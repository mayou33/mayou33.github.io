{"meta":{"title":"大雨哥","subtitle":"","description":"互联网技术","author":"zhangyu","url":"http://zhang-yu.me","root":"/"},"pages":[{"title":"about","date":"2021-03-15T13:48:47.000Z","updated":"2021-03-15T15:05:42.638Z","comments":true,"path":"about/index.html","permalink":"http://zhang-yu.me/about/index.html","excerpt":"","text":"本博客hexo主题基于hexo-theme-3-hexo 修改而来 https://github.com/yelog/hexo-theme-3-hexo"}],"posts":[{"title":"CTO观点：如何为企业选择合适的消息系统","slug":"comparing-pulsar-and-kafka-from-a-ctos-point-of-view","date":"2021-04-23T16:00:00.000Z","updated":"2021-04-25T08:01:11.076Z","comments":true,"path":"2021/04/24/comparing-pulsar-and-kafka-from-a-ctos-point-of-view/","link":"","permalink":"http://zhang-yu.me/2021/04/24/comparing-pulsar-and-kafka-from-a-ctos-point-of-view/","excerpt":"","text":"https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi 作者 | Jesse Anderson译者 | Sijia 在评估新技术时，高层管理人员的视角通常与中层管理人员、架构师、数据工程师等有所不同。高层管理人员不仅要关注基准测试结果、产品支持的特性，还要从长远角度考虑新技术的可靠性，新技术能够为企业带来哪些竞争优势，以及是否可以缩短上市时间、节约开销。 我是 Big Data Institute 的常务董事，技术评估是我的一项主要工作。我们帮助企业根据业务需求选择并落地最合适的技术。我们不与供应商合作，因此客户尤为看中我们能够客观地评估不同的技术。 在本文中，我将从 CTO 的视角出发，对比 Apache Pulsar 和 Apache Kafka。只进行理论上的对比空洞无效，也不能帮助我们作出决策，实际用例才真正值得参考。所以，在本文中，我会通过一些常见的实际使用场景来对比 Pulsar 和 Kafka，即简单消息使用场景、复杂消息使用场景和高级消息使用场景。在这些实际使用场景下，Pulsar 和 Kafka 的表现能够帮助我们更好地理解二者的性能和优势，进而作出决策。 简单消息使用场景假设有一个企业，之前从未使用过消息系统，现在需要通过一个简单的消息系统，将消息从位置 A 发送到位置 B，但不需要复制消息。 数据架构师团队在深入研究 Pulsar 和 Kafka 的业务案例后，得出如下结论：在这一使用场景中，Pulsar 和 Kafka 都没有绝对优势。并且，他们认为在短时间内，该使用场景基本不会发生改变。 对于类似这样的简单消息使用场景而言，我也赞同 Pulsar 和 Kafka 都没有绝对优势。仅从技术角度出发，Pulsar 和 Kafka 这一回合打成平局，那么我们只能考虑成本。二者的运营成本、员工培训成本分别是多少？我打算根据 Kafka 或 Pulsar 的服务提供商的收费标准进行对比。对比开销时，选好服务提供商也可以在一定程度上减少运营成本和员工培训成本。Kafka 的云服务提供商，我参考了使用 Kafka API（Azure）的 Confluent Cloud、MSK（AWS）和 Event Hubs。Pulsar 的云服务提供商，我选择 StreamNative Cloud。 对比结果出于稳妥考虑，我们决定选择 Kafka API。目前，已有多种技术支持非 Kafka broker 使用 Kafka API 或传输协议。使用 Kafka API，非 Kafka broker 可通过添加新库支持 Kafka 的传输协议，保证对 Kafka API 的兼容性，从而最大化技术选择的多样性。例如，可以通过修改 Kafka API 的实现重新编译或通过 Pulsar broker 解析 Kafka 的协议（KOP），将 Pulsar 用作 Kafka 的后端。 我们在对比单位成本后，选择了成本效益高的一方。Kafka API 可以保证后端质量，用户在后端之间的数据移动不会受到影响，有效规避风险。即使社区不活跃，技术热度不高，我们的使用也不会受到影响。 复杂消息使用场景假设一个公司需要复杂消息系统。由于需要处理世界各地的数据，必须支持跨地域复制。该企业一直在使用消息系统，因此对实时系统的复杂性有一定的了解，也发现了当前消息系统的不足之处。因此该企业对消息系统的要求是能够处理高级的消息传递和复杂的消息特性。 数据架构师团队和股东以及业务部门详细讨论了当前和未来需求。最后得出的结论是，Pulsar 和 Kafka 各有优势。同时，他们认为随着时间的推移，该使用场景和数据量都会有所增长。 在这种情况下，Pulsar 和 Kafka 难分胜负。要想作出正确决策，必须深入研究二者的使用场景。 跨地域复制Kafka 既提供私有的（价格高）跨地域复制，也提供开源的（附加服务）跨地域复制解决方案。私有的跨地域复制解决方案为其内置特性，但价格高昂。开源的解决方案（MirrorMaker）实际上就是数据复制，但由于不是其内置特性，会增加运营开销。 Pulsar 提供开源内置的跨地域复制特性，支持复杂的复制策略。对于使用场景和数据量都在增加的企业而言，显然，支持内置跨地域复制策略的 Pulsar 完胜。 就跨地域复制而言，我们选择 Pulsar。 复杂消息由于企业正在向新消息平台迁移，消息系统最好可以处理新使用场景。数据架构师团队一直在了解各个平台，尝试寻找最佳解决方案。在当前使用的消息系统中，一旦出现处理错误，必须重新生成消息，再手动重试，因此最好还可以引入消息延迟发送。另外，当前消息系统的 schema 实施功能也有待加强，各个团队选择不同的 schema 实现时，团队合作的难度显著增加。 Kafka 没有内置死信队列特性，一旦消息处理失败，必须手动处理，或修改代码重试。Kafka 也没有延迟发送消息的内置机制，延迟发送消息流程复杂、工作量大。另外，Kafka 没有内置 schema 实施机制，导致云服务提供商分别提供了不同的 schema 解决方案。 Pulsar 内置死信队列特性，当消息处理失败，收到否认 ack 时，Pulsar 可以自动重试，但次数有限。Pulsar 也支持延迟发送消息，可以设定延迟时间。对于 Pulsar 而言，schema 级别高，因此 Pulsar 有内置 schema 注册，Pulsar API 也原生支持 schema。 就复杂消息而言，我们选择 Pulsar。 高级消息传递随着对架构的深入了解，我们发现为了确保均匀分配资源，需要循环发送同一 topic 上的数据，并且需要通过排序确保消息有序排列。 Kafka 不能分发消息给指定的 consumer。当 consumer 接收到不属于它消费的消息时，要保证这些消息被正确消费，我们只能重新发送这些消息到额外的 topic 中，但这样会造成数据冗余，增加使用成本。因此，我们需要可以制定路由规则发送给指定 consumer 的产品。 Pulsar broker 可以通过制定的路由规则，把一个 topic 的不同消息根据路由规则发送到指定的 consumer 中。Pulsar broker 轻松实现了我们的目标，无需任何额外工作。 就高级消息传递而言，我们选择 Pulsar。 部署和社区为了全面比较 Pulsar 和 Kafka，我们还需要看一下二者的部署数量和社区概况。 从服务市场来看，Kafka 的提供商更多，销售和支持 Kafka 产品的团队也更多。Kafka 和 Pulsar 的开源社区都积极活跃，但 Kafka 的社区规模更大。 从使用市场来看，Kafka 和 Pulsar 都已部署在大公司的大型生产环境中。在生产环境中部署 Kafka 的公司在数量上更胜一筹。 从用户数量来看，Kafka 的用户更多。但是，数据工程师团队认为， Kafka 的使用者可以轻松学习 Pulsar。 就服务支持和社区而言，我们选择 Kafka。但值得一提的是，Pulsar 社区正在迅速发展。 对比结果由于 Pulsar 和 Kafka 在这一使用场景中都有明显的优劣势，决策难度大大增加。 Pulsar 可以在社区和部署上奋起直追，Kafka 则可以努力丰富产品特性。 在作出决策前，我们先来总结一下，该企业在技术上最看重哪方面；在技术方面，我们是否需要做最保守的选择。根据以往的经验，新的开源技术会带来更多惊喜，因此我们更倾向于选择 Pulsar。 如果选择 Kafka，我们需要承担向业务赞助商坦诚“我们无法处理这一使用场景”的风险。甚至，即使支付大笔资金购买跨地域复制许可，也无法保证顺利实现客户的需求。业务团队最终可能需要花大量时间（甚至几个月）来编写、完善、测试他们的工作方案。 如果选择 Pulsar，我们可以告诉业务赞助商“一切尽在掌握中”。由于 Pulsar 的各项内置特性都已经过测试，使用团队可以在短时间内完成部署。 在这种情况下，因为我们不需要 Kafka API 的独有特性，所以我们没有使用支持 Kafka 协议（KOP）的 Pulsar Broker，而是选择 Pulsar API，因为 Pulsar API 支持所有我们需要 Kafka API 提供的功能。 决策如下：选择 Pulsar，可以优先处理业务请求，开发团队只专注编写代码，而不是解决其他问题。选择 Pulsar 的同时，也关注 Pulsar 社区和提供商的动态。 如果采取保守决策选择 Kafka，需要接受可能无法实现某些使用场景的事实。对于相似的使用场景，我们采取相应解决方案。调整项目时间规划，增加实行预期解决方案的时间。联系运营团队，确保可以承受执行预期解决方案的开销。 高级消息使用场景假设一个公司已经在使用多种消息和队列系统。从运营、架构和开销的角度来看，我们认为有必要迁移到单个系统。同时，我们也希望降低运营成本。 数据架构师团队在和股东以及业务部门详细讨论了当前和未来需求后，给出的结论是，Pulsar 和 Kafka 各有优势。 队列和消息最大的难题是 RabbitMQ 系统。我们使用 RabbitMQ 发送太多消息，RabbitMQ 已经无法满足需求。我们调整了 RabbitMQ 的代码，将消息缓冲在内存中，并继续创建新集群来处理负载。但是我们需要的不是变通方法，而是一个能够处理大规模消息的系统。 数据架构师在研究这一使用场景时，得出结论：新系统必须可以同时处理消息流模型和队列模型。我们不仅需要继续使用 RabbitMQ 处理消息，也需要更高级的消息技术。 Kafka 擅长消息传递，也可以处理大规模消息流，但是无法处理队列。开发团队可以尝试一些解决方案，但这样就不能实现使用单个系统的预期目标。要处理队列使用场景，就同时需要 Kafka 集群和 RabbitMQ 集群。Kafka 集群更像一个缓冲区，可以有效防止 RabbitMQ 集群过载。但是 Kafka 不支持原生 RabbitMQ，我们需要与提供商合作或自己编写代码，才可以实现在 Kafka 和 RabbitMQ 之间移动数据。 Pulsar 可以在同一集群中处理队列和消息，还支持扩展集群。Pulsar 可以将所有消息流模型和队列模型的使用场景整合到一个集群中。用户可以继续使用 RabbitMQ 代码，Pulsar 支持 RabbitMQ 连接器，或者在 broker 中使用 StreamNative 开发的 AoP（AMQP 协议处理插件），该插件已获得 Apache 许可。 如果不想继续使用 RabbitMQ 代码，则可以使用 Pulsar API。Pulsar API 具有和 RabbitMQ 相同的队列功能。用户需要对代码进行相应修改，工作量取决于原代码的结构和细节，修改代码后，还需要对代码进行评估测试。 就队列模型和消息流模型而言，我们选择 Pulsar。 高级保留数据架构师分析了数据使用情况，发现 99.99% 的数据在首次使用后就未被读取。但是，他们决定采取保守策略，保留消息一周。虽然决定存储数据一周，但我们不希望增加太多运营成本。分层存储可以保存数据到本地，然后卸载其他数据到 S3，降低长期保存数据的成本。 Kafka 团队正在开发分层存储，但 Kafka 目前还不支持这一特性。一些服务商提供私有分层存储，但我们不确定是否可以直接用于生产环境中。 分层存储是 Pulsar 的原生特性，可以直接用于生产环境。目前已有多个企业在生产环境中部署该特性。 就分层存储而言，我们选择 Pulsar。Kafka 正在全力开发分层存储，这一特性的重要性不言而喻。 路由 Topic由于我们使用多个 topic 来分解数据，我们期待新系统可以创建大量 topic。数据架构师认为，我们起初需要 10 万个 topic，随着时间的推移，这个数字将会涨到 50 万。 Kafka 集群支持创建的分区数量有限且每个 topic 至少需要一个分区。Kafka 正在增加可支持 topic 的数量，但新特性尚未发布。另外，Kafka 没有命名空间和多租户，因此无法基于 topic 对资源进行分片，十万个 topic 需要存储在同一个命名空间中。 一些企业的确在使用 Kafka 集群存储甚至更多的 topic，同时进行了资源分片。但他们放弃使用单一集群，同时还需要为此支付费用。 Pulsar 支持存储数百万个 topic，这一功能早已发布并投入生产环境。Pulsar 支持命名空间和多租户，用户可以为每个 topic 设置资源配额，进而节约开销。 就 topic 而言，我们选择 Pulsar。 路由由于我们假设该企业曾经使用 RabbitMQ，在设计上，一般通过 broker 路由机制把 topic 上的数据转发到不同的 topic 中。例如，有一个用于存储世界范围数据的 topic，而 RabbitMQ broker 把它处理成以国家为单位的 topic。 数据架构师团队深入研究了如何在消息系统中使用单一 topic 存储世界范围的数据。他们发现当接收数据量增大时，下游 consumer 无法继续处理数据。对每个下游系统进行反序列化、查看数据，再丢弃数据的流程繁杂，且费时费力。 Kafka 将所有数据存储在单一 topic 中，但是，当 consumer 需要过滤的数据量增加或集群过载时，这个方法不可行。我们通常需要进行水平缩放，增加 consumer 数量，才可以读取全局 topic 并做进一步处理。用户只能选择：编写自定义 consumer / producer，编写 Kafka Streams 程序，或使用专有 KSQL。 Pulsar 支持使用 Pulsar Functions 或自定义 consumer / producer 进行路由，因此可以先读取全局 topic，再将数据保存到以国家为单位的特定 topic 上。使用独立 topic，consumer 可以按需订阅 topic，只接收相关消息。 就路由而言，我们选择 Pulsar。 最终决策时间是影响最终决策的主要原因。我们是否有时间让 Kafka 赶上 Pulsar？我们是否有时间让数据工程师来实现 Kafka 的解决方案？等待会让公司错失良机，延缓增加新的使用场景，影响业务发展。 最终决策：我们选择 Pulsar。 时间充足情况下的决策：延迟使用新架构。给 Kafka 半年时间，看 Kafka 是否可以在性能上赶超 Pulsar。如果可以，我们将在生产环境中测试这些新特性，评估稳定性。如果 Kafka 不能让人眼前一亮，我们仍然会选择 Pulsar。 结语本文涉及的三个使用场景都是我在实际工作中遇到的，希望本文给出的解决方案可以为您提供参考，帮助您根据具体使用场景进行技术评估。 原文链接： https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi","categories":[{"name":"消息系统","slug":"消息系统","permalink":"http://zhang-yu.me/categories/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Pulsar","slug":"Pulsar","permalink":"http://zhang-yu.me/tags/Pulsar/"}]},{"title":"生产环境中的Kubernetes最佳实践","slug":"kubernetes-best-practices-in-production","date":"2021-04-20T16:00:00.000Z","updated":"2021-04-21T08:50:20.907Z","comments":true,"path":"2021/04/21/kubernetes-best-practices-in-production/","link":"","permalink":"http://zhang-yu.me/2021/04/21/kubernetes-best-practices-in-production/","excerpt":"","text":"生产环境中的Kubernetes最佳实践https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&amp;mid=2649721992&amp;idx=1&amp;sn=31b9a4352a147a1fa585b8d8ef4c68b0&amp;scene=21#wechat_redirect（翻译：易理林）https://my.oschina.net/u/1787735/blog/4870582 DevOps从提出到现在，已经走过了一段很长的路。包括Docker和Kubernetes在内的多种平台也已经帮助企业用前所未有的速度实现了软件应用的交付。同时，随着应用的容器化构建和发布比率不断上升，作为事实上的容器编排工具，Kubernetes在企业用户中备受欢迎和广泛认可。 Kubernetes具有支持伸缩、零中断部署、服务发现、自动更迭和自动回滚等卓越功能特性。在管理大规模容器部署方面，Kubernetes因支持资源和工作负载的灵活分配能力，而成为了企业的必选工具，在生产环境中广泛应用。但与此同时，Kubernetes的应用需要操作人员花许多时间来熟悉和掌握它，存在一定技术门槛。鉴于目前许多公司都希望在生产中使用Kubernetes，因此有必要率先梳理这方面的最佳实践。在本文中，我们将介绍Kubernetes在生产环境中的一些最佳实践。 生产环境中Kubernetes表现 根据Garner的预测，到2022年时，全球超过75%的组织将在生产环境中运行容器化应用。这个比率在当前还不足30%，而预计到2025年时，这个比率将在2022年的基础上，继续增长到85%。快速增长的一个主要原因是云原生的软件应用在基础设施自动化、DevOps、专业操作技能方面的需求越来越强烈，而且这些工具和技术在企业的IT组织中往往很难找到。 其次，业界普遍认为在生产环境中运行容器并不容易，需要大量的计算资源和相关工作投入。目前市场上有多款容器编排平台产品可供选择，但已经获得了主要云提供商的支持和认可的平台只有Kubernetes。 再次，Kubernetes、容器化和微服务给企业用户带来的技术受益的同时，也带来了新的安全挑战。Kubernetes的Pod具备在所有基础设施类之间快速切换的能力，从而导致更多的内部流量和与之相关的安全风险，加上Kubernetes被攻击面往往比我们预期的更大，以及Kubernetes的高度动态和临时的环境与原有安全工具的融合差距等因素，可以预测使用Kubernetes并非是一件容易的事情。 最后，Kubernetes丰富的功能导致它的学习曲线复杂而陡峭，在生产环境中的操作需应尽可能小心和谨慎。企业如果没有熟悉这方面的专业人员，可以考虑外购Kubernetes-as-a-service（KaaS）提供商的服务，获取Kubernetes最佳实践。但假设用户是完全依靠自己的能力，管理生产环境中的Kubernetes集群，在这种情况下，理解和实现Kubernetes最佳实践尤其重要，特别是在可观察性、日志记录、集群监控和安全配置等方面。 综上所述，非常有必要开发一套Kubernetes管理策略，以实现在安全性、监视、网络、容器生命周期管理和平台选择等方面应用最佳实践。如下是Kubernetes应用管理需要重点考虑的措施。 使用服务状态探针进行健康检查 管理大型分布式系统是一件复杂的工作，尤其是出现问题的时候。因此为了确保应用的实例工作正常，配置Kubernetes健康检查至关重要。通过创建自定义运行状况检查，可以更好地满足用户的环境和应用的检测需要。服务状态探针包括服务就绪探针和服务活性探针。 Readiness-就绪探针：目的是让Kubernetes知道应用程序是否准备好提供服务。Kubernetes始终会在确认准备就绪探针通过检测后，然后才允许向POD发送服务请求流量。 Liveness-存活探针：目的是帮助用户确认应用程序是否正常存活，如果应用出现了异常，Kubernetes将启动新的Pod，替换异常的Pod。 资源管理 为单个容器指定资源需求和资源限制是一个很好的实践。另一个好的实践是为不同团队、部门、应用程序和客户端，划分独立的Kubernetes命名空间环境。提供相对独立的运行资源环境，减少资源使用冲突。 资源使用 Kubernetes资源使用情况掌握了生产环境中容器/Pod的资源数量使用情况。因此，密切关注Pod和容器的资源使用情况非常重要，资源使用越多，运行成本就越高。 资源利用 运维团队通常致力于优化和最大化Pod分配资源的利用百分比。资源使用情况往往也是Kubernetes优化程度的重要指标之一。可以说，优化最好的Kubernetes环境，内部运行容器的平均CPU利用率也是最优的。 开启RBAC策略 基于角色的访问控制（RBAC）是系统或网络中限制用户和应用程序的接入或访问的一种控制方法。 Kubernetes 从1.8版本开始，引入了RBAC访问控制技术，使用rbac.authorization.k8s.io程序API创建授权策略。RBAC的授权使用包括开启访问用户或帐户、添加/删除权限、设置规则等。它为Kubernetes集群添加了一个额外的安全层，限制哪些访问可以到达Kubernetes集群的生产环境。 集群配置和负载均衡 生产级Kubernetes基础设施通常需要具备高可用性，具备多控制节点、多etcd集群等关键特性。此类集群特性的配置实现通常需要借助如Terraform或Ansible等工具实现。 通常情况下，当集群的所有配置都完成，并创建了Pod时，此时的Pod基本都会配置有负载均衡器，用于将流量路由到适当的应用服务。但这其中的负载均衡器并不是Kubernetes项目的默认配置，而是由Kubernetes Ingress控制器的扩展集成工具提供的。 标注Kubernetes对象 为Kubernetes的Pod等对象打上键/值对类型的标签，通常可以用来标记重要的对象属性，特别是对用户意义重大的属性。因此，在生产环境中使用Kubernetes时，不能忽视的重要实践就是利用标签功能，它们可以帮助实现Kubernetes对象的批量查询和批量操作。同时，标签还具有将Kubernetes对象组织成集群的独特作用，这样做的一个最佳实践应用就是能够根据应用对Pod进行分组管理。除此之外，标签没有数量和内容的限制，运维团队可以任意创建和使用。 设置网络策略 网络策略设置对于生产环境中的Kubernetes平台非常重要。 网络策略本质上也是一种对象，让用户能够声明和决定哪些流量是允许或禁止传输的。Kubernetes能够阻止所有不需要的和不合规的流量。因此，强烈建议Kubernetes将网络策略配置作为基本和必要的安全措施之一，执行定义和限制集群中的网络流量。 Kubernetes中的每条网络策略都被定义成一个授权连接列表。无论何时创建的网络策略，平台全部的Pod都有权利建立或接受该连接列表。简单来说，网络策略其实就是授权和允许连接的请求白名单，无论是“输入”还是“输出”到Pod，在至少有一条网络策略允许的情况下，到该Pod流量才被允许通行。 集群监控与日志 监控对于运行状态的Kubernetes至关重要，它直接影响到平台配置、性能和流量的安全。能够帮助用户及时掌握平台状态，执行问题诊断、确保运行合规，是平台运行的必要功能部署。在开启集群监视时，必须在平台的每一层都开启日志记录，让产生的日志能够执行安全、审计和性能分析。 采用无状态应用 虽然这种观念正随着Kubernetes应用组织的增加在不断改变，但管理和运行无状态应用要比有状态应用要容易很多。事实上，对于刚接触Kubernetes的团队，建议一开始就采用无状态应用的设计。同时，还建议采用无状态的后端程序，从而让开发人员更有效地部署应用程序，实现服务的零停机时间。但前提是需要开发团队确保后端没有长时间运行的连接，不会影响到运行环境的弹性扩展。无状态应用还被认为具备根据业务需要进行简便迁移和快速扩展的能力。 启用自动扩展 Kubernetes的服务部署拥有3个自动扩展能力：Pod水平自动扩展（HPA），Pod垂直自动扩展（VPA）和集群自动扩展。 Pod水平自动扩展能够基于CPU的利用率，自动扩展运行应用的Pod数量，调整副本控制器、副本集或状态配置。 Pod垂直自动扩展建议为应用设定适当的CPU，内存的需求值和上限值。VPA能够根据情况，自动伸缩配置适当的资源数量。 集群自动扩展能够伸缩工作节点的资源池规模，从而根据当前的资源使用情况，自动调整Kubernetes集群的大小。 控制镜像拉取来源 如果允许Pod从公共库中拉取镜像，而不知道其真正运行内容的时候，用户应该控制所运行容器集群的资源，以避免资源使用的失控。而如果是从受信任的注册节点提取镜像，则可以在注册节点上采用控制策略，限制只允许提取安全且经过认证的镜像。 保持持续学习 对应用程序的状态不断评估、学习和改进。例如，通过查看容器的历史内存使用情况，确定可以分配更少的内存来节省成本。 重点保护核心服务 使用Pod优先级功能，可以为不同的服务设置重要度。例如，可以配置RabbitMQ Pod的优先级高于应用程序Pod，以获得更好的稳定性。或为输入控制器Pod配置比数据处理Pod更高的重要度，以保持服务的可用性。 保证服务零停机 服务的零停机能力可以通过全方位HA架构，支持集群和服务的零停机升级。从而为客户获得更高的服务可用性提供了保证。使用Pod反亲和性配置，确保多个副本Pod被调度到不同的节点上，从而保证计划和非计划的集群节点停机不会影响服务的可用性，或使用Pod中断预备能力，确保在可用成本内，保留最少的副本数量。 为失败指定计划 借用一句名言来理解如果应对硬件故障。硬件最终会失败，软件最终会运行。–（迈克尔·哈顿） “Hardware eventually fails. Software eventually works.”（Michael Hartung）。 结论 业界共知的Kubernetes，实际上已经是DevOps的标配编配平台。生产环境中运行的Kubernetes环境必须具备可用性、可伸缩性、安全性、弹性、资源管理和监控等功能和性能特征。由于许多公司都在生产中使用Kubernetes，因此建议遵循上面提到的Kubernetes最佳实践，以便顺利、可靠地运维和管理应用程序。 原文链接：https://containerjournal.com/topics/container-management/kubernetes-best-practices-in-production/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/tags/Kubernetes/"}]},{"title":"你的技术成长战略是什么","slug":"jishuchengzhang","date":"2021-04-18T16:00:00.000Z","updated":"2021-04-19T16:03:49.781Z","comments":true,"path":"2021/04/19/jishuchengzhang/","link":"","permalink":"http://zhang-yu.me/2021/04/19/jishuchengzhang/","excerpt":"","text":"你的技术成长战略是什么?_架构师波波的专栏-CSDN博客 https://blog.csdn.net/yang75108/article/details/112511324?spm=1001.2014.3001.5502) 一、前言在波波的微信技术交流群里头，经常有学员问关于技术人该如何学习成长的问题，虽然是微信交流，但我依然可以感受到小伙伴们焦虑的心情。 技术人为啥焦虑？恕我直言，说白了是胆识不足格局太小。胆就是胆量，焦虑的人一般对未来的不确定性怀有恐惧。识就是见识，焦虑的人一般看不清楚周围世界，也看不清自己和适合自己的道路。格局也称志向，容易焦虑的人通常视野窄志向小。如果从战略和管理的视角来看，就是对自己和周围世界的认知不足，没有一个清晰和长期的学习成长战略，也没有可执行的阶段性目标计划+严格的执行。 因为问此类问题的学员很多，让我感觉有点烦了，为了避免重复回答，所以我专门总结梳理了这篇长文，试图统一来回答这类问题。如果后面还有学员问类似问题，我会引导他们来读这篇文章，然后让他们用三个月、一年甚至更长的时间，去思考和回答这样一个问题：你的技术成长战略究竟是什么？如果你想清楚了这个问题，有清晰和可落地的答案，那么恭喜你，你只需按部就班执行就好，根本无需焦虑，你实现自己的战略目标并做出成就只是一个时间问题；否则，你仍然需要通过不断磨炼+思考，务必去搞清楚这个人生的大问题！！！ 下面我们来看一些行业技术大牛是怎么做的。 二、跟技术大牛学成长战略我们知道软件设计是有设计模式(Design Pattern)的，其实技术人的成长也是有成长模式(Growth Pattern)的。波波经常在Linkedin上看一些技术大牛的成长履历，探究其中的成长模式，从而启发制定自己的技术成长战略。 当然，很少有技术大牛会清晰地告诉你他们的技术成长战略，以及每一年的细分落地计划。但是，这并不妨碍我们通过他们的过往履历和产出成果，去溯源他们的技术成长战略。实际上，越是牛逼的技术人，他们的技术成长战略和路径越是清晰，我们越容易从中探究出一些成功的模式。 2.1 系统性能专家案例国内的开发者大都热衷于系统性能优化，有些人甚至三句话离不开高性能/高并发，但真正能深入这个领域，做到专家级水平的却寥寥无几。 我这边要特别介绍的这个技术大牛叫Brendan Gregg(布兰登·格雷格)，他是系统性能领域经典书《System Performance: Enterprise and the Cloud》(中文版《性能之巅：洞悉系统、企业和云计算》)的作者，也是著名的性能分析利器火焰图(Flame Graph)的作者。Brendan Gregg目前是Netflix公司的高级性能架构师，已经在Netflix工作近7年，之前他是Joynet公司的Lead Performance Engineer。总体上，他已经在系统性能领域深耕超过10年，Brendan Gregg的过往履历可以在linkedin上看到。在这10年间，除了书籍以外，Brendan Gregg还产出了超过上百份和系统性能相关的技术文档，演讲视频/ppt，还有各种工具软件，相关内容都整整齐齐地分享在他的技术博客上，可以说他是一个非常高产的技术大牛。 上图来自Brendan Gregg的新书《BPF Performance Tools: Linux System and Application Observability》，其中红色标注的是他开发的各种性能工具。从这个图可以看出，Brendan Gregg对系统性能领域的掌握程度，已经深挖到了硬件、操作系统和应用的每一个角落，可以说是360度无死角，整个计算机系统对他来说几乎都是透明的。波波认为，Brendan Gregg是名副其实的，世界级的，系统性能领域的大神级人物。 2.2 从开源到企业案例我要分享的第二个技术大牛是Jay Kreps(杰·克雷普斯)，他是知名的开源消息中间件Kafka的创始人/架构师，也是Confluent公司的联合创始人和CEO，Confluent公司是围绕Kafka开发企业级产品和服务的技术公司。 从Linkedin的履历上我们可以看出，Jay Kreps之前在Linkedin工作了7年多(2007.6 ~ 2014. 9)，从高级工程师、工程主管，一直做到首席资深工程师。Kafka大致是在2010年，Jay Kreps在Linkedin发起的一个项目，解决Linkedin内部的大数据采集、存储和消费问题。之后，他和他的团队一直专注Kafka的打磨，开源(2011年初)和社区生态的建设。到2014年底，Kafka在社区已经非常成功，有了一个比较大的用户群，于是Jay Kreps就和几个早期作者一起离开了Linkedin，成立了Confluent公司，开始了Kafka和周边产品的企业化服务道路。今年(2020.4月)，Confluent公司已经获得E轮2.5亿美金融资，公司估值达到45亿美金。从Kafka诞生到现在，Jay Kreps差不多在这个产品和公司上投入了整整10年。 上图是Confluent创始人三人组，一个非常有意思的组合，一个中国人(左)，一个印度人(右)，中间的Jay Kreps是美国人。 我之所以对Kafka和Jay Kreps的印象特别深刻，是因为在2012年下半年，我在携程框架部也是专门搞大数据采集的，我还开发过一套功能类似Kafka的Log Collector + Agent产品。我记得同时期有不止4个同类型的开源产品：Facebook Scribe、Apache Chukwa、Apache Flume和Apache Kafka。现在回头看，只有Kafka走到现在发展得最好，这个和创始人的专注和持续投入是分不开的，当然背后和几个创始人的技术大格局也是分不开的。 当年我对战略性思维几乎没有概念，还处在什么技术都想学、认为各种项目做得越多越牛的阶段。搞了半年的数据采集以后，我就掉头搞其它“更有趣”的项目去了(从这个事情的侧面，也可以看出我当年的技术格局是很小的)。中间我陆续关注过Jay的一些创业动向，但是没想到他能把Confluent公司发展到目前这个规模。现在回想，其实在十年前，Jay Kreps对自己的技术成长就有比较明确的战略性规划，也具有大的技术格局和成事的一些必要特质。Jay Kreps和Kafka给我上了一堂生动的技术战略和实践课。 2.3 技术媒体大V案例介绍到这里，有些同学可能会反驳说：波波你讲的这些大牛都是学历背景好，功底扎实起点高，所以他们才更能成功。其实不然，这里我再要介绍一位技术媒体界的大V叫Brad Traversy(布拉德·特沃西)，大家可以看他的Linkedin简历，背景很一般，学历差不多是一个非正规的社区大学(相当于大专)，没有正规大厂工作经历，有限几份工作一直是在做网站外包。但是Brad Traversy目前是技术媒体领域的一个大V，当前他在Youtube上有138万多的订阅量，10年累计输出Web开发和编程相关教学视频超过800个。Brad Traversy也是Udemy上的一个成功讲师，目前已经在Udemy上累计输出课程14门，购课学生数量近25万。Brad Traversy目前是自由职业者，他的Youtube广告+Udemy课程的收入相当不错。 就是这样一位技术媒体大V，你很难想象，在年轻的时候，贴在他身上的标签是：不良少年，酗酒，抽烟，吸毒，纹身，进监狱。。。直到结婚后的第一个孩子诞生，他才开始担起责任做出改变，然后凭借对技术的一腔热情，开始在Youtube平台上持续输出免费课程。从此他找到了适合自己的战略目标，然后人生开始发生各种积极的变化。。。如果大家对Brad Traversy的过往经历感兴趣，推荐观看他在Youtube上的自述视频《My Struggles &amp; Success》。 我粗略浏览了Brad Traversy在Youtube上的所有视频，10年总计输出800+视频，平均每年80+。第一个视频提交于2010年8月，刚开始几年几乎没有订阅量，2017年1月订阅量才到50k，这中间差不多隔了6年。2017.10月订阅量猛增到200k，2018年3月订阅量到300k。当前2021.1月，订阅量达到138万。可以认为从2017开始，也就是在积累了6～7年后，他的订阅量开始出现拐点。如果把这些数据画出来，将会是一条非常漂亮的复利曲线。 2.4 案例小结Brendan Gregg，Jay Kreps和Brad Traversy三个人走的技术路线各不相同，但是他们的成功具有共性或者说模式： 找到了适合自己的长期战略目标，例如： Brendan Gregg: 成为系统性能领域顶级专家 Jay Kreps：开创基于Kafka开源消息队列的企业服务公司，并将公司做到上市 Brad Traversy: 成为技术媒体领域大V和课程讲师，并以此作为自己的职业 **专注深耕一个(或有限几个相关的)细分领域(Niche)**，保持定力，不随便切换领域： Brendan Gregg：系统性能领域 Jay Kreps: 消息中间件/实时计算领域+创业 Brad Traversy: 技术媒体/教学领域，方向Web开发 + 编程语言 长期投入，三人都持续投入了10年。 **年度细分计划+持续可量化的价值产出(Persistent &amp; Measurable Value Output)**： Brendan Gregg：除公司日常工作产出以外，每年有超过10份以上的技术文档和演讲视频产出，平均每年有2.5个开源工具产出。十年共产出书籍2本，其中《System Performance》已经更新到第二版。 Jay Kreps：总体有开源产品+公司产出，1本书产出，每年有Kafka和周边产品发版若干。 Brad Traversy: 每年有Youtube免费视频产出（平均每年80+）+Udemy收费视频课产出(平均每年1.5门)。 以终为始是牛人和普通人的一大区别。普通人通常走一步算一步，很少长远规划。牛人通常是先有远大目标，然后采用倒推法，将大目标细化到每年/月/周的详细落地计划。Brendan Gregg，Jay Kreps和Brad Traversy三人都是以终为始的典型。 上面总结了几位技术大牛的成长模式，其中一个要点是：这些大牛的成长都是通过持续有价值产出Persistent Valuable Output来驱动的。持续产出为啥如此重要，这个还要从下面的学习金字塔说起。 三、学习金字塔和刻意训练 学习金字塔是美国缅因州国家训练实验室的研究成果，它认为： 我们平时上课听讲之后，学习内容平均留存率大致只有5%左右； 书本阅读的平均留存率大致只有10%左右； 学习配上视听效果的课程，平均留存率大致在20%左右， 老师实际动手做实验演示后的平均留存率大致在30%左右； 小组讨论(尤其是辩论后)的平均留存率可以达到50%左右； 在实践中实际应用所学之后，平均留存率可以达到75%左右； 在实践的基础上，再把所学梳理出来，转而再传授给他人后，平均留存率可以达到90%左右。 上面列出的7种学习方法，前四种称为被动学习，后三种称为主动学习。拿学游泳做个类比，被动学习相当于你看别人游泳，而主动学习则是你自己要下水去游。我们知道游泳或者跑步之类的运动是要燃烧身体卡路里的，这样才能达到锻炼身体和长肌肉的效果(肌肉是卡路里燃烧的结果)。如果你只是看别人游泳，自己不实际去游，是不会长肌肉的。同样的，主动学习也是要燃烧脑部卡路里的，这样才能达到训练大脑和长脑部“肌肉”的效果。 我们也知道，燃烧身体的卡路里，通常会让人感觉不舒适，如果燃烧身体卡路里会让人感觉舒适的话，估计这个世界上应该不会有胖子这类人。同样，燃烧脑部卡路里也会让人感觉不适、紧张、出汗或语无伦次，如果燃烧脑部卡路里会让人感觉舒适的话，估计这个世界上人人都很聪明，人人都能发挥最大潜能。当然，这些不舒适是短期的，长期会使你更健康和聪明。波波一直认为，人与人之间的先天身体其实都差不多，但是后天身体素质和能力有差异，这些差异，很大程度是由后天对身体和大脑的训练质量、频度和强度所造成的。 明白这个道理之后，心智成熟和自律的人就会对自己进行持续地刻意训练。这个刻意训练包括对身体的训练，比如波波现在每天坚持跑步3km，走3km，每天做60个仰卧起坐，5分钟平板撑等，每天保持让身体燃烧一定量的卡路里。刻意训练也包括对大脑的训练，比如波波现在每天做项目写代码coding(训练脑+手)，平均每天在B站上输出十分钟免费视频(训练脑+口头表达)，另外有定期总结输出公众号文章(训练脑+文字表达)，还有每天打半小时左右的平衡球(下图)或古墓丽影游戏(训练小脑+手)，每天保持让大脑燃烧一定量的卡路里，并保持一定强度(适度不适感)。如果你对刻意训练的专业原理和方法论感兴趣，推荐看书籍《刻意练习》。 注意，如果你平时从来不做举重锻炼的，那么某天突然做举重会很不适应甚至受伤。脑部训练也是一样的，如果你从来没有做过视频输出，那么刚开始做会很不适应，做出来的视频质量会很差。不过没有关系，任何训练都是一个循序渐进，不断强化的过程。等大脑相关区域的”肌肉”长出来以后，会逐步进入正循环，后面会越来越顺畅，相关”肌肉”会越来越发达。所以，和健身一样，健脑也不能遇到困难就放弃，需要循序渐进(Incremental)+持续地(Persistent)刻意训练。 理解了学习金字塔和刻意训练以后，现在再来看Brendan Gregg，Jay Kreps和Brad Traversy这些大牛的做法，他们的学习成长都是建立在持续有价值产出的基础上的，这些产出都是刻意训练+燃烧脑部卡路里的成果。他们的产出要么是建立在实践基础上的产出，例如Jay Kreps的Kafka开源项目和Confluent公司；要么是在实践的基础上，再整理传授给其他人的产出，例如，Brendan Greeg的技术演讲ppt/视频，书籍，还有Brad Traversy的教学视频等等。换句话说，他们一直在学习金字塔的5～7层主动和高效地学习。并且，他们的学习产出还可以获得用户使用，有客户价值(Customer Value)，有用户就有反馈和度量。记住，有反馈和度量的学习，也称闭环学习，它是能够不断改进提升的；反之，没有反馈和度量的学习，无法改进提升。 现在，你也应该明白，晒个书单秀个技能图谱很简单，读个书上个课也不难。但是要你给出5～10年的总体技术成长战略，再基于这个战略给出每年的细分落地计划(尤其是产出计划)，然后再严格按计划执行，这的确是很难的事情。这需要大量的实践训练+深度思考，要燃烧大量的脑部卡路里！但这是上天设置的进化法则，成长为真正的技术大牛如同成长为一流的运动员，是需要通过燃烧与之相匹配量的卡路里来交换的。成长为真正的技术大牛，也是需要通过产出与之匹配的社会价值来交换的，只有这样社会才能正常进化。你推进了社会进化，社会才会回馈你。如果不是这样，社会就无法正常进化。 四、战略思维的诞生 一般毕业生刚进入企业工作的时候，思考大都是以天/星期/月为单位的，基本上都是今天学个什么技术，明天学个什么语言，很少会去思考一年甚至更长的目标。这是个眼前漆黑看不到的懵懂时期，捕捉到机会点的能力和概率都非常小。 工作了三年以后，悟性好的人通常会以一年为思考周期，制定和实施一些年度计划。这个时期是相信天赋和比拼能力的阶段，可以捕捉到一些小机会。 工作了五年以后，一些悟性好的人会产生出一定的胆识和眼光，他们会以3～5年为周期来制定和实施计划，开始主动布局去捕捉一些中型机会点。 工作了十年以后，悟性高的人会看到模式和规则变化，例如看出行业发展模式，还有人才的成长模式等，于是开始诞生出战略性思维。然后他们会以5～10年为周期来制定和实施自己的战略计划，开始主动布局去捕捉一些中大机会点。Brendan Gregg，Jay Kreps和Brad Traversy都是属于这个阶段的人。 当然还有很少一些更牛的时代精英，他们能够看透时代和人性，他们的思考是以一生甚至更长时间为单位的，这些超人不在本文讨论范围内。 五、建议 现在大学生毕业的年龄一般在22～23岁，那么在工作了十年后，也就是在你32～33岁的时候，你差不多也看了十年了，应该对自己和周围的世界(你的行业和领域)有一个比较深刻的领悟，需要开始为下一个十年去做战略布局了。如果你到这个年纪还懵懵懂懂，今天抓东明天抓西，那么只能说你的胆识格局是相当的低。在当前IT行业竞争这么激烈的情况下，到35岁被下岗可能就在眼前。 有了战略性思考，你就会以5～10年为周期去布局谋划你的战略。以Brendan Gregg，Jay Kreps和Brad Traversy这些大牛为例，人生若真的要干点成就出来，投入周期一般都要十年。从33岁开始，你大致有3个十年，因为到60岁以后，一般人都老眼昏花干不了大事了。如果你悟性差一点，到40岁才开始规划，那么你大致还有2个十年。如果你规划好了，这2～3个十年可以成就不小的事业。否则，你很可能一生无所作为，或者一直在帮助成全别人的事业。 考虑到人生能干事业的时间也就是2～3个十年，你会发现人生其实很短暂，这时候你会把精力都投入到实现你的十年战略上去，没有时间再浪费在比如网上的闲聊和扯皮争论上去。 “图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。”～道德经。有了十年战略方向，下一步是每年的细分落地计划，尤其是产出计划。这个计划主要应该工作在学习金字塔的5/6/7层。产出应该是刻意训练+燃烧卡路里的结果，每天让身体和大脑都保持燃烧一定量的卡路里。 产出应该有客户价值，自己能学习(自己成长进化)，对别人还有用(推动社会成长进化)，这样可以得到用户回馈和度量，形成一个闭环，可以持续改进和提升你的学习。 “少则得，多则惑”～道德经。少即是多，深耕一个(或有限几个相关的)领域。所有细分计划应该紧密围绕你的战略展开。克制内心欲望，不要贪多和分心，不要被喧嚣的世界所迷惑。 战略方向+细分计划都要写下来，定期review优化。 “曲则全、枉则直”～道德经。战略实现不是直线的，而是曲折迂回的。战略方向和细分计划通常要按需调整，尤其在早期，但是最终要收敛。如果老是变不收敛，就是缺乏战略定力，是个必须思考和解决的大问题。 别人的成长战略可以参考，但是不要刻意去模仿，你有你自己的颜色，你应该成为独一无二的你。 “合抱之木，生于毫末；九层之台，起于蔂土；千里之行，始于足下”～道德经。战略方向和细分计划明确了，接下来就是按部就班执行，十年如一日铁打不动。 做长期主义者和时间的朋友，”任何一个人，不管你的能量强弱，放眼于足够长的时间，你都可以通过长期主义这种行为模式，成为时间的朋友”～罗振宇。 最后，战略目标的实现也和种树一样是生长出来的，需要时间耐心栽培，记住慢就是快。焦虑纠结的时候，像念经一样默念王阳明《传习录》中的教诲： 立志用功，如种树然。方其根芽，犹未有干；及其有干，尚未有枝；枝而后叶，叶而后花实。初种根时，只管栽培灌溉。勿作枝想，勿作花想，勿作实想。悬想何益？但不忘栽培之功，怕没有枝叶花实？ 译文： 实现战略目标，就像种树一样。刚开始只是一个小根芽，树干还没有长出来；树干长出来了，枝叶才能慢慢长出来；枝叶长出来，然后才能开花和结果。刚开始种树的时候，只管栽培灌溉，别老是纠结枝什么时候长出来，花什么时候开，果实什么时候结出来。纠结有什么好处呢？只要你坚持投入栽培，还怕没有枝叶花实吗？","categories":[{"name":"职业发展","slug":"职业发展","permalink":"http://zhang-yu.me/categories/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"}],"tags":[{"name":"职业发展","slug":"职业发展","permalink":"http://zhang-yu.me/tags/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"}]},{"title":"使用nerdctl玩转containerd","slug":"nerdctl-to-containerd","date":"2021-04-11T16:00:00.000Z","updated":"2021-04-12T13:02:31.269Z","comments":true,"path":"2021/04/12/nerdctl-to-containerd/","link":"","permalink":"http://zhang-yu.me/2021/04/12/nerdctl-to-containerd/","excerpt":"","text":"参考https://mp.weixin.qq.com/s/ZKoO041TqyR2guVooPegrg 从行业趋势来看，Docker 已经和 Kubernetes 社区渐行渐远，以?Containerd?为代表的实现了?CRI?接口的容器运行时将会受到 Kubernetes 的青睐。但纯粹使用 Containerd 还是有诸多困扰，比如不方便通过 CLI 来创建管理容器，有了?nerdctl?这个 CLI 工具，就就可以填补 Containerd 易用性的空缺 现有 CLI 的不足虽然 Docker 能干的事情，现在 Containerd 都能干，但 Containerd 还有一个非常明显的缺陷：CLI 不够友好。它无法像 Docker 和 Podman 一样通过一条简单的命令启动一个容器，它的两个 CLI 工具 ctr 和 crictl都无法实现这么一件非常简单的需求，而这个需求是大多数人都需要的，我总不能为了在本地测试容器而专门部署一个 Kubernetes 集群吧？ ctr 的设计对人类不太友好，例如缺少以下这些和 Docker 类似的功能： docker run -p &lt;PORT&gt; docker run --restart=always 通过凭证文件 ~/.docker/config.json 来拉取镜像 docker logs 除此之外还有一个 CLI 工具叫 crictl，和 ctr 一样不太友好。 为了解决这个痛点，Containerd 官方推出了一个新的 CLI 叫 nerdctl。nerdctl 的使用体验和 docker 一样顺滑 ##安裝nerdctl 你可以从?https://github.com/containerd/nerdctl中下载最新的可执行文件，每一个版本都有两种可用的发行版： `nerdctl--linux-amd64.tar.gz?: 只包含 nerdctl。 `nerdctl-full--linux-amd64.tar.gz?: 包含了 nerdctl 和相关依赖组件（containerd, runc, ###CNI, …）。 如果你已经安装了 Containerd，只需要选择前一个发行版，否则就选择完整版。 这里选择完整版nerdctl-full-0.7.3-linux-amd64.tar.gzcd /opt wget https://github.com/containerd/nerdctl/releases/download/v0.7.3/nerdctl-full-0.7.3-linux-amd64.tar.gz tar -C /usr/local -xzf nerdctl-full-0.7.3-linux-amd64.tar.gz mkdir -p /etc/containerd containerd config default &gt; /etc/containerd/config.toml sed -i “s#k8s.gcr.io#registry.aliyuncs.com/k8sxio#g” /etc/containerd/config.toml sed -i “s/systemd_cgroup = false/systemd_cgroup = true/g” /etc/containerd/config.toml export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com sed -i “s#https://registry-1.docker.io#${REGISTRY_MIRROR}#g&quot; /etc/containerd/config.toml mkdir -p /data/containerd-data-root sed -i “s#/var/lib/containerd#/data/containerd-data-root#g” /etc/containerd/config.toml sed -i “s#oom_score = 0#oom_score = -999#g” /etc/containerd/config.toml sed -i “/containerd.runtimes.runc.options/a\\ SystemdCgroup = true” /etc/containerd/config.toml sed -i “s#/opt/cni/bin#/usr/local/libexec/cni#g” /etc/containerd/config.toml 123456789101112添加私有仓库harbor---自定义--可选[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;harbor.testtest.com&quot;] endpoint &#x3D; [&quot;https:&#x2F;&#x2F;harbor.testtest.com&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor.testtest.com&quot;.tls] insecure_skip_verify &#x3D; true [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor.testtest.com&quot;.auth] username &#x3D; &quot;admin&quot; password &#x3D; &quot;Harbor12345&quot; systemctl daemon-reload systemctl enable –now containerd systemctl status containerd containerd –version ###普通用户Rootless 切换到普通用户 比如su - admin 执行 containerd-rootless-setuptool.sh install 强烈建议在Rootless模式下启用cgroup v2 请参阅https://rootlesscontaine.rs/getting-started/common/cgroup2/","categories":[{"name":"containerd","slug":"containerd","permalink":"http://zhang-yu.me/categories/containerd/"}],"tags":[{"name":"containerd","slug":"containerd","permalink":"http://zhang-yu.me/tags/containerd/"}]},{"title":"Kubernetes网络和云厂商实践浅析","slug":"Kubernetes-network","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T03:00:20.304Z","comments":true,"path":"2021/04/09/Kubernetes-network/","link":"","permalink":"http://zhang-yu.me/2021/04/09/Kubernetes-network/","excerpt":"","text":"Kubernetes网络和云厂商实践浅析 原创 张向阳 云网漫步 2020-11-22https://mp.weixin.qq.com/s?src=11&amp;timestamp=1617936055&amp;ver=2997&amp;signature=czoNtYoc1oSn7KtlQknWvo*D*Q0GOWU2VI3rEKh6YtE9kSX4TOoMWQLrj8cPiKi9jDE-u4SIMSdM1iYpGw63aXBXZlz7XHUcOgrTiL335Qm9mxs0cJFYp7j1VpUx6IkF&amp;new=1 0 前言Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 源于希腊语，意为 &quot;舵手&quot; 或 &quot;飞行员&quot;，Google 在 2014 年开源了 Kubernetes 项目，Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验的基础上（Brog系统），结合了社区中最好的想法和实践。 为了能实现Kubernetes有效的管理大规模的容器，需要优秀网络技术的支撑，本文主要从Kubernetes网络的角度去介绍Kubernetes网络的需求、网络模型、实现技术、云厂商Kubernetes的网络实践。 1 Kubernetes网络系统需求集群网络系统是 Kubernetes 的核心部分，它需要解决下面四个问题。 Pod内容器间通信。 Pod 间通信。 Pod 和服务间通信。 外部和服务间通信。 Kubernetes 的宗旨就是在应用之间共享机器，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。同时动态分配端口也会给系统带来很多复杂度，每个应用都需要设置一个端口的参数，而 API服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。 与其去解决这些问题，Kubernetes 选择了其他不同的方法，下面我们介绍一下Kubernetes 网络模型。 2 Kubernetes 网络模型 Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）： 节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信。 节点上的代理（例如，系统守护进程、kubelet）可以和节点上的所有Pod通信。 每一个Pod都有它自己的IP地址，这就意味着不需要显式地在每个Pod之间创建链接，也不需要处理容器端口到主机端口之间的映射。这样将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod可以被视作虚拟机或者物理主机。同时还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容，如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP，这样就可以和其他的虚拟机进行通信，这是基本相同的模型。 3 Kubernetes 网络技术 从上文看出，每个pod有自己唯一的IP地址，可通过一个扁平的、非NAT网络和其他Pod通信。Kubernetes是如何做到这一点呢？其实，Kubernetes不负责这块，网络是有Container Network Interface（CNI）插件进行管理。CNI是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件，CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源，如下图所示。 Kubernetes网络实现模型很多，从本质上看，使用网络技术有两大类，路由方案和Overlay网络方案。 3.1 Pod3.1.1 Pod内container(容器)通信 Pod中管理着一组容器，这些容器共享同一个网络命名空间。Pod中的每个容器拥有与Pod相同的IP和port地址空间，并且由于他们在同一个网络命名空间，他们之间可以通过localhost相互访问。 每个Pod容器有一个pause容器其有独立的网络命名空间，在Pod内启动容器时候使用 –net=container就可以让当前容器加入到Pod容器拥有的网络命名空间（pause容器）。 3.1.2 同节点的Pod通信 每个Pod拥有一个ip地址，不同的Pod之间可以直接使用改ip与彼此进行通讯。在同一个Node上，从Pod的视角看，它存在于自己的网络命名空间中，并且需要与该Node上的其他网络命名空间上的Pod进行通信。 为了让多个Pod的网络命名空间链接起来，会创建一下veth pair对，veth对的一端链接到宿主机的网络命名空间，另一端链接到Pod的网络命名空间，并重新命名为eth0。 宿主机网络命名空间的接口会绑定到容器运行时配置使用的网络桥接上。从网桥的地址段中去IP地址赋值给容器的eth0接口。应用的任何运行在容器内部的程序都会发送数据到eht0网络接口，数据从宿主机命名空间的另外一个veth接口出来，然后发送给网桥。 3.1.3 不同节点的Pod通信 不同节点上的pod能够通信，需要把这些节点的网桥以某种方式连接起来，有多种连接不同节点上的网桥的方式，例如通过三层路由，或者Overlay网络（隧道技术，例如GRE和VxLAN等）。 路由方案 Overlay方案 pod通常需要对来自集群内部其他pod，以及来自集群外部的客户端的HTTP请求作出反应。pod需要一种寻找其他pod的方法来使用其他pod提供的服务。而在Kubernetes的网络中，有特殊的地方。 一个服务经常会起多个pod，你到底访问那个pod的ip呢？ pod经常会因为各种原因被调度，调度后一个pod的ip会发生变化。 pod的ip是虚拟的且局域的，在集群内部访问没有问题，但是从Kubernetes集群的外部如何访问pod的ip呢？ 为了解决第1，2的问题，Kubernetes提供了一种资源类型，服务（service）。为了解决第3个问题，Kubernetes有将服务的类型设置为NodePort，将服务的类型设置为LoadBanlance，创建一个Ingress资源。 3.2 Service Kubernetes的service（服务）是一种为一组功能相同的pod提供单一不变的接入点的资源。当服务存在时，它的ip地址和端口不会变化，客户端通过IP地址和端口号建立连接，这些连接会被路由到提供该服务的任意一个pod上。通过这种方式，客户端不需要知道每个单独的提供服务的pod的地址，这样这些pod可以在集群中随时被创建或者移除。 Kubernetes的服务需要解决两个主要问题。 服务怎么做负载均衡？ 服务怎么被发现？ 3.2.1 负载均衡 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP的形式。其实，服务并不是和pod直接相连的，它们之间是一种EndPoint资源。EndPoint资源就是暴露一个服务的IP地址和端口列表。 3.2.1.1 userspace 代理模式 kube-proxy 会监视 Kubernetes 主控节点对 Service 对象和 Endpoints 对象的添加和移除操作。对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。任何连接到“代理端口”的请求，都会被代理到 Service 的后端 Pods 中的某个上面（如 Endpoints 所报告的一样）。使用哪个后端 Pod，是 kube-proxy 基于 SessionAffinity 来确定的。 最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP） 和 Port 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。 3.2.1.2 iptables 代理模式 kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。 使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。这种方法也可能更可靠。 如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应， 则连接失败。这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败， 并会自动使用其他后端 Pod 重试。 3.2.1.3 IPVS 代理模式 在 ipvs 模式下，kube-proxy监视Kubernetes服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。与iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。IPVS提供了更多选项来平衡后端Pod的流量。 3.2.2 服务发现Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。 通过环境变量发现服务 在pod开始运行的时候，Kubernetes会初始化一系列的环境变量指向现在存在的服务 注：当您具有需要访问服务的Pod时，并且您正在使用环境变量方法将端口和群集 IP 发布到客户端 Pod 时，必须在客户端 Pod 出现 之前 创建服务。否则，这些客户端 Pod 将不会设定其环境变量。 通过DNS发现服务 支持群集的 DNS 服务器监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。如果在整个群集中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。 3.2.3 发布服务 对一些应用（如前端）的某些部分，可能希望通过外部 Kubernetes 集群外部 IP 地址暴露 Service。 Kubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP类型。 ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。 3.3 Ingress 我们也可以使用 Ingress 来暴露自己的服务。 为什么需要Ingress呢？一个重要的原因是每个LoadBalancer服务都需要自己的负载均衡器，以及独有的公用IP地址，而Ingress只需要一个公网IP就能为很多服务提供访问。例如，当客户端向Ingress发送HTTP请求时，Ingress会根据请求的主机和路径决定请求转发到的服务。 客户端先执行DNS查询，DNS服务器返回了Ingress控制器的IP地址。 客户端然后向Ingress控制器发送HTTP请求，并在Host头部中指定访问的域名。 控制器从该Host头部确认客户端尝试访问哪个服务，通过与该服务关联的Endpoint对象查看pod IP，并将客户端的请求转发给其中一个pod。 4. 云厂商Kubernetes实践4.1 AWS Kubernetes网络方案 AWS上搭建Kubernetes集群环境有两种方式，一种是使用托管服务Amazon Elastic Kubernetes Service (Amazon EKS) ，一种是自建K8S集群。可以使用Amazon VPC CNI插件管理Pod的网络地址和通信。 EKS网络架构 4.1.1 VPC CNI插件 AWS VPC CNI 为 Kubernetes 集群提供了集成的 AWS 虚拟私有云（VPC）网络，使用该 CNI 插件，可使 Kubernetes Pod 拥有与在 VPC 网络上相同的 IP 地址。CNI 将 AWS 弹性网络接口（ENI）分配给每个 Kubernetes 节点，并将每个 ENI 的辅助 IP 范围用于该节点上的 Pod 。 Kubernetes 的 Amazon VPC 容器网络接口 (CNI) 插件随每个节点一起部署，插件包含两个主要组件。 **L-IPAM 守护程序**\\-负责创建网络接口并将网络接口附加到 Amazon EC2 实例,将辅助 IP 地址分配给网络接口,并在每个节点上维护 IP 地址的地址池,以便在安排时分配到 Kubernetes Pod。 **CNI 插件** – 负责连接主机网络(例如,配置网络接口和虚拟以太网对)并向 Pod 命名空间添加正确的网络接口。 4.1.2 Pod通信 VPC 内的通信（如 Pod 到 Pod）在私有 IP 地址之间是直接通信。 4.1.2 Pod和外部通信 当流量以 VPC 外部的地址为目标时,默认情况下,Kubernetes 的 Amazon VPC CNI 插件将每个 Pod 的私有 IP 地址转换为分配给 Pod 在其上运行的 节点的主 网络接口Amazon EC2(网络接口)的主私有IP地址，有如下两种方式。 4.1.3 Ingress AWS ALB Ingress 控制器将在Kubernetes 用户声明集群上的 Ingress 资源时触发创建 ALB 以及必要的 AWS 支持资源。Ingress 资源通过 ALB 将 HTTP\\[s\\] 流量路由至集群内的不同终端节点。 控制器观察来自 API 服务器的进站事件。如果发现 Ingress 资源满足要求，则将开始创建 AWS 资源。 为 Ingress 资源创建 ALB。 为 Ingress 资源中指定的每个后端创建目标组。 为 Ingress 资源注释中指定的每个端口创建侦听器。如果未指定端口，则将使用合理的默认值（80 或 443）。 为 Ingress 资源中指定的每个路径创建规则。这将确保指向特定路径的流量将被路由至所创建的正确目标组。 4.2 GCP Kubernetes网络方案Google Kubernetes Engine (GKE) 提供了一个托管环境，可以使用 Google 基础架构在其中部署、管理和扩缩容器化应用。 4.2.1 Pod通信 4.2.2 Service 4.2.3 Loadbalancer 具体细节，参考GCP的官方文档。 https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview 4.3 阿里云Kubernetes网络方案 阿里云容器服务产品线的整体架构 本章节是介绍阿里云容器服务Kubernetes版ACK（Alibaba Cloud Container Service for Kubernetes）。 4.3.1 网络模型 容器服务将Kubernetes网络和阿里云VPC的深度集成，提供了稳定高性能的容器网络。在容器服务中，支持以下类型的互联互通。 同一个容器集群中，Pod之间相互访问。 同一个容器集群中，Pod访问Service。 同一个容器集群中，ECS访问Service。 Pod直接访问同一个VPC下的ECS。 同一个VPC下的ECS直接访问Pod。 4.3.2 阿里云Terway网络插件 Terway网络插件是阿里云容器服务的网络插件，功能上完全兼容Flannel。 支持将阿里云的弹性网卡分配给容器。 支持基于Kubernetes标准的NetworkPolicy来定义容器间的访问策略，兼容Calico的Network Policy。 在Terway网络插件中，每个Pod拥有自己网络栈和IP地址。同一台ECS内的Pod之间通信，直接通过机器内部的转发，跨ECS的Pod通信，报文通过VPC的vRouter转发。由于不需要使用VxLAN等的隧道技术封装报文，因此具有较高的通信性能。 4.4 腾讯云Kubernetes网络方案腾讯云容器服务（Tencent Kubernetes Engine ，TKE）基于原生 kubernetes 提供以容器为核心的、高度可扩展的高性能容器管理服务。 本章节主要参考以下文章，公众号：腾讯云原生 公众号文章：腾讯云容器服务TKE推出新一代零损耗容器网络 4.4.1 GlobalRouter 模式 基于 vpc 实现的全局路由模式，目前是 TKE 默认网络方案。该模式依托于 vpc 底层路由能力，不需要在节点上配置 vxlan 等 overlay 设备，就可实现容器网络 和 vpc 网络的互访，并且相比于 calico/flannel 等网络方案，没有额外的解封包，性能也会更好。 4.4.1 VPC-CNI 模式 TKE 基于 CNI 和 VPC 弹性网卡实现的容器网络能力，适用于 Pod 固定 IP，CLB 直通 Pod，Pod 直绑 EIP 等场景。该网络模式下，容器与节点分布在同一网络平面，容器 IP 为 IPAMD 组件所分配的弹性网卡 IP。 4.4.3 VPC-CNI-**独立网卡** 依托于弹性网卡，将绑定到节点的弹性网卡通过 CNI 配置到容器网络命名空间，实现容器直接独享使用弹性网卡。 4.5 其他CNI插件 参考链接 https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/ 5 总结 本文主要介绍了Kubernetes的网络实现，包括pod的通信，服务（service），Ingress的实现，也简要介绍了云厂商的CNI插件的实现方法。Kubernetes还有其他优秀的网络插件，各个插件的实现方式有所不同，不过Kubernetes网络模型是不变。 最后欢迎大家留言沟通交流。 6 参考文献Kubernetes集群网络系统 https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/ Amazon EKS https://docs.amazonaws.cn/eks/latest/userguide/external-snat.html Amazon VPC CNI https://aws.amazon.com/cn/blogs/china/use-amazon-vpc-cni-build-default-net-kubernetes-groups/ Google Kubernetes Engine (GKE) https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview kubernetes网络和CNI简介 https://www.jianshu.com/p/88062fa25083 Understanding kubernetes networking: pods https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727 containernetworking/cni https://github.com/containernetworking/cni Amazon Elastic Container Service https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html CNI - Container Network Interface（容器网络接口） https://jimmysong.io/kubernetes-handbook/concepts/cni.html containernetworking/cni https://github.com/containernetworking/cni","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/tags/Kubernetes/"}]},{"title":"Kubernetes Ingress 控制器的技术选型技巧","slug":"Technical-selection-of-Kubernetes-Ingress-controller","date":"2021-04-07T16:00:00.000Z","updated":"2021-04-08T08:56:51.700Z","comments":true,"path":"2021/04/08/Technical-selection-of-Kubernetes-Ingress-controller/","link":"","permalink":"http://zhang-yu.me/2021/04/08/Technical-selection-of-Kubernetes-Ingress-controller/","excerpt":"","text":"Kubernetes Ingress 控制器的技术选型技巧 作者：厉辉，腾讯云中间件API网关核心研发成员 在 Kubernetes 的实践、部署中，为了解决 Pod 迁移、Node Pod 端口、域名动态分配等问题，需要开发人员选择合适的 Ingress 解决方案。面对市场上众多Ingress产品，开发者该如何分辨它们的优缺点？又该如何结合自身的技术栈选择合适的技术方案呢？在本文中，腾讯云中间件核心研发工程师厉辉将为你介绍如何进行 Kubernates Ingress 控制器的技术选型。 名词解释 阅读本文需要熟悉以下基本概念： 集群：是指容器运行所需云资源的集合，包含了若干台云服务器、负载均衡器等云资源。 实例（Pod）：由相关的一个或多个容器构成一个实例，这些容器共享相同的存储和网络空间。 工作负载（Node）：Kubernetes 资源对象，用于管理 Pod 副本的创建、调度以及整个生命周期的自动控制。 服务（Service）：由多个相同配置的实例（Pod）和访问这些实例（Pod）的规则组成的微服务。 Ingress：Ingress 是用于将外部 HTTP（S）流量路由到服务（Service）的规则集合。 Kubernetes 访问现状 Kubernetes 的外部访问方式 在 Kubernetes 中，服务跟 Pod IP 主要供服务在集群内访问使用，对于集群外的应用是不可见的。怎么解决这个问题呢？为了让外部的应用能够访问 Kubernetes 集群中的服务，通常解决办法是 NodePort 和 LoadBalancer。 这两种方案其实各自都存在一些缺点： NodePort 的缺点是一个端口只能挂载一个 Service，而且为了更高的可用性，需要额外搭建一个负载均衡。 LoadBalancer 的缺点则是每个服务都必须要有一个自己的 IP，不论是内网 IP 或者外网 IP。更多情况下，为了保证 LoadBalancer 的能力，一般需要依赖于云服务商。 在Kubernetes的实践、部署中，为了解决像 Pod 迁移、Node Pod 端口、域名动态分配，或者是 Pod 后台地址动态更新这种问题，就产生了 Ingress 解决方案 Nginx Ingress 的缺点 Ingress 是Kubernetes中非常重要的外网流量入口。在Kubernetes中所推荐的默认值为Nginx Ingress，为了与后面Nginx 提供的商业版 Ingress 区分开来，我就称它为Kubernetes Ingress。 Kubernetes Ingress，顾名思义基于 Nginx 的平台，Nginx 现在是世界上最流行的 Nginx HTTP Sever，相信大家都对 Nginx 也比较熟悉，这是一个优点。它还有一个优点是 Nginx Ingress 接入 Kubernetes 集群所需的配置非常少，而且有很多文档来指引你如何使用它。这对于大部分刚接触 Kubernetes 的人或者创业公司来说，Nginx Ingress 的确是一个非常好的选择。 但是当 Nginx Ingress 在一些大环境上使用时，就会出现很多问题： 第一个问题：Nginx Ingress用了一些 OpenResty 的特性，但最终配置加载还是依赖于原有的 Nginx config reload。当路由配置非常大时，Nginx reload 会耗时很久，时间长达几秒甚至十几秒，这样就会严重影响业务，甚至造成业务中断。 第二个问题：Nginx Ingress 的插件开发非常困难。如果你认为 Nginx Ingress 本身插件不够用，需要使用一些定制化插件，这个额外的开发任务对程序员来说是十分痛苦的。因为Nginx Ingress自身的插件能力和可扩展性非常差。 Ingress 选型原则 既然发现了 Nginx Ingress 有很多问题，那是不是考虑选择其他开源的、更好用的 Ingress？市场上比 Kubernetes Ingress 好用的Ingress起码有十几家，那么如何从这么多 Ingress 中选择适合自己的呢？ Ingress 自身是基于 HTTP 网关的，市面上 HTTP 网关主要有这么几种：Nginx、Golang 原生的网关，以及新崛起的 Envoy 。但是每个开发人员所擅长的技术栈不同，所以适合的 Ingress 也会不一样。 那么问题来了，我们如何选择一个更加好用的 Ingress 呢？或者缩小点范围，熟悉 Nginx 或 OpenResty 的开发人员，应该选择哪一个 Ingress 呢？ 下面来介绍一下我对 Ingress 控制器选型的一些经验。 选型原则 1.基本特点 首先我认为Ingress 控制器应该具备以下基本功能，如果连这些功能都没有，那完全可以直接pass。 必须开源的，不开源的无法使用。 Kubernetes 中Pod 变化非常频繁，服务发现非常重要。 现在 HTTPS 已经很普及了，TLS 或者 SSL 的能力也非常重要，比如证书管理的功能。 支持 WebSocket 等常见协议，在某些情况下，可能还需要支持 HTTP2 、QUIC 等协议。 2.基础软件前面有提到，每个人擅长的技术平台不一样，所以选择自己更加熟悉的 HTTP 网关也显得至关重要。比如 Nginx、HAProxy、Envoy 或者是 Golang 原生网关。因为你熟悉它的原理，在使用中可以实现快速落地。 在生产环境上，高性能是一个很重要的特性，但比之更重要的是高可用。这意味着你选择的网关，它的可用性、稳定性一定要非常强，只有这样，服务才能稳定。 3.功能需求抛开上述两点，就是公司业务对网关的特殊需求。你选择一个开源产品，最好肯定是开箱能用的。比如你需要 GRPC 协议转换的能力，那当然希望选的网关具备这样的功能。这里简单列一下影响选择的因素： 协议：是否支持 HTTP2、HTTP3； 负载均衡算法：最基本的WRR、一致性哈希负载均衡算法是否能够满足需求，还是需要更加复杂的类似EWMA负载均衡算法。 鉴权限流：仅需要简单的鉴权，或更进阶的鉴权方式。又或者需要集成，能够快速的开发出像腾讯云 IM 的鉴权功能。Kubernetes Ingress除了前面我们提到的存在Nginx reload 耗时长、插件扩展能力差的问题，另外它还存在后端节点调整权重的能力不够灵活的问题。 选择 APISIX 相比Kubernetes Ingress，我个人更推荐 APISIX 作为Ingress ?controller。虽然它在功能上比 Kong 会少很多，但是 APISIX 很好的路由能力、灵活的插件能力，以及本身的高性能，能够弥补在 Ingress 选型上的一些缺点。对于基于 Nginx 或 Openresty 开发的程序员，如果对现在的 Ingress 不满意，我推荐你们去使用 APISIX 作为 Ingress。 如何将 APISIX 作为 Ingress 呢？我们首先要做出一个区分，Ingress 是 Kubernetes 名称的定义或者规则定义，Ingress controller 是将 Kubernetes 集群状态同步到网关的一个组件。但 APISIX 本身只是 API 网关，怎么把 APISIX 实现成 Ingress controller 呢？我们先来简要了解一下如何实现 Ingress。 实现 Ingress，本质上就只有两部分内容： 第一部分：需要将 Kubernetes 集群中的配置、或 Kubernetes 集群中的状态同步到 APISIX 集群。 第二部分：需要将 APISIX中 的一些概念，比如像服务、upstream 等概念定义为 Kubernetes 中的 CRD。 如果实现了第二部分，通过 Kubernetes Ingress 的配置，便可以很快的产生 APISIX。通过 APISIX Ingress controller 就可以产生 APISIX 相关的配置。当前为了快速的将 APISIX 落地为能够支持 Kubernetes 的 Ingress ，我们创建了一个开源项目，叫 Ingress Controller。 ingress controller 架构图 上图为Ingress controller 项目的整体架构图。左边部分为 Kubernetes 集群，这里可以导入一些 yaml 文件，对 Kubernetes 的配置进行变更。右边部分则是 APISIX 集群，以及它的控制面和数据面。从架构图中可以看出，APISIX Ingress 充当了 Kubernetes 集群以及 APISIX 集群之间的连接者。它主要负责监听 Kubernetes 集群中节点的变化，将集群中的状态同步到 APISIX 集群。另外，由于Kubernetes 倡导所有组件都要具备高可用的特性，所以在 APISIX Ingress 设计之初，我们通过双节点或多节点的模式来保证 APISIX ?Ingress Controller 的保障高可用。 总结 各类 Ingress 横向对比 相对于市面上流行的 Ingress 控制器，我们简单对比来看看 APISIX ingress 有什么优缺点。上图是外国开发人员针对 Kubernetes Ingress 选型做的一张表格。我在原来表格的基础上，结合自己的理解，将 APISIX Ingress 的功能加入了进来。我们可以看到，最左边的是APISIX，后边就是 Kubernetes Ingress 和 Kong Ingress，后面的 Traefik，就是基于 Golang 的 Ingress。HAproxy 是比较常见的，过去是比较流行的负载均衡器。Istio 和 Ambassador 是国外非常流行的两个Ingress。 接下来我们总结下这些 Ingress各自的优缺点： APISIX Ingress：APISIX Ingress 的优点前面也提到了，它具有非常强大的路由能力、灵活的插件拓展能力，在性能上表现也非常优秀。同时，它的缺点也非常明显，尽管APISIX开源后有非常多的功能，但是缺少落地案例，没有相关的文档指引大家如何使用这些功能。 Kubernetes Ingress：即 Kubernetes 推荐默认使用的 Nginx Ingress。它的主要优点为简单、易接入。缺点是Nginx reload耗时长的问题根本无法解决。另外，虽然可用插件很多，但插件扩展能力非常弱。 Nginx Ingress：主要优点是在于它完全支持 TCP 和 UDP 协议，但是缺失了鉴权方式、流量调度等其他功能。 Kong：其本身就是一个 API 网关，它也算是开创了先河，将 API 网关引入到 Kubernetes 中当 Ingress。另外相对边缘网关，Kong 在鉴权、限流、灰度部署等方面做得非常好。Kong Ingress 还有一个很大的优点：提供了一些 API、服务的定义，可以抽象成 Kubernetes 的 CRD，通过K8S Ingress 配置便可完成同步状态至 Kong 集群。缺点就是部署特别困难，另外在高可用方面，与 APISIX 相比也是相形见绌。 Traefik ：基于 Golang 的 Ingress，它本身是一个微服务网关，在 Ingress 的场景应用比较多。他的主要平台基于 Golang，自身支持的协议也非常多，总体来说是没有什么缺点。如果大家熟悉 Golang 的话，也推荐一用。 HAproxy：是一个久负盛名的负载均衡器。它主要优点是具有非常强大的负载均衡能力，其他方面并不占优势。 Istio Ingress 和 Ambassador Ingress 都是基于非常流行的 Envoy。说实话，我认为这两个 Ingress 没有什么缺点，可能唯一的缺点是他们基于 Envoy 平台，大家对这个平台都不是很熟悉，上手门槛会比较高。 综上所述，大家在了解了各个 Ingress 的优劣势后，可以结合自身情况快速选择适合自己的 Ingress。 来自 “ ITPUB博客 ” ，链接：http://blog.itpub.net/31559354/viewspace-2677027/","categories":[{"name":"Ingress","slug":"Ingress","permalink":"http://zhang-yu.me/categories/Ingress/"}],"tags":[{"name":"Ingress","slug":"Ingress","permalink":"http://zhang-yu.me/tags/Ingress/"}]},{"title":"Kubernetes入门-进阶实战","slug":"Kubernetes-remen","date":"2021-04-07T16:00:00.000Z","updated":"2021-04-08T09:59:59.264Z","comments":true,"path":"2021/04/08/Kubernetes-remen/","link":"","permalink":"http://zhang-yu.me/2021/04/08/Kubernetes-remen/","excerpt":"","text":"Kubernetes 入门&amp;进阶实战 作者：oonamao毛江云，腾讯 CSIG 应用开发工程师 本文组织方式： 1.?K8S?是什么，即作用和目的。涉及?K8S?架构的整理，Master?和?Node?之间的关系，以及?K8S?几个重要的组件：API?Server、Scheduler、Controller、etcd?等。2.?K8S?的重要概念，即?K8S?的?API?对象，也就是常常听到的?Pod、Deployment、Service?等。3.?如何配置?kubectl，介绍kubectl工具和配置办法。4.?如何用kubectl?部署服务。5.?如何用kubectl?查看、更新/编辑、删除服务。6.?如何用kubectl?排查部署在K8S集群上的服务出现的问题 I. K8S 概览1.1 K8S 是什么？K8S 是Kubernetes的全称，官方称其是： Kubernetes is an open source system for managing containerized applications across multiple hosts. It provides basic mechanisms for deployment, maintenance, and scaling of applications. 用于自动部署、扩展和管理“容器化（containerized）应用程序”的开源系统。 翻译成大白话就是：“K8 是 S 负责自动化运维管理多个 Docker 程序的集群”。那么问题来了：Docker 运行可方便了，为什么要用 K8S，它有什么优势？ 插一句题外话： 为什么 Kubernetes 要叫 Kubernetes 呢？维基百科已经交代了（老美对星际是真的痴迷）： Kubernetes（在希腊语意为“舵手”或“驾驶员”）由 Joe Beda、Brendan Burns 和 Craig McLuckie 创立，并由其他谷歌工程师，包括 Brian Grant 和 Tim Hockin 等进行加盟创作，并由谷歌在 2014 年首次对外宣布 。该系统的开发和设计都深受谷歌的 Borg 系统的影响，其许多顶级贡献者之前也是 Borg 系统的开发者。在谷歌内部，Kubernetes 的原始代号曾经是Seven，即星际迷航中的 Borg（博格人）。Kubernetes 标识中舵轮有七个轮辐就是对该项目代号的致意。 为什么 Kubernetes 的缩写是 K8S 呢？我个人赞同Why Kubernetes is Abbreviated k8s中说的观点“嘛，写全称也太累了吧，不如整个缩写”。其实只保留首位字符，用具体数字来替代省略的字符个数的做法，还是比较常见的。 1.2 为什么是 K8S?试想下传统的后端部署办法：把程序包（包括可执行二进制文件、配置文件等）放到服务器上，接着运行启动脚本把程序跑起来，同时启动守护脚本定期检查程序运行状态、必要的话重新拉起程序。 有问题吗？显然有！最大的一个问题在于：**如果服务的请求量上来，已部署的服务响应不过来怎么办？**传统的做法往往是，如果请求量、内存、CPU 超过阈值做了告警，运维马上再加几台服务器，部署好服务之后，接入负载均衡来分担已有服务的压力。 问题出现了：从监控告警到部署服务，中间需要人力介入！那么，有没有办法自动完成服务的部署、更新、卸载和扩容、缩容呢？ 这，就是 K8S 要做的事情：自动化运维管理 Docker（容器化）程序。 1.3 K8S 怎么做？我们已经知道了 K8S 的核心功能：自动化运维管理多个容器化程序。那么 K8S 怎么做到的呢？这里，我们从宏观架构上来学习 K8S 的设计思想。首先看下图，图片来自文章Components of Kubernetes Architecture： K8S 是属于主从设备模型（Master-Slave 架构），即有 Master 节点负责核心的调度、管理和运维，Slave 节点则在执行用户的程序。但是在 K8S 中，主节点一般被称为Master Node 或者 Head Node（本文采用 Master Node 称呼方式），而从节点则被称为Worker Node 或者 Node（本文采用 Worker Node 称呼方式）。 要注意一点：Master Node 和 Worker Node 是分别安装了 K8S 的 Master 和 Woker 组件的实体服务器，每个 Node 都对应了一台实体服务器（虽然 Master Node 可以和其中一个 Worker Node 安装在同一台服务器，但是建议 Master Node 单独部署），所有 Master Node 和 Worker Node 组成了 K8S 集群，同一个集群可能存在多个 Master Node 和 Worker Node。 首先来看Master Node都有哪些组件： API Server。K8S 的请求入口服务。API Server 负责接收 K8S 所有请求（来自 UI 界面或者 CLI 命令行工具），然后，API Server 根据用户的具体请求，去通知其他组件干活。 Scheduler。K8S 所有 Worker Node 的调度器。当用户要部署服务时，Scheduler 会选择最合适的 Worker Node（服务器）来部署。 Controller Manager。K8S 所有 Worker Node 的监控器。Controller Manager 有很多具体的 Controller，在文章Components of Kubernetes Architecture中提到的有 Node Controller、Service Controller、Volume Controller 等。Controller 负责监控和调整在 Worker Node 上部署的服务的状态，比如用户要求 A 服务部署 2 个副本，那么当其中一个服务挂了的时候，Controller 会马上调整，让 Scheduler 再选择一个 Worker Node 重新部署服务。 etcd。K8S 的存储服务。etcd 存储了 K8S 的关键配置和用户配置，K8S 中仅 API Server 才具备读写权限，其他组件必须通过 API Server 的接口才能读写数据（见Kubernetes Works Like an Operating System）。 接着来看Worker Node的组件，笔者更赞同HOW DO APPLICATIONS RUN ON KUBERNETES文章中提到的组件介绍： Kubelet。Worker Node 的监视器，以及与 Master Node 的通讯器。Kubelet 是 Master Node 安插在 Worker Node 上的“眼线”，它会定期向 Worker Node 汇报自己 Node 上运行的服务的状态，并接受来自 Master Node 的指示采取调整措施。 Kube-Proxy。K8S 的网络代理。私以为称呼为 Network-Proxy 可能更适合？Kube-Proxy 负责 Node 在 K8S 的网络通讯、以及对外部网络流量的负载均衡。 Container Runtime。Worker Node 的运行环境。即安装了容器化所需的软件环境确保容器化程序能够跑起来，比如 Docker Engine。大白话就是帮忙装好了 Docker 运行环境。 Logging Layer。K8S 的监控状态收集器。私以为称呼为 Monitor 可能更合适？Logging Layer 负责采集 Node 上所有服务的 CPU、内存、磁盘、网络等监控项信息。 Add-Ons。K8S 管理运维 Worker Node 的插件组件。有些文章认为 Worker Node 只有三大组件，不包含 Add-On，但笔者认为 K8S 系统提供了 Add-On 机制，让用户可以扩展更多定制化功能，是很不错的亮点。 总结来看，K8S 的 Master Node 具备：请求入口管理（API Server），Worker Node 调度（Scheduler），监控和自动调节（Controller Manager），以及存储功能（etcd）；而 K8S 的 Worker Node 具备：状态和监控收集（Kubelet），网络和负载均衡（Kube-Proxy）、保障容器化运行环境（Container Runtime）、以及定制化功能（Add-Ons）。 到这里，相信你已经对 K8S 究竟是做什么的，有了大概认识。接下来，再来认识下 K8S 的 Deployment、Pod、Replica Set、Service 等，但凡谈到 K8S，就绕不开这些名词，而这些名词也是最让 K8S 新手们感到头疼、困惑的。 II. K8S 重要概念2.1 Pod 实例官方对于Pod的解释是： Pod是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 这样的解释还是很难让人明白究竟 Pod 是什么，但是对于 K8S 而言，Pod 可以说是所有对象中最重要的概念了！因此，我们必须首先清楚地知道“Pod 是什么”，再去了解其他的对象。 从官方给出的定义，联想下“最小的 xxx 单元”，是不是可以想到本科在学校里学习“进程”的时候，教科书上有一段类似的描述：资源分配的最小单位；还有”线程“的描述是：CPU 调度的最小单位。什么意思呢？”最小 xx 单位“要么就是事物的衡量标准单位，要么就是资源的闭包、集合。前者比如长度米、时间秒；后者比如一个”进程“是存储和计算的闭包，一个”线程“是 CPU 资源（包括寄存器、ALU 等）的闭包。 同样的，Pod 就是 K8S 中一个服务的闭包。这么说的好像还是有点玄乎，更加云里雾里了。简单来说，Pod 可以被理解成一群可以共享网络、存储和计算资源的容器化服务的集合。再打个形象的比喻，在同一个 Pod 里的几个 Docker 服务/程序，好像被部署在同一台机器上，可以通过 localhost 互相访问，并且可以共用 Pod 里的存储资源（这里是指 Docker 可以挂载 Pod 内的数据卷，数据卷的概念，后文会详细讲述，暂时理解为“需要手动 mount 的磁盘”）。笔者总结 Pod 如下图，可以看到：同一个 Pod 之间的 Container 可以通过 localhost 互相访问，并且可以挂载 Pod 内所有的数据卷；但是不同的 Pod 之间的 Container 不能用 localhost 访问，也不能挂载其他 Pod 的数据卷。 对 Pod 有直观的认识之后，接着来看 K8S 中 Pod 究竟长什么样子，具体包括哪些资源？ K8S 中所有的对象都通过 yaml 来表示，笔者从官方网站摘录了一个最简单的 Pod 的 yaml： apiVersion:?v1 kind:?Pod metadata: ??name:?memory-demo ??namespace:?mem-example spec: ??containers: ??-?name:?memory-demo-ctr ????image:?polinux/stress ????resources: ??????limits: ????????memory:?&quot;200Mi&quot; ??????requests: ????????memory:?&quot;100Mi&quot; ????command:?[&quot;stress&quot;] ????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;] ????volumeMounts: ????-?name:?redis-storage ??????mountPath:?/data/redis ??volumes: ??-?name:?redis-storage ????emptyDir:?&#123;&#125; 看不懂不必慌张，且耐心听下面的解释： apiVersion记录 K8S 的 API Server 版本，现在看到的都是v1，用户不用管。 kind记录该 yaml 的对象，比如这是一份 Pod 的 yaml 配置文件，那么值内容就是Pod。 metadata记录了 Pod 自身的元数据，比如这个 Pod 的名字、这个 Pod 属于哪个 namespace（命名空间的概念，后文会详述，暂时理解为“同一个命名空间内的对象互相可见”）。 spec记录了 Pod 内部所有的资源的详细信息，看懂这个很重要： containers记录了 Pod 内的容器信息，containers包括了：name容器名，image容器的镜像地址，resources容器需要的 CPU、内存、GPU 等资源，command容器的入口命令，args容器的入口参数，volumeMounts容器要挂载的 Pod 数据卷等。可以看到，上述这些信息都是启动容器的必要和必需的信息。 volumes记录了 Pod 内的数据卷信息，后文会详细介绍 Pod 的数据卷。 2.2 Volume 数据卷 K8S 支持很多类型的 volume 数据卷挂载，具体请参见K8S 卷。前文就“如何理解 volume”提到：“需要手动 mount 的磁盘”，此外，有一点可以帮助理解：数据卷 volume 是 Pod 内部的磁盘资源。 其实，单单就 Volume 来说，不难理解。但是上面还看到了volumeMounts，这俩是什么关系呢？ volume 是 K8S 的对象，对应一个实体的数据卷；而 volumeMounts 只是 container 的挂载点，对应 container 的其中一个参数。但是，volumeMounts 依赖于 volume，只有当 Pod 内有 volume 资源的时候，该 Pod 内部的 container 才可能有 volumeMounts。 2.3 Container 容器本文中提到的镜像 Image、容器 Container，都指代了 Pod 下的一个container。关于 K8S 中的容器，在 2.1Pod 章节都已经交代了，这里无非再啰嗦一句：一个 Pod 内可以有多个容器 container。 在 Pod 中，容器也有分类，对这个感兴趣的同学欢迎自行阅读更多资料： 标准容器 Application Container。 初始化容器 Init Container。 边车容器 Sidecar Container。 临时容器 Ephemeral Container。 一般来说，我们部署的大多是标准容器（ Application Container）。 2.4 Deployment 和 ReplicaSet（简称 RS）除了 Pod 之外，K8S 中最常听到的另一个对象就是 Deployment 了。那么，什么是 Deployment 呢？官方给出了一个要命的解释： 一个 Deployment 控制器为 Pods 和 ReplicaSets 提供声明式的更新能力。 你负责描述 Deployment 中的 _目标状态_，而 Deployment 控制器以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，并通过新的 Deployment 收养其资源。 翻译一下：Deployment 的作用是管理和控制 Pod 和 ReplicaSet，管控它们运行在用户期望的状态中。哎，打个形象的比喻，** Deployment 就是包工头 **，主要负责监督底下的工人 Pod 干活，确保每时每刻有用户要求数量的 Pod 在工作。如果一旦发现某个工人 Pod 不行了，就赶紧新拉一个 Pod 过来替换它。 新的问题又来了：那什么是 ReplicaSets 呢？ ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。 再来翻译下：ReplicaSet 的作用就是管理和控制 Pod，管控他们好好干活。但是，ReplicaSet 受控于 Deployment。形象来说，ReplicaSet 就是总包工头手下的小包工头。 笔者总结得到下面这幅图，希望能帮助理解： 新的问题又来了：如果都是为了管控 Pod 好好干活，为什么要设置 Deployment 和 ReplicaSet 两个层级呢，直接让 Deployment 来管理不可以吗？ 回答：不清楚，但是私以为是因为先有 ReplicaSet，但是使用中发现 ReplicaSet 不够满足要求，于是又整了一个 Deployment（有清楚 Deployment 和 ReplicaSet 联系和区别的小伙伴欢迎留言啊）。 但是，从 K8S 使用者角度来看，用户会直接操作 Deployment 部署服务，而当 Deployment 被部署的时候，K8S 会自动生成要求的 ReplicaSet 和 Pod。在K8S 官方文档中也指出用户只需要关心 Deployment 而不操心 ReplicaSet： This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section. 这实际上意味着您可能永远不需要操作 ReplicaSet 对象：直接使用 Deployments 并在规范部分定义应用程序。 补充说明：在 K8S 中还有一个对象 — ReplicationController（简称 RC），官方文档对它的定义是： ReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态。换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。 怎么样，和 ReplicaSet 是不是很相近？在Deployments, ReplicaSets, and pods教程中说“ReplicationController 是 ReplicaSet 的前身”，官方也推荐用 Deployment 取代 ReplicationController 来部署服务。 2.5 Service 和 Ingress吐槽下 K8S 的概念/对象/资源是真的多啊！前文介绍的 Deployment、ReplicationController 和 ReplicaSet 主要管控 Pod 程序服务；那么，Service 和 Ingress 则负责管控 Pod 网络服务。 我们先来看看官方文档中 Service 的定义： 将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。 使用 Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。 翻译下：K8S 中的服务（Service）并不是我们常说的“服务”的含义，而更像是网关层，是若干个 Pod 的流量入口、流量均衡器。 那么，为什么要 Service 呢？ 私以为在这一点上，官方文档讲解地非常清楚： Kubernetes Pod 是有生命周期的。它们可以被创建，而且销毁之后不会再启动。如果您使用 Deployment 来运行您的应用程序，则它可以动态创建和销毁 Pod。 每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。 这导致了一个问题：如果一组 Pod（称为“后端”）为群集内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用工作量的后端部分？ 补充说明：K8S 集群的网络管理和拓扑也有特别的设计，以后会专门出一章节来详细介绍 K8S 中的网络。这里需要清楚一点：K8S 集群内的每一个 Pod 都有自己的 IP（是不是很类似一个 Pod 就是一台服务器，然而事实上是多个 Pod 存在于一台服务器上，只不过是 K8S 做了网络隔离），在 K8S 集群内部还有 DNS 等网络服务（一个 K8S 集群就如同管理了多区域的服务器，可以做复杂的网络拓扑）。 此外，笔者推荐k8s 外网如何访问业务应用对于 Service 的介绍，不过对于新手而言，推荐阅读前半部分对于 service 的介绍即可，后半部分就太复杂了。我这里做了简单的总结： Service 是 K8S 服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”。举个例子，我们的一个服务 A，部署了 3 个备份，也就是 3 个 Pod；对于用户来说，只需要关注一个 Service 的入口就可以，而不需要操心究竟应该请求哪一个 Pod。优势非常明显：一方面外部用户不需要感知因为 Pod 上服务的意外崩溃、K8S 重新拉起 Pod 而造成的 IP 变更，外部用户也不需要感知因升级、变更服务带来的 Pod 替换而造成的 IP 变化，另一方面，Service 还可以做流量负载均衡。 但是，Service 主要负责 K8S 集群内部的网络拓扑。那么集群外部怎么访问集群内部呢？这个时候就需要 Ingress 了，官方文档中的解释是： Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。 翻译一下：Ingress 是整个 K8S 集群的接入层，复杂集群内外通讯。 最后，笔者把 Ingress 和 Service 的关系绘制网络拓扑关系图如下，希望对理解这两个概念有所帮助： 2.6 namespace 命名空间和前文介绍的所有的概念都不一样，namespace 跟 Pod 没有直接关系，而是 K8S 另一个维度的对象。或者说，前文提到的概念都是为了服务 Pod 的，而 namespace 则是为了服务整个 K8S 集群的。 那么，namespace 是什么呢？ 上官方文档定义： Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。这些虚拟集群被称为名字空间。 翻译一下：namespace 是为了把一个 K8S 集群划分为若干个资源不可共享的虚拟集群而诞生的。 也就是说，可以通过在 K8S 集群内创建 namespace 来分隔资源和对象。比如我有 2 个业务 A 和 B，那么我可以创建 ns-a 和 ns-b 分别部署业务 A 和 B 的服务，如在 ns-a 中部署了一个 deployment，名字是 hello，返回用户的是“hello a”；在 ns-b 中也部署了一个 deployment，名字恰巧也是 hello，返回用户的是“hello b”（要知道，在同一个 namespace 下 deployment 不能同名；但是不同 namespace 之间没有影响）。前文提到的所有对象，都是在 namespace 下的；当然，也有一些对象是不隶属于 namespace 的，而是在 K8S 集群内全局可见的，官方文档提到的可以通过命令来查看，具体命令的使用办法，笔者会出后续的实战文章来介绍，先贴下命令： `#?位于名字空间中的资源kubectl?api-resources?–namespaced=true #?不在名字空间中的资源kubectl?api-resources?–namespaced=false` 不在 namespace 下的对象有： 在 namespace 下的对象有（部分）： 2.7 其他K8S 的对象实在太多了，2.1-2.6 介绍的是在实际使用 K8S 部署服务最常见的。其他的还有 Job、CronJob 等等，在对 K8S 有了比较清楚的认知之后，再去学习更多的 K8S 对象，不是难事。 III. 配置 kubectl3.1 什么是 kubectl？官方文档中介绍 kubectl 是： Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数–kubeconfig 来指定其他位置的 kubeconfig 文件。 也就是说，可以通过 kubectl 来操作 K8S 集群，基本语法： 使用以下语法 kubectl 从终端窗口运行命令： kubectl?[command]?[TYPE]?[NAME]?[flags] 其中 command、TYPE、NAME 和 flags 分别是： command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 TYPE：指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果: ```shell kubectl?get?pod?pod1 kubectl?get?pods?pod1 kubectl?get?po?pod1 ``-?NAME：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息?kubectl get pods。 在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件： -?要按类型和名称指定资源：??-?要对所有类型相同的资源进行分组，请执行以下操作：TYPE1 name1 name2 name&lt;#&gt;。?例子：kubectl get pod example-pod1 example-pod2??-?分别指定多个资源类型：TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE&lt;#&gt;/name&lt;#&gt;。?例子：kubectl get pod/example-pod1 replicationcontroller/example-rc1-?用一个或多个文件指定资源：-f file1 -f file2 -f file&lt;#&gt;??-?使用?YAML?而不是?JSON?因为 YAML 更容易使用，特别是用于配置文件时。?例子：kubectl get -f ./pod.yaml-?flags:?指定可选的参数。例如，可以使用?-s?或?-server?参数指定 Kubernetes API 服务器的地址和端口。`` 就如何使用 kubectl 而言，官方文档已经说得非常清楚。不过对于新手而言，还是需要解释几句： kubectl 是 K8S 的命令行工具，并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。 kubectl 是通过本地的配置文件来连接到 K8S 集群的，默认保存在$HOME/.kube 目录下；也可以通过 KUBECONFIG 环境变量或设置命令参数–kubeconfig 来指定其他位置的 kubeconfig 文件【官方文档】。 接下来，一起看看怎么使用 kubectl 吧，切身感受下 kubectl 的使用。 请注意，如何安装 kubectl 的办法有许多非常明确的教程，比如《安装并配置 kubectl》，本文不再赘述。 1.2 怎么配置 kubectl？第一步，必须准备好要连接/使用的 K8S 的配置文件，笔者给出一份杜撰的配置： apiVersion:?v1 clusters: -?cluster: ????certificate-authority-data:?thisisfakecertifcateauthoritydata00000000000 ????server:? ??name:?cls-dev contexts: -?context: ????cluster:?cls-dev ????user:?kubernetes-admin ??name:?kubernetes-admin@test current-context:?kubernetes-admin@test kind:?Config preferences:?&#123;&#125; users: -?name:?kubernetes-admin ??user: ????token:?thisisfaketoken00000 解读如下： clusters记录了 clusters（一个或多个 K8S 集群）信息： name是这个 cluster（K8S 集群）的名称代号 server是这个 cluster（K8S 集群）的访问方式，一般为 IP+PORT certificate-authority-data是证书数据，只有当 cluster（K8S 集群）的连接方式是 https 时，为了安全起见需要证书数据 users记录了访问 cluster（K8S 集群）的账号信息： name是用户账号的名称代号 user/token是用户的 token 认证方式，token 不是用户认证的唯一方式，其他还有账号+密码等。 contexts是上下文信息，包括了 cluster（K8S 集群）和访问 cluster（K8S 集群）的用户账号等信息： name是这个上下文的名称代号 cluster是 cluster（K8S 集群）的名称代号 user是访问 cluster（K8S 集群）的用户账号代号 current-context记录当前 kubectl 默认使用的上下文信息 kind和apiVersion都是固定值，用户不需要关心 preferences则是配置文件的其他设置信息，笔者没有使用过，暂时不提。 第二步，给 kubectl 配置上配置文件。 --kubeconfig参数。第一种办法是每次执行 kubectl 的时候，都带上--kubeconfig=$&#123;CONFIG_PATH&#125;。给一点温馨小提示：每次都带这么一长串的字符非常麻烦，可以用 alias 别名来简化码字量，比如alias k=kubectl --kubeconfig=$&#123;CONFIG_PATH&#125;。 KUBECONFIG环境变量。第二种做法是使用环境变量KUBECONFIG把所有配置文件都记录下来，即export KUBECONFIG=$KUBECONFIG:$&#123;CONFIG_PATH&#125;。接下来就可以放心执行 kubectl 命令了。 $HOME/.kube/config 配置文件。第三种做法是把配置文件的内容放到$HOME/.kube/config 内。具体做法为： 如果$HOME/.kube/config 不存在，那么cp $&#123;CONFIG_PATH&#125; $HOME/.kube/config即可； 如果如果 $HOME/.kube/config已经存在，那么需要把新的配置内容加到 $HOME/.kube/config 下。单单只是cat $&#123;CONFIG_PATH&#125; &gt;&gt; $HOME/.kube/config是不行的，正确的做法是：KUBECONFIG=$HOME/.kube/config:$&#123;CONFIG_PATH&#125; kubectl config view --flatten &gt; $HOME/.kube/config 。解释下这个命令的意思：先把所有的配置文件添加到环境变量KUBECONFIG中，然后执行kubectl config view --flatten打印出有效的配置文件内容，最后覆盖$HOME/.kube/config 即可。 请注意，上述操作的优先级分别是 1&gt;2&gt;3，也就是说，kubectl 会优先检查--kubeconfig，若无则检查KUBECONFIG，若无则最后检查$HOME/.kube/config，如果还是没有，报错。但凡某一步找到了有效的 cluster，就中断检查，去连接 K8S 集群了。 第三步：配置正确的上下文 按照第二步的做法，如果配置文件只有一个 cluster 是没有任何问题的，但是对于有多个 cluster 怎么办呢？到这里，有几个关于配置的必须掌握的命令： kubectl config get-contexts。列出所有上下文信息。 kubectl config current-context。查看当前的上下文信息。其实，命令 1 线束出来的*所指示的就是当前的上下文信息。 kubectl config use-context $&#123;CONTEXT_NAME&#125;。更改上下文信息。 kubectl config set-context $&#123;CONTEXT_NAME&#125;|--current --$&#123;KEY&#125;=$&#123;VALUE&#125;。修改上下文的元素。比如可以修改用户账号、集群信息、连接到 K8S 后所在的 namespace。 关于该命令，还有几点要啰嗦的： config set-context可以修改任何在配置文件中的上下文信息，只需要在命令中指定上下文名称就可以。而–current 则指代当前上下文。 上下文信息所包括的内容有：cluster 集群（名称）、用户账号（名称）、连接到 K8S 后所在的 namespace，因此有config set-context严格意义上的用法： kubectl config set-context [NAME|--current] [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] [options] （备注：[options]可以通过 kubectl options 查看） 综上，如何操作 kubectl 配置都已交代。 IV. kubectl 部署服务K8S 核心功能就是部署运维容器化服务，因此最重要的就是如何又快又好地部署自己的服务了。本章会介绍如何部署 Pod 和 Deployment。 2.1 如何部署 Pod？通过 kubectl 部署 Pod 的办法分为两步：1). 准备 Pod 的 yaml 文件；2). 执行 kubectl 命令部署 第一步：准备 Pod 的 yaml 文件。关于 Pod 的 yaml 文件初步解释，本系列上一篇文章《K8S 系列一：概念入门》已经有了初步介绍，这里再复习下： apiVersion:?v1 kind:?Pod metadata: ??name:?memory-demo ??namespace:?mem-example spec: ??containers: ??-?name:?memory-demo-ctr ????image:?polinux/stress ????resources: ??????limits: ????????memory:?&quot;200Mi&quot; ??????requests: ????????memory:?&quot;100Mi&quot; ????command:?[&quot;stress&quot;] ????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;] ????volumeMounts: ????-?name:?redis-storage ??????mountPath:?/data/redis ??volumes: ??-?name:?redis-storage ????emptyDir:?&#123;&#125; 继续解读： metadata，对于新入门的同学来说，需要重点掌握的两个字段： name。这个 Pod 的名称，后面到 K8S 集群中查找 Pod 的关键字段。 namespace。命名空间，即该 Pod 隶属于哪个 namespace 下，关于 Pod 和 namespace 的关系，上一篇文章已经交代了。 spec记录了 Pod 内部所有的资源的详细信息，这里我们重点查看containers下的几个重要字段： name。Pod 下该容器名称，后面查找 Pod 下的容器的关键字段。 image。容器的镜像地址，K8S 会根据这个字段去拉取镜像。 resources。容器化服务涉及到的 CPU、内存、GPU 等资源要求。可以看到有limits和requests两个子项，那么这两者有什么区别吗，该怎么使用？在What’s the difference between Pod resources.limits and resources.requests in Kubernetes?回答了： limits是 K8S 为该容器至多分配的资源配额；而requests则是 K8S 为该容器至少分配的资源配额。打个比方，配置中要求了 memory 的requests为 100M，而此时如果 K8S 集群中所有的 Node 的可用内存都不足 100M，那么部署服务会失败；又如果有一个 Node 的内存有 16G 充裕，可以部署该 Pod，而在运行中，该容器服务发生了内存泄露，那么一旦超过 200M 就会因为 OOM 被 kill，尽管此时该机器上还有 15G+的内存。 command。容器的入口命令。对于这个笔者还存在很多困惑不解的地方，暂时挖个坑，有清楚的同学欢迎留言。 args。容器的入口参数。同上，有清楚的同学欢迎留言。 volumeMounts。容器要挂载的 Pod 数据卷等。请务必记住：Pod 的数据卷只有被容器挂载后才能使用！ 第二步：执行 kubectl 命令部署。有了 Pod 的 yaml 文件之后，就可以用 kubectl 部署了，命令非常简单：kubectl create -f $&#123;POD_YAML&#125;。 随后，会提示该命令是否执行成功，比如 yaml 内容不符合要求，则会提示哪一行有问题： 修正后，再次部署： 2.2 如何部署 Deployment？第一步：准备 Deployment 的 yaml 文件。首先来看 Deployment 的 yaml 文件内容： apiVersion:?extensions/v1beta1 ?kind:?Deployment ?metadata: ???name:?rss-site ???namespace:?mem-example ?spec: ???replicas:?2 ???template: ?????metadata: ???????labels: ?????????app:?web ?????spec: ??????containers: ???????-?name:?memory-demo-ctr ?????????image:?polinux/stress ?????????resources: ?????????limits: ???????????emory:?&quot;200Mi&quot; ?????????requests: ???????????memory:?&quot;100Mi&quot; ?????????command:?[&quot;stress&quot;] ?????????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;] ?????????volumeMounts: ?????????-?name:?redis-storage ???????????mountPath:?/data/redis ?????volumes: ?????-?name:?redis-storage ???????emptyDir:?&#123;&#125; 继续来看几个重要的字段： metadata同 Pod 的 yaml，这里提一点：如果没有指明 namespace，那么就是用 kubectl 默认的 namespace（如果 kubectl 配置文件中没有指明 namespace，那么就是 default 空间）。 spec，可以看到 Deployment 的spec字段是在 Pod 的spec内容外“包了一层”，那就来看 Deployment 有哪些需要注意的： metadata，新手同学先不管这边的信息。 spec，会发现这完完全全是上文提到的 Pod 的spec内容，在这里写明了 Deployment 下属管理的每个 Pod 的具体内容。 replicas。副本个数。也就是该 Deployment 需要起多少个相同的 Pod，如果用户成功在 K8S 中配置了 n（n&gt;1）个，那么 Deployment 会确保在集群中始终有 n 个服务在运行。 template。 第二步：执行 kubectl 命令部署。Deployment 的部署办法同 Pod：kubectl create -f $&#123;DEPLOYMENT_YAML&#125;。由此可见，K8S 会根据配置文件中的kind字段来判断具体要创建的是什么资源。 这里插一句题外话：部署完 deployment 之后，可以查看到自动创建了 ReplicaSet 和 Pod，如下图所示： 还有一个有趣的事情：通过 Deployment 部署的服务，其下属的 RS 和 Pod 命名是有规则的。读者朋友们自己总结发现哦。 综上，如何部署一个 Pod 或者 Deployment 就结束了。 V. kubectl 查看、更新/编辑、删除服务作为 K8S 使用者而言，更关心的问题应该是本章所要讨论的话题：如何通过 kubectl 查看、更新/编辑、删除在 K8S 上部署着的服务。 3.1 如何查看服务？请务必记得一个事情：在 K8S 中，一个独立的服务即对应一个 Pod。即，当我们说要 xxx 一个服务的就是，也就是操作一个 Pod。而与 Pod 服务相关的且需要用户关心的，有 Deployment。 通过 kubectl 查看服务的基本命令是： `$ kubectl?get|describe?${RESOURCE}?[-o?${FORMAT}]?-n=${NAMESPACE} ${RESOURCE}有:?pod、deployment、replicaset(rs)` 在此之前，还有一个需要回忆的事情是：Deployment、ReplicaSet 和 Pod 之间的关系 - 层层隶属；以及这些资源和 namespace 的关系是 - 隶属。如下图所示。 因此，要查看一个服务，也就是一个 Pod，必须首先指定 namespace！那么，如何查看集群中所有的 namespace 呢？kubectl get ns： 于是，只需要通过-n=$&#123;NAMESPACE&#125;就可以指定自己要操作的资源所在的 namespace。比如查看 Pod：kubectl get pod -n=oona-test，同理，查看 Deployment：kubectl get deployment -n=oona-test。 问题又来了：如果已经忘记自己所部属的服务所在的 namespace 怎么办？这么多 namespace，一个一个查看过来吗？ kubectl get pod --all-namespaces 这样子就可以看到所有 namespace 下面部署的 Pod 了！同理，要查找所有的命名空间下的 Deployment 的命令是：kubectl get deployment --all-namespaces。 于是，就可以开心地查看 Pod：kubectl get pod [-o wide] -n=oona-test，或者查看 Deployment：kubectl get deployment [-o wide] -n=oona-test。 哎，这里是否加-o wide有什么区别吗？实际操作下就明白了，其他资源亦然： 哎，我们看到之前部署的 Pod 服务 memory-demo 显示的“ImagePullBackOff”是怎么回事呢？先不着急，我们慢慢看下去。 3.2 如何更新/编辑服务？两种办法：1). 修改 yaml 文件后通过 kubectl 更新；2). 通过 kubectl 直接编辑 K8S 上的服务。 方法一：修改 yaml 文件后通过 kubectl 更新。我们看到，创建一个 Pod 或者 Deployment 的命令是kubectl create -f $&#123;YAML&#125;。但是，如果 K8S 集群当前的 namespace 下已经有该服务的话，会提示资源已经存在： 通过 kubectl 更新的命令是kubectl apply -f $&#123;YAML&#125;，我们再来试一试： （备注：命令kubectl apply -f $&#123;YAML&#125;也可以用于首次创建一个服务哦） 方法二：通过 kubectl 直接编辑 K8S 上的服务。命令为kubectl edit $&#123;RESOURCE&#125; $&#123;NAME&#125;，比如修改刚刚的 Pod 的命令为kubectl edit pod memory-demo，然后直接编辑自己要修改的内容即可。 但是请注意，无论方法一还是方法二，能修改的内容还是有限的，从笔者实战下来的结论是：只能修改/更新镜像的地址和个别几个字段。如果修改其他字段，会报错： The Pod “memory-demo” is invalid: spec: Forbidden: pod updates may not change fields other than spec.containers[*].image, spec.initContainers[*].image, spec.activeDeadlineSeconds or spec.tolerations (only additions to existing tolerations) 如果真的要修改其他字段怎么办呢？恐怕只能删除服务后重新部署了。 3.3 如何删除服务？在 K8S 上删除服务的操作非常简单，命令为kubectl delete $&#123;RESOURCE&#125; $&#123;NAME&#125;。比如删除一个 Pod 是：kubectl delete pod memory-demo，再比如删除一个 Deployment 的命令是：kubectl delete deployment $&#123;DEPLOYMENT_NAME&#125;。但是，请注意： 如果只部署了一个 Pod，那么直接删除该 Pod 即可； 如果是通过 Deployment 部署的服务，那么仅仅删除 Pod 是不行的，正确的删除方式应该是：先删除 Deployment，再删除 Pod。 关于第二点应该不难想象：仅仅删除了 Pod 但是 Deployment 还在的话，Deployment 定时会检查其下属的所有 Pod，如果发现失败了则会再拉起。因此，会发现过一会儿，新的 Pod 又被拉起来了。 另外，还有一个事情：有时候会发现一个 Pod 总也删除不了，这个时候很有可能要实施强制删除措施，命令为kubectl delete pod --force --grace-period=0 $&#123;POD_NAME&#125;。 VI. kubectl 排查服务问题上文说道：部署的服务 memory-demo 失败了，是怎么回事呢？本章就会带大家一起来看看常见的 K8S 中服务部署失败、服务起来了但是不正常运行都怎么排查呢？ 首先，祭出笔者最爱的一张 K8S 排查手册，来自博客《Kubernetes Deployment 故障排除图解指南》： 哈哈哈，对于新手同学来说，上图还是不够友好，下面我们简单来看两个例子： 4.1 K8S 上部署服务失败了怎么排查？请一定记住这个命令：kubectl describe $&#123;RESOURCE&#125; $&#123;NAME&#125;。比如刚刚的 Pod 服务 memory-demo，我们来看： 拉到最后看到Events部分，会显示出 K8S 在部署这个服务过程的关键日志。这里我们可以看到是拉取镜像失败了，好吧，大家可以换一个可用的镜像再试试。 一般来说，通过kubectl describe pod $&#123;POD_NAME&#125;已经能定位绝大部分部署失败的问题了，当然，具体问题还是得具体分析。大家如果遇到具体的报错，欢迎分享交流。 4.2 K8S 上部署的服务不正常怎么排查？如果服务部署成功了，且状态为running，那么就需要进入 Pod 内部的容器去查看自己的服务日志了： 查看 Pod 内部某个 container 打印的日志：kubectl log $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125;。 进入 Pod 内部某个 container：kubectl exec -it [options] $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125; [args]，嗯，这个命令的作用是通过 kubectl 执行了docker exec xxx进入到容器实例内部。之后，就是用户检查自己服务的日志来定位问题。 显然，线上可能会遇到更复杂的问题，需要借助更多更强大的命令和工具。 写在后面本文希望能够帮助对 K8S 不了解的新手快速了解 K8S。笔者一边写文章，一边查阅和整理 K8S 资料，过程中越发感觉 K8S 架构的完备、设计的精妙，是值得深入研究的，K8S 大受欢迎是有道理的。 来自 “ ITPUB博客 ” ，链接：http://blog.itpub.net/31559354/viewspace-2746071/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/tags/Kubernetes/"}]},{"title":"Flannel-Calico怎么选择","slug":"Flannel-Calico","date":"2021-03-23T16:00:00.000Z","updated":"2021-03-24T09:01:18.991Z","comments":true,"path":"2021/03/24/Flannel-Calico/","link":"","permalink":"http://zhang-yu.me/2021/03/24/Flannel-Calico/","excerpt":"","text":"网络插件Flannel-Calico怎么选择 Kubernetes中常见的网络插件有哪些？1.flannel：能提供ip地址，但不支持网络策略 2.calico：既提供ip地址，又支持网络策略 3.canal：flannel和calico结合，通过flannel提供ip地址，calico提供网络策略 什么叫做网络策略？网络策略：可以达到多租户网络隔离，可以控制入网和出网流量，入网和出网ip访问的一种策略 各种CNI网络方案的差异对比参考https://helpcdn.aliyun.com/document_detail/97621.html flannel和calico网络性能分析官方指标如下 flannel host-gw = flannel vxlan-directrouting = calico bgp&gt; calico ipip &gt; flannel vxlan-vxlan&gt;flannel-udp 官方推荐使用的网络方案：所有节点在同一个网段推荐使用calico的bgp模式和flannel的host-gw模式 结论：1.如果需要多集群的跨网络分段的网络，选择Calico 2.如果需要管理网络策略，做网络隔离等，选择Calico 3.大部分公司生产环境业务不复杂的，开发测试环境就几台机器的，不存在多数据中心的。 选择用Flannel 就行了。 部署在公有云上，封装 backend 选择vxlan-directrouting。 部署在私有云上，封装 backend 选择host-gw。 参考Kubernetes集群网络规划 https://helpcdn.aliyun.com/document_detail/86500.html 使用网络策略（Network Policy） https://helpcdn.aliyun.com/document_detail/97621.html","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/categories/Kubernetes/"}],"tags":[{"name":"flannel","slug":"flannel","permalink":"http://zhang-yu.me/tags/flannel/"}]},{"title":"nginx核心知识100讲知识图谱","slug":"nginx-knowledge-graph","date":"2021-03-14T16:00:00.000Z","updated":"2021-03-15T15:07:57.974Z","comments":true,"path":"2021/03/15/nginx-knowledge-graph/","link":"","permalink":"http://zhang-yu.me/2021/03/15/nginx-knowledge-graph/","excerpt":"","text":"nginx核心知识100讲知识图谱","categories":[{"name":"nginx","slug":"nginx","permalink":"http://zhang-yu.me/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://zhang-yu.me/tags/nginx/"}]},{"title":"性能之癫-优化你的程序","slug":"performance-optimize-your-program","date":"2021-03-14T16:00:00.000Z","updated":"2021-03-15T15:33:48.965Z","comments":true,"path":"2021/03/15/performance-optimize-your-program/","link":"","permalink":"http://zhang-yu.me/2021/03/15/performance-optimize-your-program/","excerpt":"","text":"原创 码砖杂役性能之巅-优化你的程序​ outline：关注&amp;指标&amp;度量，基础理论知识，工具&amp;方法，最佳实践，参考资料性能优化关注：CPU、内存、磁盘IO、网络IO等四个方面。性能指标：吞吐率、响应时间、QPS/IOPS、TP99、资源使用率是我们经常关注的指标。时间度量：从cpu cycle到网络IO，自上到下，时间量级越大。监控、分析、优化，三部曲，以终为始，循环往复。优化性能，需要一些系统编程知识。提升处理能力、减少计算量是优化的2个根本方向。优化大师格雷格画的图，吊炸天，你应该很熟悉，gregg亲手实现了一些工具。借助工具定位性能瓶颈。gprof2dot.py可以处理多种采样输出数据 建议使用perf等非侵入式的profiling工具。perf不仅仅可以定位cpu瓶颈，还可以查看很多方面，比如缺页，分支预测失败，上下文切换等。IO瓶颈，你应该知道的知识。有关锁的知识，你应该知道的。多线程的学问很大内存管理的方方面面最佳实践，没有足够理由，你不应该违背。你应该懂得的。 关于排序，你应该知道的。这些资料不错，你值得拥有。 如果对你有帮助，请帮忙转发，让更多朋友收益。 一般性原则依据数据而不是凭空猜测忌过早优化忌过度优化深入理解业务性能优化是持久战选择合适的衡量指标、测试用例、测试环境性能优化的层次需求阶段设计阶段实现阶段一般性方法缓存并发惰性批量，合并更高效的实现缩小解空间性能优化与代码质量总结依据数据而不是凭空猜测 这是性能优化的第一原则，当我们怀疑性能有问题的时候，应该通过测试、日志、profillig来分析出哪里有问题，有的放矢，而不是凭感觉、撞运气。一个系统有了性能问题，瓶颈有可能是CPU，有可能是内存，有可能是IO（磁盘IO，网络IO），大方向的定位可以使用top以及stat系列来定位（vmstat，iostat，netstat…），针对单个进程，可以使用pidstat来分析。 在本文中，主要讨论的是CPU相关的性能问题。按照80/20定律，绝大多数的时间都耗费在少量的代码片段里面，找出这些代码唯一可靠的办法就是profile，我所知的编程语言，都有相关的profile工具，熟练使用这些profile工具是性能优化的第一步。 忌过早优化 The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.我并不十分清楚Donald Knuth说出这句名言的上下文环境，但我自己是十分认同这个观念的。在我的工作环境（以及典型的互联网应用开发）与编程模式下，追求的是快速的迭代与试错，过早的优化往往是无用功。而且，过早的优化很容易拍脑袋，优化的点往往不是真正的性能瓶颈。 忌过度优化 As performance is part of the specification of a program – a program that is unusably slow is not fit for purpose性能优化的目标是追求合适的性价比。 在不同的阶段，我们对系统的性能会有一定的要求，比如吞吐量要达到多少多少。如果达不到这个指标，就需要去优化。如果能满足预期，那么就无需花费时间精力去优化，比如只有几十个人使用的内部系统，就不用按照十万在线的目标去优化。 而且，后面也会提到，一些优化方法是“有损”的，可能会对代码的可读性、可维护性有副作用。这个时候，就更不能过度优化。 深入理解业务 代码是服务于业务的，也许是服务于最终用户，也许是服务于其他程序员。不了解业务，很难理解系统的流程，很难找出系统设计的不足之处。后面还会提及对业务理解的重要性。 性能优化是持久战 当核心业务方向明确之后，就应该开始关注性能问题，当项目上线之后，更应该持续的进行性能检测与优化。 现在的互联网产品，不再是一锤子买卖，在上线之后还需要持续的开发，用户的涌入也会带来性能问题。因此需要自动化的检测性能问题，保持稳定的测试环境，持续的发现并解决性能问题，而不是被动地等到用户的投诉。 选择合适的衡量指标、测试用例、测试环境 正因为性能优化是一个长期的行为，所以需要固定衡量指标、测试用例、测试环境，这样才能客观反映性能的实际情况，也能展现出优化的效果。 衡量性能有很多指标，比如系统响应时间、系统吞吐量、系统并发量。不同的系统核心指标是不一样的，首先要明确本系统的核心性能诉求，固定测试用例；其次也要兼顾其他指标，不能顾此失彼。 测试环境也很重要，有一次突然发现我们的QPS高了许多，但是程序压根儿没优化，查了半天，才发现是换了一个更牛逼的物理机做测试服务器。","categories":[{"name":"优化","slug":"优化","permalink":"http://zhang-yu.me/categories/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"性能","slug":"性能","permalink":"http://zhang-yu.me/tags/%E6%80%A7%E8%83%BD/"}]}],"categories":[{"name":"消息系统","slug":"消息系统","permalink":"http://zhang-yu.me/categories/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/categories/Kubernetes/"},{"name":"职业发展","slug":"职业发展","permalink":"http://zhang-yu.me/categories/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"},{"name":"containerd","slug":"containerd","permalink":"http://zhang-yu.me/categories/containerd/"},{"name":"Ingress","slug":"Ingress","permalink":"http://zhang-yu.me/categories/Ingress/"},{"name":"nginx","slug":"nginx","permalink":"http://zhang-yu.me/categories/nginx/"},{"name":"优化","slug":"优化","permalink":"http://zhang-yu.me/categories/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"Pulsar","slug":"Pulsar","permalink":"http://zhang-yu.me/tags/Pulsar/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhang-yu.me/tags/Kubernetes/"},{"name":"职业发展","slug":"职业发展","permalink":"http://zhang-yu.me/tags/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"},{"name":"containerd","slug":"containerd","permalink":"http://zhang-yu.me/tags/containerd/"},{"name":"Ingress","slug":"Ingress","permalink":"http://zhang-yu.me/tags/Ingress/"},{"name":"flannel","slug":"flannel","permalink":"http://zhang-yu.me/tags/flannel/"},{"name":"nginx","slug":"nginx","permalink":"http://zhang-yu.me/tags/nginx/"},{"name":"性能","slug":"性能","permalink":"http://zhang-yu.me/tags/%E6%80%A7%E8%83%BD/"}]}