{"meta":{"title":"张天师","subtitle":"","description":"专业，团队","author":"张天师","url":"http://zhangyu.info","root":"/"},"pages":[{"title":"about","date":"2021-03-15T13:48:47.000Z","updated":"2021-03-15T15:05:42.638Z","comments":true,"path":"about/index.html","permalink":"http://zhangyu.info/about/index.html","excerpt":"","text":"本博客hexo主题基于hexo-theme-3-hexo 修改而来 https://github.com/yelog/hexo-theme-3-hexo"}],"posts":[{"title":"云原生微服务最佳实践","slug":"云原生微服务最佳实践","date":"2022-05-27T16:00:00.000Z","updated":"2022-05-28T06:20:51.855Z","comments":true,"path":"2022/05/28/云原生微服务最佳实践/","link":"","permalink":"http://zhangyu.info/2022/05/28/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"https://developer.aliyun.com/article/891714?spm=5176.21213303.J_6704733920.7.22db53c9yXA1Zr&amp;scm=20140722.S_community%40%40%E6%96%87%E7%AB%A0%40%40891714._.ID_community%40%40%E6%96%87%E7%AB%A0%40%40891714-RL_%E4%BA%91%E5%8E%9F%E7%94%9F%E6%9E%B6%E6%9E%84%E4%B8%8B%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%80%89%E5%9E%8B%E5%92%8C%E6%BC%94%E8%BF%9B-LOC_main-OR_ser-V_2-P0_0 作者：彦林本文整理自阿里云智能高级技术专家彦林的线上直播分享《云原生微服务最佳实践》。视频回放地址：https://yqh.aliyun.com/live/detail/28454 云原生架构下的微服务选型和演进-阿里云开发者社区 随着云原生的演进，微服务作为主流应用架构被广泛使用，其落地的难题逐步从如何建好延伸到如何用好。今天跟各位小伙伴分享一下我在微服务领域 10 余年的实践经验，如何以更高效的姿势把微服务这件事做扎实。 阿里微服务发展历程微服务 1.0 （1w 实例/微服务拆分/同城容灾）2008 年随着阿里业务规模不断增大，单体胖应用+硬负载的架构逐渐暴露性能瓶颈；随着研发人员逐步增多，协调效率也逐步下降，不能满足日益复杂的业务挑战，因此急需技术升级解决这些问题。 在当时 SOA 架构非常流行，也就成为我们技术演进的主要方向，当时有两种解决方案，一个是 Server Based 的解决方案，这种模式侵入小、方便集中管控，但是这种中心化方案会带来成本高、稳定性风险高、扩展性差；一个是 Client Based 的解决方案，这种模式去中心化，扩展性强，成本低，但是会带来一定侵入性，比较难以管理；当然很多人会问为什么不直接用 DNS 呢？主要是 DNS 不能满足 IDC 内部服务发现实时性，服务列表更新不能及时通知下有业务会导致业务流量损失。 在评估两种方案利弊之后，我们在网关这种需要集中管理安全和简单路由场景采用了 Server Based 的方案，基于 Nginx 演进出了阿里 Tengine 网关技术体系，从入口处解决安全、高可用、简单路由能力；在 IDC 内部采用了 Client Base 模式，孵化出 HSF/Dubbo+Nacos 技术体系，支撑了业务微服务拆分。 随着第一代微服务架构落地，由于引入注册中心带来了稳定性风险，注册中心挂会导致调用链路全部中断；业务集中发布的时候注册中心压力会比较大。 针对可用性问题我们提供了推空保护能力，即使注册中心挂也不会影响业务正常运行；为了提供更好性能我们提供了全异步架构；为了支持同城容灾我们提供了 AP 一致性协议，具体协议可以参考《Nacos 架构与原理》电子书。 随着阿里微服务 1.0 架构落地，帮助业务完成微服务拆分，解决了扩展性和协同效率问题，同时支撑了阿里同城容灾能力。对于正在做微服务的小伙伴可能问阿里如何做微服务架构演进的： 前后端分离是第一步，因为前端变化多，变化快，后端相对变化小，演进慢，因此需要解耦发展，让前端更快的适应市场变化，以便在竞争中保持先机； 后端无状态改造是第二步，把内存状态外置到 Redis，把持久化状态外置到 Mysql，这样业务就可以随意进行切分； 第三步是模块化拆分，这块是最考验架构师的，因为拆分一个是按照业务属性拆分，一个是按照应用复杂度进行拆分，这个是一个相对动态过程，建议拆分模块后 2-3 人负责一个模块，拆到太细会有比较高的运维成本，拆的太粗又会带来研发协同问题，阿里内部也经历过合久必分，分久必合的几波震荡，最终走到相对稳态。这里值得一提就是 HSF/Dubbo 的一个优势，因为早期采用 SOA 架构思想设计，一个接口就是一个服务，这样其实非常方便服务的拆分和合并，当然同时带来一个问题是对注册中心性能压力比较大，这是一个架构选择和平衡问题。 微服务 2.0（10w 实例/业务中台/异地多活）微服务 1.0 架构帮助阿里极大缓解性能和效率问题，但是由于阿里双十一的成功，技术上面临一个洪峰的技术挑战，我们必须在用户体验、资源成本、高可用之间做一个平衡。这个阶段我们最大的挑战是扩展性和稳定性，扩展性是要支撑业务 10w+实例扩容，但是单地资源有限，双十一商家投入的资金越来越大，导致我们双十一当天也不能出严重问题，不然损失非常大，因此对业务稳定性提出非常高的要求。 因此阿里演进到微服务 2.0 支撑了异地多活的高可用体系，让阿里业务可以按照 IDC 级别水平扩展，新的机房，新的技术体系都可以在单元中进行验证，也加速了阿里技术体系演进速度。 在此期间 Nacos Server 间水平通知压力巨大，业务发布窗口容易把网卡打满，频繁推送会消耗业务大量内存和 CPU，进而影响业务的稳定性。 针对上述问题，我们在 Nacos Server 间做了聚合推送，将一定时间窗的变更合并聚合推送，推送过程中做了压缩推送，从而解决了上述问题。 在微服务解决扩展性和高可用的同时，业务系统变多，重复建设，业务孤岛也越来越多，协同效率也越来越低，因此阿里业务在这个时候推出了业务中台能力，将扁平的微服务抽象分层，将基础服务抽象为中台服务解决上述问题，业务分层后支撑了阿里业务高速增长，也加速了技术架构统一。 微服务 3.0（100w 实例/业务域拆分/云原生）微服务 2.0 架构支撑了阿里双十一的技术奇迹，阿里也陆续开启业务扩张，构建更完整的互联网版图。在这个阶段阿里收购了比较多的公司，技术体系不统一如何形成合力；从线上走到线下后，线下系统对系统稳定性要求更高；云计算发展，如何利用好云的弹性做双十一，这个阶段我们也推出了微服务的云产品，期望通过云产品支撑阿里双十一。 业务域切分比较容易，切完之后如何更好的互联互通是一个关键，因此我们内部推出了 Nacos-sync 和云原生网关两个产品。Nacos-sync 适合业务流量超大，协议一致场景。云原生网关适合网络不通，协议不同，跨 Region 等场景。 即使从顶层做了业务域拆分，但是最大的电商集群往百万实例演进过程中对注册中心的压力越来越大，我们把聚合窗口时间不断拉长，推送慢了会导致业务发布时间变长，推送快了会对业务消耗较大，因此陷入了两难境地。 这个阶段我们进行问题的分解，首先根据服务列表大小做了一个切分，服务列表多的可以推送慢一些问题也不大，服务列表小的需要及时推送，因此我们优化了聚合推送逻辑，根据服务列表大小做了分级推送。还有一个优化思路是变更只有几个列表变化，因此我们提供了增量推送能力，大幅降低服务变更推送数据量。 通过微服务 3.0 架构演进很好的解决了跨域互通和平滑上云的问题，新业务可以先上云，或者部分业务上云，通过网关做云上云下互通等问题，同时支撑了百万实例微服务架构演进。 期望通过我分享阿里微服务发展历程给大家做微服务架构演进提供一些思路和启发。 云原生微服务趋势随着云原生技术演进，容器以不可变基础设施为理念，解决运维标准和资源利用率问题；微服务以可变运行时为理念，解决研发效率问题，提升系统整体扩展性和高可用。经常有人问我，为什么有了容器的服务发现机制，还需要微服务的注册中心呢？从架构上首先是分层的，小的时候确实也看不到明显区别，大一些就会发现问题，如阿里中心最大微服务集群，底层是多个 Kubernetes 集群，防止一个 Kubernetes 出问题影响全局，底层 Kubernetes 也可以水平扩展，如果依赖了 Kubernetes 的服务发现机制，跨 Kubernetes 服务发现就成了第一个问题。当然底层是一个 Kubernetes 上面也可以是多个微服务环境，微服务可以按照业务域切分。两层可以做解耦，自由环境组合。还有就是阿里微服务体系积累了推空保护、服务治理完整体系，而 Kubernetes 的 CoreDNS 将服务发现强制拉到业务调用链路，每次调用都会做域名解析，因此 CoreDNS 挂的时候业务全部中断。 对于阿里整体正在从百万实例往千万实例的规模演进，这部分也是阿里微服务 4.0 的内容，这部分给大部分公司的借鉴意义有限，因此不做展开。 微服务最佳实践阿里微服务体系经过 10 余年的发展，目前已经通过开源被广泛使用，通过阿里云支撑了成千上万家企业做数字化升级。借此机会把我们的最佳实践总结分享给大家，期望都对大家用好微服务有所帮助。 阿里微服务体系简介通过 MSE + ACK 能够完成第一步云原生技术升级，释放云弹性红利，释放研发效率红利，可以通过可观测和高可用进一步用好微服务体系。 微服务最佳实践通过注册&amp;配置中心完成微服务拆分；通过网关统一入口，从入口处解决安全和高可用问题；最后通过服务治理提升用户微服务的问题。 网关最佳实践云原生网关作为下一代网关，提供高集成、高可用、高性能、安全的一站式网关解决方案。 统一接入：将流量网关、 微服务网关、 WAF 三合一大幅降低资源和运维成本，需要强调的是云原生网关集成 WAF 的方案有非常好的性能优势，WAF 做为控制面下发防护规则到云原生网关，流量直接在云原生网关清洗完毕直接路由到后端机器，RT 短，运维成本低。 统一入口安全防线：自动更新证书防过期，支持 JWT/OAuth2/OIDC/IDaaS 认证机制，支持黑白名单机制。 统一东西南北流量：统一解决跨域互通问题，包括跨网络域，跨业务域，跨地域，跨安全域等。 统一服务发现机制：支持 Nacos/Kubernetes/DNS/ 固定 IP 多种服务发现方式。 统一观测平台：从入口做好 tracing 埋点全链路诊断，丰富业务大盘和告警模板大幅降低网关运维成本。 统一服务治理：从入口做限流、降级、熔断等高可用能力，提供全链路灰度方案控制变更风险。统一性能优化：采用硬件加速性能提升 80%，Ingress 场景比 Nginx 性能高 90%，参数调优+模块优化提升 40%。 云原生网关支持 WASM 扩展网关自定义功能，并且通过插件市场提供丰富的插件能力。 服务治理最佳实践提供零业务侵入，开发，测试，运维全覆盖服务治理能力，提升系统高可用。如发布阶段即使注册中心是毫秒级推送也会有延迟，这个期间就会导致流量损失，因此我们提供了无损上下线能力解决这个痛点。本月我们将服务治理能力通过 OpenSergo 开源，欢迎各位小伙伴参与共建！ 日常环境隔离最佳实践共享一套环境联调开发相互影响，所有环境都独立联调机器成本太高，这个是一个矛盾，我们通过全链路打标能力将流量隔离，让大家可以在一套环境隔离多个逻辑联调环境，巧妙的解决这个问题。 配置管理最佳实践随着应用规模变大，到每个机器去修改配置运维成本太高，因此需要配置中心统一维护应用配置，将静态业务动态化，动态修改业务运行时行为，提升应用运行时灵活性。 服务网格最佳实践对于多语言开发有诉求和对服务网关感兴趣的小伙伴可以通过 MSE+ASM 快速构建服务网格解决方案，完成服务互通，快速体验新的技术。 微服务高可用最佳实践随着业务复杂度变高，业务峰值不可测，面对失败的设计和微服务高可用工具使用就非常重要，可以通过 Sentinel 完成限流、降级、熔断的保护，可以通过 PTS 完成压测，可以通过混沌工程完成破坏性测试，从体整体提升系统高可用。 注册中心平滑迁移实践目前大规模场景推荐双注册，如 1w 实例以上，这样发布周期长，稳定性更高一些。如果不到 1w 实例可以通过 Nacos-sync 同步完成注册中心平滑前一，这样通用型强一些。 网关平衡迁移实践由于前面云原生网关三合一和性能优势，大家可以通过入口 DNS 灰度切换到云原生网关。 微服务标杆客户用户上云中有两类典型客户，一类是传统的单体胖应用客户，一类是已经采用了微服务需要用好微服务的用户，我们通过两个标杆客户分享一下。 斯凯奇微服务＋业务中台实践斯凯奇 2021 年找到我们做数字化升级时间非常紧急，需要双十一前 3 个月左右要完成数字化升级，采用 MSE 微服务+中台解决方案，斯凯奇借助云原生网关完成了东西南北流量的统一控制，借助南北向云原生网关完成安全认证和入口限流，从入口做好流量防护；借助东西向网关完成了多个业务域的互通，新老系统的互通，1 个月左右完成了整个系统的搭建，1 个月左右完成了整个系统压测和高可用验证，并且最终大促业务非常成功，助力斯凯奇双十一 12 亿营收规模。 来电微服务全链路灰度最佳实践来电的技术挑战来电科技的业务场景丰富且系统众多，在技术架构上已完成容器化以及微服务化改造，微服务框架使用的是 Spring Cloud 与 Dubbo。随着近年来的高速发展，充电宝设备节点以及业务量都在快速增加，系统的稳定性面临几点挑战: 1.在系统服务的发布过程中如何避免业务流量的损失；2.系统缺少简单有效的灰度能力，每次系统发布都存在一定的稳定性风险。MSE 微服务治理提供了开箱即用且无侵入的线上发布稳定性解决方案以及全链路灰度解决方案，帮助来电科技消除发布风险、提升线上稳定性。 来电全链路灰度最佳实践1.来电科技选用 MSE 微服务治理专业版来实现无侵入微服务治理能力，无缝支持市面上近 5 年所有的 Spring Cloud 和 Dubbo 的版本，不用改一行代码，不需要改变业务的现有架构就可以使用，没有绑定。 2.MSE 微服务治理专业版提供了全链路灰度解决方案帮助来电科技快速落地可灰度、可观测、可回滚的安全生产三板斧能力，满足业务高速发展情况下快速迭代和小心验证的诉求； 3.MSE 微服务治理的无损上下线能力，对系统服务的全流程进行防护，通过服务预热、无损下线、与 Kubernetes 微服务生命周期对齐、延迟发布等一系列能力，保证在服务冷启动或销毁过程中，业务连续无损。 4.MSE 微服务治理的离群实例摘除能力，可以做到让服务消费者自动检测其所调用提供者实例的可用性并进行实时的权重动态调整，以保证服务调用的成功率，从而提升业务稳定性和服务质量。 阿里云微服务生态与规划阿里开源微服务会贴着服务治理帮助开发者用户微服务，云产品做好产品集成提升大家的使用体验。 ACK+MSE = 云原生架构升级解决方案 ASM+MSE = 服务网格解决方案 AHAS + MSE = 微服务高可用解决方案 ARMS + MSE = 微服务可观测解决方案 EDAS + MSE = APaaS解决方案 SAE + MSE = 微服务 Serverless 解决方案 WAF + 云盾 + IDaaS + MSE = 微服务安全解决方案","categories":[{"name":"微服务","slug":"微服务","permalink":"http://zhangyu.info/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://zhangyu.info/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"微服务之间的最佳调用方式","slug":"微服务之间的最佳调用方式","date":"2022-05-27T16:00:00.000Z","updated":"2022-05-28T06:23:50.040Z","comments":true,"path":"2022/05/28/微服务之间的最佳调用方式/","link":"","permalink":"http://zhangyu.info/2022/05/28/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E4%BD%B3%E8%B0%83%E7%94%A8%E6%96%B9%E5%BC%8F/","excerpt":"","text":"https://blog.csdn.net/weixin_38748858/article/details/101062272 微服务之间的最佳调用方式_倚天码农的博客-CSDN博客_微服务调用 在微服务架构中，需要调用很多服务才能完成一项功能。服务之间如何互相调用就变成微服务架构中的一个关键问题。服务调用有两种方式，一种是RPC方式，另一种是事件驱动（Event-driven）方式，也就是发消息方式。消息方式是松耦合方式，比紧耦合的RPC方式要优越，但RPC方式如果用在适合的场景也有它的一席之地. 耦合的种类：我们总在谈耦合，那么耦合到底意味着什么呢？ 时间耦合：客户端和服务端必须同时上线才能工作。发消息时，接受消息队列必须运行，但后台处理程序暂时不工作也不影响。 容量耦合：客户端和服务端的处理容量必须匹配。发消息时，如果后台处理能力不足也不要紧，消息队列会起到缓冲的作用。 接口耦合：RPC调用有函数标签，而消息队列只是一个消息。例如买了商品之后要调用发货服务，如果是发消息，那么就只需发送一个商品被买消息。 发送方式耦合：RPC是点对点方式，需要知道对方是谁，它的好处是能够传回返回值。消息既可以点对点，也可以用广播的方式，这样减少了耦合，但也使返回值比较困难。 下面我们来逐一分析这些耦合的影响。 第一，时间耦合，对于多数应用来讲，你希望能马上得到回答，因此即使使用消息队列，后台也需要一直工作。第二，容量耦合，如果你对回复有时间要求，那么消息队列的缓冲功能作用不大，因为你希望及时响应。真正需要的是自动伸缩（Auto-scaling），它能自动调整服务端处理能力去匹配请求数量。第三和第四，接口耦合和发送方式耦合，这两个确实是RPC方式的软肋。 事件驱动（Event-Driven）方式：Martin Fowler把事件驱动分成四种方式(What do you mean by “Event-Driven”)，简化之后本质上只有两种方式。 一种就是我们熟悉的的事件通知（Event Notification），另一种是事件溯源（Event Sourcing）。事件通知就是微服务之间不直接调用，而是通过发消息来进行合作。事件溯源有点像记账，它把所有的事件都记录下来，作为永久存储层，再在它的基础之上构建应用程序。实际上从应用的角度来讲，它们并不应该分属一类，它们的用途完全不同。事件通知是微服务的调用（或集成）方式，应该和RPC分在一起。事件溯源是一种存储数据的方式，应该和数据库分在一起。 事件通知（Event Notification）方式：让我们用具体的例子来看一下。在下面的例子中，有三个微服务，“Order Service”， “Customer Service” 和“Product Service”. 图片来源 先说读数据，假设要创建一个“Order”，在这个过程中需要读取“Customer”的数据和“Product”数据。如果用事件通知的方式就只能在“Order Service”本地也创建只读“Customer”和“Product”表，并把数据用消息的方式同步过来。 再说写数据，如果在创建一个“Order”时需要创建一个新的“Customer”或要修改“Customer”的信息，那么可以在界面上跳转到用户创建页面，然后在“Customer Service”创建用户之后再发”用户已创建“的消息，“Order Service”接到消息，更新本地“Customer”表。 这并不是一个很好的使用事件驱动的例子，因为事件驱动的优点就是不同的程序之间可以独立运行，没有绑定关系。但现在“Order Service”需要等待“Customer Service”创建完了之后才能继续运行，来完成整个创建“Order”的工作。主要是因为“Order”和“Customer”本身从逻辑上来讲就是紧耦合关系，没有“Customer”你是不能创建“Order”的。 在这种紧耦合的情况下，也可以使用RPC。你可以建立一个更高层级的管理程序来管理这些微服务之间的调用，这样“Order Service”就不必直接调用“Customer Service”了。当然它从本质上来讲并没有解除耦合，只是把耦合转移到了上一层，但至少现在“order Service”和“Customer Service”可以互不影响了。之所以不能根除这种紧耦合关系是因为它们在业务上是紧耦合的。 再举一个购物的例子。用户选好商品之后进行“Checkout”，生成“Order”，然后需要“payment”，再从“Inventory”取货，最后由“Shipment”发货，它们每一个都是微服务。这个例子用RPC方式和事件通知方式都可以完成。当用RPC方式时，由“Order”服务调用其他几个服务来完成整个功能。用事件通知方式时，“Checkout”服务完成之后发送“Order Placed”消息，“Payment”服务收到消息，接收用户付款，发送“Payment received”消息。“Inventory”服务收到消息，从仓库里取货，并发送“Goods fetched”消息。“Shipment”服务得到消息，发送货物，并发送“Goods shipped”消息。 图片来源 对这个例子来讲，使用事件驱动是一个不错的选择，因为每个服务发消息之后它不需要任何反馈，这个消息由下一个模块接收来完成下一步动作，时间上的要求也比上一个要宽松。用事件驱动的好处是降低了耦合度，坏处是你现在不能在程序里找到整个购物过程的步骤。如果一个业务逻辑有它自己相对固定的流程和步骤，那么使用RPC或业务流程管理（BPM）能够更方便地管理这些流程。在这种情况下选哪种方案呢？在我看来好处和坏处是大致相当的。从技术上来讲要选事件驱动，从业务上来讲要选RPC。不过现在越来越多的人采用事件通知作为微服务的集成方式，它似乎已经成了微服务之间的标椎调用方式。 事件溯源(Event Sourcing)：这是一种具有颠覆性质的的设计，它把系统中所有的数据都以事件（Event）的方式记录下来，它的持久存储叫Event Store， 一般是建立在数据库或消息队列（例如Kafka）基础之上，并提供了对事件进行操作的接口，例如事件的读写和查询。事件溯源是由领域驱动设计(Domain-Driven Design)提出来的。DDD中有一个很重要的概念，有界上下文（Bounded Context），可以用有界上下文来划分微服务，每个有界上下文都可以是一个微服务。 下面是有界上下文的示例。下图中有两个服务“Sales”和“Support”。有界上下文的一个关键是如何处理共享成员， 在图中是“Customer”和“Product”。在不同的有界上下文中，共享成员的含义、用法以及他们的对象属性都会有些不同，DDD建议这些共享成员在各自的有界上下文中都分别建自己的类（包括数据库表），而不是共享。可以通过数据同步的手段来保持数据的一致性。下面还会详细讲解。 图片来源 事件溯源是微服务的一种存储方式，它是微服务的内部实现细节。因此你可以决定哪些微服务采用事件溯源方式，哪些不采用，而不必所有的服务都变成事件溯源的。 通常整个应用程序只有一个Event Store， 不同的微服务都通过向Event Store发送和接受消息而互相通信。Event Store内部可以分成不同的stream（相当于消息队列中的Topic）， 供不同的微服务中的领域实体（Domain Entity）使用。 事件溯源的一个短板是数据查询，它有两种方式来解决。第一种是直接对stream进行查询，这只适合stream比较小并且查询比较简单的情况。查询复杂的话，就要采用第二种方式，那就是建立一个只读数据库，把需要的数据放在库中进行查询。数据库中的数据通过监听Event Store中相关的事件来更新。 数据库存储方式只能保存当前状态，而事件溯源则存储了所有的历史状态，因而能根据需要回放到历史上任何一点的状态，具有很大优势。但它也不是一点问题都没有。第一，它的程序比较复杂，因为事件是一等公民，你必须把业务逻辑按照事件的方式整理出来，然后用事件来驱动程序。第二，如果你要想修改事件或事件的格式就比较麻烦，因为旧的事件已经存储在Event Store里了（事件就像日志，是只读的），没有办法再改。 由于事件溯源和事件通知表面上看起来很像，不少人都搞不清楚它们的区别。事件通知只是微服务的集成方式，程序内部是不使用事件溯源的，内部实现仍然是传统的数据库方式。只有当要与其他微服务集成时才会发消息。而在事件溯源中，事件是一等公民，可以不要数据库，全部数据都是按照事件的方式存储的。 虽然事件溯源的践行者有不同的意见，但有不少人都认为事件溯源不是微服务的集成方式，而是微服务的一种内部实现方式。因此，在一个系统中，可以某些微服务用事件溯源，另外一些微服务用数据库。当你要集成这些微服务时，你可以用事件通知的方式。注意现在有两种不同的事件需要区分开，一种是微服务的内部事件，是颗粒度比较细的，这种事件只发送到这个微服务的stream中，只被事件溯源使用。另一种是其他微服务也关心的，是颗粒度比较粗的，这种事件会放到另外一个或几个stream中，被多个微服务使用，是用来做服务之间集成的。这样做的好处是限制了事件的作用范围，减少了不相关事件对程序的干扰。详见”Domain Events vs. Event Sourcing“. 事件溯源出现已经很长时间了，虽然热度一直在上升（尤其是这两年），但总的来说非常缓慢，谈论的人不少，但生产环境使用的不多。究其原因就是应为它对现在的体系结构颠覆太大，需要更改数据存储结构和程序的工作方式，还是有一定风险的。另外，微服务已经形成了一整套体系，从程序部署，服务发现与注册，到监控，服务韧性（Service Resilience），它们基本上都是针对RPC的，虽然也支持消息，但成熟度就差多了，因此有不少工作还是要自己来做。有意思的是Kafka一直在推动它作为事件驱动的工具，也取得了很大的成功。但它却没有得到事件溯源圈内的认可（详见这里）。多数事件溯源都使用一个叫evenstore的开源Event Store，或是基于某个数据库的Event Store，只有比较少的人用Kafka做Event Store。 但如果用Kafka实现事件通知就一点问题都没有。总的来说，对大多数公司来讲事件溯源是有一定挑战的，应用时需要找到合适的场景。如果你要尝试的话，可以先拿一个微服务试水。 虽然现在事件驱动还有些生涩，但从长远来讲，还是很看好它的。像其他全新的技术一样，事件溯源需要大规模的适用场景来推动。例如容器技术就是因为微服务的流行和推动，才走向主流。事件溯源以前的适用场景只限于记账和源代码库，局限性较大。区块链可能会成为它的下一个机遇，因为它用的也是事件溯源技术。另外AI今后会渗入到具体程序中，使程序具有学习功能。而RPC模式注定没有自适应功能。事件驱动本身就具有对事件进行反应的能力，这是自我学习的基础。因此，这项技术长远来讲定会大放异彩，但短期内（3-5年）大概不会成为主流。 RPC方式：RPC的方式就是远程函数调用，像RESTFul，gRPC, DUBBO 都是这种方式。它一般是同步的，可以马上得到结果。在实际中，大多数应用都要求立刻得到结果，这时同步方式更有优势，代码也更简单。 服务网关（API Gateway）:熟悉微服务的人可能都知道服务网关（API Gateway）。当UI需要调用很多微服务时，它需要了解每个服务的接口，这个工作量很大。于是就用服务网关创建了一个Facade，把几个微服务封装起来，这样UI就只调用服务网关就可以了，不需要去对付每一个微服务。下面是API Gateway示例图： 图片来源 服务网关（API Gateway）不是为了解决微服务之间调用的紧耦合问题，它主要是为了简化客户端的工作。其实它还可以用来降低函数之间的耦合度。 有了API Gateway之后，一旦服务接口修改，你可能只需要修改API Gateway， 而不必修改每个调用这个函数的客户端，这样就减少了程序的耦合性。 服务调用：可以借鉴API Gateway的思路来减少RPC调用的耦合度，例如把多个微服务组织起来形成一个完整功能的服务组合，并对外提供统一的服务接口。这种想法跟上面的API Gateway有些相似，都是把服务集中起来提供粗颗粒（Coarse Granular）服务，而不是细颗粒的服务（Fine Granular）。但这样建立的服务组合可能只适合一个程序使用，没有多少共享价值。因此如果有合适的场景就采用，否侧也不必强求。虽然我们不能降低RPC服务之间的耦合度，却可以减少这种紧耦合带来的影响。 降低紧耦合的影响：什么是紧耦合的主要问题呢？就是客户端和服务端的升级不同步。服务端总是先升级，客户端可能有很多，如果要求它们同时升级是不现实的。它们有各自的部署时间表，一般都会选择在下一次部署时顺带升级。 一般有两个办法可以解决这个问题： 同时支持多个版本：这个工作量比较大，因此大多数公司都不会采用这种方式。 服务端向后兼容：这是更通用的方式。例如你要加一个新功能或有些客户要求给原来的函数增加一个新的参数，但别的客户不需要这个参数。这时你只好新建一个函数，跟原来的功能差不多，只是多了一个参数。这样新旧客户的需求都能满足。它的好处是向后兼容（当然这取决于你使用的协议）。它的坏处是当以后新的客户来了，看到两个差不多的函数就糊涂了，不知道该用那个。而且时间越长越严重，你的服务端可能功能增加的不多，但相似的函数却越来越多，无法选择。 它的解决办法就是使用一个支持向后兼容的RPC协议，现在最好的就是Protobuf gRPC，尤其是在向后兼容上。它给每个服务定义了一个接口，这个接口是与编程语言无关的中性接口，然后你可以用工具生成各个语言的实现代码，供不同语言使用。函数定义的变量都有编号，变量可以是可选类型的，这样就比较好地解决了函数兼容的问题。就用上面的例子，当你要增加一个可选参数时，你就定义一个新的可选变量。由于它是可选的，原来的客户端不需要提供这个参数，因此不需要修改程序。而新的客户端可以提供这个参数。你只要在服务端能同时处理这两种情况就行了。这样服务端并没有增加新的函数，但用户的新需求满足了，而且还是向后兼容的。 微服务的数量有没有上限？总的来说微服务的数量不要太多，不然会有比较重的运维负担。有一点需要明确的是微服务的流行不是因为技术上的创新，而是为了满足管理上的需要。单体程序大了之后，各个模块的部署时间要求不同，对服务器的优化要求也不同，而且团队人数众多，很难协调管理。把程序拆分成微服务之后，每个团队负责几个服务，就容易管理了，而且每个团队也可以按照自己的节奏进行创新，但它给运维带来了巨大的麻烦。所以在微服务刚出来时，我一直觉得它是一个退步，弊大于利。但由于管理上的问题没有其他解决方案，只有硬着头皮上了。值得庆幸的是微服务带来的麻烦都是可解的。直到后来，微服务建立了全套的自动化体系，从程序集成到部署，从全链路跟踪到日志，以及服务检测，服务发现和注册，这样才把微服务的工作量降了下来。虽然微服务在技术上一无是处，但它的流行还是大大推动了容器技术，服务网格（Service Mesh）和全链路跟踪等新技术的发展。不过它本身在技术上还是没有发现任何优势。。直到有一天，我意识到单体程序其实性能调试是很困难的（很难分离出瓶颈点），而微服务配置了全链路跟踪之后，能很快找到症结所在。看来微服务从技术来讲也不全是缺点，总算也有好的地方。但微服务的颗粒度不宜过细，否则工作量还是太大。 一般规模的公司十几个或几十个微服务都是可以承受的，但如果有几百个甚至上千个，那么绝不是一般公司可以管理的。尽管现有的工具已经很齐全了，而且与微服务有关的整个流程也已经基本上全部自动化了，但它还是会增加很多工作。Martin Fowler几年以前建议先从单体程序开始（详见 MonolithFirst），然后再逐步把功能拆分出去，变成一个个的微服务。但是后来有人反对这个建议，他也有些松口了。如果单体程序不是太大，这是个好主意。可以用数据额库表的数量来衡量程序的大小，我见过大的单体程序有几百张表，这就太多了，很难管理。正常情况下，一个微服务可以有两、三张表到五、六张表，一般不超过十张表。但如果要减少微服务数量的话，可以把这个标准放宽到不要超过二十张表。用这个做为大致的指标来创建微程序，如果使用一段时间之后还是觉得太大了，那么再逐渐拆分。当然，按照这个标准建立的服务更像是服务组合，而不是单个的微服务。不过它会为你减少工作量。只要不影响业务部门的创新进度，这是一个不错的方案。 到底应不应该选择微服务呢？如果单体程序已经没法管理了，那么你别无选择。如果没有管理上的问题，那么微服务带给你的只有问题和麻烦。其实，一般公司都没有太多选择，只能采用微服务，不过你可以选择建立比较少的微服务。如果还是没法决定，有一个折中的方案，“内部微服务设计”。 内部微服务设计：这种设计表面上看起来是一个单体程序，它只有一个源代码存储仓库，一个数据库，一个部署，但在程序内部可以按照微服务的思想来进行设计。它可以分成多个模块，每个模块是一个微服务，可以由不同的团队管理。 图片来源 用这张图做例子。这个图里的每个圆角方块大致是一个微服务，但我们可以把它作为一个单体程序来设计，内部有五个微服务。每个模块都有自己的数据库表，它们都在一个数据库中，但模块之间不能跨数据库访问（不要建立模块之间数据库表的外键）。“User”（在Conference Management模块中）是一个共享的类，但在不同的模块中的名字不同，含义和用法也不同，成员也不一样（例如，在“Customer Service”里叫“Customer”）。DDD（Domain-Driven Design）建议不要共享这个类，而是在每一个有界上下文（模块）中都建一个新类，并拥有新的名字。虽然它们的数据库中的数据应该大致相同，但DDD建议每一个有界上下文中都建一个新表，它们之间再进行数据同步。 这个所谓的“内部微服务设计”其实就是DDD，但当时还没有微服务，因此外表看起来是单体程序，但内部已经是微服务的设计了。它的书在2003就出版了，当时就很有名。但它更偏重于业务逻辑的设计，践行起来也比较困难，因此大家谈论得很多，真正用的较少。直到十年之后，微服务出来之后，人们发现它其实内部就是微服务，而且微服务的设计需要用它的思想来指导，于是就又重新焕发了青春，而且这次更猛，已经到了每个谈论微服务的人都不得不谈论DDD的地步。不过一本软件书籍，在十年之后还能指导新技术的设计，非常令人钦佩。 这样设计的好处是它是一个单体程序，省去了多个微服务带来的部署、运维的麻烦。但它内部是按微服务设计的，如果以后要拆分成微服务会比较容易。至于什么时候拆分不是一个技术问题。如果负责这个单体程序的各个团队之间不能在部署时间表，服务器优化等方面达成一致，那么就需要拆分了。当然你也要应对随之而来的各种运维麻烦。内部微服务设计是一个折中的方案，如果你想试水微服务，但又不愿意冒太大风险时，这是一个不错的选择。微服务的数据库设计也有很多内容，包括如何把服务从单体程序一步步里拆分出来请参见“微服务的数据库设计”. 结论：微服务之间的调用有两种方式，RPC和事件驱动。事件驱动是更好的方式，因为它是松耦合的。但如果业务逻辑是紧耦合的，RPC方式也是可行的（它的好处是代码更简单），而且你还可以通过选取合适的协议（Protobuf gRPC）来降低这种紧耦合带来的危害。由于事件溯源和事件通知的相似性，很多人把两者弄混了，但它们实际上是完全不同的东西。微服务的数量不宜太多，可以先创建比较大的微服务（更像是服务组合）。如果你还是不能确定是否采用微服务架构，可以先从“内部微服务设计”开始，再逐渐拆分。 索引：[1] What do you mean by “Event-Driven” [2] Domain-Driven Design [3] BoundedContext [4] Domain Events vs. Event Sourcing [5] Using Kafka as a (CQRS) Eventstore. Good idea [6] Evenstore [7] MonolithFirst [8] 微服务的数据库设计","categories":[{"name":"微服务","slug":"微服务","permalink":"http://zhangyu.info/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://zhangyu.info/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"解决gateway使用nacos重启报503ServiceUnavailable问题","slug":"解决gateway使用nacos重启报503ServiceUnavailable问题","date":"2022-04-30T16:00:00.000Z","updated":"2022-05-01T06:10:29.038Z","comments":true,"path":"2022/05/01/解决gateway使用nacos重启报503ServiceUnavailable问题/","link":"","permalink":"http://zhangyu.info/2022/05/01/%E8%A7%A3%E5%86%B3gateway%E4%BD%BF%E7%94%A8nacos%E9%87%8D%E5%90%AF%E6%8A%A5503ServiceUnavailable%E9%97%AE%E9%A2%98/","excerpt":"","text":"https://www.jianshu.com/p/70f4c2ce6ac8 问题描述项目使用spring cloud gateway作为网关，nacos作为微服务注册中心，项目搭建好后正常访问都没问题，但是有个很烦人的小瑕疵： 当某个微服务重启后，通过网关调用这个服务时有时会出现503 Service Unavailable(服务不可用)的错误，但过了一会儿又可以访问了，这个等待时间有时很长有时很短，甚至有时候还不会出现 导致每次重启某个项目都要顺便启动gateway项目才能保证立即可以访问，时间长了感觉好累，想彻底研究下为什么，并彻底解决 接下来介绍我在解决整个过程的思路，如果没兴趣，可以直接跳到最后的最终解决方案 gateway感知其它服务上下线首先在某个微服务上下线时，gateway的控制台可以立即看到有对应的输出 某服务下线gateway输出 某服务上线gateway输出 这说明nacos提供了这种监听功能，在注册中心服务列表发生时可以第一时间通知客户端，而在我们的依赖spring-cloud-starter-alibaba-nacos-discovery中显然已经帮我们实现了这个监听 所以也就说明gateway是可以立即感知其它服务的上下线事件，但问题是明明感知到某个服务的上线，那为什么会出现503 Service Unavailable的错误，而且上面的输出有时出现了很久，但调用依然是503 Service Unavailable，对应的某服务明明下线，这是应该是503 Service Unavailable状态，可有时确会有一定时间的500错误 ribbon为了调查事情的真相，我打开了gateway的debug日志模式，找到了503的罪魁祸首 503的控制台输出 在503错误输出前，有一行这样的日志Zone aware logic disabled or there is only one zone，而报这个信息的包就是ribbon-loadbalancer，也就是gateway默认所使用的负载均衡器 我的gateway配置文件路由方面设置如下 routes: - id: auth uri: lb://demo-auth predicates: - Path=/auth/** filters: - StripPrefix=1 其中在uri这一行，使用了lb:// ,代表使用了gateway的ribbon负载均衡功能，官方文档说明如下Note that this example also demonstrates (optional) Spring Cloud Netflix Ribbon load-balancing (defined the lb prefix on the destination URI) ribbon再调用时首先会获取所有服务列表(ip和端口信息)，然后根据负载均衡策略调用其中一个服务，选择服务的代码如下 package com.netflix.loadbalancer; public class ZoneAwareLoadBalancer&lt;T extends Server&gt; extends DynamicServerListLoadBalancer&lt;T&gt; &#123; // 选择服务的方法 public Server chooseServer(Object key) &#123; if (!ENABLED.get() || getLoadBalancerStats().getAvailableZones().size() &lt;= 1) &#123; logger.debug(&quot;Zone aware logic disabled or there is only one zone&quot;); return super.chooseServer(key); &#125; ... 这就是上面的Zone aware logic..这行日志的出处，经调试发现在getLoadBalancerStats().getAvailableZones()这一步返回的服务是空列表，说明这里没有存储任何服务信息，所以才导致最终的503 Service Unavailable继续跟进去看getAvailableZones的代码，如下 public class LoadBalancerStats implements IClientConfigAware &#123; // 一个缓存所有服务的map volatile Map&lt;String, List&lt;? extends Server&gt;&gt; upServerListZoneMap = new ConcurrentHashMap&lt;String, List&lt;? extends Server&gt;&gt;(); // 获取可用服务keys public Set&lt;String&gt; getAvailableZones() &#123; return upServerListZoneMap.keySet(); &#125; 可以看到ribbon是在LoadBalancerStats中维护了一个map来缓存所有可用服务，而问题的原因也大概明了了：gateway获取到了服务变更事件，但并没有及时更新ribbon的服务列表缓存 ribbon的刷新缓存机制现在的实际情况是：gateway获取到了服务变更事件，但并没有马上更新ribbon的服务列表缓存，但过一段时间可以访问说明缓存又刷新了，那么接下来就要找到ribbon的缓存怎么刷新的，进而进一步分析为什么没有及时刷新 在LoadBalancerStats查找到更新缓存的方法是updateZoneServerMapping public class LoadBalancerStats implements IClientConfigAware &#123; // 一个缓存所有服务的map volatile Map&lt;String, List&lt;? extends Server&gt;&gt; upServerListZoneMap = new ConcurrentHashMap&lt;String, List&lt;? extends Server&gt;&gt;(); // 更新缓存 public void updateZoneServerMapping(Map&lt;String, List&lt;Server&gt;&gt; map) &#123; upServerListZoneMap = new ConcurrentHashMap&lt;String, List&lt;? extends Server&gt;&gt;(map); // make sure ZoneStats object exist for available zones for monitoring purpose for (String zone: map.keySet()) &#123; getZoneStats(zone); &#125; &#125; 那么接下来看看这个方法的调用链，调用链有点长，最终找到了DynamicServerListLoadBalancer下的updateListOfServers方法，首先看DynamicServerListLoadBalancer翻译过来”动态服务列表负载均衡器”，说明它有动态获取服务列表的功能，那我们的bug它肯定难辞其咎，而updateListOfServers就是它刷新缓存的手段，那么就看看这个所谓的”动态服务列表负载均衡器”是如何使用updateListOfServers动态刷新缓存的 public class DynamicServerListLoadBalancer&lt;T extends Server&gt; extends BaseLoadBalancer &#123; // 封装成一个回调 protected final ServerListUpdater.UpdateAction updateAction = new ServerListUpdater.UpdateAction() &#123; @Override public void doUpdate() &#123; updateListOfServers(); &#125; &#125;; // 初始化 public DynamicServerListLoadBalancer(IClientConfig clientConfig, IRule rule, IPing ping, ServerList&lt;T&gt; serverList, ServerListFilter&lt;T&gt; filter, ServerListUpdater serverListUpdater) &#123; ... this.serverListUpdater = serverListUpdater; // serverListUpdate赋值 ... // 初始化时刷新服务 restOfInit(clientConfig); &#125; void restOfInit(IClientConfig clientConfig) &#123; ... // 开启动态刷新缓存 enableAndInitLearnNewServersFeature(); // 首先刷新一遍缓存 updateListOfServers(); ... &#125; // 开启动态刷新缓存 public void enableAndInitLearnNewServersFeature() &#123; // 把更新的方法传递给serverListUpdater serverListUpdater.start(updateAction); &#125; 可以看到初始化DynamicServerListLoadBalancer时，首先updateListOfServers获取了一次服务列表并缓存，这只能保证项目启动获取一次服务列表，而真正的动态更新实现是把updateListOfServers方法传递给内部serverListUpdater.start方法，serverListUpdater翻译过来就是“服务列表更新器”，所以再理一下思路： DynamicServerListLoadBalancer只所以敢自称“动态服务列表负载均衡器”，是因为它内部有个serverListUpdater(“服务列表更新器”)，也就是serverListUpdater.start才是真正为ribbon提供动态更新服务列表的方法，也就是罪魁祸首 那么就看看ServerListUpdater到底是怎么实现的动态更新，首先ServerListUpdater是一个接口，它的实现也只有一个PollingServerListUpdater，那么肯定是它了，看一下它的start方法实现 public class PollingServerListUpdater implements ServerListUpdater &#123; @Override public synchronized void start(final UpdateAction updateAction) &#123; if (isActive.compareAndSet(false, true)) &#123; // 定义一个runable，运行doUpdate放 final Runnable wrapperRunnable = new Runnable() &#123; @Override public void run() &#123; .... try &#123; updateAction.doUpdate(); // 执行更新服务列表方法 lastUpdated = System.currentTimeMillis(); &#125; catch (Exception e) &#123; logger.warn(&quot;Failed one update cycle&quot;, e); &#125; &#125; &#125;; // 定时执行 scheduledFuture = getRefreshExecutor().scheduleWithFixedDelay( wrapperRunnable, initialDelayMs, refreshIntervalMs, // 默认30 * 1000 TimeUnit.MILLISECONDS ); &#125; else &#123; logger.info(&quot;Already active, no-op&quot;); &#125; &#125; 至此真相大白了，原来ribbon默认更新服务列表依靠的是定时任务，而且默认30秒一次，也就是说假如某个服务重启了，gateway的nacos客户端也感知到了，但是ribbon内部极端情况需要30秒才会重新获取服务列表，这也就解释了为什么会有那么长时间的503 Service Unavailable问题 而且因为定时任务，所以等待时间是0-30秒不等，有可能你刚重启完就获取了正常调用没问题，也有可能刚重启完时刚获取完一次，结果就得等30秒才能访问到新的节点 解决思路问题的原因找到了，接下来就是解决了，最简单暴力的方式莫过于修改定时任务的间隔时间，默认30秒，可以改成10秒，5秒，1秒，只要你机器配置够牛逼 但是有没有更优雅的解决方案，我们的gateway明明已经感知到服务的变化，如果通知ribbon直接更新，问题不就完美解决了吗，这种思路定时任务都可以去掉了，性能还优化了 具体解决步骤如下 写一个新的更新器，替换掉默认的PollingServerListUpdater更新器 更新器可以监听nacos的服务更新 收到服务更新事件时，调用doUpdate方法更新ribbon缓存 接下来一步步解决 首先看上面DynamicServerListLoadBalancer的代码，发现更新器是构造方法传入的，所以要找到构造方法的调用并替换成自己信息的更新器 在DynamicServerListLoadBalancer构造方法上打了个断点，看看它是如何被初始化的(并不是gateway启动就会初始化，而是首次调用某个服务，给对应的服务创建一个LoadBalancer，有点懒加载的意思) 构造方法断点 debugger 看一下debugger的函数调用，发现一个doCreateBean&gt;&gt;&gt;createBeanInstance的调用，其中createBeanInstance执行到如下地方 createBeanInstance 熟悉spring源码的朋友应该看得出来DynamicServerListLoadBalancer是spring容器负责创建的，而且是FactoryBean模式。 这个bean的定义在spring-cloud-netflix-ribbon依赖中的RibbonClientConfiguration类 package org.springframework.cloud.netflix.ribbon; @Configuration(proxyBeanMethods = false) @EnableConfigurationProperties @Import(&#123; HttpClientConfiguration.class, OkHttpRibbonConfiguration.class, RestClientRibbonConfiguration.class, HttpClientRibbonConfiguration.class &#125;) public class RibbonClientConfiguration &#123; ... @Bean @ConditionalOnMissingBean public ServerListUpdater ribbonServerListUpdater(IClientConfig config) &#123; return new PollingServerListUpdater(config); &#125; ... &#125; 也就是通过我们熟知的@Configuration+@Bean模式创建的PollingServerListUpdater更新器，而且加了个注解@ConditionalOnMissingBean 也就是说我们自己实现一个ServerListUpdater更新器，并加入spring容器，就可以代替PollingServerListUpdater成为ribbon的更新器 最终解决方案我们的更新器是要订阅nacos的，收到事件做update处理，为了避免ribbon和nacos耦合抽象一个监听器再用nacos实现 1.抽象监听器/** * @Author pq * @Date 2022/4/26 17:19 * @Description 抽象监听器 */ public interface ServerListListener &#123; /** * 监听 * @param serviceId 服务名 * @param eventHandler 回调 */ void listen(String serviceId, ServerEventHandler eventHandler); @FunctionalInterface interface ServerEventHandler &#123; void update(); &#125; &#125; 自定义ServerListUpdaterpublic class NotificationServerListUpdater implements ServerListUpdater &#123; private static final Logger logger = LoggerFactory.getLogger(NotificationServerListUpdater.class); private final ServerListListener listener; public NotificationServerListUpdater(ServerListListener listener) &#123; this.listener = listener; &#125; /** * 开始运行 * @param updateAction */ @Override public void start(UpdateAction updateAction) &#123; // 创建监听 String clientName = getClientName(updateAction); listener.listen(clientName, ()-&gt; &#123; logger.info(&quot;&#123;&#125; 服务变化, 主动刷新服务列表缓存&quot;, clientName); // 回调直接更新 updateAction.doUpdate(); &#125;); &#125; /** * 通过updateAction获取服务名，这种方法比较粗暴 * @param updateAction * @return */ private String getClientName(UpdateAction updateAction) &#123; try &#123; Class&lt;?&gt; bc = updateAction.getClass(); Field field = bc.getDeclaredField(&quot;this$0&quot;); field.setAccessible(true); BaseLoadBalancer baseLoadBalancer = (BaseLoadBalancer) field.get(updateAction); return baseLoadBalancer.getClientConfig().getClientName(); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new IllegalStateException(e); &#125; &#125; 实现ServerListListener监控nacos并注入bean容器@Slf4j @Component public class NacosServerListListener implements ServerListListener &#123; @Autowired private NacosServiceManager nacosServiceManager; private NamingService namingService; @Autowired private NacosDiscoveryProperties properties; @PostConstruct public void init() &#123; namingService = nacosServiceManager.getNamingService(properties.getNacosProperties()); &#125; /** * 创建监听器 */ @Override public void listen(String serviceId, ServerEventHandler eventHandler) &#123; try &#123; namingService.subscribe(serviceId, event -&gt; &#123; if (event instanceof NamingEvent) &#123; NamingEvent namingEvent = (NamingEvent) event; // log.info(&quot;服务名：&quot; + namingEvent.getServiceName()); // log.info(&quot;实例：&quot; + namingEvent.getInstances()); // 实际更新 eventHandler.update(); &#125; &#125;); &#125; catch (NacosException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 把自定义Updater注入bean@Configuration @ConditionalOnRibbonNacos public class RibbonConfig &#123; @Bean public ServerListUpdater ribbonServerListUpdater(NacosServerListListener listener) &#123; return new NotificationServerListUpdater(listener); &#125; &#125; 到此，大工告成，效果是gateway访问的某微服务停止后，调用马上503，启动后，马上可以调用 总结本来想解决这个问题首先想到的是nacos或ribbon肯定留了扩展，比如说改了配置就可以平滑感知服务下线，但结果看了文档和源码，并没有发现对应的扩展点，所以只能大动干戈来解决问题，其实很多地方都觉得很粗暴，比如获取clientName，但也实在找不到更好的方案，如果谁知道，麻烦评论告诉我一下 实际上我的项目更新器还保留了定时任务刷新的逻辑，一来刚接触cloud对自己的修改自信不足，二来发现nacos的通知都是udp的通知方式，可能不可靠，不知道是否多余 nacos的监听主要使用namingService的subscribe方法，里面还有坑，还有一层缓存，以后细讲","categories":[{"name":"nacos","slug":"nacos","permalink":"http://zhangyu.info/categories/nacos/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"http://zhangyu.info/tags/nacos/"}]},{"title":"从监控到可观测性，设计思想、技术选型、职责分工都有哪些变化","slug":"从监控到可观测性，设计思想、技术选型、职责分工都有哪些变化","date":"2022-04-30T16:00:00.000Z","updated":"2022-05-01T13:57:37.956Z","comments":true,"path":"2022/05/01/从监控到可观测性，设计思想、技术选型、职责分工都有哪些变化/","link":"","permalink":"http://zhangyu.info/2022/05/01/%E4%BB%8E%E7%9B%91%E6%8E%A7%E5%88%B0%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7%EF%BC%8C%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E3%80%81%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%E3%80%81%E8%81%8C%E8%B4%A3%E5%88%86%E5%B7%A5%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8F%98%E5%8C%96/","excerpt":"","text":"https://www.51cto.com/article/707455.html 从监控到可观测性，设计思想、技术选型、职责分工都有哪些变化-51CTO.COM 随着大量云原生技术的应用，IT系统日益复杂，主动感知、预测故障并迅速定位、排障的难度变得越来越大，传统监控方式已无法跟上需求，由此应运而生的可观测性，被视为未来云环境生产部署不可或缺的技术支撑。 目前大多数传统企业对可观测性仍处于初步了解阶段，不少互联网公司在可观测性建设上也是起步不久。因此，围绕“从监控到可观测性应如何转变与升级”这一话题，本期dbaplus话题接力专栏，特别采访到知乎全链路可观测系统和接入层网络负责人-熊豹、虎牙直播SRE平台研发团队负责人-匡凌轩、好大夫基础架构部高级工程师-方勇三位老师，希望能通过他们在可观测性领域的研究心得和实践经验，帮助广大技术从业者准确认识可观测性、给企业搭建适配自身发展的可观测体系提供建议和启发。 Q1监控与可观测性是什么关系？有什么区别？可否从两者的关注点、应用场景、作用、局限性等方面进行解析？熊豹“正在发生什么 与 为什么会这样”监控是常见的运维手段，一般是指以观测系统的外部资源使用情况和接口表现来推测系统运行状态，即感知到“正在发生什么”。 可观测性是一种属性，是指在可以感知系统当前运行状态的性质，提升系统的可被观测的性质有助于我们了解“正在发生什么”以及“为什么会这样”。 云原生架构在业内逐步落地，给稳定性建设带来了更多新的挑战：迭代发布更迅速、业务系统更庞大、网络链路更复杂、运行环境更动态。在这样的混沌系统中仅仅只是知道问题发生是不够的，在这样纷繁复杂的环境下赤手空拳的我们很难去进行问题的追踪和溯源。我们要依托分层、多维度的观测数据来构建更立体和智能的诊断系统，以更多样的视角来观察和解读系统。 匡凌轩“可观测性更多是对业务应用系统自身的要求”我认为监控是可观测性能力的一部分，初期监控主要是外部对业务应用系统的主动行为，运维是传统监控的使用主体，如：通过业务进程状态、系统资源等监控数据的分析和告警来发现问题。而现在可观测性更多是对业务应用系统自身的要求，如何设计去暴露出更多可被观测的应用运行时的数据，并为这些数据之间建立关联，如：微服务框架在请求处理和RPC调用时提供一些AOP扩展的设计，可以更方便地对请求进行Metric度量和Trace追踪，以及异常情况的上下文关联。 方勇“从局部到全局可用性视角的延伸”两者的关系：监控和可观测性都旨在辅助建设高可用的服务，缩短故障处理时长，两者往往是密切协作的，界限相对模糊。 两者的区别：监控往往关注告警触发的瞬时状态，一般围绕告警事件展开，涉及从告警事件的产生到应急响应等一系列动作。关注的视角一般是局部可用性，关注每个具体的监控项，如CPU负载、剩余内存等。监控是个老生常谈的话题，最常见的场景是系统资源监控、进程或服务状态的粗粒度监控。对定制化的业务指标监控不太友好，另外传统的监控体系对云原生的支持、对微服务体系监控的支持也不太友好。 可观测性可以看作是监控的一种延续，涉及面较广，包括全链路分析（APM）、业务服务质量（SLA）、业务容量等，聚焦服务的整体可用性。关注的视角一般是全局可用性，会忽略不影响服务质量的一些指标，如CPU负载高，服务整体时延波动不大就会忽略这个CPU负载指标。 可观测性的应用场景一般与业务能力相绑定，通过可视化聚合展示影响SLA的相关指标（SLI），再配合监控告警，通过可观测性看板下钻分析异常根因。另外可观测性打通Metrics/Traces/Logs后可主动识别出服务的潜在风险，能先于用户发现问题。 可观测性也有所局限，由于需要收集业务数据，对业务具有一定的侵入性，加上打造可视化平台投入成本较高。另外可观测性整体处于初期阶段，很多工具链还不太完善，价值预期其实是被高估了。 Q2从监控到可观测性都有哪些变化？对运维、开发、架构师等岗位人员分别提出了怎样的新要求？ 熊豹“要把可观测性理念贯穿到架构和程序设计中”目标不一样了，除了要知道“正在发生什么”，还要尝试解释“为什么会这样”。我们需要把可观测性的理念贯穿到架构和程序设计中，而不是到事发或事后再来补救。我们需要有意识地设计一些机制来观察业务指标的关联变化、系统架构的数据漏斗模型、程序内逻辑分支的运行开销、外部资源依赖的健康状态，还要暴露程序内的一些资源并发度、池的填充率和命中率、运行时的状态等情况，当运行错误时也要在错误信息中携带足量的上下文信息。 运维同学要为可观测场景提供更坚实的工具基础，在上述庞大的数据压力下，保障和解决数据存储和查询的性能、资源开销、集群的拓展性和稳定性等问题。 匡凌轩“从被动监控向主动发现与定位问题的转变”我认为最大的变化是应用系统自身角色的转变，从被动监控转向主动发现与定位问题，在设计应用系统架构之初就需要考虑到系统自身的可观测性建设。运维、开发、架构师都是各环节设计的参与者，在协作方式也有一定的改变： 运维：深入熟悉产品业务和应用服务，定义并关联业务指标、应用服务指标、系统资源指标等。 开发：在框架层设计和实现对分布式应用服务运行时的Metric、Trace、Log数据采集。 架构师：业务应用系统和可观测性系统的整体架构设计，需要关注无侵入式采集上报、多维度量聚合、错误寻根分析、整体海量数据处理和存储等。 总体来说，需要各角色有更多跨技术领域的知识储备、业务思维和模型抽象能力。 方勇“职责分工、认知意识、排障效率的转变和升级”个人认为主要变化有以下几个方面： 职责分工的转变，研发关注服务质量后，部分职责从运维侧开始迁移到研发侧。研发上线后不再当甩手掌柜，开始对自己的服务负责。 认知意识的提高，从被动响应告警到主动提升服务质量。 排障效率的提升，从原先的黑盒排障模式逐渐朝可视化发展。 对不同岗位人员也有新的要求： 运维，需要摆脱传统监控的意识枷锁，拥抱云原生监控体系，同时和其他岗位人员达成共识，共建高可用服务。 开发，接棒部分运维职责，聚焦服务可用性，需要有MDD（Metrics-Driven Developmen）的思想，建设具有高韧性的服务。 架构师，在架构设计的过程中需要暴露可观测性的指标，同时需要提升数据分析的能力，建模分析Metrics/Traces/Logs数据，识别服务中潜在的风险。围绕可观测性打造相应的工具链及服务治理平台。 Q3可观测性的核心方法论/关键技术有哪些？熊豹“数据的采集、存储、分析是核心关注点”可观测性建设的核心关注点还是在数据的采集、存储、分析环节。 数据采集的覆盖可以以多种角度来看：可尝试梳理完整的数据链路，来覆盖从终端发起、网关、业务、基础设施中间的每一层组件；可以不同的观测视角进行覆盖，比如Metrics、Traces、Logs、Exception Collection、Profiler、Debuger、Changelog等类别的数据或能力都已建设齐备；可以多种维度来观察系统，比如业务维度、资源瓶颈、关联组件等维度进行覆盖的建设。 数据存储环节要关注多种类型数据的存储和查询系统选型。最为常见的是Metrics、Traces、Logs相关的存储系统，这三者都有非常广泛的基础软件选型。其中相对棘手的是指标维度爆炸、日志和Trace存储成本及性能相关的问题，一般需要搭配预聚合、前采样和后采样、存储分级等策略来解决。 数据分析环节要关联不同数据源的元信息，糅合以多维视角来构建查询界面。同时，我们也要关注如何在海量的原始数据中找到一些突出和异常的数据，一般需要建设一些流式检测和聚类分析的能力。 匡凌轩“采集数据，建立关联，设计模型”可观测性的核心思考：需要采集什么数据、如何建立关联、如何设计模型，我们以应用服务场景为例： 采集：请求量、耗时、错误和容量等，以及线程池、队列、连接池等资源指标。 关联：纵向关联请求上下游链路和调用栈，横向关联请求和处理请求所消耗的应用资源。 模型：数据采集和关联、异常定义和分析、全链路错误寻根三方面统一的模型化设计。 以上可指导我们针对不同的业务应用系统进行合理抽象，建设更标准的可观测性能力。 方勇“MDD思想主张指标驱动开发”常用方法论： 1、SLI选择： 参考Google VALET（Volume、Available、Latency、Error、Ticket）模型。 Netflix的USE方法，USE是Utilization（使用率）、Saturation（饱和度）、Error（错误）。 Weave Cloud的RED方法，Request-Rate（每秒接收的请求数）/Request-Errors（每秒失败的请求数）/Request-Duration（每个请求所花费的时间，用时间间隔表示）。 2、MDD（Metrics-Driven Development）思想：MDD主张整个应用开发过程由指标驱动，通过实时指标来驱动快速、精确和细粒度的软件迭代。指标驱动开发的理念，不但可以让程序员实时感知生产状态，及时定位并终结问题，还可以帮助产品经理和运维人员一起关注相关的业务指标。 关键技术： 1、数据收集：如果是基于Prometheus生态，有丰富的Exporte可用，还可以自研相应的Exporter。如果基于文件日志收集，可考虑Flume、Fluentd等等。 2、数据分析：可基于Clickhouse SQL分析提炼日志指标，如果是Prometheus体系，也有丰富的PromQL可用来分析相关指标。针对Traces、Logs分析一般采用自研分析引擎，并与Metrics打通。 3、数据存储：Prometheus本身就是一款很好的时序数据库，但不支持分布式存储。一般采用远程存储引擎搭配使用，常用Clickhouse、InfluxDB等。Traces和Logs一般可采用Elasticsearch存储。 4、数据展示：数据最终呈现形式，需要契合可视化设计规划，支持上卷/下钻。大部分需求可采用Grafana呈现，Grafana提供了丰富的插件，支持丰富的数据库类型，也可基于Echarts自研。如果托管公有云，可充分利用公有云自有的体系，不过有些需要单独付费。 Q4如何将Metrics、Traces、Logs三者打通并发挥最大价值？熊豹“基于时间范围内的统计关系或Label和TraceID关联”我们已知的有两类方式： 1、基于时间范围内的统计关系：一般的使用习惯是在Metric异常的时间区间里去找到对应时间区间出现异常行为的Traces和Logs，这种方式会依赖对Traces和Logs的聚类分析能力。 2、基于Label和TraceID关联：基于OpenTelemetry Collector可观测数据采集的框架，我们可以以插件的形式、以Trace Span元数据Label来生成访问指标，也同时将TraceID携带记录到日志的元信息中，这样就能以同样的TraceID或Label维度进行关联查看了。另外当前Prometheus实现了一个exemplar特性可以将Metric与TraceID关联存储，这个设计也挺有意思的。 匡凌轩“全链路错误寻根是三者打通的最大价值”三者打通最大的价值是能做到全链路错误寻根，即从发现请求Metric指标异常，通过指标关联分析，并逐层下钻到明细Trace追踪和具体Error Log，全流程自动化从宏观到明细的错误发现和根因定位。 虎牙为三者统一设计了应用监控模型，包括应用服务的透明零成本SDK接入，三者数据自动采集和关联，以及在虎牙大型分布式系统充分实践的全链路错误寻根算法。就整体实践经验来说，最终业务价值在于帮助研发和运维提高了应用服务的排障和治理效率。 方勇“打通后可立体、全息分析整个服务的可用性”从投入成本（CapEx）、运维成本（OpEx）、响应能力（Reaction）、查问题的有效程度（Investigation）几个方面分析。Metrics、Logs、Traces具有以下特征： Logs和Traces一般采用trace_id打通，trace_id一般在端入口生成，贯穿整个请求的生命周期，业务记录Logs的时候可记录当前的trace_id，这样Logs和Traces就能打通了。 与Metrics打通一般是采用标签Tags模式，如某个服务servername产生的metrics可与Traces中的servername关联。 打通后可以服务名的维度，立体、全息分析整个服务的可用性。 Q5可观测性工具如何选型？有通用的标准吗？熊豹“高可用、可伸缩、降成本、易运维”我们关注可观测工具系统的这些特性： 高可用：可观测系统作为稳定性的守卫者，本身要求更高的可靠性。 可伸缩：我们关注存储写入和查询能力的可拓展性，以支持更大的数据量级。 降成本：观测类数据会随着时间的推移逐渐失去价值，历史数据最好能低成本地失效或能对存储介质进行降级。 易运维：拥有一定的自动化能力或者本身架构足够简单。 匡凌轩“是否基于业界标准且方便扩展”虎牙主要是基于OpenTracing标准进行的深度自研和扩展，通过业界标准来做会有充分的开源代码和社区支持，可以节省很多基础代码的工作，让我们更关注自身的业务系统特性和模型设计。现在OpenTelemetry对Metrics、Traces、Logs三者提供了统一标准，开源社区热度也比较大，是个值得去研究和实践的方向。 可观测性工具选型建议可考虑两个方面： 是否基于业界标准，有更多社区和厂商支持。 是否方便扩展，更容易把共性和个性结合，最终在此基础上建设符合自身业务特性的可观测性系统。 方勇“根据已有技术栈按需选择，不必盲从主流”可观测性分析整个技术栈可参考如下图： 工具选型： Metrics：常用Zabbix、Nagios、Prometheus，及相关高可用部署方案如Prometheus-operator、Thanos。 Logging：ELK Stack、Fluentd、Loki等。 Traceing：常用Jaeger、SkyWalking、Pinpoint、Zipkin、Spring Cloud Sleuth等。 可视化：Grafana。 其实技术选型没什么特定的标准，每个企业不同阶段可能有不同的选择，适合自己的才是最好的，这里总结几点心得： 控制成本预算，企业一般需要从自身的发展阶段实际情况考虑，不必一上来就整全链路可观测性，也许初期只用传统的Zabbix就满足需求了。理性按需选择，大可不必盲从主流。 拥抱开源，初期一般采用开源产品，开箱即用，搭顺风车。另外，选型时还需要考虑周边生态的丰富度。 根据团队技术栈选择，中间件、业务服务、云原生、物理机监控等选型都要贴合团队已有的技术栈。","categories":[{"name":"监控","slug":"监控","permalink":"http://zhangyu.info/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"监控","slug":"监控","permalink":"http://zhangyu.info/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"SpringBoot性能太差,教你几招轻松搞定","slug":"SpringBoot性能太差,教你几招轻松搞定","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T10:42:45.102Z","comments":true,"path":"2022/04/30/SpringBoot性能太差,教你几招轻松搞定/","link":"","permalink":"http://zhangyu.info/2022/04/30/SpringBoot%E6%80%A7%E8%83%BD%E5%A4%AA%E5%B7%AE,%E6%95%99%E4%BD%A0%E5%87%A0%E6%8B%9B%E8%BD%BB%E6%9D%BE%E6%90%9E%E5%AE%9A/","excerpt":"","text":"SpringBoot性能太差,教你几招轻松搞定 Java派 2022-04-30 09:27 https://mp.weixin.qq.com/s/lreQia3XL5cl0XKrN47KCA Spring Boot性能太差，教你几招轻松搞定 目录 异步执行 增加内嵌 Tomcat 的最大连接数 使用 @ComponentScan() 默认 Tomcat 容器改为 Undertow 使用 BufferedWriter 进行缓冲 Deferred 方式实现异步调用 异步调用可以使用 AsyncHandlerInterceptor 进行拦截 异步执行 实现方式二种： 使用异步注解 @aysnc、启动类：添加 @EnableAsync 注解 JDK 8 本身有一个非常好用的 Future 类——CompletableFuture 123456789101112131415161718192021@AllArgsConstructorpublic class AskThread implements Runnable&#123; private CompletableFuture&lt;Integer&gt; re &#x3D; null; public void run() &#123; int myRe &#x3D; 0; try &#123; myRe &#x3D; re.get() * re.get(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(myRe); &#125; public static void main(String[] args) throws InterruptedException &#123; final CompletableFuture&lt;Integer&gt; future &#x3D; new CompletableFuture&lt;&gt;(); new Thread(new AskThread(future)).start(); &#x2F;&#x2F;模拟长时间的计算过程 Thread.sleep(1000); &#x2F;&#x2F;告知完成结果 future.complete(60); &#125;&#125; 在该示例中，启动一个线程，此时 AskThread 对象还没有拿到它需要的数据，执行到 myRe = re.get() * re.get() 会阻塞。 我们用休眠 1 秒来模拟一个长时间的计算过程，并将计算结果告诉 future 执行结果，AskThread 线程将会继续执行。 123456789101112131415161718public class Calc &#123; public static Integer calc(Integer para) &#123; try &#123; &#x2F;&#x2F;模拟一个长时间的执行 Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return para * para; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; final CompletableFuture&lt;Void&gt; future &#x3D; CompletableFuture.supplyAsync(() -&gt; calc(50)) .thenApply((i) -&gt; Integer.toString(i)) .thenApply((str) -&gt; &quot;\\&quot;&quot; + str + &quot;\\&quot;&quot;) .thenAccept(System.out::println); future.get(); &#125;&#125; CompletableFuture.supplyAsync 方法构造一个 CompletableFuture 实例，在 supplyAsync() 方法中，它会在一个新线程中，执行传入的参数。 在这里它会执行 calc() 方法，这个方法可能是比较慢的，但这并不影响 CompletableFuture 实例的构造速度，supplyAsync() 会立即返回。 而返回的 CompletableFuture 实例就可以作为这次调用的契约，在将来任何场合，用于获得最终的计算结果。 supplyAsync 用于提供返回值的情况，CompletableFuture 还有一个不需要返回值的异步调用方法 runAsync(Runnable runnable)，一般我们在优化 Controller 时，使用这个方法比较多。 这两个方法如果在不指定线程池的情况下，都是在 ForkJoinPool.common 线程池中执行，而这个线程池中的所有线程都是 Daemon（守护）线程，所以，当主线程结束时，这些线程无论执行完毕都会退出系统。 核心代码： 123CompletableFuture.runAsync(() -&gt; this.afterBetProcessor(betRequest,betDetailResult,appUser,id)); 异步调用使用 Callable 来实现： 12345678910111213141516171819202122232425262728293031323334@RestController public class HelloController &#123; private static final Logger logger &#x3D; LoggerFactory.getLogger(HelloController.class); @Autowired private HelloService hello; @GetMapping(&quot;&#x2F;helloworld&quot;) public String helloWorldController() &#123; return hello.sayHello(); &#125; &#x2F;** * 异步调用restful * 当controller返回值是Callable的时候，springmvc就会启动一个线程将Callable交给TaskExecutor去处理 * 然后DispatcherServlet还有所有的spring拦截器都退出主线程，然后把response保持打开的状态 * 当Callable执行结束之后，springmvc就会重新启动分配一个request请求，然后DispatcherServlet就重新 * 调用和处理Callable异步执行的返回结果， 然后返回视图 * * @return *&#x2F; @GetMapping(&quot;&#x2F;hello&quot;) public Callable&lt;String&gt; helloController() &#123; logger.info(Thread.currentThread().getName() + &quot; 进入helloController方法&quot;); Callable&lt;String&gt; callable &#x3D; new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; logger.info(Thread.currentThread().getName() + &quot; 进入call方法&quot;); String say &#x3D; hello.sayHello(); logger.info(Thread.currentThread().getName() + &quot; 从helloService方法返回&quot;); return say; &#125; &#125;; logger.info(Thread.currentThread().getName() + &quot; 从helloController方法返回&quot;); return callable; &#125;&#125; 异步调用的方式 WebAsyncTask： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@RestController public class HelloController &#123; private static final Logger logger &#x3D; LoggerFactory.getLogger(HelloController.class); @Autowired private HelloService hello; &#x2F;** * 带超时时间的异步请求 通过WebAsyncTask自定义客户端超时间 * * @return *&#x2F; @GetMapping(&quot;&#x2F;world&quot;) public WebAsyncTask&lt;String&gt; worldController() &#123; logger.info(Thread.currentThread().getName() + &quot; 进入helloController方法&quot;); &#x2F;&#x2F; 3s钟没返回，则认为超时 WebAsyncTask&lt;String&gt; webAsyncTask &#x3D; new WebAsyncTask&lt;&gt;(3000, new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; logger.info(Thread.currentThread().getName() + &quot; 进入call方法&quot;); String say &#x3D; hello.sayHello(); logger.info(Thread.currentThread().getName() + &quot; 从helloService方法返回&quot;); return say; &#125; &#125;); logger.info(Thread.currentThread().getName() + &quot; 从helloController方法返回&quot;); webAsyncTask.onCompletion(new Runnable() &#123; @Override public void run() &#123; logger.info(Thread.currentThread().getName() + &quot; 执行完毕&quot;); &#125; &#125;); webAsyncTask.onTimeout(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; logger.info(Thread.currentThread().getName() + &quot; onTimeout&quot;); &#x2F;&#x2F; 超时的时候，直接抛异常，让外层统一处理超时异常 throw new TimeoutException(&quot;调用超时&quot;); &#125; &#125;); return webAsyncTask; &#125; &#x2F;** * 异步调用，异常处理，详细的处理流程见MyExceptionHandler类 * * @return *&#x2F; @GetMapping(&quot;&#x2F;exception&quot;) public WebAsyncTask&lt;String&gt; exceptionController() &#123; logger.info(Thread.currentThread().getName() + &quot; 进入helloController方法&quot;); Callable&lt;String&gt; callable &#x3D; new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; logger.info(Thread.currentThread().getName() + &quot; 进入call方法&quot;); throw new TimeoutException(&quot;调用超时!&quot;); &#125; &#125;; logger.info(Thread.currentThread().getName() + &quot; 从helloController方法返回&quot;); return new WebAsyncTask&lt;&gt;(20000, callable); &#125;&#125; 增加内嵌 Tomcat 的最大连接数 代码如下： 123456789101112131415161718192021@Configurationpublic class TomcatConfig &#123; @Bean public ConfigurableServletWebServerFactory webServerFactory() &#123; TomcatServletWebServerFactory tomcatFactory &#x3D; new TomcatServletWebServerFactory(); tomcatFactory.addConnectorCustomizers(new MyTomcatConnectorCustomizer()); tomcatFactory.setPort(8005); tomcatFactory.setContextPath(&quot;&#x2F;api-g&quot;); return tomcatFactory; &#125; class MyTomcatConnectorCustomizer implements TomcatConnectorCustomizer &#123; public void customize(Connector connector) &#123; Http11NioProtocol protocol &#x3D; (Http11NioProtocol) connector.getProtocolHandler(); &#x2F;&#x2F;设置最大连接数 protocol.setMaxConnections(20000); &#x2F;&#x2F;设置最大线程数 protocol.setMaxThreads(2000); protocol.setConnectionTimeout(30000); &#125; &#125;&#125; 使用 @ComponentScan() 使用 @ComponentScan() 定位扫包比 @SpringBootApplication 扫包更快。默认 Tomcat 容器改为 Undertow 默认 Tomcat 容器改为 Undertow（Jboss 下的服务器，Tomcat 吞吐量 5000，Undertow 吞吐量 8000） 123456&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt;&lt;&#x2F;exclusions&gt; 改为： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-undertow&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt; 使用 BufferedWriter 进行缓冲 这里不给大家举例，可自行尝试。 Deferred 方式实现异步调用 代码如下： 1234567891011121314151617181920212223242526272829303132333435@RestControllerpublic class AsyncDeferredController &#123; private final Logger logger &#x3D; LoggerFactory.getLogger(this.getClass()); private final LongTimeTask taskService; @Autowired public AsyncDeferredController(LongTimeTask taskService) &#123; this.taskService &#x3D; taskService; &#125; @GetMapping(&quot;&#x2F;deferred&quot;) public DeferredResult&lt;String&gt; executeSlowTask() &#123; logger.info(Thread.currentThread().getName() + &quot;进入executeSlowTask方法&quot;); DeferredResult&lt;String&gt; deferredResult &#x3D; new DeferredResult&lt;&gt;(); &#x2F;&#x2F; 调用长时间执行任务 taskService.execute(deferredResult); &#x2F;&#x2F; 当长时间任务中使用deferred.setResult(&quot;world&quot;);这个方法时，会从长时间任务中返回，继续controller里面的流程 logger.info(Thread.currentThread().getName() + &quot;从executeSlowTask方法返回&quot;); &#x2F;&#x2F; 超时的回调方法 deferredResult.onTimeout(new Runnable()&#123; @Override public void run() &#123; logger.info(Thread.currentThread().getName() + &quot; onTimeout&quot;); &#x2F;&#x2F; 返回超时信息 deferredResult.setErrorResult(&quot;time out!&quot;); &#125; &#125;); &#x2F;&#x2F; 处理完成的回调方法，无论是超时还是处理成功，都会进入这个回调方法 deferredResult.onCompletion(new Runnable()&#123; @Override public void run() &#123; logger.info(Thread.currentThread().getName() + &quot; onCompletion&quot;); &#125; &#125;); return deferredResult; &#125;&#125; 异步调用可以使用 AsyncHandlerInterceptor 进行拦截 代码如下： 12345678910111213141516171819202122232425262728293031@Componentpublic class MyAsyncHandlerInterceptor implements AsyncHandlerInterceptor &#123; private static final Logger logger &#x3D; LoggerFactory.getLogger(MyAsyncHandlerInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123;&#x2F;&#x2F; HandlerMethod handlerMethod &#x3D; (HandlerMethod) handler; logger.info(Thread.currentThread().getName()+ &quot;服务调用完成，返回结果给客户端&quot;); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; if(null !&#x3D; ex)&#123; System.out.println(&quot;发生异常:&quot;+ex.getMessage()); &#125; &#125; @Override public void afterConcurrentHandlingStarted(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; &#x2F;&#x2F; 拦截之后，重新写回数据，将原来的hello world换成如下字符串 String resp &#x3D; &quot;my name is chhliu!&quot;; response.setContentLength(resp.length()); response.getOutputStream().write(resp.getBytes()); logger.info(Thread.currentThread().getName() + &quot; 进入afterConcurrentHandlingStarted方法&quot;); &#125;&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://zhangyu.info/categories/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://zhangyu.info/tags/SpringBoot/"}]},{"title":"API网关的功能用途及实现方式","slug":"API网关的功能用途及实现方式","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T08:52:54.499Z","comments":true,"path":"2022/04/30/API网关的功能用途及实现方式/","link":"","permalink":"http://zhangyu.info/2022/04/30/API%E7%BD%91%E5%85%B3%E7%9A%84%E5%8A%9F%E8%83%BD%E7%94%A8%E9%80%94%E5%8F%8A%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"API网关的功能用途及实现方式 - 东风微鸣技术博客https://ewhisper.cn/posts/52291/ 1. API 网关诞生背景前言API 经济生态链已经在全球范围覆盖， 绝大多数企业都已经走在数字化转型的道路上，API 成为企业连接业务的核心载体， 并产生巨大的盈利空间。快速增长的 API 规模以及调用量，使得企业 IT 在架构上、模式上面临着更多的挑战。 API 是什么API 网关是一个服务器，是系统的唯一入口。从面向对象设计的角度看，它与外观模式类似。API 网关封装了系统内部架构，为每个客户端提供一个定制的 API。它可能还具有其它职责，如身份验证、监控、负载均衡、缓存、请求分片与管理、静态响应处理。API 网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。通常，网关也是提供 REST/HTTP 的访问 API。服务端通过 API-GW 注册和管理服务。 1. API 开放数量不断增加毋庸置疑，随着企业的数据化进展，微服务改造，不同领域的 API 层出不穷，早在 2014 年 ProgrammableWeb 便预测 API 矢量可达到 100,000 到 200,000，并会不断增长。API 开发数量的增加给边缘系统带来机会，也随即演变了 API 网关的出现。大规模的 API 管理系统成为核心的发展趋势。 The API Economy Disruption and the Business of APIs，Nordic APIs 2. API 服务平台多样化最初的 API 主要针对不同单体应用的网络单元之间信息交互，现已演变到服务间快速通讯。随着人工智能 EI，IOT 的不断演进，依赖 API 的平台不断更新，如 Web，Mobile，终端等，未来将会出现更多的服务体系。包括不限于： 浏览器 IOS Android macOS Windows Linux IOT 其他移动端 小程序 终端设备（如智慧零售、工业的终端等） … 3. 逐步替换原有企业的服务模式，API 即商品卖计算，卖软件，卖能力，最终的企业的销售模式会逐步转变，能力变现，释放数据价值，依托不同的 API 管理平台创造新的盈利。 API 网关诞生背景随着 API 的整体趋势发展，每个时期都面临着不同的挑战，架构也随之变化，具体如下图： 1960-1980：阿帕网、ATTP、TCP 1980-1990：点对点 1990-2000：消息中间件、ESB（企业服务总线，Enterprise service bus），SOA（面向服务的架构） 2000 至今：Integration as a service，RESTful services，API 管理，云上编排 API economy From systems to business services 从最原始的“传输协议通讯” -&gt; “简单的接口集成” -&gt; “消息中间件” -&gt; “标准 REST”， 可以看到 API 的发展更趋向于简洁， 集成，规范化， 这也促使更多的系统边界组件不断涌现，在承载了万亿级的 API 经济的背景下， API 网关应运而生。 如果没有合适的 API 管理工具， API 经济不可能顺利开展。 同时提出了对于 API 管理系统的生命周期定义： planning（规划）, design（设计）， implementation（实施）， publication（发布），operation（运维）, consumption（消费）, maintenance（维护） and retirement of APIs（下架） 如果没有合适的 API 管理工具， API 经济不可能顺利开展。 同时提出了对于 API 管理系统的生命周期定义： planning（规划）, design（设计）， implementation（实施）， publication（发布），operation（运维）, consumption（消费）, maintenance（维护） and retirement of APIs（下架） – Magic Quadrant for Full Life Cycle API Management，Gartner, 2016-10-27 2. API 网关核心功能 API 生命周期管理 planning（规划） design（设计） implementation（实施） publication（发布） operation（运维） consumption（消费） maintenance（维护） retirement（下架） API 网关基础功能 认证 鉴权 服务发现和集成 负载均衡 日志 链路追踪 监控 重试 限流 QoS 熔断器 映射 缓存 Header、query 字符串 等 转义 API 文档 API 测试 SDK 生成 API 多版本、多环境管理 插件 API 集中式 metrics、logging、tracing 管理 安全 HTTPS IP 黑白名单 高可用 可热重启 高性能 可扩展性 无状态横向扩展 3. API 网关的用途OpenAPI企业需要将自身数据、能力等作为开发平台向外开放，通常会以 rest 的方式向外提供。最好的例子就是淘宝开放平台、腾讯公司的 QQ 开发平台、微信开放平台。 Open API 开放平台必然涉及到客户应用的接入、API 权限的管理、调用次数管理等，必然会有一个统一的入口进行管理，这正是 API 网关可以发挥作用的时候。 微服务网关在微服务架构中，有一个组件可以说是必不可少的，那就是微服务网关，微服务网关处理了负载均衡，缓存，路由，访问控制，服务代理，监控，日志等。 API 网关在微服务架构中正是以微服务网关的身份存在。 API 中台上述的微服务架构对企业来说有可能实施上是困难的，企业有很多遗留系统，要全部抽取为微服务改动太大，对企业来说成本太高。 但是由于不同系统间存在大量的 API 服务互相调用，因此需要对系统间服务调用进行管理，清晰地看到各系统调用关系，对系统间调用进行监控等。 API 网关可以解决这些问题，我们可以认为如果没有大规模的实施微服务架构，那么对企业来说微服务网关就是企业的 API 中台。 4. API 网关的价值通过 API 网关，可以封装后端各种服务，以 API 的形式，提供给各方使用。API 网关产品的优势总结如下： API 全生命周期管理：协助开发者轻松完成 API 的创建、维护、发布、监控等整个生命周期的管理。 丰富的服务治理能力：支持 API 限流，参数校验，元数据维护，SDK 生成，批量操作等能力，协助开发者高效管理服务。 可观察性：通过 API 网关，支持对调用次数，前后端错误次数等丰富监控指标的可视和告警能力；通过全面的监控告警，保证用户服务的可用性。 可运营性：支持 企业 OpenAPI 定价，账单等运营功能 服务安全：通过接入多种认证方式，确保用户 API 的访问安全性；通过严格的流量控制，避免用户服务的过载。 前后端业务解耦 多类型后端打通 5. API 网关的实现方式主流 API 网关 Istio Linkerd NGINX 及其商业版 KONG Traefik APISIX RedHat 3scale Netflix Zuul Spring Cloud Gateway Amazon API Gateway 阿里云 API 网关 腾讯云 API 网关 MuleSoft OpenAPI对于定位 OpenAPI 平台的 API 网关，目前只能选择专业的 API 网关作为解决方案。 微服务网关对于定位为「微服务网关」的 API 网关，业务有多种实现方式： Service Mesh典型的如 Istio，架构如下： 通用反向代理基于 NGINX 或 NGINX + LUA + OpenResty 的实现。典型如： Nginx 及其 商业版 NGINX Controller（API 管理、App 交付） NGINX Plus（API Gateway，负载均衡，仪表板） NGINX Ingress Controller NGINX Service Mesh KONG Traefik 3scale API 网关框架 Netflix Zuul，zuul 是 spring cloud 的一个推荐组件 Spring Cloud Gateway 公有云解决方案其实公有云的解决方案也是基于以上方案的定制化开发并产品化后发布到公有云上，主流的也是基于：NGINX + LUA + OpenResty 的实现 Amazon API Gateway 阿里云 API 网关 腾讯云 API 网关 其他方案 基于 Netty、非阻塞 IO 模型。 基于 Node.js 的方案。这种方案是应用了 Node.js 的非阻塞的特性。 基于 Java，如 MuleSoft","categories":[{"name":"API网关","slug":"API网关","permalink":"http://zhangyu.info/categories/API%E7%BD%91%E5%85%B3/"}],"tags":[{"name":"API网关","slug":"API网关","permalink":"http://zhangyu.info/tags/API%E7%BD%91%E5%85%B3/"}]},{"title":"基于容器的PaaS混合云的几种形式","slug":"基于容器的PaaS混合云的几种形式","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T09:25:35.098Z","comments":true,"path":"2022/04/30/基于容器的PaaS混合云的几种形式/","link":"","permalink":"http://zhangyu.info/2022/04/30/%E5%9F%BA%E4%BA%8E%E5%AE%B9%E5%99%A8%E7%9A%84PaaS%E6%B7%B7%E5%90%88%E4%BA%91%E7%9A%84%E5%87%A0%E7%A7%8D%E5%BD%A2%E5%BC%8F/","excerpt":"","text":"基于容器的 PaaS 混合云的几种形式 - 东风微鸣技术博客https://ewhisper.cn/posts/57201/) 概述这是 Gartner 的一个图，提供了全球的基于容器的 PaaS 公有云、混合云服务的梳理展示： 这里提供一个其他的视角：中国市场，基于容器的 PaaS 混合云（公有云 + 私有云）的相关厂商及产品。 ❗️ 注意： 文章目前还是初版，只是厂商和产品的一个简单罗列，后面会进一步细化。另外由于作者认知所限，无法罗列所有相关厂商和产品。请见谅。 软件 - 容器平台指的是通过售卖软件形式提供的容器平台（可能的售卖方式包括: 买断 + 维保；订阅），供应商不提供算力。这里的「容器平台」指的是：基于 Kubernetes 的容器平台，有的容器平台会提供更丰富的功能，如：镜像仓库，监控，日志，Tracing，DevOps，微服务治理，ServiceMesh、Servless 等 RedHat - OpenShift Container Platform Rancher - RKE 青云 - Kubesphere 时速云 - TCS（TenxCloud Container Service） 灵雀云 - ACP（Alauda Container Platform） 博云 - BeyondContainer DaoCLoud - DaoCloud Enterprise 腾讯 - TKE Enterprise（基于灵雀云） VMware - VSphere 7+ 软件 - 多云容器管理平台指的是通过售卖软件形式提供的多云容器管理平台（可能的售卖方式包括: 买断 + 维保；订阅），供应商不提供算力。这里的「多云容器管理平台」指的是：基于 Kubernetes 的容器平台，或基于 Kubernetes 联邦（如华为 MCP），或基于自研多集群能力（如 Rancher），实现对异构、公有云及私有云的 Kubernetes 集群的纳管、甚至安装、运维、统一监控等能力。 ❗️ 注意： 这类「多云容器管理平台」虽然可以纳管异构 Kubernetes 集群，但是某些高级功能，只有使用供应商推荐的 Kubernetes 产品才能使用。如：Rancher 的安装、监控、日志等高级功能；RedHat 的安装、安全策略、GitOps 等功能 优劣优势： 灵活 适用于：全内网环境（对于安全级别要求高的如金融行业会非常关注） 劣势： 购买方需要提供硬件 需要安装搭建，无法开箱即用 供应商及产品 Rancher - Rancher 华为 - MCP（多云容器平台） DaoCloud - DaoCloud Service Platform RedHat - ACM（Advanced Cluster Management for Kubernetes） 青云 - Kubesphere（Kubesphere 3.0 以后支持多集群管理） VMware - Tanzu 托管 - 公有云托管 K8S 集群指的是公有云提供的 K8S 集群，提供公有云算力，也提供托管 Kubernetes 服务。计费方式为：按量计费或包年包月等。 ✍️ 备注： 暂不包括公有云实例服务及 Servless 服务。 Amazon - EKS（Elastic Kubernetes Service） 阿里 - ACK（Alibaba Cloud Container Service for Kubernetes） 腾讯 - TKE（Tencent Kubernetes Engine） 微软 - AKS（Azure Kubernetes Service） 华为 - CCE（云容器引擎） 青云 - QKE（KubeSphere on QingCloud） 软件 - 公有云 K8S 集群产品私有化输出指的是通过售卖软件形式提供的和公有云架构类似的「公有云 K8S 集群产品私有化输出」（可能的售卖方式包括: 买断 + 维保；订阅），供应商不提供算力。 华为 - CCE（云容器引擎） 阿里 - 阿里飞天专有云敏捷版 腾讯 - TCS（Tencent Cloud-Native Suite） 托管 - 公有云多云容器管理平台指的是公有云提供的 多云容器管理平台，提供公有云算力，也提供管理 Kubernetes 服务。计费方式为：按量计费或包年包月等。但是有个前提：如果是私有云 Kubernetes 集群或其他公有云提供商的 Kubernetes 集群，必须通过专线或互联网等形式与供应商网络联通。 优劣优势： 标准化产品，灵活性欠缺 适用于：互联网环境 按需付费 开箱即用 劣势： 无法纳管 没有互联网或连接公有云专线的 Kubernetes 集群 安全性担忧 对于被纳管集群的要求较多（如：EKS Anywhere 目前仅支持两种特定 Kubernetes 集群的纳管） 供应商及产品 华为 - MCP（多云容器平台） 腾讯 - TKE Everywhere（❗️ 注意：这个和其他 2 家的 Anywhere 还不太一样，云上云下是 一个 集群，云下的 Node 由云上的 Master 纳管。本质上是一个边缘容器管理方案。而且还在内测中。） Amazon - EKS Anywhere 阿里云 - AKS Anywhere 🧠 思考：2 家 xxx Anywhere 具体是啥做法？ 2 家的 Anywhere 做法是极为一致的。本质上就是「公有云私有化，线上线下我全都要」。优势是：（兼听则明啊，经过实战检验才知道效果如何…） 一致体验 统一集群管理 统一资源调度 统一数据容灾 统一应用交付 弹性算力 能力下沉 云原生可观测 安全防护能力 中间件 数据库 数据分析 AI 简化容灾 以阿里云为例：阿里云推出了一云多形态的部署架构，提供中心云、本地云、边缘云、云盒等多种部署形态，ACKAnywhere 的全面升级意味着公共云能力向本地化进一步延伸，客户在自建的数据中心内也能体验到低成本、低延迟、本地化的公共云产品。随着云计算的普及和云原生技术的发展，容器服务已成为各大公司上云用云的必备基础设施。…此次升级的 ACK Anywhere 拥有「一致体验、弹性算力、能力下沉、简化容灾」四大核心能力，使企业在任何业务场景下使用容器服务时，都能实现「统一集群管理、统一资源调度、统一数据容灾和统一应用交付」。得益于阿里云公共云丰富的产品能力，ACK Anywhere 可将成熟的云原生可观测、安全防护能力部署到用户环境，更可以将云端先进的中间件、数据分析和 AI 能力下沉到本地，满足客户对于产品丰富度以及数据管控的需求，加速业务创新。 业务连续性是现代企业 IT 架构关注的重点，ACK Anywhere 内建的备份中心，实现了备份、容灾、迁移一体化；支持 Kubernetes 集群配置与数据卷的备份恢复。结合阿里云丰富的业务多活容灾经验，帮助企业全面提升系统稳定性和业务连续性。 ❗️ 注意： 由于上面所说的原因：「对于被纳管集群的要求较多」，所以这类产品往往也会推荐用户安装自己提供的 Kubernetes 产品，如：华为的 CCE，腾讯的 TKE 开源版，或 Amazon EKS Anywhere 的 EKS Distro 产品，或阿里云的 ACK Distro。 其他玩家 京东云 UCloud 百度云 金山云","categories":[{"name":"PaaS","slug":"PaaS","permalink":"http://zhangyu.info/categories/PaaS/"}],"tags":[{"name":"PaaS","slug":"PaaS","permalink":"http://zhangyu.info/tags/PaaS/"}]},{"title":"打破Dockershim移除焦虑,且看Rancher如何应对","slug":"打破Dockershim移除焦虑,且看Rancher如何应对","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T14:43:29.818Z","comments":true,"path":"2022/04/30/打破Dockershim移除焦虑,且看Rancher如何应对/","link":"","permalink":"http://zhangyu.info/2022/04/30/%E6%89%93%E7%A0%B4Dockershim%E7%A7%BB%E9%99%A4%E7%84%A6%E8%99%91,%E4%B8%94%E7%9C%8BRancher%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9/","excerpt":"","text":"打破 Dockershim 移除焦虑，且看 Rancher 如何应对https://mp.weixin.qq.com/s/swhQtu0pb_1AwQtMfTyd9Q 前 言 早在 2020 年 12 月，Kubernetes 就宣布将要弃用 Dockershim（https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/）。在 Kubernetes 中，Dockershim 是一个适配器组件，Dockershim 适配器允许 Kubelet 与 Docker 交互，就好像 Docker 是一个与 CRI 兼容的运行时一样。 Kubernetes 即将发布的 v1.24 版本最主要的变化就是删除了 Dockershim（https://github.com/kubernetes/enhancements/issues/2221）。也就是说，Kubernetes v1.24 无法再通过 in-tree 的形式来支持 Docker 作为它的 CRI 运行时。 随着 Kubernetes 的发展， 虽然 Docker 日渐式微，但还是有大量用户群体离不开 Docker，或者说暂时无法切换到 containerd 或 CRI-O 作为它的 CRI 运行时。 Rancher 为了满足继续使用 Docker 作为 CRI 运行时的需求，通过 RKE 集群支持外部 Dockershim 继续使用 Docker 作为 CRI 运行时。 虽然 Rancher 最新的 v2.6.4 目前还不支持 Kubernetes v1.24，但早在 Kubernetes v1.21 中就采用了 Mirantis 和 Docker 宣布的上游开源社区外部 Dockershim （该项目称为 cri-dockerd：https://github.com/kubernetes/enhancements/issues/2221）来确保 RKE 集群可以继续使用 Docker。换句话说，你可以像之前一样继续基于 Docker Engine 构建 Kubernetes，唯一的区别就是 Dockershim 由内置方案变成了外部方案。 要启用外部 Dockershim，只需要在 RKE 配置中设置以下选项： enable_cri_dockerd: true 由于外部 Dockershim 的支持是从 RKE 创建的 Kubernetes 1.21 及以上的版本中开始支持，所以我们需要通过 RKE 创建一个 Kubernetes 1.21 及以上的 Kubernetes 版本才能支持这种方案。 下面将演示如何在 RKE 创建的 Kubernetes 集群中启用外部 Dockershim。 通过 RKE CLI 创建集群 说明： RKE 的安装及使用，请参考官方文档（http://docs.rancher.cn/rke）， 这里不再详细说明。 本次 demo 使用的 RKE 版本为 `v1.3.9`。 配置RKE cluster.yml 文件 在 RKE 的集群配置文件 cluster.yml 中通过增加 enable_cri_dockerd: true 选项来启用外部 Dockershim 支持。本例使用最精简文件示例，如需个性化设置，请根据需求调整选项： 123456789~$ cat cluster.ymlnodes: - address: 192.168.205.19 user: ubuntu role: - controlplane - etcd - workerenable_cri_dockerd: true 通过 RKE 创建 Kubernetes 集群 12345678910111213~$ rke upINFO[0000] Running RKE version: v1.3.9INFO[0000] Initiating Kubernetes clusterINFO[0000] cri-dockerd is enabled for cluster version [v1.22.7-rancher1-2]INFO[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused Kubelet certificatesINFO[0000] [certificates] Generating admin certificates and kubeconfig.........INFO[1130] [ingress] ingress controller nginx deployed successfullyINFO[1130] [addons] Setting up user addonsINFO[1130] [addons] no user addons definedINFO[1130] Finished building Kubernetes cluster successfully 确认启用 cri-dockerd 集群创建成功后，连接到下游集群的主机查看进程，可以发现增加了一个 cri-dockerd 的进程： 12root@dev-1:~# ps -ef | grep cri-dockerdroot 26211 25813 3 11:26 ? 00:04:13 &#x2F;opt&#x2F;rke-tools&#x2F;bin&#x2F;cri-dockerd --network-plugin&#x3D;cni --cni-conf-dir&#x3D;&#x2F;etc&#x2F;cni&#x2F;net.d --cni-bin-dir&#x3D;&#x2F;opt&#x2F;cni&#x2F;bin --pod-infra-container-image&#x3D;rancher&#x2F;mirrored-pause:3.6 Cri-dockerd 其实就是从被移除的 Dockershim 中，独立出来的一个项目。为 Docker Engine 提供了一个垫片（shim），可以通过 Kubernetes CRI 控制 Docker。这意味着你可以像以前一样继续基于 Docker Engine 构建 Kubernetes，只需从内置的 Dockershim 切换到外部的 Dockershim 即可。 接下来，我们再观察 Kubelet 的参数变化： 1234567root@dev-1:~# docker inspect kubelet &quot;Entrypoint&quot;: [ ... &quot;--container-runtime&#x3D;remote&quot;, &quot;--container-runtime-endpoint&#x3D;&#x2F;var&#x2F;run&#x2F;Dockershim.sock&quot;, ... ], 可以看到，增加 enable_cri_dockerd: true 参数启动的 Kubernetes 集群增加了 --container-runtime=remote 和 --container-runtime-endpoint=/var/run/Dockershim.sock 两个 Kubelet 参数。通过这两个 Kubelet 参数可以设置 Kubernetes 集群利用外部 Dockershim 继续使用 Docker 作为 CRI 运行时。 通过 Rancher 创建 RKE 集群 如果你是 Rancher 的长期用户，你肯定会知道从 Rancher UI 上创建的自定义集群就是通过 RKE 来去实现的。只不过通过 Rancher UI 创建的 RKE 集群可以省去配置 RKE cluster.yml 的烦恼，只需要从 UI 上做一些简单的配置即可。 本节，将给大家介绍如何通过 Rancher 创建 RKE 集群并启用外部 Dockershim 支持。 安装 Rancher Rancher 的安装及使用，请参考官方文档，这里不再详细说明。因为 RKE 创建的Kubernetes 1.21 及以上的版本中才开始支持外部 Dockershim，并且只有 Rancher v2.6.x 才支持 Kubernetes 1.21 或以上版本。所以，我们本次示例选择 Rancher 2.6.4 作为 demo 环境。 创建自定义集群 通过 Edit as YAML 来设置 enable_cri_dockerd 参数值为 true 将 enable_cri_dockerd 的值修改为 true ，保存并创建集群 确认启用 cri-dockerd 可以参考上面“通过 RKE CLI 创建集群”章节的步骤去检查下游集群是否成功启用了 cri-docker，为了节省篇幅，这里就不重复说明。 常见问题 问：如果要获得 Rancher 对上游 Dockershim 的支持，需要升级 Rancher 吗？ 答：对于 RKE，Dockershim 的上游支持从 Kubernetes 1.21 开始。你需要使用支持 RKE 1.21 的 Rancher 版本。详情请参见我们的支持矩阵。 问：我目前的 RKE 使用 Kubernetes 1.20。为了避免出现不再支持 Dockershim 的情况，我是否需要尽早将 RKE 升级到 Kubernetes 1.21？ 答：在使用 Kubernetes 1.20 的 RKE 中，Dockershim 版本依然可用，而且在下一个发行版发行之前不会被弃用。有关时间线的更多信息，请参见 [Kubernetes Dockershim 弃用相关的常见问题](https://kubernetes.io/blog/2020/12/02/Dockershim-faq/#when-will-Dockershim-be-removed)。Kubernetes 会发出将会弃用 Dockershim 的警告，而 Rancher 在 RKE 中已经用 Kubernetes 1.21 缓解了这个问题。你可以按照计划正常升级到 1.21。 问：如果不想再依赖 Dockershim，还有什么选择？ 答：你可以为 Kubernetes 使用不需要 Dockershim 支持的运行时，如 Containerd。RKE2 和 K3s 就是其中的两个选项。 问：如果目前使用 RKE1，但想切换到 RKE2，可以怎样进行迁移？ 答：你可以构建一个新集群，然后将工作负载迁移到使用 Containerd 的新 RKE2 集群。Rancher 也在探索就地升级路径的可能性。 问：如果已经通过 RKE 创建了 Kubernetes v1.21 以上的集群，当我切换到外部 Dockershim 时，是否会对集群有影响？ 答：没影响，因为容器运行时没有变化，Dockershim 只是由内置方案变成了外部方案。","categories":[{"name":"k8s","slug":"k8s","permalink":"http://zhangyu.info/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://zhangyu.info/tags/k8s/"}]},{"title":"API网关为K8s容器应用集群提供强大的接入能力","slug":"API网关为K8s容器应用集群提供强大的接入能力","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T07:58:02.175Z","comments":true,"path":"2022/04/30/API网关为K8s容器应用集群提供强大的接入能力/","link":"","permalink":"http://zhangyu.info/2022/04/30/API%E7%BD%91%E5%85%B3%E4%B8%BAK8s%E5%AE%B9%E5%99%A8%E5%BA%94%E7%94%A8%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BE%9B%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%8E%A5%E5%85%A5%E8%83%BD%E5%8A%9B/","excerpt":"","text":"API网关为K8s容器应用集群提供强大的接入能力https://help.aliyun.com/document_detail/71623.html 1. Kubernetes 集群介绍Kubernetes（k8s）作为自动化容器操作的开源平台已经名声大噪，目前已经成为容器玩家主流选择。Kubernetes在容器技术的基础上，增加调度和节点集群间扩展能力，可以非常轻松地让你快速建立一个企业级容器应用集群，这个集群主要拥有以下能力： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它 下面是一个典型的Kubernetes架构图： 2. API网关作为Kubernetes集群的接入层架构我们可以看到Kubernetes集群是有足够理由作为应用服务的首选，但是Kubernetes集群没有足够的接入能力，特别在大型应用中，它是不能够直接对用户提供服务的，否则会有非常大的安全风险。而API网关作为成熟的云产品，已经集成了非常丰富的接入能力，把API网关放在Kubernetes集群前面作为应用集群的接入服务使用，将大大提高Kubernetes集群的服务能力，可以作为标准的大型互联网应用的标准架构。下面是使用阿里云架构图： 是否启用Ingress Control？ 从架构图中我们可以看到，API网关作为Kubernetes集群的桥头堡，负责处理所有客户端的接入及安全工作，API网关和Kubernetes集群中Ingress Control的内网SLB或者服务的内网SLB进行通信。具体什么时候用Ingress Control呢？如果Kubernetes集群内只有一个服务，网关直接和此服务关联的内网SLB进行通信最高效。如果Kubernetes集群内有多个服务，如果使用服务的内网SLB对网关提供服务，将会生成很多SLB，资源管理起来会比较麻烦，此时我们可以使用Ingress Control做Kubernetes中服务发现与七层代理工作，API网关的所有请求发送到Ingress Control关联的内网SLB上，由Ingress Control将请求分发到Kubernetes集群内的容器内，这样我们也只需要一个内网SLB就能将Kubernetes集群内的所有服务暴露出去了。 3. API网关接入能力读者要问了，接入了API网关具体能为整个架构带来哪些好处呢？下面我们列一下这种架构中，API网关具体能给整个应用带来什么价值。 1.API网关允许客户端和API网关使用多种协议进行通信，其中包括： * HTTP * HTTP2 * WebScoket 2.API网关使用多种方法保证和客户端之间的通信安全： * 允许定义API和APP之间的授权关系，只有授权的APP允许调用； * 全链路通信都使用签名验证机制，包括客户端和API网关之间的通信和API网关和后端服务之间的通信，保证请求在整个链路上不会被篡改； * 支持用户使用自己的SSL证书进行HTTPS通信； * 支持OPENID CONNECT； 3.API网关具备iOS/Android/Java三种SDK的自动生成能力，并且具备API调用文档自动生成能力； 4.API网关支持入参混排能力，请求中的参数可以映射到后端请求中的任何位置； 5.API网关提供参数清洗能力，用户定义API的时候可以指定参数的类型，正则等规则，API网关会帮用户确认传输给后端服务的请求是符合规则的数据； 6.API网关支持流量控制能力，支持的维度为用户/APP/API； 7.API网关提供双向通信的能力； 8.API网关提供基于请求数/错误数/应答超时时间/流量监控报警能力，所有的报警信息会使用短信或者邮件在一分钟内发出； 9.API网关已经和阿里云的SLS产品打通，用户可以将所有请求日志自动上传到用户自己的SLS中，后继好对访问日志进行统计分析； 10.API网关支持Mock模式，在联调中这个能力非常方便； 11.API网关支持用户配置调用方的IP白名单和黑名单； 12.API网关结合阿里云的云市场，为Provider提供向API使用者收费的能力。 阿里云的API网关是一个上线数年的成熟云产品，在稳定性和性能方面，经过了时间和阿里云的工程师的不断打磨，有高性能需求的用户尽管放马过来。 4. 在阿里云快速配置Kubernetes集群和API网关阿里云支持快速创建Kubernetes集群，同一个Region内的Kubernetes集群和API网关的集成也非常简单，下面我们来一步一步地在阿里云中配置出本文第二节中架构设计。第二节中的架构设计中，API网关和Kubernetes有两种结合的模式，一种是API网关将请求发送到Ingress Control前的SLB，由Ingress Control将请求路由到Kubernetes对应的节点中，第二种是API网关直接将请求发送到Kubernetes中服务前的SLB，由SLB直接将请求转发到Kubernetes中服务对应的节点中。第一种模式只需要Ingress Control前有一个SLB就可以了，由Ingress Contro做服务发现与路由，第二种模式每个服务前都需要申请一个SLB，适合并发量大的场景。 4.1 Ingress Control模式的配置方式4.1.1 创建带有Ingress Control组件的Kubernetes集群首先我们来通过控制台创建一个具备Ingress Control组件的Kubernetes集群。 1.进入Kubernetes集群管理控制台界面：https://cs.console.aliyun.com/#/k8s/cluster/list 2.点击左上角“创建Kubernetes集群”按钮，进入 3.在创建Kubernetes集群页面选择不同规格的，具体创建选项和常规创建参数一致，在组件配置子页面，需要注意勾选创建Ingress组件： 4.创建成功后可以在Kubernetes列表页面看到刚才创建的集群。 4.1.2 在Kubernetes集群内创建一个多容器的服务现在集群有了，我们需要在集群内创建一个服务，这个服务由2个容器组成，每个容器都由最新的Tomcat镜像生成。容器的端口是8080，Ingress Control提供服务的端口是80。 1.进入Kubernetes集群管理控制台界面：https://cs.console.aliyun.com/#/k8s/cluster/list 2.进入Kubernetes集群的控制台页面后，点击左边菜单栏的“应用”菜单下的“无状态”按钮，进入应用列表页面后，点击右上角的“使用模板创建”按钮进入创建页面： 3.进入创建页面后，点击使用文本创建按钮，输入下面的资源编排文本点击上传按钮进行创建： apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-demo labels: app: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: tomcat:latest ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-service spec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: tomcat sessionAffinity: None type: NodePort 对这段编排模板创建文本做下解释：使用最新的Tomcat镜像创建两个容器的意思，容器的服务端口是8080，并且创建一个命名为tomcat-service的服务，对外服务的端口为80，映射到容器8080的端口； 好了，目前为止，我们已经创建了一个Kubernetes集群，并且在这个集群下面创建了两个容器，每个容器上面跑着一个最新的Tomcat。这两个容器组成一个无状态应用，并且组成了一个命名为tomcat-service的服务。我们可以进入无状态应用详情页面看到整个应用的运行情况。目前我们的API网关没有办法访问到这个服务，需要我们在Ingress Control上建立一条到这个服务的路由才能把全链路打通 4.1.3 在Ingress Control上给服务加上路由现在我们有应用了，还需要在Ingress Control上为这个应用建立一个服务，然后在服务上建立一条路由，这样API网关将请求发送给Ingress Control时，Ingress Control会根据配置的路由信息将请求代理到对应的应用节点上。 1.在容器服务控制台上点击“路由与负载均衡”菜单下的“服务”按钮，点击右上角的“创建”按钮； 2.在创建路由页面填写服务对应的域名，所监听的端口等： 4.1.4 在API网关对Ingress Control的内网SLB授权API网关如果要访问Ingress Control的内网SLB，需要增加VPC网络的授权，首先我们需要准备VPC的标识和内网SLB的实例ID，我们可以在SLB控制台获取。 1.在Kubernetes管理控制台的“服务于负载均衡”菜单下点击“服务”菜单，进入服务管理页面后，选择命名空间为”所有命名空间”后可以在列表中看到Ingress Control的内网SLB地址： 2.登录SLB控制台（https://slb.console.aliyun.com/），找到刚才创建Kubernetes服务时自动创建的VPC，点击进去查看详情，我们可以在这个页面看到VPC的标识： 好了，目前我们已经获取到了VPC的ID和SLB的实例ID,下面我们来创建API网关的授权： 3.进入API网关授权页面：https://apigateway.console.aliyun.com/#/cn-beijing/vpcAccess/list ，点击右上角的创建授权按钮，弹出创建VPC授权的小页面，将刚才查询到的VPC标识和SLB实例ID填入到对应的内容中： 点击确认按钮后，授权关系就创建好了，需要记住刚才填写的授权名称。 4.1.5 创建API在API网关控制台创建API的时候，后端服务这块，有两点需要注意的： 1.我们需要填写刚才创建的VPC授权名称； 2.在创建API页面的常量参数添加一个名字为host，位置header的参数，参数值设置为4.1.3节中设置的路由中填写的域名。 4.1.6 调用测试API建立好了以后，我们把API发布到线上就可以使用API网关的测试工具进行测试，看看能否将请求发送到刚才创建的Kubernetes集群中去。 我们进入刚才创建好的API的详情页面，点击左侧菜单中的调试API，进入调试页面。在调试页面填写好相应的参数，点击“发起请求”按钮。 我们可以看到，请求发送到了Kubernetes的集群中的容器中，并且收到了容器中tomcate的404的应答（因为没有配置对应的页面）。 4.2 Kubernetes服务内网SLB结合模式的配置方式通过Kubernetes服务的SLB结合API网关与Kubernetes的模式的配置方式与前一节中通过Ingress Control结合模式的配置方式有两点不同：1.申请Kubernetes中的容器服务时，需要指定生成内网SLB，2.找到这个SLB的VPC ID与SLB ID，使用这两个ID到API网关去进行授权即可。上一节中描述的创建Kubernetes集群的步骤，本节不再冗余描述。本节主要描述创建携带内网SLB的Kubernetes服务，并且找到这个内网SLB的IP地址。在找到SLB的IP地址后，具体授权方法和4.1.4中描述的一致，本节也不再重复描述。 4.2.1 生成携带内网SLB的Kubernetes服务在4.1.1操作之后，我们有了一个Kubernetes集群，现在我们通过资源编排文本在这个集群中创建带有内网SLB的服务。我们注意下，本段资源编排代码和4.1.2中的资源编排代码不同的是，我们把最后一行的Type变成了LoadBalancer，并且指定了这个SLBLoadBalancer为内网： apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-demo labels: app: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: tomcat:latest ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-service annotations: service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet spec: ports: - port: 80 targetPort: 8080 protocol: TCP selector: app: tomcat type: LoadBalancer 对这段编排模板创建文本做下解释：使用最新的Tomcat镜像创建两个容器的意思，容器的服务端口是8080，并且创建一个命名为tomcat-service的服务，这个服务前有一个内网SLB，对外服务的端口为80，映射到容器8080的端口。 4.2.2 在Kubernetes控制台找到服务的内网SLB地址现在我们成功创建了一个携带内网SLB的服务，我们可以在Kubernetes控制台的“路由与负载均衡”菜单的“服务”子页面找到这个内网SLB的内网IP： 找到内网SLB之后，就可以像4.1.4中一样，去SLB的控制台找到这个SLB的VPC ID和SLB ID，并且使用这个SLB的VPC ID和SLB ID到API网关去授权了。 5 总结让我们总结一下本文的内容，在前三节中，我们描述了Kubernetes集群和API网关的各项能力，并且画出了结合他俩作为后端应用服务生产的架构图。我们结合API网关+Kubernetes集群的架构，让系统具备了动态伸缩，动态路由，支持多协议接入，SDK自动生成，双向通信等各项能力，成为可用性更高，更灵活，更可靠的一套架构。 5.1 配置总结在最后一节，我们描述了在阿里云的公有云如何创建一整套API网关加Kubernetes的流程，关键点在于Kubernetes集群通过Ingress Control组件服务发现，在API网关对Ingress Control组件的内网SLB进行授权后，将所有请求发送到SLB上。下面我们再总结一下通过Ingress结合API网关与Kubernetes的步骤： 创建一个带有Ingress Control组件的Kubernetes的集群； 使用资源编排命令，在Kubernetes集群中创建两个运行这最新版本的Tomcat的容器，并且基于这两个容器创建对应的服务； 在控制台上给Ingress Control加上一条服务路由； 到SLB控制台找到Ingress Control内网SLB的实例ID，在API网关创建一个VPC授权； 在API网关创建API，后端服务使用刚才创建的VPC授权，并且设定一个名为host的参数，参数值使用Ingress Control服务路由设置的域名。API的请求将发送到Kubernetes集群的Ingress Control的SLB上，由Ingress Control将请求路由到容器内部的Tomcat服务上。 在高并发场景或者只有一个服务的场景，我们可以跳过Ingress Control，直接在服务前面架设一个内网SLB，并且将这个内网SLB授权给API网关，供API网关进行访问。 5.2 Ingress Control和SLB两种方案对比1. 使用SLB+Ingress Control。Ingress Control可以做Kubernetes集群的服务发现和七层代理工作，如果Kubernetes集群中有多个服务，可以统一使用Ingress Control进行路由，同时只需要一个内网SLB就可以对外暴露多个服务。便于运维管理和Kubernetes集群的服务扩充。推荐使用此种方式。 2. 直接使用SLB。如果集群中某个服务的业务压力很大，可以考虑为此服务单独建立一个SLB，API网关直接连接此SLB，从而达到更高的通信效率。此种方式的弊端也比较明显，如果Kubernetes集群内有多个服务，需要为每个服务配置一个SLB，因此给运维管理带来较大工作量。","categories":[{"name":"API网关","slug":"API网关","permalink":"http://zhangyu.info/categories/API%E7%BD%91%E5%85%B3/"}],"tags":[{"name":"API网关","slug":"API网关","permalink":"http://zhangyu.info/tags/API%E7%BD%91%E5%85%B3/"}]},{"title":"云原生环境下的日志采集、存储、分析实践","slug":"云原生环境下的日志采集-存储-分析实践","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T09:51:45.713Z","comments":true,"path":"2022/04/30/云原生环境下的日志采集-存储-分析实践/","link":"","permalink":"http://zhangyu.info/2022/04/30/%E4%BA%91%E5%8E%9F%E7%94%9F%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86-%E5%AD%98%E5%82%A8-%E5%88%86%E6%9E%90%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"以下文章来源于火山引擎开发者社区作者：刘卯银｜火山引擎日志系统架构师 云原生环境下的日志采集、存储、分析实践https://mp.weixin.qq.com/s/L2bPGFL2cNamaFrxeyU48Q 谈到日志系统，首先要从日志说起，日志在 IT 系统里无处不在，也是 IT系统大数据的关键来源。日志的种类和样式非常多，以在线教育系统为例，日志包括客户端日志、服务端日志。服务端日志又包括业务的运行/运维日志以及业务使用的云产品产生的日志。要管理诸多类型的日志，就需要一套统一的日志系统，对日志进行采集、加工、存储、查询、分析、可视化、告警以及消费投递，将日志的生命周期进行闭环。 Kubernetes 下日志采集的开源自建方案 开源自建 火山引擎早期为了快速上线业务，各团队基于开源项目搭建了自己的日志系统，以满足基本的日志查询需求，例如使用典型的开源日志平台 Filebeat+Logstash+ES+Kibana 的方案。但是在使用过程中，我们发现了开源日志系统的不足： 各业务模块自己搭建日志系统，造成重复建设。 以 ES 为中心的日志架构可以利用 ES 查询便利的优势，但是资源开销大、成本高。而且 ES 与 Kibana 在界面上强绑定，不利于功能扩展。 开源方案一般采用单机 yaml 做采集配置，当节点数很多的时候，配置非常繁琐。 开源系统的采集配置难以管理，数据源也比较单一。 Kubernetes 下的日志采集 Kubernetes 下如何采集日志呢？官方推荐了四种日志采集方案： DaemonSet：在每台宿主机上搭建一个 DaemonSet 容器来部署 Agent。业务容器将容器标准输出存储到宿主机上的文件，Agent 采集对应宿主机上的文件。 Streaming Sidecar：有一些业务系统的日志不是标准输出，而是文件输出。Streaming Sidecar 的方式可以把这些文件输出通过 Sidecar 容器转换成容器的标准输出，然后采集。 Sidecar Logging Agent：业务 Pod 内单独部署 Agent 的 Sidecar 容器。这种方式的资源隔离性强。 API/SDK：直接在容器内使用 API 或 SDK 接口将日志采集到后端。 以上前三种采集方案都只支持采集容器的标准输出，第四种方案需要改造业务代码，这几种方式对采集容器文件都不友好。但用户对于日志文件有分类的需求，标准输出将所有日志混在一起，不利于用户进行分类。如果用户要把所有日志都转到标准输出上，还需要开发或者配置，难以推广。因此 Kubernetes 官方推荐的方案无法完全满足用户需求，给我们的实际使用带来了很多不便。 自建日志采集系统的困境与挑战 云原生场景下日志种类多、数量多、动态非永久，开源系统在采集云原生日志时面临诸多困难，主要包括以下问题： 一、采集难 配置复杂：系统规模越来越大，节点数越来越多，每个节点的配置都不一样，手工配置很容易出错，系统的变更变得非常困难。 需求不满足：开源系统无法完全满足实际场景的用户需求，例如不具备多行日志采集、完整正则匹配、过滤、时间解析等功能，容器文件的采集也比较困难。 运维难度高：大规模场景下大量 Agent 的升级是个挑战，系统无法实时监控 Agent 的状态，当Agent 状态异常时也没有故障告警。 二、产品化能力不足 可用性低：因为缺少流控，突发的业务容易使后端系统过载，业务之间容易相互影响。 资源使用效率低：如果配置的资源是固定的，在突发场景下容易造成性能不足的问题；但如果配置的资源过多，普通场景下资源利用率就会很低；不同的组件配置不均衡还会导致性能瓶颈浪费资源。ES 的原始数据和索引使用相同的资源配置，也会导致高成本。 功能不足：比如 ES 的投递和消费能力弱、分析能力固化、没有告警能力、可视化能力有限。 火山引擎统一日志平台 TLS 在遇到这些问题以后，我们研发了一套统一的日志管理平台——**火山引擎日志服务（Tinder Log Service，简称为 TLS)**。TLS 的整体架构如下： 面对各种日志源，TLS 通过自研的 LogCollector/SDK/API，可支持专有协议、 OpenTelemetry 和 Kafka 协议上传日志。支持多种类型的终端、多种开发语言以及开源生态标准协议。 采集到的日志首先会存入高速缓冲集群，削峰填谷，随后日志会匀速流入存储集群，根据用户配置再流转到数据加工集群进行日志加工，或者到索引集群建立索引。建立索引后用户可以进行实时查询和分析。TLS 提供标准的 Lucene 查询语法、SQL 92 分析语法、可视化仪表盘以及丰富的监控告警能力。 当日志存储达到一定周期，不再需要实时分析之后，用户可以把日志投递到成本更低的火山引擎对象存储服务中，或者通过 Kafka 协议投递到其他云产品。如果用户有更高阶的分析需求，TLS 也支持把日志消费到实时计算、流式计算或离线计算进行更深入的分析。 TLS 的系统设计遵循高可用、高性能、分层设计的原则。 高可用：通过存算分离，所有服务都是无状态的，故障快速恢复。 高性能：所有集群都可横向扩展，没有热点。 分层设计：各模块之间低耦合，模块之间定义标准接口，组件可替换。 以上就是火山引擎自研的日志存储平台 TLS 的系统架构，下面将详细介绍 TLS 相较于开源系统做的优化。 系统优化 中心化白屏化的配置管理 当日志系统中采集 Agent 数量较多时，不再适合在每台机器上手工配置，因此我们开发了中心化、白屏化的配置管理功能，支持动态下发采集配置，并支持查看 Agent 运行状态监控、支持客户端自动升级。 中心化配置的实现流程如下： 客户端主动向服务端发起心跳，携带自身版本信息。 服务端收到心跳，检查版本。 服务端判断是否需要下发配置信息给客户端。 客户端收到配置信息，热加载到本地配置，以新的配置进行采集。 中心化配置管理的优势在于： 可靠：中心化管理，配置不丢失，白屏化配置不容易出错。 高效：各种环境下所有的配置都是统一处理，无论 LogCollector 部署在移动端、容器还是物理机上，用户都可以在服务端相同的界面上配置，配置以机器组为单位批量下发，快速高效。 轻松运维：用户可以在服务端查看客户端的运行状态，对客户端的异常发出告警。通过中心化配置，TLS 可以向客户端推送最新版本，自动升级。 CRD 云原生配置方式 中心化、白屏化的配置方式是适合运维人员的配置方式。在开发测试自动化的场景下，最优的方式是 CRD。传统的方式通过 API 接口去做采集配置，用户通常需要写数千行代码来实现。TLS 提供了云原生 CRD 的方式来进行配置，用户只需要在 yaml 文件里配置要采集的容器、容器内的日志路径以及采集规则即可完成采集配置。因为不再需要编写代码，CRD 方式大幅提高了日志接入效率。 CRD 的配置流程如下： 使用 Kubectl 命令创建 TLS LogConfig CRD； TLS Controller 监听到 CRD 配置更新； TLS Controller 根据 CRD 配置向 TLS Server 发送命令，创建 topic、创建机器组，创建日志采集配置； LogCollector 定期请求 TLS Server，获取新的采集配置并进行热加载； LogCollector 根据采集配置采集各个容器上的标准输出或文本日志； LogCollector 将采集到的日志发送给 TLS Server。 适合大规模、多租户场景的客户端 开源日志采集客户端一般只支持一个 Output，多个 Input 采用相同的 Pipeline，相互影响。为了适应大规模、多租户场景，火山引擎自研了日志采集的客户端 LogCollector。LogCollector 对不同的 Input 采用不同的 Pipeline 做资源隔离，减少多租户之间的相互影响。一个 LogCollector 支持多个 Output，可以根据不同的 Output 单独做租户鉴权。同时我们还在 LogCollector 内实现了自适应反压机制，如果服务端忙碌，LogCollector 会自动延迟退避，服务端空闲时再发送，减少算力负担。 产品优化 可用性提升 在可用性方面，TLS 支持多级全局流控，能杜绝因业务突发导致的故障扩散问题。 在日志采集到高速缓冲集群时，按照用户的 Shard 数控制写入高速缓冲区的流量。 当数据从高速缓冲区流向存储集群时，按存储集群控制单个存储集群的流量。 从存储集群到索引集群，按索引集群控制单个索引集群的写入流控以及查询分析并发数。 效率提升 索引和原始数据分离 ES 的索引和数据存在一起，我们在设计过程发现索引和原始数据分离会更优，主要表现在： 提升数据流动性：存储集群支持批量消费接口，消费数据不经过索引集群。相对于从索引集群先查询后消费的模式，直接从存储集群消费性能更高，有效提升数据流动性。 降低成本：索引和存储可以采用不同成本的存储，整体的存储成本就会降低。用户可以随时按需创建索引，进一步降低索引成本。 提升可用性：索引可以异步创建，流量突发时创建索引慢不会影响存储写入速率。 索引管理和调度 索引的流量是不可预测的，因此我们在效率方面的另一个优化是支持索引的管理和调度，实现弹性伸缩，从而提升可用性，解决规模问题。我们的解决方案是在多个索引集群之间做数据流动，基于负载、资源、容量自动迁移索引，支持动态跨集群在线迁移索引，平衡索引集群负载。 功能优化 消费投递：在消费投递方面我们支持了丰富的消费投递接口，包括： 消费者 消费组 Kafka 协议：通过 Kafka 协议进行标准协议的消费； S3 协议：支持通过 S3 对象存储的协议把日志投递到对象存储。 查询分析：支持先查询过滤后分析，减少分析数据量提高性能。分析支持标准的 SQL 92，分析结果支持图表可视化。 日志告警：通过实时监控日志，基于用户配置的监控规则组合以及告警触发条件，满足条件就可以通过短信、邮件、飞书等方式发送告警给用户或用户组。 可视化仪表盘：TLS 提供多种可视化仪表盘，实现实时监测，且仪表盘可以关联告警。 TLS 实践案例 接下来为大家介绍两个 TLS 的典型案例。 火山引擎内部业务及运维日志采集 TLS 目前支撑了火山引擎全国多个 Region 运维日志的采集分析。日志类型包括业务的文件日志、容器标准输出。业务分别部署在内网、外网以及混合云，日志都通过 TLS 平台统一做采集和分析。 相较于前期各业务模块自己搭建日志系统，采用 TLS 获得了如下收益： 经济高效：资源利用率由之前的 20% 提升到现在的 **80%**，大幅降低资源成本； 可用性较高：多级流控加缓存，抗突发能力强，即使在索引系统故障的时候也不会影响原始数据的流量； 轻松运维：TLS 的统一运维提升了运维人员的能力，少量运维人员即可完成整个系统的运维。 快速接入：TLS 可以在一小时内完成一个新业务的采集、查询、分析、消费的快速接入。 某教育行业头部客户日志采集 该客户的系统业务主要采集的日志包括： 文件日志 App 日志 Kubernetes 集群后端业务的日志 用户行为日志 TLS 把这几个平台的日志统一采集到云端，进行实时查询分析以及进行告警。客户自建有大数据分析平台，TLS 可将日志数据通过消费的方式流转到该平台进行在线、离线等更高阶的大数据分析。对于时间长的历史数据，则投递到对象存储进行归档，从而降低整个系统的成本。 用户的管理员可在 TLS 上统一查看所有平台的各种日志，整个系统的建设和运维成本也降低了。TLS 使用标准接口，可以兼容云上自建的分析平台，用户在快速上线的同时也能保证系统的高度兼容。 展望未来 未来，TLS 平台会不断进行更深层次的优化： 云产品的一键日志采集 搜索引擎的深度优化 数据清洗和加工的函数式接口 集成更多第三方平台，火山引擎云产品深度融合 Q&amp;A Q：中心化配置，各个业务的日志采集配置是 OP 负责还是 RD 负责？A：日志采集的中心化配置是 Web 方式，配置非常简单，无论是 RD 或是 OP 负责都可以。火山引擎上 Web 配置由 OP 来负责，容器自动化采集是用 CRD 的方式，一般是 RD 负责。 Q：采集端 Agent 的使用资源可以限制吗？是否会影响业务的资源使用？A：CPU 占用量、内存占用量这些是可以配置的，不会影响业务的资源使用。 Q：CRD 和中心化配置不会冲突吗？A：通常情况下不会冲突。CRD 有特定的命名规则，只要 Web 配置和 CRD 配置的名字不冲突就不会报错。如果名字冲突，配置会失败，改名字后重试即可。 Q：Node 节点宕机是否会丢日志？A：不会。LogCollector 有 Checkpoint，Checkpoint 会定期更新。如果节点宕机没有更新 Checkpoint，日志会从上次 Checkpoint 点重新采集，所以是不会丢的。 Q：日志采集的延迟情况如何？A：一般在秒级延迟，后端业务忙的时候可能是几秒到十几秒的延迟。 Q：Kafka 协议是如何暴露的？通过实现 Kafka Server 吗？A：我们是在服务端实现的 Kafka 协议。用户以 Kafka 的协议方式接入，鉴权也是以 Kafka 的鉴权协议来做的。用户看到的其实就是一个 Kafka。这样可以对用户做到透明。","categories":[{"name":"日志","slug":"日志","permalink":"http://zhangyu.info/categories/%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"日志","slug":"日志","permalink":"http://zhangyu.info/tags/%E6%97%A5%E5%BF%97/"}]},{"title":"公有云降本增效最佳实践","slug":"公有云降本增效最佳实践","date":"2022-04-29T16:00:00.000Z","updated":"2022-04-30T07:57:55.662Z","comments":true,"path":"2022/04/30/公有云降本增效最佳实践/","link":"","permalink":"http://zhangyu.info/2022/04/30/%E5%85%AC%E6%9C%89%E4%BA%91%E9%99%8D%E6%9C%AC%E5%A2%9E%E6%95%88%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"https://ewhisper.cn/posts/59535/ 最佳实践整体首选公有云服务而非自建公有云除了提供 IaaS（计算、存储、网络等）外，也会提供 PaaS（微服务、中间件、数据库、大数据、开发套件等）和 SaaS 服务。 在公有云提供的服务（如 MySQL 数据库）可以满足需求的前提下，建议首选公有云上的 MySQL 数据库服务，而非自建。 理由简单说明如下： 对比项 成本 性能 伸缩性 维护方 可靠性 监控 易用性 自建 高 低 弱 我方 低 无 难 云上服务 中 高 高 云提供商 高 有 易用 成本： 自建，需要人员维护和优化的成本，需要自行考虑高可靠（可能需要购买多台服务器）和高性能（可能需要购买高性能存储），使得成本偏高。 云上服务，通过规模效应、资源池化、参数调优等实现成本相对不高。 性能： 自建，不一定知道所有的参数优化项，也不一定同价位能买到专用的高性能硬件。 云上服务，性能明码标价，按需选择适合自己的性能配置。 伸缩性 自建，伸缩较麻烦，要不手动，要不通过 历经检验的 DevOps 脚本，伸缩性弱。 云上服务，很多 PaaS 类服务可以一键升配。 维护方 自建，我方自行兜底 云上服务，云提供商提供 SLA 兜底。 可靠性 自建，不一定能实现该组件的集群模式或高可用模式的全部最佳实践。 云上服务，会做好网络高可用（甚至是跨 AZ 的高可用）、存储多副本、计算跨物理服务器 / 机架 /AZ 甚至 region、服务监控及自愈、备份等多种措施保障可靠性。 监控： 自建，要不没监控，要不监控需要从头（采集端）到尾（告警通知）实现一遍 云上服务，监控具备，且和公有云监控无缝对接。 易用性： 自建：一般没有 Web 界面，需要通过线下或流程平台或 CLI 来申请和操作 云上服务：有易用的 web 界面，可以在 web 界面上完成大部分功能。 比如云数据库： 运维架构： 存储的数据规模及后期扩展，采用的高可用架构； 异常切换 硬件及基础环境部署 选择什么配置的服务器，服务器型号及对应磁盘阵列； 操作系统环境及内核设置； 数据库安装及优化 数据库版本安装部署及配置； 数据库配置参数调优； SQL 语句优化； 慢查询，对 SQL 语句及索引做优化 数据库日常备份及恢复。 备份； 热备还是冷备？物理备份还是逻辑备份？ 备份策略、周期、频率 使用云数据库，这些步骤云数据库都帮你做了。其他 PaaS（中间件、大数据、微服务、DevOps 等）也类似。 做好安全防护公有云最大的风险就是数据泄露。所以一定要做好安全防护。这个安全防护是多方面的。详细见 安全 部分。 云的优势是「分布式」如果对比单台服务器，可能云主机的性能差一些。「分布式」是云计算的最大优势。在实践中，不要只追求单台机器的性能，而是要通过分布式的设计思想来保障业务的高性能。最佳实践推荐，服务器标配 4C8G，低配也可以采用 2C4G 的配置。通过分布式充分压榨了单台服务器的资源，从而最大限度地保障了最终的低成本。所以，在云上，一般情况下应用服务器的选择条件是：更多的低配的云服务胜于更少的高配的云服务器。所以，在云上，对于数据库来说，如果数据量非常大，也推荐使用「分布式数据库」，而非在云上自建 Oracle。 云的优势是「弹性」所以，在云上，不要按照业务峰值购买全量的资源，而是推荐： 买满足日常需求的资源 高峰时，再提前购买一些弹性的资源，弹性扩容。 另外，不仅仅是服务器资源，对于网络也适用，如果您的系统经常搞活动，网络负载差距很大，那么推荐：「大带宽按量付费」而不是「固定带宽固定计费」。 动静分离静态：放 CDN + 对象存储上，或者放 NGINX 服务器上也好，不要直接用应用服务器（如 tomcat 或 nodejs）来处理静态资源。（浪费，术业有专攻）动态：典型架构是 LB - NGINX - 应用服务器 - redis - 数据库。 上云前做好业务量的评估上云前做好业务量的评估，为云上的资源规划做好资源预算。可以通过： 压测 已有监控数据分析 等方式评估业务量。 常用的业务量指标如下： 指标 计算周期 指标含义 PV 天 Page View。指 B/S 架构中的 Web 类业务一天内页面的访问次数，每打开或刷新一次页面，算一个 PV。 UV 天 UV 是 Unique Visitor 的简写。指 B/S 架构中 Web 类业务一天内访问站点的用户数（以 cookie 为依据） IP 天 B/S 架构中 Web 类业务一天内有多少个独立的 IP 浏览了页面，即统计不同的 IP 浏览用户数量。 用户数 – 业务系统的注册用户数 活跃用户数 天 注册用户数中，一天中实际使用了业务系统的用户数量，跟 UV 的概念一样 在线用户数 天 一天的活跃用户数中，用户同时在一定的时间段内在线的数量 并发用户数 – 指在线用户数基础上，某一时刻同时指向服务器发送请求的用户数 具体的性能监控指标如何和业务指标进行转换就先略过了。 推荐几个公有云云产品这些公有云产品是基本上都会用到的，历经检验，且比较实用的产品。 云服务器 关系型数据库 负载均衡 对象存储 VPC（Virtual Private Cloud）：专有网络 CDN Redis 安全类的基本产品（如：安全组、ACL、漏扫、WAF、DDoS 防护等） 计算云服务器配置以中低配为主云服务器一般用于承载应用，推荐以更多台数的中低配为主，避免资源的浪费。建议一般配置不要超过：16C32G，主流配置为： 4C8G 甚至更低 8C16G 推荐使用容器服务容器服务有诸多优势，推荐无状态应用使用容器服务。 资源粒度更细，细粒度到: 0.1C, 内存 MB。 自动扩缩容更方便 扩容后 pod 自动加入负载均衡 … 按需购买在云上，不要按照业务峰值购买全量的资源，而是推荐：买满足日常需求的资源 弹性扩容在云上，如果需要搞活动，再通过「容器」或「API + 镜像 + 快照」批量购买、弹性扩容。 比如在某手机的秒杀活动中，会瞬间开启 200 台机器且持续 2H 来应对，IT 资源花费 600 元人民币： 搭建好环境，制作好镜像； 活动前通过 API 秒开 200 台服务器来应对活动； 活动结束后，通过 API 瞬间释放资源 这在传统架构中，根本不可想象。比如传统架构，搞「双十一」，都要提前一个月准备 IT 资源。 另外上面的场景也可以利用 「弹性伸缩服务」或「容器 HPA」来动态调整。 使用 Ansible 等 DevOps 工具既然云的优势是「分布式」，资源多，那么 Ansible 这种批量的 DevOps 工具是必不可少的，可以大幅度提升效率。具体应用，可以通过 Ansible，定制对应的 Playbook，自动化批量安装和运维。 通过镜像提升云端部署效率先开通一台云服务器，并对这台云服务器做运维规范方面的系统调优、安全加固等措施。然后把这台云服务器做成一个基础镜像，批量开通 其他同样环境的服务器，可以大大提升部署效率。 网络域名备案要先行上云的最后一步，是要将域名的 IP 解析到 负载均衡 公网 IP 上。但需要提前在共有云上对域名进行备案，如果到最后域名解析到公有云上后才发现域名被拉黑，业务访问被拒绝，这将会变得非常麻烦。因此需要提前通过公有云进行域名备案，或者已经在其他供应商进行备案，那么需要将域名备案转接入公有云。 推荐必备 .CN 域名近期国际形势愈演愈烈，中美摩擦进一步升级，形势紧张。要做最坏的打算：美国可能会断掉您的 .COM 域名的解析。另外国家最近有指引，不要使用外国管控的根域名作为基础设施的一级域名。.cn 是国家根域，.com.cn、[net.cn]、[org.cn] 等这些都是可以的。 严禁每台服务器都能访问公网出于安全（受攻击面太大）和成本（公网 IP 都是钱）的考虑。而且没必要，如果是业务访问，入向通过负载均衡进来，出向通过 NAT 网关出去。如果是运维，推荐通过 VPN + 跳板机（中小企业）或专线 + 堡垒机（大企业）来实现运维管理。 如果需要出公网，建议使用 NAT 网关而非在某台机器绑定公网 IP原因：可靠性更高，更安全。 利用低成本高负载的按量带宽对于中小规模企业，如果您的系统经常搞活动，网络负载差距很大，那么推荐：「大带宽按量付费」而不是「固定带宽固定计费」，比如：「1Gbps 峰值带宽按量计费」对比「100Mbps 固定带宽」： 费用可能更低 带宽更大，活动期间可能会超过 100Mbps，那这时候固定带宽就会影响用户体验，而 1Gbps 峰值带宽是完全可以支撑的住的。 以某客户上云前后为例，在 IDC 机房，200Mbps 的独享电信带宽，一年的成本大概是 1Mbps/100 元 / 月 x 12 个月 x 200 = 24 万。而在云端，采用 1Gbps 峰值的 BGP 多线 SLB 带宽，在带宽质量上面提升了几个量级。另外，带宽费用采用按量付费，大大降低了维护成本。 推荐使用云上软负载均衡推荐使用公有云提供的负载均衡，可以作为反向代理，防止客户端直连云服务器带来的安全和稳定性风险。 加入 负载均衡 可以保障架构灵活扩展性：加入 负载均衡 后，架构变得更加灵活。典型场景是将所有域名先绑定到 负载均衡 上，然后转到后端 Nginx，通过 Nginx 做虚拟主机等七层更灵活的控制。 高并发情况下，推荐使用 4 层负载均衡采用 4 层 负载均衡 保障性能：在实践中，面对高并发性能的场景时，发现 7 层的负载均衡，相比 4 层的负载均衡，在性能上面有很大差距。7 层负载均衡只能达到万级别并发，而 4 层的负载均衡能达到几十万级，甚至上百万级的并发量。所以在电商等网站应用中，对于 负载均衡，优先选择 TCP 层。四层 LB 能扛得住 10w-50w 的并发。 DNS 记录调整要注意用户的 DNS TTL 我们是无法控制的，如果调整了某域名的 DNS 记录，可能某些用户已生效，某些用户没有生效。针对这种情况，需要在原有 IP 上做 302 重定向跳转，将依旧访问原 IP 的客户引流到新 IP 上，这将大大提高用户的访问体验。 大型企业 - DNS 负载均衡实践大规模应用。当后端有一两百台云服务器，而一台负载均衡 性能有限时，可以采用多个 负载均衡，前边通过 DNS 负载均衡。典型如：淘宝、阿里云官网。 DNS 有个最大的问题，就是 本地 DNS 缓存。 可以让 DNS TTL 生效快一点； DNS 配置的是负载均衡 IP，而不是云服务器的 IP。 如果还是有部分用户出问题，指导用户清理 DNS 缓存，或强制绑定本机 host 指向域名解析。 智能解析 – 跨地域分布式架构中必不可少。根据 ClientIP，选择返回对应地域、运营商的 IP。 运营商线路解析如：DNS 记录： 默认线路：电信服务器 IP 网通：网通 IP 移动：移动 IP 教育网：教育网 IP 海外：海外 IP 如果有 BGP 线路，那就更简单了： 默认线路：负载均衡的公网 IP 地域线路解析如：用户请求访问域名，DNS 自动判断访问者 IP 是「上海联通」还是「北京联通」，然后智能返回设置的对应的「上海联通」和「北京联通」的服务器 IP 地址完成域名解析。 海外：可以选择「海外、海外大洲、海外（国家 / 地区）」来细分解析。 如: 海外 - 亚洲地区 - 新加坡线路：指向新加坡服务器的 IP 海外 - 北美洲 - 美国线路：指向美国服务器的 IP 海外 - 欧洲 - 德国线路：指向德国服务器的 IP 默认线路：指向新加坡服务器的 IP CDN 就是智能解析的最佳实践存储云上善用「对象存储服务」云上建议尽量不要使用类 NAS 的共享文件存储服务，而应该使用 对象存储服务 来替代。在传统环境，NAS 的典型使用场景如下： 负载均衡：使用 LB + 多台 云服务器（如：Web 服务器）部署的业务。多台 云服务器 需要访问同一个存储空间，以便多台 云服务器 共享数据。 替代方案：直接使用普通云数据盘，通过 DevOps 等工具实现批量部署及数据一致。 代码共享：多台 云服务器 应用，部署的代码一致。将代码放在同一个存储空间，提供给多台 云服务器 同时访问。代码集中管理。 替代方案：代码放在代码仓库中集中管理。 日志共享：多台 云服务器 应用，需要将日志写入到同一个存储空间，以便做集中的日志数据处理和分析 替代方案：日志定期存储到对象存储中，可以根据策略、冷热数据的实际情况选择分别存储到「标准对象存储」、「低频对象存储」和「归档存储」中进一步压缩成本；或直接使用云上的「日志服务」。 企业办公文件共享场景：企业有公共的文件需要共享给多组业务使用，需要集中的共享存储来存放数据。 替代方案：使用对象存储来替代。 容器服务的场景：部署的容器服务需要共享访问某个文件数据源，特别是在资源编排的容器服务。对应的容器可能会在不同的服务器中进行服务漂移，所以文件共享访问尤为重要。 替代方案：这种场景确实需要用到云上文件系统服务。 备份的场景：用户希望将数据备份到云上，可以通过挂载文件系统来存储数据备份。 替代方案：备份到对象存储的「归档存储」中，进一步降低成本。 错误用法：NGINX 做公网转发到对象存储在某个客户场景中，静态资源放到 对象存储 中，前端对静态资源的请求通过 Nginx 反向代理转发给 对象存储。但这种做法，在云端架构上是不推荐的，因为它会带来几个问题： 访问静态资源的流量走 云服务器 的带宽流量，特别是中大型的 Web 应用中。流量走 云服务器 的带宽，很可能出现性能瓶颈。 Nginx 是通过公网将请求反向代理转发给 对象存储 的，所以在网络传输上会影响速度性能。 通过 Nginx 反向代理，不仅增加运维成本，还要维护 Nginx 配置文件等。 所以，添加 Nginx 做反向代理是多此一举。云端不推荐这么做。该客户这么用，主要原因是业务代码侧，静态资源的请求，都是通过目录划分。如果将静态资源单独放在二级域名，跨域等问题代码侧没很好地解决，从而产生这种不伦不类的架构。最终在业务代码侧进行了优化调整，对 对象存储 静态资源的使用规范如下： 业务侧使用单独的二级域名来管理静态资源（如：&lt;pic-cdn.ewhisper.cn&gt;)，静态资源统一放在 对象存储 中； 静态资源的二级域名直接将 CNAME 绑定在 对象存储 的 URL 地址上（访问量很少的情况下），这样就直接跳过「使用 Nginx 做反向代理」这个冗余的步骤了 如果想要进一步提升 对象存储 中存放的静态资源的访问速度，可以无缝接入 CDN。 CDN 的回源请求，会直接通过内网回源请求 对象存储 中的源数据。相比 Nginx 反向代理走公网请求 对象存储，速度和效率会提升得更高，价格特定情况下也会更划算。 👉 典型使用场景：CDN 对象存储 数据库数据库推荐云服务 且必须有高可用保障数据库不推荐自建，推荐直接使用云提供商的相关数据库服务，且推荐必备高可用保障，如集群模式或多副本，以及数据备份。数据库优先采用云提供商的相关数据库服务 ，低成本高效率：如果在云上购买云服务器自建 MySQL 主从部署并维护的模式，使得后期的维护管理成本很大。即我们要监控及维护主从状态，并且在出现问题时需要及时处理，保障业务对数据库读写的连续性。在采用云提供商的相关数据库服务 后，这些问题都可以自动化解决。即对数据库主从的监控、备份、后期维护、故障切换等，都是全自动。 对于可靠性要求特别高的 DB，可以选择跨 AZ 高可用的集群方案对于可靠性要求特别高的 DB，可以选择跨 AZ 高可用的集群方案。比如：Redis、MongoDB、MySQL 都有类似的跨 AZ 高可用的集群方案提供。 按需选择合适的数据库数据库多种多样，根据自己的实际需求进行选择，以下列出部分： 关系型数据库 MySQL SQL Server Postgresql MariaDB 分布式数据库（如 OceanBase 或 TDSQL 等） 非关系型：内存数据库 Redis Memcache 文档数据库：MongoDB 列数据库 HBase 等 时序数据库 InfluxDB TSDB CDN典型使用场景：CDN + 对象存储 数据分发：适用于搭建下载行为较多的 APP、音视频平台、网站等，用户可结合 CDN + 对象存储 的能力，将静态内容（包括音视频、图片等文件）托管在对象存储中，并将热点文件提前下发至 CDN 边缘节点，降低下载访问延迟 网站托管：适用于官方网站等偏静态的站点，将网站的静态资源快速托管存储在对象存储中，同时通过 CDN + 对象存储 分发，通过 CDN 配置的域名作为静态网站访客的访问地址入口，快速建好一个网站 安全必须设置强密码典型如：MongoDB、Redis、ES，默认无密码或弱密码，已经发生过多轮、大规模的数据泄露事件，所以针对这些服务，一定要设置强密码。至于云服务器、云账户、关系型数据库等，更是要保障强密码或者更强力的安全措施。 客户端访问必须 HTTPS这个就不多说了。 给域名申请证书，放在 Nginx 或 LB 上 管理。 业务侧，保留 HTTP 80 端口，做 80 -&gt; 443 的重定向。LB 上 80 和 443 端口监听都要开启。 一定要配置安全组和 ACL最基本的安全防护 不要 root 直连不要 root 直连，用普通用户，登陆过去按需 sudo 切换到 root 建议暴露公网的 SSH 端口不要用 22建议不要用默认的 22 端口，防止被扫描。另外还有建议用证书认证等方式，就不一一赘述了。 免费安全产品别忘领如每开通一台云服务器，都会赠送一些免费额度的「DDoS 防护和主机安全防护」。有基本的防护，会比裸奔安全很多。","categories":[{"name":"公有云","slug":"公有云","permalink":"http://zhangyu.info/categories/%E5%85%AC%E6%9C%89%E4%BA%91/"}],"tags":[{"name":"公有云","slug":"公有云","permalink":"http://zhangyu.info/tags/%E5%85%AC%E6%9C%89%E4%BA%91/"}]},{"title":"为什么互联网大厂一边大规模裁员又一边招聘","slug":"为什么互联网大厂一边大规模裁员又一边招聘","date":"2022-04-22T16:00:00.000Z","updated":"2022-06-09T04:56:47.045Z","comments":true,"path":"2022/04/23/为什么互联网大厂一边大规模裁员又一边招聘/","link":"","permalink":"http://zhangyu.info/2022/04/23/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BA%92%E8%81%94%E7%BD%91%E5%A4%A7%E5%8E%82%E4%B8%80%E8%BE%B9%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%A3%81%E5%91%98%E5%8F%88%E4%B8%80%E8%BE%B9%E6%8B%9B%E8%81%98/","excerpt":"","text":"作者：东岳老师 https://www.zhihu.com/people/tian-tian-quan-75-47 真实在大厂工作过，十几年的互联网老兵告诉你事实。 大厂里面有很多的业务线，也有很多的部门，每个部门负责的都不一样，阿里不是只做淘宝，腾讯也不是只做微信，一个大厂有数百条业务线，有的赚钱，有的赔钱。但是通常赔钱的最多。 通常大厂是这样玩的: 上层领导看中了一个方向，比如说游戏是赚钱的，于是就大量开始招聘游戏岗位。 公司往这方面投钱，比如说一年投入1000万，然后制定一个目标，实现三年盈利。通过招聘，你顺利进入了他们的游戏业务线，成为大厂员工。 光环加顶，有些刚进入大厂的员工觉以为祖坟冒青烟了，但也可能冒黑烟。 因为公司业务线刚开辟，所以就大量招人，高层来赌这个业务三年后一定盈利。把人力，金钱，物力投入进去。至于高层哪来的那么大自信赌成功，说真的，他们也都是懵逼状态。 高层之间也内卷，总裁副总裁一大批，负责的业务都不同。 我在某厂做总监时，经常跟一些总裁开会讨论方案，他们真的啥也不懂。因为这群人年龄太大了，很多都是投资人，根本不懂互联网，都是瞎指挥。 懂互联网的也是极个别人，总裁眼里只有钱。其他都没有。各种不着调的想法每次和他们开完会恨不能摔门而去。 但你还得执行他们不靠谱的想法，谁让咱们是打工人。就算方案是屎也得给老板做出臭豆腐的口感。实现不了怎么办，假装很努力的加班啊，得让老板看到咱很努力的干啊。 高层画大饼不要紧，咱也得吃饭啊。 高层天天开会，传达给中层的就TM一句话，今年我们要实现十个亿的目标! 卧槽，毛线都没有呢，那怎么办，上有政策下有对策，中层也有办法，你们知道中层每天都在干什么吗，天天写PPT!天天给高层做实现十个亿目标的汇报，一次一次被打回重改，直到改到高层认为PPT可以真的能实现十个亿目标位置。 PPT是个好东西，不仅能造车，还能造梦! 很遗憾的是，高层和中层一本正经的搞了PPT很长时间，大家一致认为从PPT上面已经证明能够实现十个亿的目标。 但多数业务实际根本不可能像预想的那样盈利，PPT终究是PPT，当不能盈利时，这条业务线就会被砍掉，你经常发现大厂裁员是整条业务线从上到下全部被裁掉就是这个问题。 老板层总是有各种想法，每年都要想我要做什么，总之都是为了赚钱。 有的老板都是拍脑门，反正人家有钱，玩得起。瞎折腾怕什么，万一折腾成功了呢。就像有个人说的那样，梦想总是有的，万一实现了呢。 把钱往里面一投资，找一堆写手，做几篇新闻，搞几个概念，开始忽悠了。全都给我上，产品，技术，运营，招起来!上层要干什么事情呢，上层要拿着PPT去资本市场忽悠钱。 有一次和某个总裁喝酒，无意之间他说了一句话，我们不这样的话，股票就会跌的，我们只有这样做股票才能涨。 我忽然恍然大悟，其实他们根本不是做互联网的，他们就是一群在股市圈钱的人。是的，只要折腾起来，股民才会不断的被割韭菜。原来赔掉的钱可以通过股市赚回来!资本市场才是真正赚钱的地方。 互联网从诞生开始，就是靠资本一轮一轮融资的吹起来的，互联网公司本质就是投资公司，而高层就是投资人，靠着一个一个的故事融钱，用钱赚钱，而至于这个业务，他们并不关心能不能做成。他们只关心手里的股票能不能升值。 但是问题是，你不可能所有开辟的业务线都赔钱吧，股东也不是傻子啊，靠股市画大饼早晚会被做空。所以他们总得有点赚钱的业务啊。 十条业务线九条可能都赔钱，但有一条业务线赚钱就成功。公司高层都是赌徒心理，因为他们也没办法判断哪个业务能成功。多生几个孩子，总有一个孩子能成器吧。 不成器的孩子怎么办，放弃吧。然后继续生孩子，继续招人。大厂靠着自己的招牌不用担心招不到人，反正人人都想进大厂。 就算全都裁掉，照样能够招到。就算赔掉一个亿对大厂来说只不过是交学费而已。毕竟人家赚一个亿也就是小目标。 那问题在于，为什么他们不用原有的团队做呢，因为原有团队在高层看来就是败军之将，给你三年时间都没搞成你在高层眼里已经没有价值，不裁你才怪呢。老板看你不顺眼，他们眼中只有一个单词：loser! 这类新闻屡见不鲜: 字节跳动方面，本地生活和房产业务受到影响。去年10月，字节跳动本地生活被曝出从22个城市撤退，仅保留了北京、上海等几个城市。 字节跳动HR相关负责人回应媒体称，裁员信息属实，系公司正常业务调整。 大部分的公司都是公司业务正常调整，简单来说就是，这孩子不成器，赶出家门。 那你说，我这条业务线搞成了，那总不会被裁吧。 呵呵，你想的天真了，一条业务线搞成了照样裁掉一半，这叫组织人员优化，本身做这条业务线就需要大量人参与进来，就像你建一座桥，建设时候需要几千人，建完了只需要几十人维护就可以了。 那你说，我只要努力就不会被裁吧，呵呵，裁掉你和你努力不努力无关系，什么末尾淘汰制只不过是裁你的理由，制定一个规则，让员工内卷，因为员工内卷对企业最有好处。 只有裁员，才能让员工感到危机。 你虽然花费了大量时间精力，什么996啊，公司是不会看在眼里的，公司只看你成本太高了。 三十五岁为什么会被裁，你知道，你在一家公司干十年你的薪资得有多高，不给你加薪资你不满意，给你加薪资老板不满意，反正有的是人干活，这么高薪资不需要你了，就裁掉了。 与你能力无关系。只和你成本有关系。 在大厂的业务线，中层压力最大，因为裁员先把中层干掉。中层在公司的定位就是背锅的，中层一般都是总监或者级别副总裁级，负责承上启下，只要业务快玩完了，为了给公司交代，稳定军心，高层首先要把中层拿来祭旗了。 中层天天要逼着底层加班，也并不是真的很忙，因为他要做给高层看，让高层觉得他很努力，一定能成功。但是中层消息也很灵敏，见势不妙，没等裁员就脚底抹油提前跑路了。 跑之前中层这群老油条们还得给底层没有经验的职场小白PUA，兄弟们，挺住，困难只是暂时的。只要团结一心，一定可以的！ 你会说中层难道不想要补偿吗？呵呵，你太小看中层了，在业务没有倒闭前另谋其主，还TM能吹牛皮一把说这业务做的很成功。 你看，离开我就倒闭了吧。真要耗到业务干倒闭了拿那个裁员补偿，对他们来说找工作都不好找。中层早就提前谋划好了出路，重要的人该走的都走光了。 多说句中层的话题，中层之间也经常在一起喝酒，不同的业务线之间也会互相交流经验，我参加过很多聚餐，喝酒前大家牛皮吹一波，我们做的是十个亿的大项目，这算什么，我们做的是一百亿的大项目。 你们说的都不是什么，我们定下目标一千亿。酒过三巡，真情流露，大家互相安慰，兄弟，早撤吧，我看透了，这活没希望。 等你有一天做了中层，就知道中层才是互联网公司最苦逼的，上面领导骂你，底下员工骂你，回家老婆骂你，辞职不敢，没有一边讨好。多少底层想要往上爬到中层，等你爬上去，就知道这哪是人生巅峰，是TM火山口! 上也上不去，下也下不来。每件事处理起来都是贼烫手。做中层久了就知道，有些事不能硬撑，关键时候跑路才是上上策，孙子兵法得作为案头书天天阅读，不然你怎么在这么复杂的环境中生存下去。 这就是你看到的类似新闻: 某某大厂某事业部负责人离职，加入某某公司。 但你看到这种消息后一般还不到裁员时候，因为裁员需要一个过程。 三个月后，轮到HR上场了，HR会在一夜之间发个通告，因某某原因，公司无法经营，宣布裁员，底层员工被打的措手不及! 前一天还在加班到凌晨十二点，这时候你忽然看到HR也在加班，你心里想，嗯，公司又开始招人了。 公司肯定发展越来越好了。其实人家加班是制定裁员名单呢，今天一上班就被告知裁员。没等你反应过来，整个部门都没有了。 HR才是互联网公司效率最高的职位!昨天，还许诺你加薪，今天你一脸懵逼的发呆，看着同事一个一个打包离开。 HR就告诉你一句话，今天必须走。按劳动合同，给你n+1补偿。其实发布裁员公告之前，所有准备都已经提前三个月准备好了。连给你n+1的钱都准备好了。 有的大厂裁员也很有情怀，临走还发给职场小白一个毕业证，同学!恭喜你在某厂顺利毕业了! 呵呵，有的应届生刚入职第一天就毕业了，这速度真TM的快啊!给我的50万年薪呢？这么快就没了?咱好歹号称是大厂啊，别这样糊弄人行不？ 这时候你看到的新闻就是: 某厂内部员工在某APP传闻裁员，整个事业部都被裁撤，未经官方证实。 于是一轮从招聘到裁员的过程就结束了，宣布一条不靠谱的业务线彻底消失，老板的大饼没有画成。 从项目立项，到招人，到投资扩建，到疯狂炒概念，再到负责人离职，内部传出裁员，公司证实属实。 众位朋友，等你经历互联网十年你就知道这种招聘裁员戏天天上演。只是大部分都没有爆出来而已。因为很多业务线都不起眼，还没有人知道就已经消亡了。 然后公司继续开辟新业务线，继续靠着大厂这个招牌白嫖打工者的青春，反正你不来是有别人来，我反正是给钱的，你不做还有别人做。真招不到人就开始画大饼，给应届生开高薪。 一毕业就来个年薪五十万，卧槽，我这工作十几年的都没一个应届生薪资高，你招他来做什么？ 后来我明白，很多人都是凑数的。裁员的时候容易点。要招我这种老油条，连签合同我都得看三遍，敢裁我，分分钟给你讲劳动法。别TM忽悠我，罗翔的刑法讲义我天天看。 应届生就容易多了，签合同都不看一眼，裁你时候给你一个绩效不合格，他们还觉得自己没有尽全力，对不起公司。也不用n+1补偿，因为连n都没有。看起来薪资那么高，其实用工成本真的很廉价。很多都是做给外界看的。 其实很多人，不过是陪着高层赌未来，高层赌不对没关系，可以继续赌，毕竟人家不会担心自己被裁掉。本身高层眼里也没有员工，只有利益，员工自己堵不对，只能被裁了。 老板赌上的是钱，员工陪赌的是未来。老板赌输了钱，大不了再赌一把，员工赌没了未来，就真的啥也没有了。有的员工连命都赔赌进去了。 所谓某些互联网大厂，也不过是披着一层炫酷的外衣，进去也是996的工作。因为你得陪着老板赌这种不靠谱的未来。 只是，你人生需要做的是淡定！看庭前花开花落，云卷云舒，莫纠结！很多事情，对打工人来说都很无奈，最后苦的还是打工人。 你方唱罢我登场，今天他被裁，明天你被裁，也只是打工人的命运。与你能力高低真没关系。如果有关系，那你不过是个背锅侠而已。裁你，也只是杀鸡给猴看。 领导说，我们给社会每年输送一千人才。呵呵，确实是这样，陪你玩几年，你赚的盆满钵满，我们成了人才。两全其美，何乐不为呢。 我TM混了互联网十五年不是做人才就是走在做人才的路上! 青春就那么几年，你从小到大都是很优秀，拿着985的学历拼进大厂，恋爱都不敢谈，每天996的为公司奋斗，养着房东，养着这个城市，养着身体几十万亿的细胞。 不到三十岁的小伙子头发秃顶，肾虚无力，腰间盘突出，赌上了自己一切，用自己拼搏卷走了无数人，梦想有一天出人头地，你做好了最优秀的自己，无一刻休息，只为了明天更好，最后结局被裁了。最后如梦初醒，才知道自己是小丑。 真实，上面的话都不敢说，我敢说只因为我不在大厂干了，也不想再进去。就算封杀我也无所谓。我坚决反对拿员工前途做赌注的公司! 我要说这些话，也希望企业裁员慎重考虑!同时也希望企业不要盲目的招聘，因为真的很多同学因为你们的招聘赌上了未来。我不怕得罪那么多的大厂，我只希望彼此都真诚些，大家都是为了更好的未来。 我没有针对任何大厂，我只说一种现象，希望我写这些不会被封掉。 人生苦短，善待自己!","categories":[{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/categories/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"}],"tags":[{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/tags/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"}]},{"title":"Linux的CPU上下文切换深入探讨","slug":"Linux的CPU上下文切换深入探讨","date":"2022-04-22T16:00:00.000Z","updated":"2022-04-23T15:03:30.595Z","comments":true,"path":"2022/04/23/Linux的CPU上下文切换深入探讨/","link":"","permalink":"http://zhangyu.info/2022/04/23/Linux%E7%9A%84CPU%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8/","excerpt":"","text":"https://mp.weixin.qq.com/s/3fJvAjgPmi6N8XUQ4QII4w 链接：https://medium.com/geekculture/linux-cpu-context-switch-deep-dive-764bfdae4f01 我们都知道 Linux 是一个多任务操作系统，它支持的任务同时运行的数量远远大于 CPU 的数量。当然，这些任务实际上并不是同时运行的（Single CPU），而是因为系统在短时间内将 CPU 轮流分配给任务，造成了多个任务同时运行的假象。 CPU 上下文（CPU Context）在每个任务运行之前，CPU 需要知道在哪里加载和启动任务。这意味着系统需要提前帮助设置 CPU 寄存器和程序计数器。 CPU 寄存器是内置于 CPU 中的小型但速度极快的内存。程序计数器用于存储 CPU 正在执行的或下一条要执行指令的位置。 它们都是 CPU 在运行任何任务之前必须依赖的依赖环境，因此也被称为 “CPU 上下文”。如下图所示： 知道了 CPU 上下文是什么，我想你理解 CPU 上下文切换就很容易了。“CPU上下文切换”指的是先保存上一个任务的 CPU 上下文（CPU寄存器和程序计数器），然后将新任务的上下文加载到这些寄存器和程序计数器中，最后跳转到程序计数器。 这些保存的上下文存储在系统内核中，并在重新安排任务执行时再次加载。这确保了任务的原始状态不受影响，并且任务似乎在持续运行。 CPU 上下文切换的类型你可能会说 CPU 上下文切换无非就是更新 CPU 寄存器和程序计数器值，而这些寄存器是为了快速运行任务而设计的，那为什么会影响 CPU 性能呢？ 在回答这个问题之前，请问，你有没有想过这些“任务”是什么？你可能会说一个任务就是一个进程或者一个线程。是的，进程和线程正是最常见的任务，但除此之外，还有其他类型的任务。 别忘了硬件中断也是一个常见的任务，硬件触发信号，会引起中断处理程序的调用。 因此，CPU 上下文切换至少有三种不同的类型： 进程上下文切换 线程上下文切换 中断上下文切换 让我们一一来看看。 进程上下文切换Linux 按照特权级别将进程的运行空间划分为内核空间和用户空间，分别对应下图中 Ring 0 和 Ring 3 的 CPU 特权级别的 。 内核空间（Ring 0）拥有最高权限，可以直接访问所有资源 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备。它必须通过系统调用被陷入（trapped）内核中才能访问这些特权资源。 从另一个角度看，一个进程既可以在用户空间也可以在内核空间运行。当一个进程在用户空间运行时，称为该进程的用户态，当它落入内核空间时，称为该进程的内核态。 从用户态到内核态的转换需要通过系统调用来完成。例如，当我们查看一个文件的内容时，我们需要以下系统调用： open()：打开文件 read()：读取文件的内容 write()：将文件的内容写入到输出文件（包括标准输出） close()：关闭文件 那么在上述系统调用过程中是否会发生 CPU 上下文切换呢？当然是的。 这需要先保存 CPU 寄存器中原来的用户态指令的位置。接下来，为了执行内核态的代码，需要将 CPU 寄存器更新到内核态指令的新位置。最后是跳转到内核态运行内核任务。 那么系统调用结束后，CPU 寄存器需要恢复原来保存的用户状态，然后切换到用户空间继续运行进程。 因此，在一次系统调用的过程中，实际上有两次 CPU 上下文切换。 但需要指出的是，系统调用进程不会涉及进程切换，也不会涉及虚拟内存等系统资源切换。这与我们通常所说的“进程上下文切换”不同。进程上下文切换是指从一个进程切换到另一个进程，而系统调用期间始终运行同一个进程 系统调用过程通常被称为特权模式切换，而不是上下文切换。但实际上，在系统调用过程中，CPU 的上下文切换也是不可避免的。 进程上下文切换 vs 系统调用那么进程上下文切换和系统调用有什么区别呢？首先，进程是由内核管理的，进程切换只能发生在内核态。因此，进程上下文不仅包括虚拟内存、栈和全局变量等用户空间资源，还包括内核栈和寄存器等内核空间的状态。 所以进程上下文切换比系统调用要多出一步： 在保存当前进程的内核状态和 CPU 寄存器之前，需要保存进程的虚拟内存、栈等；并加载下一个进程的内核状态。 根据 Tsuna 的测试报告，每次上下文切换需要几十纳秒至微秒的 CPU 时间。这个时间是相当可观的，尤其是在大量进程上下文切换的情况下，很容易导致 CPU 花费大量时间来保存和恢复寄存器、内核栈、虚拟内存等资源。这正是我们在上一篇文章中谈到的，一个导致平均负载上升的重要因素。 那么，该进程何时会被调度/切换到在 CPU 上运行？其实有很多场景，下面我为大家总结一下： 当一个进程的 CPU 时间片用完时，它会被系统挂起，并切换到其他等待 CPU 运行的进程。 当系统资源不足（如内存不足）时，直到资源充足之前，进程无法运行。此时进程也会被挂起，系统会调度其他进程运行。 当一个进程通过 sleep 函数自动挂起自己时，自然会被重新调度。 当优先级较高的进程运行时，为了保证高优先级进程的运行，当前进程会被高优先级进程挂起运行。 当发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 了解这些场景是非常有必要的，因为一旦上下文切换出现性能问题，它们就是幕后杀手。 线程上下文切换线程和进程最大的区别在于，线程是任务调度的基本单位，而进程是资源获取的基本单位。 说白了，内核中所谓的任务调度，实际的调度对象是线程；而进程只为线程提供虚拟内存和全局变量等资源。所以，对于线程和进程，我们可以这样理解： 当一个进程只有一个线程时，可以认为一个进程等于一个线程 当一个进程有多个线程时，这些线程共享相同的资源，例如虚拟内存和全局变量。 此外，线程也有自己的私有数据，比如栈和寄存器，在上下文切换时也需要保存。 这样，线程的上下文切换其实可以分为两种情况： 首先，前后两个线程属于不同的进程。此时，由于资源不共享，切换过程与进程上下文切换相同。 其次，前后两个线程属于同一个进程。此时，由于虚拟内存是共享的，所以切换时虚拟内存的资源保持不变，只需要切换线程的私有数据、寄存器等未共享的数据。 显然，同一个进程内的线程切换比切换多个进程消耗的资源要少。这也是多线程替代多进程的优势。 中断上下文切换除了前面两种上下文切换之外，还有另外一种场景也输出 CPU 上下文切换的，那就是中断。 为了快速响应事件，硬件中断会中断正常的调度和执行过程，进而调用中断处理程序。 在中断其他进程时，需要保存进程的当前状态，以便中断后进程仍能从原始状态恢复。 与进程上下文不同，中断上下文切换不涉及进程的用户态。因此，即使中断进程中断了处于用户态的进程，也不需要保存和恢复进程的虚拟内存、全局变量等用户态资源。 另外，和进程上下文切换一样，中断上下文切换也会消耗 CPU。过多的切换次数会消耗大量的 CPU 资源，甚至严重降低系统的整体性能。因此，当您发现中断过多时，需要注意排查它是否会对您的系统造成严重的性能问题。 结论综上所述，无论哪种场景导致上下文切换，你都应该知道： CPU 上下文切换是保证 Linux 系统正常运行的核心功能之一，一般不需要我们特别关注。 但是过多的上下文切换会消耗 CPU 的时间来保存和恢复寄存器、内核栈、虚拟内存等数据，从而缩短进程的实际运行时间，导致系统整体性能显着下降。","categories":[{"name":"linux","slug":"linux","permalink":"http://zhangyu.info/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://zhangyu.info/tags/linux/"}]},{"title":"微服务架构及设计模式","slug":"微服务架构及设计模式","date":"2022-04-22T16:00:00.000Z","updated":"2022-04-23T14:12:56.733Z","comments":true,"path":"2022/04/23/微服务架构及设计模式/","link":"","permalink":"http://zhangyu.info/2022/04/23/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E5%8F%8A%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"https://colstuwjx.github.io/2020/01/%E7%BF%BB%E8%AF%91-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E5%8F%8A%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/ 原文：https://medium.com/@madhukaudantha/microservice-architecture-and-design-patterns-for-microservices-e0e5013fd58a 本文介绍了主流常见的微服务模式。 微服务能够对企业产生积极影响。因此，了解如何处理微服务架构（MSA）以及一些微服务设计模式，一个微服务架构的一些通用目标或者设计原则是很有价值的。下面是在微服务架构方案中值得考虑的四个目标[1]。 1、缩减成本：MSA将会降低设计、实现和维护IT服务的总体成本 2、加快发布速度：MSA将会加快服务从想法到部署的落地速度 3、增强弹性：MSA将会提升我们服务网络的弹性 4、开启可见性：MSA支持为服务和网络提供更好的可见性 你需要了解建设微服务架构背后的几个设计原则： 可扩展性 可用性 韧性 灵活性 独立自主性，自治性 去中心化治理 故障隔离 自动装配 通过 DevOps 持续交付 听取上述原则，在你实施的解决方案或系统付诸实践的同时，这也会带来一些挑战和问题。这些问题在许多解决方案中也很常见。使用正确及匹配的设计模式可以克服这些问题。微服务有一些设计模式，这可以大体分为五类。每类都包含许多具体的设计模式。下图展示了这些设计模式。 图1 微服务设计模式 分解模式按业务功能进行分解说白了，微服务就是要应用单一职责原则，把服务改造成松耦合式的。它可以按照业务功能进行分解。定义和业务功能相对应的服务。业务功能是一个来自业务架构建模 [2] 的概念。它是一个企业为了创造价值而要去做的某些事情。一个业务功能往往对应于一个业务对象，比如： 订单管理负责订单 客户管理则是负责客户 按问题子域进行分解按照业务功能来分解一个应用程序可能会是一个不错的开始，但是你终将会遇到所谓的“神类”，它很难再被分解。这些类将在多个服务之间都是通用的。可以定义一些和领域驱动设计（DDD）里面的子域相对应的服务。DDD 把应用程序的问题空间 —— 也即是业务 —— 称之为域。一个域由多个子域组成。每个子域对应业务的各个不同部分。 子域可以分为如下几类： 核心 —— 业务的核心竞争力以及应用程序最有价值的部分 支撑 —— 和业务有关但并不是一个核心竞争力。这些可以在内部实现也可以外包 通用 —— 不特定于业务，而且在理想情况下可以使用现成的软件实现 一个订单管理的子域包括： 产品目录服务 库存管理服务 订单管理服务 配送管理服务 按事务/两阶段提交（2pc）模式进行分解你可以通过事务分解服务。然后，这样一来系统里将会存在多个事务。事务处理协调器[3]是分布式事务处理的重要参与者之一。分布式事务包括两个步骤： 准备阶段 —— 在这个阶段，事务的所有参与者都准备提交并通知协调员他们已准备好完成事务 提交或回滚阶段 —— 在这个阶段，事务协调器向所有参与者发出提交或回滚命令 2PC 的问题在于，和单个微服务的运行时间相比，它显得相当慢。即便这些微服务跑在相同的网络里，它们之间的事务协调也确实会减慢系统速度，因此这种方法通常不适用于高负载情况。 绞杀者模式（Strangler Pattern）上面三种，我们看到的这几个设计模式都是用来分解绿场（Greenfield）的应用程序，但是往往我们所做的工作中有 80％ 是针对灰场（brownfield）应用程序，它们是一些大型的单体应用程序（历史遗留的代码库）。绞杀者模式可以解决这类问题。它会创建两个单独的应用程序，它们并排跑在同一个 URI 空间里。随着时间的流逝，直到最后，新重构的应用程序会“干掉”或替换原有的应用程序，此时就可以关掉那个老的单体应用程序。绞杀应用程序的步骤分别是转换，共存和消除[4]： 转换（Transform） —— 使用现代方法创建一个并行的全新站点。 共存（Coexist） —— 让现有站点保留一段时间。把针对现有站点的访问重定向到新站点，以便逐步实现所需功能。 消除（Eliminate） —— 从现有站点中删除旧功能。 隔舱模式（Bulkhead Pattern）让一个应用程序的元素和池子相对隔离，这样一来，其他应用程序将可以继续正常工作。这种模式被称为“隔舱”，因为它类似于船体的分段分区。根据使用者负载和可用性要求，将服务实例分成不同的组。这种设计有助于隔离故障，并允许用户即使在故障期间仍可为某些使用者维持服务。 边车模式该模式将一个应用程序的组件部署到一个单独的处理器容器里以提供隔离和封装。它还允许应用程序由异构的组件和技术组成。这种模式被称为边车模式（Sidecar），因为它类似于连接到摩托车的侧边车。在该模式中，侧边车会附加到父应用程序，并为该应用程序提供功能支持。Sidecar 还与父应用程序共享相同的生命周期，并与父应用程序一起创建和退出。Sidecar 模式有时也称为 sidekick 模式，这是我们在文章中列出的最后一个分解模式。 集成模式API 网关模式当一个应用程序被分解成多个较小的微服务时，这里会出现一些需要解决的问题： 存在不同渠道对多个微服务的多次调用 需要处理不同类型的协议 不同的消费者可能需要不同的响应格式 API 网关有助于解决微服务实现引发的诸多问题，而不仅限于上述提到的这些。 API 网关是任何微服务调用的单一入口点 它可以用作将请求路由到相关微服务的代理服务 它可以汇总结果并发送回消费者 该解决方案可以为每种特定类型的客户端创建一个细粒度的 API 它还可以转换协议请求并做出响应 它也可以承担微服务的身份验证/授权的责任。 聚合器模式（Aggregator Pattern）将业务功能分解成几个较小的逻辑代码段后就有必要考虑如何协同每个服务返回的数据。不能把这个职责留给消费者。 聚合器模式有助于解决这个问题。它讨论了如何聚合来自不同服务的数据，然后将最终响应发送给消费者。这里有两种实现方式[6]： 1、一个组合微服务将调用所有必需的微服务，合并数据，然后在发送回数据之前对其进行转换合成 2、一个 API 网关还可以将请求划分成多个微服务，然后在将数据发送给使用者之前汇总数据 如果要应用一些业务逻辑的话，建议选择一个组合式的微服务。除此之外，API 网关作为这个问题的解决方案已经是既定的事实标准。 代理模式针对 API 网关，我们只是借助它来对外公开我们的微服务。引入 API 网关后，我们得以获得一些像安全性和对 API 进行分类这样的 API 层面功能。在这个例子里，API 网关有三个 API 模块： 1、移动端 API，它实现了 FTGO 移动客户端的 API 2、浏览器端 API，它实现了在浏览器里运行的 JavaScript 应用程序的 API 3、公共API，它实现了一些第三方开发人员需要的 API 网关路由模式API 网关负责路由请求。一个 API 网关通过将请求路由到相应的服务来实现一些 API 操作。当 API 网关接收到请求时，它会查询一个路由映射，该路由映射指定了将请求路由到哪个服务。一个路由映射可以将一个 HTTP 方法和路径映射到服务的 HTTP URL。这种做法和像 NGINX 这样的 Web 服务器提供的反向代理功能一样。 链式微服务模式（Chained Microservice Pattern）单个服务或者微服务将会有多级依赖，举个例子：Sale 的微服务依赖 Product 微服务和 Order 微服务。链式微服务设计模式将帮助你提供合并后的请求结果。microservice-1 接收到请求后，该请求随后与 microservice-2 进行通信，还有可能正在和 microservice-3 通信。所有这些服务都是同步调用。 分支模式一个微服务可能需要从包括其他微服务在内的多个来源获取数据。分支微服务模式是聚合器和链式设计模式的混合，并允许来自两个或多个微服务的同时请求/响应处理。调用的微服务可以是一个微服务链。分支模式还可用于根据你的业务需求调用不同的微服务链或单个链。 客户端UI组合模式通过分解业务功能/子域来开发服务时，负责用户体验的服务必须从多个微服务中提取数据。在一个单体世界里，过去只有一个从 UI 到后端服务的调用，它会检索所有数据然后刷新/提交 UI 页面。但是，现在不一样了。对于微服务而言，我们必须把 UI 设计成一个具有屏幕/页面的多个板块/区域的框架。每个板块都将调用一个单独的后端微服务以提取数据。诸如 AngularJS 和 ReactJS 之类的框架可以帮助我们轻松地实现这一点。这些屏幕称为单页应用程序（SPA）。每个团队都开发一个客户端 UI 组件，比如一个 AngularJS 指令，该组件实现其服务的页面/屏幕区域。UI 团队负责通过组合多个特定服务的 UI 组件来实现构建页面/屏幕的页面框架。 数据库模式给微服务定义数据库架构时，我们需要考虑以下几点： 1、服务必须是松耦合的。这样它们可以独立开发，部署和扩展 2、业务事务可能会强制跨越多个服务的不变量 3、一些业务事务需要查询多个服务的数据 4、为了可扩展性考虑，数据库有时候必须是可复制和共享的 5、不同服务存在不同的数据存储要求 每个服务一套数据库为了解决上述问题，必须为每个微服务设计一个数据库。它必须仅专用于该服务。应当只能通过微服务的 API 访问它。其他服务无法直接访问它。比如，针对关系型数据库，我们可以采用每个服务使用单独的专用表（private-tables-per-service），每个服务单独的数据库模式（schema-per-service）或每个服务单独的数据库服务器（database-server-per-service）。 服务之间共享数据库我们已经说过，在微服务里，为每个服务分配一套单独的数据库是理想方案。采用共享数据库在微服务里属于反模式。但是，如果应用程序是一个单体应用而且试图拆分成微服务，那么反正规化就不那么容易了。在后面的阶段里，我们可以转到每个服务一套数据库的模式，直到我们完全做到了这一点。服务之间共享数据库并不理想，但是对于上述情况，它是一个切实可行的解决方案。大多数人认为这是微服务的反模式，但是对于灰场应用程序，这是将应用程序分解成更小逻辑部分的一个很好的开始。值得一提的是，这不应当应用于绿场应用程序。 命令和查询职责分离 (CQRS)一旦实现了每个服务分配单独一套数据库（database-per-service），自然就会产生查询需求，这需要联合来自多个服务的数据。然而这是不可能的。CQRS 建议将应用程序分成两部分 —— 命令端和查询端。 命令端处理创建，更新和删除请求 查询端通过使用物化视图来处理查询部分 这通常会搭配事件驱动模式（event sourcing pattern）一起使用，一旦有任何数据更改便会创建对应的事件。通过订阅事件流，我们便可以让物化视图保持更新。 事件驱动绝大多数应用程序需要用到数据，典型的做法就是应用程序要维护当前状态。例如，在传统的创建，读取，更新和删除（CRUD）模型中，典型的数据流程是从存储中读取数据。它也包含了经常使用事务导致锁定数据的限制。 事件驱动模式[7]定义了一种方法，用于处理由一系列事件驱动的数据操作，每个事件都记录在一个 append-only 的存储中。应用程序代码向事件存储发送一系列事件，这些事件命令式的描述了对数据执行的每个操作，它们会被持久化到事件存储。每个事件代表一组数据更改（例如，AddedItemToOrder）。 这些事件将保留在充当记录系统的一个事件存储里。事件存储发布的事件的典型用途是在应用程序触发的一些动作更改实体时维护这些实体的物化视图，以及与外部系统集成。例如，一个系统可以维护一个用于填充 UI 部分所有客户订单的物化视图。当应用程序添加新订单，添加或删除订单中的项目以及添加运输信息时，描述这些更改的事件将会得到处理并用于更新物化视图。下图展示了该模式的一个概览。 图2 事件驱动模式[8] Saga模式当每个服务都有它们自己的数据库，并且一个业务事务跨越多个服务时，我们该如何确保各个服务之间的数据一致性呢？ 每个请求都有一个补偿请求，它会在请求失败时执行。这可以通过两种方式实现： 编舞（Choreography） —— 在没有中央协调的情况下，每个服务都会生成并侦听另一个服务的事件，并决定是否应该采取措施。编舞是一种指定两个或多个参与方的方案。任何一方都无法控制对方的流程，或者对这些流程有任何可见性，无法协调他们的活动和流程以共享信息和值。当需要跨控制/可见性域进行协调时，请使用编舞的方式。参考一个简单场景，你可以把编舞看作和网络协议类似。它规定了各方之间可接受的请求和响应模式。 图3 Saga模式 —— 编舞 编排（Orchestration） —— 一个编排器（对象）会负责 saga 的决策和业务逻辑排序。此时你可以控制流程中的所有参与者。当它们全部处于一个控制域时，你可以控制该活动的流程。当然，这通常是你被指派到一个拥有控制权的组织里制定业务流程。 图4 Saga模式 —— 编排 可观测性模式日志聚合考虑一个应用程序包含多个服务的用例。请求通常跨越多个服务实例。每个服务实例均采用标准格式生成日志文件。我们需要一个集中式的日志记录服务，该服务可以汇总每个服务实例的日志。用户可以搜索和分析日志。他们可以配置在某些消息出现在日志中时触发告警。例如，PCF 就有日志聚合器，它在应用侧从 PCF 平台的每个组件（router、controller、diego等）收集日志。AWS Cloud Watch 也是这样做的。 性能指标当服务组合由于引入了微服务架构而增加时，保持对事务的监控就变得尤为关键了，如此一来就可以监控这些模式，而当有问题发生时便会发送告警。 此外，需要一个度量服务来收集有关单个操作的统计信息。它应当聚合一个应用服务的指标数据，它会用来报告和告警。这里有两种用于汇总指标的模型： 推送 —— 服务将指标推送到指标服务，例如 NewRelic，AppDynamics 提取 —— 指标服务从服务中提取指标，例如 Prometheus 分布式链路追踪在微服务架构里，请求通常跨越多个服务。每个服务通过跨越多个服务执行一个或多个操作来处理请求。在排障时，有一个 Trace ID 是很有帮助的，我们可以端对端地跟踪一个请求。 解决方案便是引入一个事务ID。可以采用如下方式： 为每个外部请求分配一个唯一的外部请求ID 将外部请求ID传递给处理该请求链路的所有服务 在所有日志消息中加入该外部请求ID 健康检查实施微服务架构后，服务可能会出现启动了但是无法处理事务的情况。每个服务都需要有一个可用于检查应用程序运行状况的 API 端点，例如 /health。该 API 应该检查主机的状态，与其他服务/基础设施的连接以及任何其他特定的逻辑。 横切关注点模式（Cross-Cutting Concern Patterns）外部配置一个服务通常还会调用其他服务和数据库。对于dev，QA，UAT，Prod等每个环境而言，API 端点的 URL 或某些配置属性可能会有所不同。这些属性中的任何一个更改都可能需要重新构建和重新部署服务。 为避免代码修改，可以使用配置。把所有配置放到外面，包括端点 URL 和证书。应用程序应该在启动时或运行时加载它们。这些可以在启动时由应用程序访问，也可以在不重新启动服务器的情况下进行刷新。 服务发现模式在微服务出现时，我们需要在调用服务方面解决一些问题。 借助容器技术，IP地址可以动态地分配给服务实例。每次地址更改时，消费端服务都会中断并且需要手动更改。 对于消费端服务来说，它们必须记住每个上游服务的 URL ，这就变成紧耦合了。 为此，需要创建一个服务注册中心，该注册表将保留每个生产者服务的元数据和每个服务的配置。服务实例在启动时应当注册到注册中心，而在关闭时应当注销。服务发现有两种类型： 客户端：例如：Netflix Eureka 服务端：例如：AWS ALB 图5 服务发现[9] 熔断器模式一个服务通常会通过调用其他服务来检索数据，而这时候下游服务可能已经挂了。这样的话，有两个问题：首先，请求将继续抵达挂了的服务，耗尽网络资源，并且降低性能。其次，用户体验将是糟糕且不可预测的。 消费端服务应通过代理来调用远程服务，该代理的表现和一个电流断路器类似。当连续的故障数超过阈值时，断路器将跳闸，并且在超时期间内，所有调用远程服务的尝试都会立即失败。超时到期后，断路器将允许有限数量的测试请求通过。如果这些请求成功，断路器则将恢复正常运行。否则，如果发生故障的话，超时时间则将再次重新开始计算。如果某些操作失败概率很高的话，采取此模式有助于防止应用程序在故障发生后仍然不断尝试调用远程服务或访问共享资源。 图6 熔断器模式[10] 蓝绿部署模式使用微服务架构时，一个应用可以被拆分成许多个微服务。如果我们采用停止所有服务然后再部署改进版本的方式的话，宕机时间将是非常可观的，并且会影响业务。同样，回滚也将是一场噩梦。 蓝绿部署模式可以避免这种情况。 实施蓝绿部署策略可以用来减少或消除宕机。它通过运行两个相同的生产环境，Blue 和Green 来实现这一目标。 假设 Green 是现有的活动实例，Blue 是该应用程序的新版本。在任何时候，只有一个环境处于活动状态，该活动环境为所有生产流量提供服务。所有云平台均提供了用于实施蓝绿部署的选项。 图7 蓝绿部署模式 参考链接[1] “Microservice Architecture: Aligning Principles, Practices, and Culture” Book by Irakli Nadareishvili, Matt McLarty, and Michael Amundsen [2] https://microservices.io/patterns/decomposition/decompose-by-business-capability.html [3] https://www.baeldung.com/transactions-across-microservices [4] https://developer.ibm.com/articles/cl-strangler-application-pattern-microservices-apps-trs/ [5] https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead [6] https://dzone.com/articles/design-patterns-for-microservices [7] https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs#event-sourcing-and-cqrs [8] https://docs.microsoft.com/en-us/azure/architecture/patterns/event-sourcing [9] https://www.dineshonjava.com/microservices-with-spring-boot/ [10] https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker","categories":[{"name":"架构","slug":"架构","permalink":"http://zhangyu.info/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://zhangyu.info/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"打造一套客户端功能最全的APM监控系统","slug":"打造一套客户端功能最全的APM监控系统","date":"2022-04-22T16:00:00.000Z","updated":"2022-04-23T14:24:50.485Z","comments":true,"path":"2022/04/23/打造一套客户端功能最全的APM监控系统/","link":"","permalink":"http://zhangyu.info/2022/04/23/%E6%89%93%E9%80%A0%E4%B8%80%E5%A5%97%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8A%9F%E8%83%BD%E6%9C%80%E5%85%A8%E7%9A%84APM%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"打造一套客户端功能最全的 APM 监控系统 杭城小刘 发布于 2021-07-02 APM 是 Application Performance Monitoring 的缩写，监视和管理软件应用程序的性能和可用性。应用性能管理对一个应用的持续稳定运行至关重要。所以这篇文章就从一个 iOS App 的性能管理的纬度谈谈如何精确监控以及数据如何上报等技术点 App 的性能问题是影响用户体验的重要因素之一。性能问题主要包含：Crash、网络请求错误或者超时、UI 响应速度慢、主线程卡顿、CPU 和内存使用率高、耗电量大等等。大多数的问题原因在于开发者错误地使用了线程锁、系统函数、编程规范问题、数据结构等等。解决问题的关键在于尽早的发现和定位问题。 本篇文章着重总结了 APM 的原因以及如何收集数据。APM 数据收集后结合数据上报机制，按照一定策略上传数据到服务端。服务端消费这些信息并产出报告。请结合姊妹篇， 总结了如何打造一款灵活可配置、功能强大的数据上报组件。 一、卡顿监控卡顿问题，就是在主线程上无法响应用户交互的问题。影响着用户的直接体验，所以针对 App 的卡顿监控是 APM 里面重要的一环。 FPS（frame per second）每秒钟的帧刷新次数，iPhone 手机以 60 为最佳，iPad 某些型号是 120，也是作为卡顿监控的一项参考参数，为什么说是参考参数？因为它不准确。先说说怎么获取到 FPS。CADisplayLink 是一个系统定时器，会以帧刷新频率一样的速率来刷新视图。 [CADisplayLink displayLinkWithTarget:self selector:@selector(###:)]。至于为什么不准我们来看看下面的示例代码 _displayLink = [CADisplayLink displayLinkWithTarget:self selector:@selector(p_displayLinkTick:)];[_displayLink setPaused:YES];[_displayLink addToRunLoop:[NSRunLoop currentRunLoop] forMode:NSRunLoopCommonModes]; 代码所示，CADisplayLink 对象是被添加到指定的 RunLoop 的某个 Mode 下。所以还是 CPU 层面的操作，卡顿的体验是整个图像渲染的结果：CPU + GPU。请继续往下看 1. 屏幕绘制原理 讲讲老式的 CRT 显示器的原理。 CRT 电子枪按照上面方式，从上到下一行行扫描，扫面完成后显示器就呈现一帧画面，随后电子枪回到初始位置继续下一次扫描。为了把显示器的显示过程和系统的视频控制器进行同步，显示器（或者其他硬件）会用硬件时钟产生一系列的定时信号。当电子枪换到新的一行，准备进行扫描时，显示器会发出一个水平同步信号（horizonal synchronization），简称 HSync；当一帧画面绘制完成后，电子枪恢复到原位，准备画下一帧前，显示器会发出一个垂直同步信号（Vertical synchronization），简称 VSync。显示器通常以固定的频率进行刷新，这个固定的刷新频率就是 VSync 信号产生的频率。虽然现在的显示器基本都是液晶显示屏，但是原理保持不变。 通常，屏幕上一张画面的显示是由 CPU、GPU 和显示器是按照上图的方式协同工作的。CPU 根据工程师写的代码计算好需要现实的内容（比如视图创建、布局计算、图片解码、文本绘制等），然后把计算结果提交到 GPU，GPU 负责图层合成、纹理渲染，随后 GPU 将渲染结果提交到帧缓冲区。随后视频控制器会按照 VSync 信号逐行读取帧缓冲区的数据，经过数模转换传递给显示器显示。 在帧缓冲区只有一个的情况下，帧缓冲区的读取和刷新都存在效率问题，为了解决效率问题，显示系统会引入2个缓冲区，即双缓冲机制。在这种情况下，GPU 会预先渲染好一帧放入帧缓冲区，让视频控制器来读取，当下一帧渲染好后，GPU 直接把视频控制器的指针指向第二个缓冲区。提升了效率。 目前来看，双缓冲区提高了效率，但是带来了新的问题：当视频控制器还未读取完成时，即屏幕内容显示了部分，GPU 将新渲染好的一帧提交到另一个帧缓冲区并把视频控制器的指针指向新的帧缓冲区，视频控制器就会把新的一帧数据的下半段显示到屏幕上，造成画面撕裂的情况。 为了解决这个问题，GPU 通常有一个机制叫垂直同步信号（V-Sync），当开启垂直同步信号后，GPU 会等到视频控制器发送 V-Sync 信号后，才进行新的一帧的渲染和帧缓冲区的更新。这样的几个机制解决了画面撕裂的情况，也增加了画面流畅度。但需要更多的计算资源 答疑 可能有些人会看到「当开启垂直同步信号后，GPU 会等到视频控制器发送 V-Sync 信号后，才进行新的一帧的渲染和帧缓冲区的更新」这里会想，GPU 收到 V-Sync 才进行新的一帧渲染和帧缓冲区的更新，那是不是双缓冲区就失去意义了？ 设想一个显示器显示第一帧图像和第二帧图像的过程。首先在双缓冲区的情况下，GPU 首先渲染好一帧图像存入到帧缓冲区，然后让视频控制器的指针直接直接这个缓冲区，显示第一帧图像。第一帧图像的内容显示完成后，视频控制器发送 V-Sync 信号，GPU 收到 V-Sync 信号后渲染第二帧图像并将视频控制器的指针指向第二个帧缓冲区。 看上去第二帧图像是在等第一帧显示后的视频控制器发送 V-Sync 信号。是吗？真是这样的吗？ 😭 想啥呢，当然不是。 🐷 不然双缓冲区就没有存在的意义了 揭秘。请看下图 当第一次 V-Sync 信号到来时，先渲染好一帧图像放到帧缓冲区，但是不展示，当收到第二个 V-Sync 信号后读取第一次渲染好的结果（视频控制器的指针指向第一个帧缓冲区），并同时渲染新的一帧图像并将结果存入第二个帧缓冲区，等收到第三个 V-Sync 信号后，读取第二个帧缓冲区的内容（视频控制器的指针指向第二个帧缓冲区），并开始第三帧图像的渲染并送入第一个帧缓冲区，依次不断循环往复。 请查看资料，需要梯子：Multiple buffering 2. 卡顿产生的原因 VSync 信号到来后，系统图形服务会通过 CADisplayLink 等机制通知 App，App 主线程开始在 CPU 中计算显示内容（视图创建、布局计算、图片解码、文本绘制等）。然后将计算的内容提交到 GPU，GPU 经过图层的变换、合成、渲染，随后 GPU 把渲染结果提交到帧缓冲区，等待下一次 VSync 信号到来再显示之前渲染好的结果。在垂直同步机制的情况下，如果在一个 VSync 时间周期内，CPU 或者 GPU 没有完成内容的提交，就会造成该帧的丢弃，等待下一次机会再显示，这时候屏幕上还是之前渲染的图像，所以这就是 CPU、GPU 层面界面卡顿的原因。 目前 iOS 设备有双缓存机制，也有三缓冲机制，Android 现在主流是三缓冲机制，在早期是单缓冲机制。iOS 三缓冲机制例子 CPU 和 GPU 资源消耗原因很多，比如对象的频繁创建、属性调整、文件读取、视图层级的调整、布局的计算（AutoLayout 视图个数多了就是线性方程求解难度变大）、图片解码（大图的读取优化）、图像绘制、文本渲染、数据库读取（多读还是多写乐观锁、悲观锁的场景）、锁的使用（举例：自旋锁使用不当会浪费 CPU）等方面。开发者根据自身经验寻找最优解（这里不是本文重点）。 3. APM 如何监控卡顿并上报CADisplayLink 肯定不用了，这个 FPS 仅作为参考。一般来讲，卡顿的监测有2种方案：监听 RunLoop 状态回调、子线程 ping 主线程 3.1 RunLoop 状态监听的方式RunLoop 负责监听输入源进行调度处理。比如网络、输入设备、周期性或者延迟事件、异步回调等。RunLoop 会接收2种类型的输入源：一种是来自另一个线程或者来自不同应用的异步消息（source0事件）、另一种是来自预定或者重复间隔的事件。 RunLoop 状态如下图 第一步：通知 Observers，RunLoop 要开始进入 loop，紧接着进入 loop if (currentMode-&gt;_observerMask &amp; kCFRunLoopEntry ) // 通知 Observers: RunLoop 即将进入 loop __CFRunLoopDoObservers(rl, currentMode, kCFRunLoopEntry);// 进入loopresult = __CFRunLoopRun(rl, currentMode, seconds, returnAfterSourceHandled, previousMode); 第二步：开启 do while 循环保活线程，通知 Observers，RunLoop 触发 Timer 回调、Source0 回调，接着执行被加入的 block if (rlm-&gt;_observerMask &amp; kCFRunLoopBeforeTimers) // 通知 Observers: RunLoop 即将触发 Timer 回调 __CFRunLoopDoObservers(rl, rlm, kCFRunLoopBeforeTimers);if (rlm-&gt;_observerMask &amp; kCFRunLoopBeforeSources) // 通知 Observers: RunLoop 即将触发 Source 回调 __CFRunLoopDoObservers(rl, rlm, kCFRunLoopBeforeSources);// 执行被加入的block__CFRunLoopDoBlocks(rl, rlm); 第三步：RunLoop 在触发 Source0 回调后，如果 Source1 是 ready 状态，就会跳转到 handle_msg 去处理消息。 // 如果有 Source1 (基于port) 处于 ready 状态，直接处理这个 Source1 然后跳转去处理消息if (MACH_PORT_NULL != dispatchPort &amp;&amp; !didDispatchPortLastTime) {#if DEPLOYMENT_TARGET_MACOSX || DEPLOYMENT_TARGET_EMBEDDED || DEPLOYMENT_TARGET_EMBEDDED_MINI msg = (mach_msg_header_t *)msg_buffer; if (\\_\\_CFRunLoopServiceMachPort(dispatchPort, &amp;msg, sizeof(msg\\_buffer), &amp;livePort, 0, &amp;voucherState, NULL)) &#123; goto handle\\_msg; &#125; #elif DEPLOYMENT_TARGET_WINDOWS if (__CFRunLoopWaitForMultipleObjects(NULL, &amp;dispatchPort, 0, 0, &amp;livePort, NULL)) { goto handle_msg; }#endif} 第四步：回调触发后，通知 Observers 即将进入休眠状态 Boolean poll = sourceHandledThisLoop || (0ULL == timeout_context-&gt;termTSR);// 通知 Observers: RunLoop 的线程即将进入休眠(sleep)if (!poll &amp;&amp; (rlm-&gt;_observerMask &amp; kCFRunLoopBeforeWaiting)) __CFRunLoopDoObservers(rl, rlm, kCFRunLoopBeforeWaiting); __CFRunLoopSetSleeping(rl); 第五步：进入休眠后，会等待 mach_port 消息，以便再次唤醒。只有以下4种情况才可以被再次唤醒。 基于 port 的 source 事件 Timer 时间到 RunLoop 超时 被调用者唤醒 do { if (kCFUseCollectableAllocator) &#123; // objc\\_clear\\_stack(0); // &lt;rdar://problem/16393959&gt; memset(msg\\_buffer, 0, sizeof(msg\\_buffer)); &#125; msg = (mach\\_msg\\_header\\_t \\*)msg\\_buffer; \\_\\_CFRunLoopServiceMachPort(waitSet, &amp;msg, sizeof(msg\\_buffer), &amp;livePort, poll ? 0 : TIMEOUT\\_INFINITY, &amp;voucherState, &amp;voucherCopy); if (modeQueuePort != MACH\\_PORT\\_NULL &amp;&amp; livePort == modeQueuePort) &#123; // Drain the internal queue. If one of the callout blocks sets the timerFired flag, break out and service the timer. while (\\_dispatch\\_runloop\\_root\\_queue\\_perform\\_4CF(rlm-&gt;\\_queue)); if (rlm-&gt;\\_timerFired) &#123; // Leave livePort as the queue port, and service timers below rlm-&gt;\\_timerFired = false; break; &#125; else &#123; if (msg &amp;&amp; msg != (mach\\_msg\\_header\\_t \\*)msg\\_buffer) free(msg); &#125; &#125; else &#123; // Go ahead and leave the inner loop. break; &#125; } while (1); 第六步：唤醒时通知 Observer，RunLoop 的线程刚刚被唤醒了 // 通知 Observers: RunLoop 的线程刚刚被唤醒了if (!poll &amp;&amp; (rlm-&gt;_observerMask &amp; kCFRunLoopAfterWaiting)) __CFRunLoopDoObservers(rl, rlm, kCFRunLoopAfterWaiting); // 处理消息 handle_msg:; __CFRunLoopSetIgnoreWakeUps(rl); 第七步：RunLoop 唤醒后，处理唤醒时收到的消息 如果是 Timer 时间到，则触发 Timer 的回调 如果是 dispatch，则执行 block 如果是 source1 事件，则处理这个事件 #if USE_MK_TIMER_TOO // 如果一个 Timer 到时间了，触发这个Timer的回调 else if (rlm-&gt;\\_timerPort != MACH\\_PORT\\_NULL &amp;&amp; livePort == rlm-&gt;\\_timerPort) &#123; CFRUNLOOP\\_WAKEUP\\_FOR\\_TIMER(); // On Windows, we have observed an issue where the timer port is set before the time which we requested it to be set. For example, we set the fire time to be TSR 167646765860, but it is actually observed firing at TSR 167646764145, which is 1715 ticks early. The result is that, when \\_\\_CFRunLoopDoTimers checks to see if any of the run loop timers should be firing, it appears to be &#39;too early&#39; for the next timer, and no timers are handled. // In this case, the timer port has been automatically reset (since it was returned from MsgWaitForMultipleObjectsEx), and if we do not re-arm it, then no timers will ever be serviced again unless something adjusts the timer list (e.g. adding or removing timers). The fix for the issue is to reset the timer here if CFRunLoopDoTimers did not handle a timer itself. 9308754 if (!\\_\\_CFRunLoopDoTimers(rl, rlm, mach\\_absolute\\_time())) &#123; // Re-arm the next timer \\_\\_CFArmNextTimerInMode(rlm, rl); &#125; &#125; #endif // 如果有dispatch到main\\_queue的block，执行block else if (livePort == dispatchPort) &#123; CFRUNLOOP\\_WAKEUP\\_FOR\\_DISPATCH(); \\_\\_CFRunLoopModeUnlock(rlm); \\_\\_CFRunLoopUnlock(rl); \\_CFSetTSD(\\_\\_CFTSDKeyIsInGCDMainQ, (void \\*)6, NULL); #if DEPLOYMENT_TARGET_WINDOWS void \\*msg = 0; #endif \\_\\_CFRUNLOOP\\_IS\\_SERVICING\\_THE\\_MAIN\\_DISPATCH\\_QUEUE\\_\\_(msg); \\_CFSetTSD(\\_\\_CFTSDKeyIsInGCDMainQ, (void \\*)0, NULL); \\_\\_CFRunLoopLock(rl); \\_\\_CFRunLoopModeLock(rlm); sourceHandledThisLoop = true; didDispatchPortLastTime = true; &#125; // 如果一个 Source1 (基于port) 发出事件了，处理这个事件 else &#123; CFRUNLOOP\\_WAKEUP\\_FOR\\_SOURCE(); // If we received a voucher from this mach\\_msg, then put a copy of the new voucher into TSD. CFMachPortBoost will look in the TSD for the voucher. By using the value in the TSD we tie the CFMachPortBoost to this received mach\\_msg explicitly without a chance for anything in between the two pieces of code to set the voucher again. voucher\\_t previousVoucher = \\_CFSetTSD(\\_\\_CFTSDKeyMachMessageHasVoucher, (void \\*)voucherCopy, os\\_release); CFRunLoopSourceRef rls = \\_\\_CFRunLoopModeFindSourceForMachPort(rl, rlm, livePort); if (rls) &#123; #if DEPLOYMENT_TARGET_MACOSX || DEPLOYMENT_TARGET_EMBEDDED || DEPLOYMENT_TARGET_EMBEDDED_MINI mach\\_msg\\_header\\_t \\*reply = NULL; sourceHandledThisLoop = \\_\\_CFRunLoopDoSource1(rl, rlm, rls, msg, msg-&gt;msgh\\_size, &amp;reply) || sourceHandledThisLoop; if (NULL != reply) &#123; (void)mach\\_msg(reply, MACH\\_SEND\\_MSG, reply-&gt;msgh\\_size, 0, MACH\\_PORT\\_NULL, 0, MACH\\_PORT\\_NULL); CFAllocatorDeallocate(kCFAllocatorSystemDefault, reply); &#125; #elif DEPLOYMENT_TARGET_WINDOWS sourceHandledThisLoop = \\_\\_CFRunLoopDoSource1(rl, rlm, rls) || sourceHandledThisLoop; #endif 第八步：根据当前 RunLoop 状态判断是否需要进入下一个 loop。当被外部强制停止或者 loop 超时，就不继续下一个 loop，否则进入下一个 loop if (sourceHandledThisLoop &amp;&amp; stopAfterHandle) { // 进入loop时参数说处理完事件就返回 retVal = kCFRunLoopRunHandledSource; } else if (timeout_context-&gt;termTSR &lt; mach_absolute_time()) { // 超出传入参数标记的超时时间了 retVal = kCFRunLoopRunTimedOut;} else if (__CFRunLoopIsStopped(rl)) { __CFRunLoopUnsetStopped(rl); // 被外部调用者强制停止了 retVal = kCFRunLoopRunStopped;} else if (rlm-&gt;_stopped) { rlm-&gt;_stopped = false; retVal = kCFRunLoopRunStopped;} else if (__CFRunLoopModeIsEmpty(rl, rlm, previousMode)) { // source/timer一个都没有 retVal = kCFRunLoopRunFinished;} 完整且带有注释的 RunLoop 代码见此处。 Source1 是 RunLoop 用来处理 Mach port 传来的系统事件的，Source0 是用来处理用户事件的。收到 Source1 的系统事件后本质还是调用 Source0 事件的处理函数。 RunLoop 6个状态 typedef CF_OPTIONS(CFOptionFlags, CFRunLoopActivity) { kCFRunLoopEntry , // 进入 loop kCFRunLoopBeforeTimers , // 触发 Timer 回调 kCFRunLoopBeforeSources , // 触发 Source0 回调 kCFRunLoopBeforeWaiting , // 等待 mach_port 消息 kCFRunLoopAfterWaiting ), // 接收 mach_port 消息 kCFRunLoopExit , // 退出 loop kCFRunLoopAllActivities // loop 所有状态改变} RunLoop 在进入睡眠前的方法执行时间过长而导致无法进入睡眠，或者线程唤醒后接收消息时间过长而无法进入下一步，都会阻塞线程。如果是主线程，则表现为卡顿。 一旦发现进入睡眠前的 KCFRunLoopBeforeSources 状态，或者唤醒后 KCFRunLoopAfterWaiting，在设置的时间阈值内没有变化，则可判断为卡顿，此时 dump 堆栈信息，还原案发现场，进而解决卡顿问题。 开启一个子线程，不断进行循环监测是否卡顿了。在 n 次都超过卡顿阈值后则认为卡顿了。卡顿之后进行堆栈 dump 并上报（具有一定的机制，数据处理在下一 part 讲）。 WatchDog 在不同状态下具有不同的值。 启动（Launch）：20s 恢复（Resume）：10s 挂起（Suspend）：10s 退出（Quit）：6s 后台（Background）：3min（在 iOS7 之前可以申请 10min；之后改为 3min；可连续申请，最多到 10min） 卡顿阈值的设置的依据是 WatchDog 的机制。APM 系统里面的阈值需要小于 WatchDog 的值，所以取值范围在 [1, 6] 之间，业界通常选择3秒。 通过 long dispatch_semaphore_wait(dispatch_semaphore_t dsema, dispatch_time_t timeout) 方法判断是否阻塞主线程，Returns zero on success, or non-zero if the timeout occurred. 返回非0则代表超时阻塞了主线程。 可能很多人纳闷 RunLoop 状态那么多，为什么选择 KCFRunLoopBeforeSources 和 KCFRunLoopAfterWaiting？因为大部分卡顿都是在 KCFRunLoopBeforeSources 和 KCFRunLoopAfterWaiting 之间。比如 Source0 类型的 App 内部事件等 Runloop 检测卡顿流程图如下： 关键代码如下： // 设置Runloop observer的运行环境CFRunLoopObserverContext context = {0, (__bridge void *)self, NULL, NULL};// 创建Runloop observer对象_observer = CFRunLoopObserverCreate(kCFAllocatorDefault, kCFRunLoopAllActivities, YES, 0, &amp;runLoopObserverCallBack, &amp;context);// 将新建的observer加入到当前thread的runloopCFRunLoopAddObserver(CFRunLoopGetMain(), _observer, kCFRunLoopCommonModes);// 创建信号_semaphore = dispatch_semaphore_create(0); __weak __typeof(self) weakSelf = self;// 在子线程监控时长dispatch_async(dispatch_get_global_queue(0, 0), ^{ __strong __typeof(weakSelf) strongSelf = weakSelf; if (!strongSelf) { return; } while (YES) { if (strongSelf.isCancel) { return; } // N次卡顿超过阈值T记录为一次卡顿 long semaphoreWait = dispatch_semaphore_wait(self-&gt;_semaphore, dispatch_time(DISPATCH_TIME_NOW, strongSelf.limitMillisecond * NSEC_PER_MSEC)); if (semaphoreWait != 0) { if (self-&gt;_activity == kCFRunLoopBeforeSources || self-&gt;_activity == kCFRunLoopAfterWaiting) { if (++strongSelf.countTime &lt; strongSelf.standstillCount){ continue; } // 堆栈信息 dump 并结合数据上报机制，按照一定策略上传数据到服务器。堆栈 dump 会在下面讲解。数据上报会在 [打造功能强大、灵活可配置的数据上报组件](https://github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1%20-%20iOS/1.80.md) 讲 } } strongSelf.countTime = 0; }}); 3.2 子线程 ping 主线程监听的方式开启一个子线程，创建一个初始值为0的信号量、一个初始值为 YES 的布尔值类型标志位。将设置标志位为 NO 的任务派发到主线程中去，子线程休眠阈值时间，时间到后判断标志位是否被主线程成功（值为 NO），如果没成功则认为主线程发生了卡顿情况，此时 dump 堆栈信息并结合数据上报机制，按照一定策略上传数据到服务器。数据上报会在 打造功能强大、灵活可配置的数据上报组件 讲 while (self.isCancelled == NO) { @autoreleasepool { __block BOOL isMainThreadNoRespond = YES; dispatch_semaphore_t semaphore = dispatch_semaphore_create(0); dispatch\\_async(dispatch\\_get\\_main\\_queue(), ^&#123; isMainThreadNoRespond = NO; dispatch\\_semaphore\\_signal(semaphore); &#125;); \\[NSThread sleepForTimeInterval:self.threshold\\]; if (isMainThreadNoRespond) &#123; if (self.handlerBlock) &#123; self.handlerBlock(); // 外部在 block 内部 dump 堆栈（下面会讲），数据上报 &#125; &#125; dispatch\\_semaphore\\_wait(semaphore, DISPATCH\\_TIME\\_FOREVER); &#125; &#125; 4. 堆栈 dump方法堆栈的获取是一个麻烦事。理一下思路。[NSThread callStackSymbols] 可以获取当前线程的调用栈。但是当监控到卡顿发生，需要拿到主线程的堆栈信息就无能为力了。从任何线程回到主线程这条路走不通。先做个知识回顾。 在计算机科学中，调用堆栈是一种栈类型的数据结构，用于存储有关计算机程序的线程信息。这种栈也叫做执行堆栈、程序堆栈、控制堆栈、运行时堆栈、机器堆栈等。调用堆栈用于跟踪每个活动的子例程在完成执行后应该返回控制的点。 维基百科搜索到 “Call Stack” 的一张图和例子，如下上图表示为一个栈。分为若干个栈帧（Frame），每个栈帧对应一个函数调用。下面蓝色部分表示 DrawSquare 函数，它在执行的过程中调用了 DrawLine 函数，用绿色部分表示。 可以看到栈帧由三部分组成：函数参数、返回地址、局部变量。比如在 DrawSquare 内部调用了 DrawLine 函数：第一先把 DrawLine 函数需要的参数入栈；第二把返回地址(控制信息。举例：函数 A 内调用函数 B，调用函数B 的下一行代码的地址就是返回地址)入栈；第三函数内部的局部变量也在该栈中存储。 栈指针 Stack Pointer 表示当前栈的顶部，大多部分操作系统都是栈向下生长，所以栈指针是最小值。帧指针 Frame Pointer 指向的地址中，存储了上一次 Stack Pointer 的值，也就是返回地址。 大多数操作系统中，每个栈帧还保存了上一个栈帧的帧指针。因此知道当前栈帧的 Stack Pointer 和 Frame Pointer 就可以不断回溯，递归获取栈底的帧。 接下来的步骤就是拿到所有线程的 Stack Pointer 和 Frame Pointer。然后不断回溯，还原案发现场。 5. Mach Task 知识Mach task: App 在运行的时候，会对应一个 Mach Task，而 Task 下可能有多条线程同时执行任务。《OS X and iOS Kernel Programming》 中描述 Mach Task 为：任务（Task）是一种容器对象，虚拟内存空间和其他资源都是通过这个容器对象管理的，这些资源包括设备和其他句柄。简单概括为：Mack task 是一个机器无关的 thread 的执行环境抽象。 作用： task 可以理解为一个进程，包含它的线程列表。 结构体：task_threads，将 target_task 任务下的所有线程保存在 act_list 数组中，数组个数为 act_listCnt kern_return_t task_threads( task_t traget_task, thread_act_array_t *act_list, //线程指针列表 mach_msg_type_number_t *act_listCnt //线程个数) thread_info: kern_return_t thread_info( thread_act_t target_act, thread_flavor_t flavor, thread_info_t thread_info_out, mach_msg_type_number_t *thread_info_outCnt); 如何获取线程的堆栈数据： 系统方法 kern_return_t task_threads(task_inspect_t target_task, thread_act_array_t *act_list, mach_msg_type_number_t *act_listCnt); 可以获取到所有的线程，不过这种方法获取到的线程信息是最底层的 mach 线程。 对于每个线程，可以用 kern_return_t thread_get_state(thread_act_t target_act, thread_state_flavor_t flavor, thread_state_t old_state, mach_msg_type_number_t *old_stateCnt); 方法获取它的所有信息，信息填充在 _STRUCT_MCONTEXT 类型的参数中，这个方法中有2个参数随着 CPU 架构不同而不同。所以需要定义宏屏蔽不同 CPU 之间的区别。 _STRUCT_MCONTEXT 结构体中，存储了当前线程的 Stack Pointer 和最顶部栈帧的 Frame pointer，进而回溯整个线程调用堆栈。 但是上述方法拿到的是内核线程，我们需要的信息是 NSThread，所以需要将内核线程转换为 NSThread。 pthread 的 p 是 POSIX 的缩写，表示「可移植操作系统接口」（Portable Operating System Interface）。设计初衷是每个系统都有自己独特的线程模型，且不同系统对于线程操作的 API 都不一样。所以 POSIX 的目的就是提供抽象的 pthread 以及相关 API。这些 API 在不同的操作系统中有不同的实现，但是完成的功能一致。 Unix 系统提供的 task_threads 和 thread_get_state 操作的都是内核系统，每个内核线程由 thread_t 类型的 id 唯一标识。pthread 的唯一标识是 pthread_t 类型。其中内核线程和 pthread 的转换（即 thread_t 和 pthread_t）很容易，因为 pthread 设计初衷就是「抽象内核线程」。 memorystatus_action_neededpthread_create 方法创建线程的回调函数为 nsthreadLauncher。 static void *nsthreadLauncher(void* thread){ NSThread *t = (NSThread*)thread; [nc postNotificationName: NSThreadDidStartNotification object:t userInfo: nil]; [t _setName: [t name]]; [t main]; [NSThread exit]; return NULL;} NSThreadDidStartNotification 其实就是字符串 @”_NSThreadDidStartNotification”。 &lt;NSThread: 0x…&gt;{number = 1, name = main} 为了 NSThread 和内核线程对应起来，只能通过 name 一一对应。 pthread 的 API pthread_getname_np 也可获取内核线程名字。np 代表 not POSIX，所以不能跨平台使用。 思路概括为：将 NSThread 的原始名字存储起来，再将名字改为某个随机数（时间戳），然后遍历内核线程 pthread 的名字，名字匹配则 NSThread 和内核线程对应了起来。找到后将线程的名字还原成原本的名字。对于主线程，由于不能使用 pthread_getname_np，所以在当前代码的 load 方法中获取到 thread_t，然后匹配名字。 static mach_port_t main_thread_id; (void)load { main_thread_id = mach_thread_self();} 二、 App 启动时间监控1. App 启动时间的监控应用启动时间是影响用户体验的重要因素之一，所以我们需要量化去衡量一个 App 的启动速度到底有多快。启动分为冷启动和热启动。 冷启动：App 尚未运行，必须加载并构建整个应用。完成应用的初始化。冷启动存在较大优化空间。冷启动时间从 application: didFinishLaunchingWithOptions: 方法开始计算，App 一般在这里进行各种 SDK 和 App 的基础初始化工作。 热启动：应用已经在后台运行（常见场景：比如用户使用 App 过程中点击 Home 键，再打开 App），由于某些事件将应用唤醒到前台，App 会在 applicationWillEnterForeground: 方法接受应用进入前台的事件 思路比较简单。如下 在监控类的 load 方法中先拿到当前的时间值 监听 App 启动完成后的通知 UIApplicationDidFinishLaunchingNotification 收到通知后拿到当前的时间 步骤1和3的时间差就是 App 启动时间。 mach_absolute_time 是一个 CPU/总线依赖函数，返回一个 CPU 时钟周期数。系统休眠时不会增加。是一个纳秒级别的数字。获取前后2个纳秒后需要转换到秒。需要基于系统时间的基准，通过 mach_timebase_info 获得。 mach_timebase_info_data_t g_apmmStartupMonitorTimebaseInfoData = 0;mach_timebase_info(&amp;g_apmmStartupMonitorTimebaseInfoData);uint64_t timelapse = mach_absolute_time() - g_apmmLoadTime;double timeSpan = (timelapse * g_apmmStartupMonitorTimebaseInfoData.numer) / (g_apmmStartupMonitorTimebaseInfoData.denom * 1e9); 2. 线上监控启动时间就好，但是在开发阶段需要对启动时间做优化。要优化启动时间，就先得知道在启动阶段到底做了什么事情，针对现状作出方案。 pre-main 阶段定义为 App 开始启动到系统调用 main 函数这个阶段；main 阶段定义为 main 函数入口到主 UI 框架的 viewDidAppear。 App 启动过程： 解析 Info.plist：加载相关信息例如闪屏；沙盒建立、权限检查； Mach-O 加载：如果是胖二进制文件，寻找合适当前 CPU 架构的部分；加载所有依赖的 Mach-O 文件（递归调用 Mach-O 加载的方法）；定义内部、外部指针引用，例如字符串、函数等；加载分类中的方法；c++ 静态对象加载、调用 Objc 的 +load() 函数；执行声明为 __attribute_((constructor)) 的 c 函数； 程序执行：调用 main()；调用 UIApplicationMain()；调用 applicationWillFinishLaunching()； Pre-Main 阶段 Main 阶段 2.1 加载 Dylib每个动态库的加载，dyld 需要 分析所依赖的动态库 找到动态库的 Mach-O 文件 打开文件 验证文件 在系统核心注册文件签名 对动态库的每一个 segment 调用 mmap（） 优化： 减少非系统库的依赖 使用静态库而不是动态库 合并非系统动态库为一个动态库 2.2 Rebase &amp;&amp; Binding优化： 减少 Objc 类数量，减少 selector 数量，把未使用的类和函数都可以删掉 减少 c++ 虚函数数量 转而使用 Swift struct（本质就是减少符号的数量） 2.3 Initializers优化： 使用 +initialize 代替 +load 不要使用过 attribute*((constructor)) 将方法显示标记为初始化器，而是让初始化方法调用时才执行。比如使用 dispatch_one、pthread_once() 或 std::once()。也就是第一次使用时才初始化，推迟了一部分工作耗时也尽量不要使用 c++ 的静态对象 2.4 pre-main 阶段影响因素 动态库加载越多，启动越慢。 ObjC 类越多，函数越多，启动越慢。 可执行文件越大启动越慢。 C 的 constructor 函数越多，启动越慢。 C++ 静态对象越多，启动越慢。 ObjC 的 +load 越多，启动越慢。 优化手段： 减少依赖不必要的库，不管是动态库还是静态库；如果可以的话，把动态库改造成静态库；如果必须依赖动态库，则把多个非系统的动态库合并成一个动态库 检查下 framework应当设为optional和required，如果该framework在当前App支持的所有iOS系统版本都存在，那么就设为required，否则就设为optional，因为optional会有些额外的检查 合并或者删减一些OC类和函数。关于清理项目中没用到的类，使用工具AppCode代码检查功能，查到当前项目中没有用到的类（也可以用根据linkmap文件来分析，但是准确度不算很高） 有一个叫做FUI的开源项目能很好的分析出不再使用的类，准确率非常高，唯一的问题是它处理不了动态库和静态库里提供的类，也处理不了C++的类模板 删减一些无用的静态变量 删减没有被调用到或者已经废弃的方法 将不必须在 +load 方法中做的事情延迟到 +initialize中，尽量不要用 C++ 虚函数(创建虚函数表有开销) 类和方法名不要太长：iOS每个类和方法名都在 __cstring 段里都存了相应的字符串值，所以类和方法名的长短也是对可执行文件大小是有影响的 因还是 Object-c 的动态特性，因为需要通过类/方法名反射找到这个类/方法进行调用，Object-c 对象模型会把类/方法名字符串都保存下来； 用 dispatch_once() 代替所有的 attribute((constructor)) 函数、C++ 静态对象初始化、ObjC 的 +load 函数； 在设计师可接受的范围内压缩图片的大小，会有意外收获。 压缩图片为什么能加快启动速度呢？因为启动的时候大大小小的图片加载个十来二十个是很正常的， 图片小了，IO操作量就小了，启动当然就会快了，比较靠谱的压缩算法是 TinyPNG。 2.5 main 阶段优化 减少启动初始化的流程。能懒加载就懒加载，能放后台初始化就放后台初始化，能延迟初始化的就延迟初始化，不要卡主线程的启动时间，已经下线的业务代码直接删除 优化代码逻辑。去除一些非必要的逻辑和代码，减小每个流程所消耗的时间 启动阶段使用多线程来进行初始化，把 CPU 性能发挥最大 使用纯代码而不是 xib 或者 storyboard 来描述 UI，尤其是主 UI 框架，比如 TabBarController。因为 xib 和 storyboard 还是需要解析成代码来渲染页面，多了一步。 3. 启动时间加速内存缺页异常？在使用中，访问虚拟内存的一个 page 而对应的物理内存缺不存在（没有被加载到物理内存中），则发生缺页异常。影响耗时，在几毫秒之内。 什么时候发生大量的缺页异常？一个应用程序刚启动的时候。 启动时所需要的代码分布在 VM 的第一页、第二页、第三页…，这样的情况下启动时间会影响较大，所以解决思路就是将应用程序启动刻所需要的代码（二进制优化一下），统一放到某几页，这样就可以避免内存缺页异常，则优化了 App 启动时间。 二进制重排提升 App 启动速度是通过「解决内存缺页异常」（内存缺页会有几毫秒的耗时）来提速的。 一个 App 发生大量「内存缺页」的时机就是 App 刚启动的时候。所以优化手段就是「将影响 App 启动的方法集中处理，放到某一页或者某几页」（虚拟内存中的页）。Xcode 工程允许开发者指定 「Order File」，可以「按照文件中的方法顺序去加载」，可以查看 linkMap 文件（需要在 Xcode 中的 「Buiild Settings」中设置 Order File、Write Link Map Files 参数）。 其实难点是如何拿到启动时刻所调用的所用方法？代码可能是 Swift、block、c、OC，所以hook 肯定不行、fishhook 也不行，用 clang 插桩可以满足需求。 三、 CPU 使用率监控1. CPU 架构CPU（Central Processing Unit）中央处理器，市场上主流的架构有 ARM（arm64）、Intel（x86）、AMD 等。其中 Intel 使用 CISC（Complex Instruction Set Computer），ARM 使用 RISC（Reduced Instruction Set Computer）。区别在于不同的 CPU 设计理念和方法。 早期 CPU 全部是 CISC 架构，设计目的是用最少的机器语言指令来完成所需的计算任务。比如对于乘法运算，在 CISC 架构的 CPU 上。一条指令 MUL ADDRA, ADDRB 就可以将内存 ADDRA 和内存 ADDRB 中的数香乘，并将结果存储在 ADDRA 中。做的事情就是：将 ADDRA、ADDRB 中的数据读入到寄存器，相乘的结果写入到内存的操作依赖于 CPU 设计，所以 CISC 架构会增加 CPU 的复杂性和对 CPU 工艺的要求。 RISC 架构要求软件来指定各个操作步骤。比如上面的乘法，指令实现为 MOVE A, ADDRA; MOVE B, ADDRB; MUL A, B; STR ADDRA, A;。这种架构可以降低 CPU 的复杂性以及允许在同样的工艺水平下生产出功能更加强大的 CPU，但是对于编译器的设计要求更高。 目前市场是大部分的 iPhone 都是基于 arm64 架构的。且 arm 架构能耗低。 2. 获取线程信息讲完了区别来讲下如何做 CPU 使用率的监控 开启定时器，按照设定的周期不断执行下面的逻辑 获取当前任务 task。从当前 task 中获取所有的线程信息（线程个数、线程数组） 遍历所有的线程信息，判断是否有线程的 CPU 使用率超过设置的阈值 假如有线程使用率超过阈值，则 dump 堆栈 组装数据，上报数据 线程信息结构体 struct thread_basic_info { time_value_t user_time; /* user run time（用户运行时长） */ time_value_t system_time; /* system run time（系统运行时长） */ integer_t cpu_usage; /* scaled cpu usage percentage（CPU使用率，上限1000） */ policy_t policy; /* scheduling policy in effect（有效调度策略） */ integer_t run_state; /* run state (运行状态，见下) */ integer_t flags; /* various flags (各种各样的标记) */ integer_t suspend_count; /* suspend count for thread（线程挂起次数） */ integer_t sleep_time; /* number of seconds that thread * has been sleeping（休眠时间） */}; 代码在讲堆栈还原的时候讲过，忘记的看一下上面的分析 thread_act_array_t threads;mach_msg_type_number_t threadCount = 0;const task_t thisTask = mach_task_self();kern_return_t kr = task_threads(thisTask, &amp;threads, &amp;threadCount);if (kr != KERN_SUCCESS) { return ;}for (int i = 0; i &lt; threadCount; i++) { thread_info_data_t threadInfo; thread_basic_info_t threadBaseInfo; mach_msg_type_number_t threadInfoCount; kern\\_return\\_t kr = thread\\_info((thread\\_inspect\\_t)threads\\[i\\], THREAD\\_BASIC\\_INFO, (thread\\_info\\_t)threadInfo, &amp;threadInfoCount); if (kr == KERN\\_SUCCESS) &#123; threadBaseInfo = (thread\\_basic\\_info\\_t)threadInfo; // todo：条件判断，看不明白 if (!(threadBaseInfo-&gt;flags &amp; TH\\_FLAGS\\_IDLE)) &#123; integer\\_t cpuUsage = threadBaseInfo-&gt;cpu\\_usage / 10; if (cpuUsage &gt; CPUMONITORRATE) &#123; NSMutableDictionary \\*CPUMetaDictionary = \\[NSMutableDictionary dictionary\\]; NSData \\*CPUPayloadData = \\[NSData data\\]; NSString \\*backtraceOfAllThread = \\[BacktraceLogger backtraceOfAllThread\\]; // 1. 组装卡顿的 Meta 信息 CPUMetaDictionary\\[@&quot;MONITOR\\_TYPE&quot;\\] = APMMonitorCPUType; // 2. 组装卡顿的 Payload 信息（一个JSON对象，对象的 Key 为约定好的 STACK\\_TRACE， value 为 base64 后的堆栈信息） NSData \\*CPUData = \\[SAFE\\_STRING(backtraceOfAllThread) dataUsingEncoding:NSUTF8StringEncoding\\]; NSString \\*CPUDataBase64String = \\[CPUData base64EncodedStringWithOptions:0\\]; NSDictionary \\*CPUPayloadDictionary = @&#123;@&quot;STACK\\_TRACE&quot;: SAFE\\_STRING(CPUDataBase64String)&#125;; NSError \\*error; // NSJSONWritingOptions 参数一定要传0，因为服务端需要根据 \\\\n 处理逻辑，传递 0 则生成的 json 串不带 \\\\n NSData \\*parsedData = \\[NSJSONSerialization dataWithJSONObject:CPUPayloadDictionary options:0 error:&amp;error\\]; if (error) &#123; APMMLog(@&quot;%@&quot;, error); return; &#125; CPUPayloadData = \\[parsedData copy\\]; // 3. 数据上报会在 \\[打造功能强大、灵活可配置的数据上报组件\\](https://github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1%20-%20iOS/1.80.md) 讲 \\[\\[HermesClient sharedInstance\\] sendWithType:APMMonitorCPUType meta:CPUMetaDictionary payload:CPUPayloadData\\]; &#125; &#125; &#125; } 四、 OOM 问题1. 基础知识准备硬盘：也叫做磁盘，用于存储数据。你存储的歌曲、图片、视频都是在硬盘里。 内存：由于硬盘读取速度较慢，如果 CPU 运行程序期间，所有的数据都直接从硬盘中读取，则非常影响效率。所以 CPU 会将程序运行所需要的数据从硬盘中读取到内存中。然后 CPU 与内存中的数据进行计算、交换。内存是易失性存储器（断电后，数据消失）。内存条区是计算机内部（在主板上）的一些存储器，用来保存 CPU 运算的中间数据和结果。内存是程序与 CPU 之间的桥梁。从硬盘读取出数据或者运行程序提供给 CPU。 虚拟内存 是计算机系统内存管理的一种技术。它使得程序认为它拥有连续的可用内存，而实际上，它通常被分割成多个物理内存碎片，可能部分暂时存储在外部磁盘（硬盘）存储器上（当需要使用时则用硬盘中数据交换到内存中）。Windows 系统中称为 “虚拟内存”，Linux/Unix 系统中称为 ”交换空间“。 iOS 不支持交换空间？不只是 iOS 不支持交换空间，大多数手机系统都不支持。因为移动设备的大量存储器是闪存，它的读写速度远远小电脑所使用的硬盘，也就是说手机即使使用了交换空间技术，也因为闪存慢的问题，不能提升性能，所以索性就没有交换空间技术。 2. iOS 内存知识内存（RAM）与 CPU 一样都是系统中最稀少的资源，也很容易发生竞争，应用内存与性能直接相关。iOS 没有交换空间作为备选资源，所以内存资源尤为重要。 什么是 OOM？是 out-of-memory 的缩写，字面意思是超过了内存限制。分为 FOOM（Foreground Out Of Memory，应用在前台运行的过程中崩溃。用户在使用的过程中产生的，这样的崩溃会使得活跃用户流失，业务上是非常不愿意看到的）和 BOOM（Background Out Of Memory，应用在后台运行的过程崩溃）。它是由 iOS 的 Jetsam 机制造成的一种非主流 Crash，它不能通过 Signal 这种监控方案所捕获。 什么是 Jetsam 机制？Jetsam 机制可以理解为系统为了控制内存资源过度使用而采用的一种管理机制。Jetsam 机制是运行在一个独立的进程中，每个进程都有一个内存阈值，一旦超过这个内存阈值，Jetsam 会立即杀掉这个进程。 为什么设计 Jetsam 机制？因为设备的内存是有限的，所以内存资源非常重要。系统进程以及其他使用的 App 都会抢占这个资源。由于 iOS 不支持交换空间，一旦触发低内存事件，Jetsam 就会尽可能多的释放 App 所在内存，这样 iOS 系统上出现内存不足时，App 就会被系统杀掉，变现为 crash。 2种情况触发 OOM：系统由于整体内存使用过高，会基于优先级策略杀死优先级较低的 App；当前 App 达到了 “highg water mark“ ，系统也会强杀当前 App（超过系统对当前单个 App 的内存限制值）。 读了源码（xnu/bsd/kern/kern_memorystatus.c）会发现内存被杀也有2种机制，如下 highwater 处理 -&gt; 我们的 App 占用内存不能超过单个限制 从优先级列表里循环寻找线程 判断是否满足 p_memstat_memlimit 的限制条件 DiagonoseActive、FREEZE 过滤 杀进程，成功则 exit，否则循环 memorystatus_act_aggressive 处理 -&gt; 内存占用高，按照优先级杀死 根据 policy 家在 jld_bucket_count，用来判断是否被杀 从 JETSAM_PRIORITY_ELEVATED_INACTIVE 开始杀 Old_bucket_count 和 memorystatus_jld_eval_period_msecs 判断是否开杀 根据优先级从低到高开始杀，直到 memorystatus_avail_pages_below_pressure 内存过大的几种情况 App 内存消耗较低，同时其他 App 内存管理也很棒，那么即使切换到其他 App，我们自己的 App 依旧是“活着”的，保留了用户状态。体验好 App 内存消耗较低，但其他 App 内存消耗太大（可能是内存管理糟糕，也可能是本身就耗费资源，比如游戏），那么除了在前台的线程，其他 App 都会被系统杀死，回收内存资源，用来给活跃的进程提供内存。 App 内存消耗较大，切换到其他 App 后，即使其他 App 向系统申请的内存不大，系统也会因为内存资源紧张，优先把内存消耗大的 App 杀死。表现为用户将 App 退出到后台，过会儿再次打开会发现 App 重新加载启动。 App 内存消耗非常大，在前台运行时就被系统杀死，造成闪退。 App 内存不足时，系统会按照一定策略来腾出更多的空间供使用。比较常见的做法是将一部分优先级低的数据挪到磁盘上，该操作为称为 page out。之后再次访问这块数据的时候，系统会负责将它重新搬回到内存中，该操作被称为 page in。 Memory page** 是内存管理中的最小单位，是系统分配的，可能一个 page 持有多个对象，也可能一个大的对象跨越多个 page。通常它是 16KB 大小，且有3种类型的 page。 Clean Memory Clean memory 包括3类：可以 page out 的内存、内存映射文件、App 使用到的 framework（每个 framework 都有 _DATA_CONST 段，通常都是 clean 状态，但使用 runtime swizling，那么变为 dirty）。 一开始分配的 page 都是干净的（堆里面的对象分配除外），我们 App 数据写入时候变为 dirty。从硬盘读进内存的文件，也是只读的、clean page。 Dirty Memory Dirty memory 包括4类：被 App 写入过数据的内存、所有堆区分配的对象、图像解码缓冲区、framework（framework 都有 _DATA 段和 _DATA_DIRTY 段，它们的内存都是 dirty）。 在使用 framework 的过程中会产生 Dirty memory，使用单例或者全局初始化方法有助于帮助减少 Dirty memory（因为单例一旦创建就不销毁，一直在内存中，系统不认为是 Dirty memory）。 Compressed Memory 由于闪存容量和读写限制，iOS 没有交换空间机制，而是在 iOS7 引入了 memory compressor。它是在内存紧张时候能够将最近一段时间未使用过的内存对象，内存压缩器会把对象压缩，释放出更多的 page。在需要时内存压缩器对其解压复用。在节省内存的同时提高了响应速度。 比如 App 使用某 Framework，内部有个 NSDictionary 属性存储数据，使用了 3 pages 内存，在近期未被访问的时候 memory compressor 将其压缩为 1 page，再次使用的时候还原为 3 pages。 App 运行内存 = pageNumbers * pageSize。因为 Compressed Memory 属于 Dirty memory。所以 Memory footprint = dirtySize + CompressedSize 设备不同，内存占用上限不同，App 上限较高，extension 上限较低，超过上限 crash 到 EXC_RESOURCE_EXCEPTION。 接下来谈一下如何获取内存上限，以及如何监控 App 因为占用内存过大而被强杀。 3. 获取内存信息3.1 通过 JetsamEvent 日志计算内存限制值当 App 被 Jetsam 机制杀死时，手机会生成系统日志。查看路径：Settings-Privacy-Analytics &amp; Improvements- Analytics Data（设置-隐私- 分析与改进-分析数据），可以看到 JetsamEvent-2020-03-14-161828.ips 形式的日志，以 JetsamEvent 开头。这些 JetsamEvent 日志都是 iOS 系统内核强杀掉那些优先级不高（idle、frontmost、suspended）且占用内存超过系统内存限制的 App 留下的。 日志包含了 App 的内存信息。可以查看到 日志最顶部有 pageSize 字段，查找到 per-process-limit，该节点所在结构里的 rpages ，将 rpages * pageSize 即可得到 OOM 的阈值。 日志中 largestProcess 字段代表 App 名称；reason 字段代表内存原因；states 字段代表奔溃时 App 的状态（ idle、suspended、frontmost…）。 为了测试数据的准确性，我将测试2台设备（iPhone 6s plus/13.3.1，iPhone 11 Pro/13.3.1）的所有 App 彻底退出，只跑了一个为了测试内存临界值的 Demo App。 循环申请内存，ViewController 代码如下 - (void)viewDidLoad { [super viewDidLoad]; NSMutableArray *array = [NSMutableArray array]; for (NSInteger index = 0; index &lt; 10000000; index++) { UIImageView *imageView = [[UIImageView alloc] initWithFrame:CGRectMake(0, 0, 100, 100)]; UIImage *image = [UIImage imageNamed:@”AppIcon”]; imageView.image = image; [array addObject:imageView]; }} iPhone 6s plus/13.3.1 数据如下： {“bug_type”:”298”,”timestamp”:”2020-03-19 17:23:45.94 +0800”,”os_version”:”iPhone OS 13.3.1 (17D50)”,”incident_id”:”DA8AF66D-24E8-458C-8734-981866942168”}{ “crashReporterKey” : “fc9b659ce486df1ed1b8062d5c7c977a7eb8c851”, “kernel” : “Darwin Kernel Version 19.3.0: Thu Jan 9 21:10:44 PST 2020; root:xnu-6153.82.3~1\\/RELEASE_ARM64_S8000”, “product” : “iPhone8,2”, “incident” : “DA8AF66D-24E8-458C-8734-981866942168”, “date” : “2020-03-19 17:23:45.93 +0800”, “build” : “iPhone OS 13.3.1 (17D50)”, “timeDelta” : 332, “memoryStatus” : { “compressorSize” : 48499, “compressions” : 7458651, “decompressions” : 5190200, “zoneMapCap” : 744407040, “largestZone” : “APFS_4K_OBJS”, “largestZoneSize” : 41402368, “pageSize” : 16384, “uncompressed” : 104065, “zoneMapSize” : 141606912, “memoryPages” : { “active” : 26214, “throttled” : 0, “fileBacked” : 14903, “wired” : 20019, “anonymous” : 37140, “purgeable” : 142, “inactive” : 23669, “free” : 2967, “speculative” : 2160 }}, “largestProcess” : “Test”, “genCounter” : 0, “processes” : [ { “uuid” : “39c5738b-b321-3865-a731-68064c4f7a6f”, “states” : [ “daemon”, “idle” ], “lifetimeMax” : 188, “age” : 948223699030, “purgeable” : 0, “fds” : 25, “coalition” : 422, “rpages” : 177, “pid” : 282, “idleDelta” : 824711280, “name” : “com.apple.Safari.SafeBrowsing.Se”, “cpuTime” : 10.275422000000001 }, // … { “uuid” : “83dbf121-7c0c-3ab5-9b66-77ee926e1561”, “states” : [ “frontmost” ], “killDelta” : 2592, “genCount” : 0, “age” : 1531004794, “purgeable” : 0, “fds” : 50, “coalition” : 1047, “rpages” : 92806, “reason” : “per-process-limit”, “pid” : 2384, “cpuTime” : 59.464373999999999, “name” : “Test”, “lifetimeMax” : 92806 }, // … ]} iPhone 6s plus/13.3.1 手机 OOM 临界值为：(16384*92806)/(1024*1024)=1450.09375M iPhone 11 Pro/13.3.1 数据如下： {“bug_type”:”298”,”timestamp”:”2020-03-19 17:30:28.39 +0800”,”os_version”:”iPhone OS 13.3.1 (17D50)”,”incident_id”:”7F111601-BC7A-4BD7-A468-CE3370053057”}{ “crashReporterKey” : “bc2445adc164c399b330f812a48248e029e26276”, “kernel” : “Darwin Kernel Version 19.3.0: Thu Jan 9 21:11:10 PST 2020; root:xnu-6153.82.3~1\\/RELEASE_ARM64_T8030”, “product” : “iPhone12,3”, “incident” : “7F111601-BC7A-4BD7-A468-CE3370053057”, “date” : “2020-03-19 17:30:28.39 +0800”, “build” : “iPhone OS 13.3.1 (17D50)”, “timeDelta” : 189, “memoryStatus” : { “compressorSize” : 66443, “compressions” : 25498129, “decompressions” : 15532621, “zoneMapCap” : 1395015680, “largestZone” : “APFS_4K_OBJS”, “largestZoneSize” : 41222144, “pageSize” : 16384, “uncompressed” : 127027, “zoneMapSize” : 169639936, “memoryPages” : { “active” : 58652, “throttled” : 0, “fileBacked” : 20291, “wired” : 45838, “anonymous” : 96445, “purgeable” : 4, “inactive” : 54368, “free” : 5461, “speculative” : 3716 }}, “largestProcess” : “杭城小刘”, “genCounter” : 0, “processes” : [ { “uuid” : “2dd5eb1e-fd31-36c2-99d9-bcbff44efbb7”, “states” : [ “daemon”, “idle” ], “lifetimeMax” : 171, “age” : 5151034269954, “purgeable” : 0, “fds” : 50, “coalition” : 66, “rpages” : 164, “pid” : 11276, “idleDelta” : 3801132318, “name” : “wcd”, “cpuTime” : 3.430787 }, // … { “uuid” : “63158edc-915f-3a2b-975c-0e0ac4ed44c0”, “states” : [ “frontmost” ], “killDelta” : 4345, “genCount” : 0, “age” : 654480778, “purgeable” : 0, “fds” : 50, “coalition” : 1718, “rpages” : 134278, “reason” : “per-process-limit”, “pid” : 14206, “cpuTime” : 23.955463999999999, “name” : “杭城小刘”, “lifetimeMax” : 134278 }, // … ]} iPhone 11 Pro/13.3.1 手机 OOM 临界值为：(16384*134278)/(1024*1024)=2098.09375M iOS 系统如何发现 Jetsam ？ MacOS/iOS 是一个 BSD 衍生而来的系统，其内核是 Mach，但是对于上层暴露的接口一般是基于 BSD 层对 Mach 的包装后的。Mach 是一个微内核的架构，真正的虚拟内存管理也是在其中进行的，BSD 对内存管理提供了上层接口。Jetsam 事件也是由 BSD 产生的。bsd_init 函数是入口，其中基本都是在初始化各个子系统，比如虚拟内存管理等。 // 1. Initialize the kernel memory allocator, 初始化 BSD 内存 Zone，这个 Zone 是基于 Mach 内核的zone 构建kmeminit(); // 2. Initialise background freezing, iOS 上独有的特性，内存和进程的休眠的常驻监控线程#if CONFIG_FREEZE#ifndef CONFIG_MEMORYSTATUS #error “CONFIG_FREEZE defined without matching CONFIG_MEMORYSTATUS”#endif /* Initialise background freezing */ bsd_init_kprintf(“calling memorystatus_freeze_init\\n”); memorystatus_freeze_init();#endif&gt; // 3. iOS 独有，JetSAM（即低内存事件的常驻监控线程）#if CONFIG_MEMORYSTATUS /* Initialize kernel memory status notifications */ bsd_init_kprintf(“calling memorystatus_init\\n”); memorystatus_init();#endif /* CONFIG_MEMORYSTATUS */ 主要作用就是开启了2个优先级最高的线程，来监控整个系统的内存情况。 CONFIG_FREEZE 开启时，内核对进程进行冷冻而不是杀死。冷冻功能是由内核中启动一个 memorystatus_freeze_thread 进行，这个进程在收到信号后调用 memorystatus_freeze_top_process 进行冷冻。 iOS 系统会开启优先级最高的线程 vm_pressure_monitor 来监控系统的内存压力情况，并通过一个堆栈来维护所有 App 进程。iOS 系统还会维护一个内存快照表，用于保存每个进程内存页的消耗情况。有关 Jetsam 也就是 memorystatus 相关的逻辑，可以在 XNU 项目中的 kern_memorystatus.h 和 kern_memorystatus.c 源码中查看。 iOS 系统因内存占用过高会强杀 App 前，至少有 6秒钟可以用来做优先级判断，JetsamEvent 日志也是在这6秒内生成的。 上文提到了 iOS 系统没有交换空间，于是引入了 MemoryStatus 机制（也称为 Jetsam）。也就是说在 iOS 系统上释放尽可能多的内存供当前 App 使用。这个机制表现在优先级上，就是先强杀后台应用；如果内存还是不够多，就强杀掉当前应用。在 MacOS 中，MemoryStatus 只会强杀掉标记为空闲退出的进程。 MemoryStatus 机制会开启一个 memorystatus_jetsam_thread 的线程，它负责强杀 App 和记录日志，不会发送消息，所以内存压力检测线程无法获取到强杀 App 的消息。 当监控线程发现某 App 有内存压力时，就发出通知，此时有内存的 App 就去执行 didReceiveMemoryWarning 代理方法。在这个时机，我们还有机会做一些内存资源释放的逻辑，也许会避免 App 被系统杀死。 源码角度查看问题 iOS 系统内核有一个数组，专门维护线程的优先级。数组的每一项是一个包含进程链表的结构体。结构体如下： #define MEMSTAT_BUCKET_COUNT (JETSAM_PRIORITY_MAX + 1) typedef struct memstat_bucket { TAILQ_HEAD(, proc) list; int count;} memstat_bucket_t; memstat_bucket_t memstat_bucket[MEMSTAT_BUCKET_COUNT]; 在 kern_memorystatus.h 中可以看到进行优先级信息 #define JETSAM_PRIORITY_IDLE_HEAD -2/* The value -1 is an alias to JETSAM_PRIORITY_DEFAULT */#define JETSAM_PRIORITY_IDLE 0#define JETSAM_PRIORITY_IDLE_DEFERRED 1 /* Keeping this around till all xnu_quick_tests can be moved away from it.*/#define JETSAM_PRIORITY_AGING_BAND1 JETSAM_PRIORITY_IDLE_DEFERRED#define JETSAM_PRIORITY_BACKGROUND_OPPORTUNISTIC 2#define JETSAM_PRIORITY_AGING_BAND2 JETSAM_PRIORITY_BACKGROUND_OPPORTUNISTIC#define JETSAM_PRIORITY_BACKGROUND 3#define JETSAM_PRIORITY_ELEVATED_INACTIVE JETSAM_PRIORITY_BACKGROUND#define JETSAM_PRIORITY_MAIL 4#define JETSAM_PRIORITY_PHONE 5#define JETSAM_PRIORITY_UI_SUPPORT 8#define JETSAM_PRIORITY_FOREGROUND_SUPPORT 9#define JETSAM_PRIORITY_FOREGROUND 10#define JETSAM_PRIORITY_AUDIO_AND_ACCESSORY 12#define JETSAM_PRIORITY_CONDUCTOR 13#define JETSAM_PRIORITY_HOME 16#define JETSAM_PRIORITY_EXECUTIVE 17#define JETSAM_PRIORITY_IMPORTANT 18#define JETSAM_PRIORITY_CRITICAL 19 #define JETSAM_PRIORITY_MAX 21 可以明显的看到，后台 App 优先级 JETSAM_PRIORITY_BACKGROUND 为3，前台 App 优先级 JETSAM_PRIORITY_FOREGROUND 为10。 优先级规则是：内核线程优先级 &gt; 操作系统优先级 &gt; App 优先级。且前台 App 优先级高于后台运行的 App；当线程的优先级相同时， CPU 占用多的线程的优先级会被降低。 在 kern_memorystatus.c 中可以看到 OOM 可能的原因： /* For logging clarity */static const char *memorystatus_kill_cause_name[] = { “” , /* kMemorystatusInvalid */ “jettisoned” , /* kMemorystatusKilled */ “highwater” , /* kMemorystatusKilledHiwat */ “vnode-limit” , /* kMemorystatusKilledVnodes */ “vm-pageshortage” , /* kMemorystatusKilledVMPageShortage */ “proc-thrashing” , /* kMemorystatusKilledProcThrashing */ “fc-thrashing” , /* kMemorystatusKilledFCThrashing */ “per-process-limit” , /* kMemorystatusKilledPerProcessLimit */ “disk-space-shortage” , /* kMemorystatusKilledDiskSpaceShortage */ “idle-exit” , /* kMemorystatusKilledIdleExit */ “zone-map-exhaustion” , /* kMemorystatusKilledZoneMapExhaustion */ “vm-compressor-thrashing” , /* kMemorystatusKilledVMCompressorThrashing */ “vm-compressor-space-shortage” , /* kMemorystatusKilledVMCompressorSpaceShortage */}; 查看 memorystatus_init 这个函数中初始化 Jetsam 线程的关键代码 __private_extern__ voidmemorystatus_init(void) { // … /* Initialize the jetsam_threads state array */ jetsam_threads = kalloc(sizeof(struct jetsam_thread_state) * max_jetsam_threads); /\\* Initialize all the jetsam threads \\*/ for (i = 0; i &lt; max\\_jetsam\\_threads; i++) &#123; result = kernel\\_thread\\_start\\_priority(memorystatus\\_thread, NULL, 95 /\\* MAXPRI\\_KERNEL \\*/, &amp;jetsam\\_threads\\[i\\].thread); if (result == KERN\\_SUCCESS) &#123; jetsam\\_threads\\[i\\].inited = FALSE; jetsam\\_threads\\[i\\].index = i; thread\\_deallocate(jetsam\\_threads\\[i\\].thread); &#125; else &#123; panic(&quot;Could not create memorystatus\\_thread %d&quot;, i); &#125; &#125; } /* * High-level priority assignments * ************************************************************************* * 127 Reserved (real-time) * A * + * (32 levels) * + * V * 96 Reserved (real-time) * 95 Kernel mode only * A * + * (16 levels) * + * V * 80 Kernel mode only * 79 System high priority * A * + * (16 levels) * + * V * 64 System high priority * 63 Elevated priorities * A * + * (12 levels) * + * V * 52 Elevated priorities * 51 Elevated priorities (incl. BSD +nice) * A * + * (20 levels) * + * V * 32 Elevated priorities (incl. BSD +nice) * 31 Default (default base for threads) * 30 Lowered priorities (incl. BSD -nice) * A * + * (20 levels) * + * V * 11 Lowered priorities (incl. BSD -nice) * 10 Lowered priorities (aged pri’s) * A * + * (11 levels) * + * V * 0 Lowered priorities (aged pri’s / idle) ************************************************************************* */ 可以看出：用户态的应用程序的线程不可能高于操作系统和内核。而且，用户态的应用程序间的线程优先级分配也有区别，比如处于前台的应用程序优先级高于处于后台的应用程序优先级。iOS 上应用程序优先级最高的是 SpringBoard；此外线程的优先级不是一成不变的。Mach 会根据线程的利用率和系统整体负载动态调整线程优先级。如果耗费 CPU 太多就降低线程优先级，如果线程过度挨饿，则会提升线程优先级。但是无论怎么变，程序都不能超过其所在线程的优先级区间范围。 可以看出，系统会根据内核启动参数和设备性能，开启 max_jetsam_threads 个（一般情况为1，特殊情况下可能为3）jetsam 线程，且这些线程的优先级为 95，也就是 MAXPRI_KERNEL（注意这里的 95 是线程的优先级，XNU 的线程优先级区间为：0～127。上文的宏定义是进程优先级，区间为：-2~19）。 紧接着，分析下 memorystatus_thread 函数，主要负责线程启动的初始化 static voidmemorystatus_thread(void *param __unused, wait_result_t wr __unused) { //… while (memorystatus_action_needed()) { boolean_t killed; int32_t priority; uint32_t cause; uint64_t jetsam_reason_code = JETSAM_REASON_INVALID; os_reason_t jetsam_reason = OS_REASON_NULL; cause = kill\\_under\\_pressure\\_cause; switch (cause) &#123; case kMemorystatusKilledFCThrashing: jetsam\\_reason\\_code = JETSAM\\_REASON\\_MEMORY\\_FCTHRASHING; break; case kMemorystatusKilledVMCompressorThrashing: jetsam\\_reason\\_code = JETSAM\\_REASON\\_MEMORY\\_VMCOMPRESSOR\\_THRASHING; break; case kMemorystatusKilledVMCompressorSpaceShortage: jetsam\\_reason\\_code = JETSAM\\_REASON\\_MEMORY\\_VMCOMPRESSOR\\_SPACE\\_SHORTAGE; break; case kMemorystatusKilledZoneMapExhaustion: jetsam\\_reason\\_code = JETSAM\\_REASON\\_ZONE\\_MAP\\_EXHAUSTION; break; case kMemorystatusKilledVMPageShortage: /\\* falls through \\*/ default: jetsam\\_reason\\_code = JETSAM\\_REASON\\_MEMORY\\_VMPAGESHORTAGE; cause = kMemorystatusKilledVMPageShortage; break; &#125; /\\* Highwater \\*/ boolean\\_t is\\_critical = TRUE; if (memorystatus\\_act\\_on\\_hiwat\\_processes(&amp;errors, &amp;hwm\\_kill, &amp;post\\_snapshot, &amp;is\\_critical)) &#123; if (is\\_critical == FALSE) &#123; /\\* \\* For now, don&#39;t kill any other processes. \\*/ break; &#125; else &#123; goto done; &#125; &#125; jetsam\\_reason = os\\_reason\\_create(OS\\_REASON\\_JETSAM, jetsam\\_reason\\_code); if (jetsam\\_reason == OS\\_REASON\\_NULL) &#123; printf(&quot;memorystatus\\_thread: failed to allocate jetsam reason\\\\n&quot;); &#125; if (memorystatus\\_act\\_aggressive(cause, jetsam\\_reason, &amp;jld\\_idle\\_kills, &amp;corpse\\_list\\_purged, &amp;post\\_snapshot)) &#123; goto done; &#125; /\\* \\* memorystatus\\_kill\\_top\\_process() drops a reference, \\* so take another one so we can continue to use this exit reason \\* even after it returns \\*/ os\\_reason\\_ref(jetsam\\_reason); /\\* LRU \\*/ killed = memorystatus\\_kill\\_top\\_process(TRUE, sort\\_flag, cause, jetsam\\_reason, &amp;priority, &amp;errors); sort\\_flag = FALSE; if (killed) &#123; if (memorystatus\\_post\\_snapshot(priority, cause) == TRUE) &#123; post\\_snapshot = TRUE; &#125; /\\* Jetsam Loop Detection \\*/ if (memorystatus\\_jld\\_enabled == TRUE) &#123; if ((priority == JETSAM\\_PRIORITY\\_IDLE) || (priority == system\\_procs\\_aging\\_band) || (priority == applications\\_aging\\_band)) &#123; jld\\_idle\\_kills++; &#125; else &#123; /\\* \\* We&#39;ve reached into bands beyond idle deferred. \\* We make no attempt to monitor them \\*/ &#125; &#125; if ((priority &gt;= JETSAM\\_PRIORITY\\_UI\\_SUPPORT) &amp;&amp; (total\\_corpses\\_count() &gt; 0) &amp;&amp; (corpse\\_list\\_purged == FALSE)) &#123; /\\* \\* If we have jetsammed a process in or above JETSAM\\_PRIORITY\\_UI\\_SUPPORT \\* then we attempt to relieve pressure by purging corpse memory. \\*/ task\\_purge\\_all\\_corpses(); corpse\\_list\\_purged = TRUE; &#125; goto done; &#125; if (memorystatus\\_avail\\_pages\\_below\\_critical()) &#123; /\\* \\* Still under pressure and unable to kill a process - purge corpse memory \\*/ if (total\\_corpses\\_count() &gt; 0) &#123; task\\_purge\\_all\\_corpses(); corpse\\_list\\_purged = TRUE; &#125; if (memorystatus\\_avail\\_pages\\_below\\_critical()) &#123; /\\* \\* Still under pressure and unable to kill a process - panic \\*/ panic(&quot;memorystatus\\_jetsam\\_thread: no victim! available pages:%llu\\\\n&quot;, (uint64\\_t)memorystatus\\_available\\_pages); &#125; &#125; done: } 可以看到它开启了一个 循环，memorystatus_action_needed() 来作为循环条件，持续释放内存。 static boolean_tmemorystatus_action_needed(void) {#if CONFIG_EMBEDDED return (is_reason_thrashing(kill_under_pressure_cause) || is_reason_zone_map_exhaustion(kill_under_pressure_cause) || memorystatus_available_pages &lt;= memorystatus_available_pages_pressure);#else /* CONFIG_EMBEDDED */ return (is_reason_thrashing(kill_under_pressure_cause) || is_reason_zone_map_exhaustion(kill_under_pressure_cause));#endif /* CONFIG_EMBEDDED */} 它通过 vm_pagepout 发送的内存压力来判断当前内存资源是否紧张。几种情况：频繁的页面换出换进 is_reason_thrashing, Mach Zone 耗尽了 is_reason_zone_map_exhaustion、以及可用的页低于了 memory status_available_pages 这个门槛。 继续看 memorystatus_thread，会发现内存紧张时，将先触发 High-water 类型的 OOM，也就是说假如某个进程使用过程中超过了其使用内存的最高限制 hight water mark 时会发生 OOM。在 memorystatus_act_on_hiwat_processes() 中，通过 memorystatus_kill_hiwat_proc() 在优先级数组 memstat_bucket 中查找优先级最低的进程，如果进程的内存小于阈值（footprint_in_bytes &lt;= memlimit_in_bytes）则继续寻找次优先级较低的进程，直到找到占用内存超过阈值的进程并杀死。 通常来说单个 App 很难触碰到 high water mark，如果不能结束任何进程，最终走到 memorystatus_act_aggressive，也就是大多数 OOM 发生的地方。 static boolean_tmemorystatus_act_aggressive(uint32_t cause, os_reason_t jetsam_reason, int *jld_idle_kills, boolean_t *corpse_list_purged, boolean_t *post_snapshot) { // … if ( (jld_bucket_count == 0) || (jld_now_msecs &gt; (jld_timestamp_msecs + memorystatus_jld_eval_period_msecs))) { /\\* \\* Refresh evaluation parameters \\*/ jld\\_timestamp\\_msecs = jld\\_now\\_msecs; jld\\_idle\\_kill\\_candidates = jld\\_bucket\\_count; \\*jld\\_idle\\_kills = 0; jld\\_eval\\_aggressive\\_count = 0; jld\\_priority\\_band\\_max = JETSAM\\_PRIORITY\\_UI\\_SUPPORT; &#125; //…} 上述代码看到，判断要不要真正执行 kill 是根据一定的时间间判断的，条件是 jld_now_msecs &gt; (jld_timestamp_msecs + memorystatus_jld_eval_period_msecs。 也就是在 memorystatus_jld_eval_period_msecs 后才发生条件里面的 kill。 /* Jetsam Loop Detection */if (max_mem &lt;= (512 * 1024 * 1024)) { /* 512 MB devices */memorystatus_jld_eval_period_msecs = 8000; /* 8000 msecs == 8 second window */} else { /* 1GB and larger devices */memorystatus_jld_eval_period_msecs = 6000; /* 6000 msecs == 6 second window */} 其中 memorystatus_jld_eval_period_msecs 取值最小6秒。所以我们可以在6秒内做些处理。 3.2 开发者们整理所得stackoverflow 上有一份数据，整理了各种设备的 OOM 临界值 device crash amount:MB total amount:MB percentage of total iPad1 127 256 49% iPad2 275 512 53% iPad3 645 1024 62% iPad4(iOS 8.1) 585 1024 57% Pad Mini 1st Generation 297 512 58% iPad Mini retina(iOS 7.1) 696 1024 68% iPad Air 697 1024 68% iPad Air 2(iOS 10.2.1) 1383 2048 68% iPad Pro 9.7”(iOS 10.0.2 (14A456)) 1395 1971 71% iPad Pro 10.5”(iOS 11 beta4) 3057 4000 76% iPad Pro 12.9” (2015)(iOS 11.2.1) 3058 3999 76% iPad 10.2(iOS 13.2.3) 1844 2998 62% iPod touch 4th gen(iOS 6.1.1) 130 256 51% iPod touch 5th gen 286 512 56% iPhone4 325 512 63% iPhone4s 286 512 56% iPhone5 645 1024 62% iPhone5s 646 1024 63% iPhone6(iOS 8.x) 645 1024 62% iPhone6 Plus(iOS 8.x) 645 1024 62% iPhone6s(iOS 9.2) 1396 2048 68% iPhone6s Plus(iOS 10.2.1) 1396 2048 68% iPhoneSE(iOS 9.3) 1395 2048 68% iPhone7(iOS 10.2) 1395 2048 68% iPhone7 Plus(iOS 10.2.1) 2040 3072 66% iPhone8(iOS 12.1) 1364 1990 70% iPhoneX(iOS 11.2.1) 1392 2785 50% iPhoneXS(iOS 12.1) 2040 3754 54% iPhoneXS Max(iOS 12.1) 2039 3735 55% iPhoneXR(iOS 12.1) 1792 2813 63% iPhone11(iOS 13.1.3) 2068 3844 54% iPhone11 Pro Max(iOS 13.2.3) 2067 3740 55% 3.3 触发当前 App 的 high water mark我们可以写定时器，不断的申请内存，之后再通过 phys_footprint 打印当前占用内存，按道理来说不断申请内存即可触发 Jetsam 机制，强杀 App，那么最后一次打印的内存占用也就是当前设备的内存上限值。 timer = [NSTimer scheduledTimerWithTimeInterval:0.01 target:self selector:@selector(allocateMemory) userInfo:nil repeats:YES]; (void)allocateMemory { UIImageView *imageView = [[UIImageView alloc] initWithFrame:CGRectMake(0, 0, 100, 100)]; UIImage *image = [UIImage imageNamed:@”AppIcon”]; imageView.image = image; [array addObject:imageView]; memoryLimitSizeMB = [self usedSizeOfMemory]; if (memoryWarningSizeMB &amp;&amp; memoryLimitSizeMB) { NSLog(@&quot;----- memory warnning:%dMB, memory limit:%dMB&quot;, memoryWarningSizeMB, memoryLimitSizeMB); }} (int)usedSizeOfMemory { task_vm_info_data_t taskInfo; mach_msg_type_number_t infoCount = TASK_VM_INFO_COUNT; kern_return_t kernReturn = task_info(mach_task_self(), TASK_VM_INFO, (task_info_t)&amp;taskInfo, &amp;infoCount); if (kernReturn != KERN_SUCCESS) { return 0; } return (int)(taskInfo.phys_footprint/1024.0/1024.0);} 3.4 适用于 iOS13 系统的获取方式iOS13 开始 &lt;os/proc.h&gt; 中 size_t os_proc_available_memory(void); 可以查看当前可用内存。 Return Value The number of bytes that the app may allocate before it hits its memory limit. If the calling process isn’t an app, or if the process has already exceeded its memory limit, this function returns 0. Discussion Call this function to determine the amount of memory available to your app. The returned value corresponds to the current memory limit minus the memory footprint of your app at the time of the function call. Your app’s memory footprint consists of the data that you allocated in RAM, and that must stay in RAM (or the equivalent) at all times. Memory limits can change during the app life cycle and don’t necessarily correspond to the amount of physical memory available on the device. Use the returned value as advisory information only and don’t cache it. The precise value changes when your app does any work that affects memory, which can happen frequently. Although this function lets you determine the amount of memory your app may safely consume, don’t use it to maximize your app’s memory usage. Significant memory use, even when under the current memory limit, affects system performance. For example, when your app consumes all of its available memory, the system may need to terminate other apps and system processes to accommodate your app’s requests. Instead, always consume the smallest amount of memory you need to be responsive to the user’s needs. If you need more detailed information about the available memory resources, you can call task_info. However, be aware that task_info is an expensive call, whereas this function is much more efficient. if (@available(iOS 13.0, *)) { return os_proc_available_memory() / 1024.0 / 1024.0;} App 内存信息的 API 可以在 Mach 层找到，mach_task_basic_info 结构体存储了 Mach task 的内存使用信息，其中 phys_footprint 就是应用使用的物理内存大小，virtual_size 是虚拟内存大小。 #define MACH_TASK_BASIC_INFO 20 /* always 64-bit basic info */struct mach_task_basic_info { mach_vm_size_t virtual_size; /* virtual memory size (bytes) */ mach_vm_size_t resident_size; /* resident memory size (bytes) */ mach_vm_size_t resident_size_max; /* maximum resident memory size (bytes) */ time_value_t user_time; /* total user run time for terminated threads */ time_value_t system_time; /* total system run time for terminated threads */ policy_t policy; /* default policy for new threads */ integer_t suspend_count; /* suspend count for task */}; 所以获取代码为 task_vm_info_data_t vmInfo;mach_msg_type_number_t count = TASK_VM_INFO_COUNT;kern_return_t kr = task_info(mach_task_self(), TASK_VM_INFO, (task_info_t)&amp;vmInfo, &amp;count); if (kr != KERN_SUCCESS) { return ;}CGFloat memoryUsed = (CGFloat)(vmInfo.phys_footprint/1024.0/1024.0); 可能有人好奇不应该是 resident_size 这个字段获取内存的使用情况吗？一开始测试后发现 resident_size 和 Xcode 测量结果差距较大。而使用 phys_footprint 则接近于 Xcode 给出的结果。且可以从 WebKit 源码中得到印证。 所以在 iOS13 上，我们可以通过 os_proc_available_memory 获取到当前可以用内存，通过 phys_footprint 获取到当前 App 占用内存，2者的和也就是当前设备的内存上限，超过即触发 Jetsam 机制。 - (CGFloat)limitSizeOfMemory { if (@available(iOS 13.0, *)) { task_vm_info_data_t taskInfo; mach_msg_type_number_t infoCount = TASK_VM_INFO_COUNT; kern_return_t kernReturn = task_info(mach_task_self(), TASK_VM_INFO, (task_info_t)&amp;taskInfo, &amp;infoCount); if (kernReturn != KERN\\_SUCCESS) &#123; return 0; &#125; return (CGFloat)((taskInfo.phys\\_footprint + os\\_proc\\_available\\_memory()) / (1024.0 \\* 1024.0); &#125; return 0; } 当前可以使用内存：1435.936752MB；当前 App 已占用内存：14.5MB，临界值：1435.936752MB + 14.5MB= 1450.436MB， 和 3.1 方法中获取到的内存临界值一样「iPhone 6s plus/13.3.1 手机 OOM 临界值为：(16384*92806)/(1024*1024)=1450.09375M」。 3.5 通过 XNU 获取内存限制值在 XNU 中，有专门用于获取内存上限值的函数和宏，可以通过 memorystatus_priority_entry 这个结构体得到所有进程的优先级和内存限制值。 typedef struct memorystatus_priority_entry { pid_t pid; int32_t priority; uint64_t user_data; int32_t limit; uint32_t state;} memorystatus_priority_entry_t; 其中，priority 代表进程优先级，limit 代表进程的内存限制值。但是这种方式需要 root 权限，由于没有越狱设备，我没有尝试过。 相关代码可查阅 kern_memorystatus.h 文件。需要用到函数 int memorystatus_control(uint32_t command, int32_t pid, uint32_t flags, void *buffer, size_t buffersize); /* Commands */#define MEMORYSTATUS_CMD_GET_PRIORITY_LIST 1#define MEMORYSTATUS_CMD_SET_PRIORITY_PROPERTIES 2#define MEMORYSTATUS_CMD_GET_JETSAM_SNAPSHOT 3#define MEMORYSTATUS_CMD_GET_PRESSURE_STATUS 4#define MEMORYSTATUS_CMD_SET_JETSAM_HIGH_WATER_MARK 5 /* Set active memory limit = inactive memory limit, both non-fatal */#define MEMORYSTATUS_CMD_SET_JETSAM_TASK_LIMIT 6 /* Set active memory limit = inactive memory limit, both fatal */#define MEMORYSTATUS_CMD_SET_MEMLIMIT_PROPERTIES 7 /* Set memory limits plus attributes independently */#define MEMORYSTATUS_CMD_GET_MEMLIMIT_PROPERTIES 8 /* Get memory limits plus attributes */#define MEMORYSTATUS_CMD_PRIVILEGED_LISTENER_ENABLE 9 /* Set the task’s status as a privileged listener w.r.t memory notifications */#define MEMORYSTATUS_CMD_PRIVILEGED_LISTENER_DISABLE 10 /* Reset the task’s status as a privileged listener w.r.t memory notifications */#define MEMORYSTATUS_CMD_AGGRESSIVE_JETSAM_LENIENT_MODE_ENABLE 11 /* Enable the ‘lenient’ mode for aggressive jetsam. See comments in kern_memorystatus.c near the top. */#define MEMORYSTATUS_CMD_AGGRESSIVE_JETSAM_LENIENT_MODE_DISABLE 12 /* Disable the ‘lenient’ mode for aggressive jetsam. */#define MEMORYSTATUS_CMD_GET_MEMLIMIT_EXCESS 13 /* Compute how much a process’s phys_footprint exceeds inactive memory limit */#define MEMORYSTATUS_CMD_ELEVATED_INACTIVEJETSAMPRIORITY_ENABLE 14 /* Set the inactive jetsam band for a process to JETSAM_PRIORITY_ELEVATED_INACTIVE */#define MEMORYSTATUS_CMD_ELEVATED_INACTIVEJETSAMPRIORITY_DISABLE 15 /* Reset the inactive jetsam band for a process to the default band (0)*/#define MEMORYSTATUS_CMD_SET_PROCESS_IS_MANAGED 16 /* (Re-)Set state on a process that marks it as (un-)managed by a system entity e.g. assertiond */#define MEMORYSTATUS_CMD_GET_PROCESS_IS_MANAGED 17 /* Return the ‘managed’ status of a process */#define MEMORYSTATUS_CMD_SET_PROCESS_IS_FREEZABLE 18 /* Is the process eligible for freezing? Apps and extensions can pass in FALSE to opt out of freezing, i.e., 伪代码 struct memorystatus_priority_entry memStatus[NUM_ENTRIES];size_t count = sizeof(struct memorystatus_priority_entry) * NUM_ENTRIES;int kernResult = memorystatus_control(MEMORYSTATUS_CMD_GET_PRIORITY_LIST, 0, 0, memStatus, count);if (rc &lt; 0) { NSLog(@”memorystatus_control”); return ;} int entry = 0;for (; rc &gt; 0; rc -= sizeof(struct memorystatus_priority_entry)){ printf (“PID: %5d\\tPriority:%2d\\tUser Data: %llx\\tLimit:%2d\\tState:%s\\n”, memstatus[entry].pid, memstatus[entry].priority, memstatus[entry].user_data, memstatus[entry].limit, state_to_text(memstatus[entry].state)); entry++;} for 循环打印出每个进程（也就是 App）的 pid、Priority、User Data、Limit、State 信息。从 log 中找出优先级为10的进程，即我们前台运行的 App。为什么是10？ 因为 #define JETSAM_PRIORITY_FOREGROUND 10 我们的目的就是获取前台 App 的内存上限值。 4. 如何判定发生了 OOMOOM 导致 crash 前，app 一定会收到低内存警告吗？ 做2组对比实验： // 实验1NSMutableArray *array = [NSMutableArray array];for (NSInteger index = 0; index &lt; 10000000; index++) { NSString *filePath = [[NSBundle mainBundle] pathForResource:@”Info” ofType:@”plist”]; NSData *data = [NSData dataWithContentsOfFile:filePath]; [array addObject:data];} // 实验2// ViewController.m (void)viewDidLoad { [super viewDidLoad]; dispatch_async(dispatch_get_global_queue(0, 0), ^{ NSMutableArray \\*array = \\[NSMutableArray array\\]; for (NSInteger index = 0; index &lt; 10000000; index++) &#123; NSString \\*filePath = \\[\\[NSBundle mainBundle\\] pathForResource:@&quot;Info&quot; ofType:@&quot;plist&quot;\\]; NSData \\*data = \\[NSData dataWithContentsOfFile:filePath\\]; \\[array addObject:data\\]; &#125; });} (void)didReceiveMemoryWarning{ NSLog(@”2”);} // AppDelegate.m (void)applicationDidReceiveMemoryWarning:(UIApplication *)application{ NSLog(@”1”);} 现象： 在 viewDidLoad 也就是主线程中内存消耗过大，系统并不会发出低内存警告，直接 Crash。因为内存增长过快，主线程很忙。 多线程的情况下，App 因内存增长过快，会收到低内存警告，AppDelegate 中的applicationDidReceiveMemoryWarning 先执行，随后是当前 VC 的 didReceiveMemoryWarning。 结论： 收到低内存警告不一定会 Crash，因为有6秒钟的系统判断时间，6秒内内存下降了则不会 crash。发生 OOM 也不一定会收到低内存警告。 5. 内存信息收集要想精确的定位问题，就需要 dump 所有对象及其内存信息。当内存接近系统内存上限的时候，收集并记录所需信息，结合一定的数据上报机制，上传到服务器，分析并修复。 还需要知道每个对象具体是在哪个函数里创建出来的，以便还原“案发现场”。 源代码（libmalloc/malloc），内存分配函数 malloc 和 calloc 等默认使用 nano_zone，nano_zone 是小于 256B 以下的内存分配，大于 256B 则使用 scalable_zone 来分配。 主要针对大内存的分配监控。malloc 函数用的是 malloc_zone_malloc, calloc 用的是 malloc_zone_calloc。 使用 scalable_zone 分配内存的函数都会调用 malloc_logger 函数，因为系统为了有个地方专门统计并管理内存分配情况。这样的设计也满足「收口原则」。 void *malloc(size_t size) { void *retval; retval = malloc_zone_malloc(default_zone, size); if (retval == NULL) { errno = ENOMEM; } return retval;} void *calloc(size_t num_items, size_t size) { void *retval; retval = malloc_zone_calloc(default_zone, num_items, size); if (retval == NULL) { errno = ENOMEM; } return retval;} 首先来看看这个 default_zone 是什么东西, 代码如下 typedef struct { malloc_zone_t malloc_zone; uint8_t pad[PAGE_MAX_SIZE - sizeof(malloc_zone_t)];} virtual_default_zone_t; static virtual_default_zone_t virtual_default_zone__attribute__((section(“__DATA,__v_zone”)))__attribute__((aligned(PAGE_MAX_SIZE))) = { NULL, NULL, default_zone_size, default_zone_malloc, default_zone_calloc, default_zone_valloc, default_zone_free, default_zone_realloc, default_zone_destroy, DEFAULT_MALLOC_ZONE_STRING, default_zone_batch_malloc, default_zone_batch_free, &amp;default_zone_introspect, 10, default_zone_memalign, default_zone_free_definite_size, default_zone_pressure_relief, default_zone_malloc_claimed_address,}; static malloc_zone_t *default_zone = &amp;virtual_default_zone.malloc_zone; static void *default_zone_malloc(malloc_zone_t *zone, size_t size) { zone = runtime_default_zone(); return zone-&gt;malloc(zone, size); } MALLOC_ALWAYS_INLINEstatic inline malloc_zone_t *runtime_default_zone() { return (lite_zone) ? lite_zone : inline_malloc_default_zone();} 可以看到 default_zone 通过这种方式来初始化 static inline malloc_zone_t *inline_malloc_default_zone(void) { _malloc_initialize_once(); // malloc_report(ASL_LEVEL_INFO, “In inline_malloc_default_zone with %d %d\\n”, malloc_num_zones, malloc_has_debug_zone); return malloc_zones[0];} 随后的调用如下_malloc_initialize -&gt; create_scalable_zone -&gt; create_scalable_szone 最终我们创建了 szone_t 类型的对象，通过类型转换，得到了我们的 default_zone。 malloc_zone_t *create_scalable_zone(size_t initial_size, unsigned debug_flags) { return (malloc_zone_t *) create_scalable_szone(initial_size, debug_flags);} void *malloc_zone_malloc(malloc_zone_t *zone, size_t size) { MALLOC_TRACE(TRACE_malloc | DBG_FUNC_START, (uintptr_t)zone, size, 0, 0); void *ptr; if (malloc_check_start &amp;&amp; (malloc_check_counter++ &gt;= malloc_check_start)) { internal_check(); } if (size &gt; MALLOC_ABSOLUTE_MAX_SIZE) { return NULL; } ptr = zone-&gt;malloc(zone, size); // 在 zone 分配完内存后就开始使用 malloc_logger 进行进行记录 if (malloc_logger) { malloc_logger(MALLOC_LOG_TYPE_ALLOCATE | MALLOC_LOG_TYPE_HAS_ZONE, (uintptr_t)zone, (uintptr_t)size, 0, (uintptr_t)ptr, 0); } MALLOC_TRACE(TRACE_malloc | DBG_FUNC_END, (uintptr_t)zone, size, (uintptr_t)ptr, 0); return ptr;} 其分配实现是 zone-&gt;malloc 根据之前的分析，就是szone_t结构体对象中对应的malloc实现。 在创建szone之后，做了一系列如下的初始化操作。 // Initialize the security token.szone-&gt;cookie = (uintptr_t)malloc_entropy[0]; szone-&gt;basic_zone.version = 12;szone-&gt;basic_zone.size = (void *)szone_size;szone-&gt;basic_zone.malloc = (void *)szone_malloc;szone-&gt;basic_zone.calloc = (void *)szone_calloc;szone-&gt;basic_zone.valloc = (void *)szone_valloc;szone-&gt;basic_zone.free = (void *)szone_free;szone-&gt;basic_zone.realloc = (void *)szone_realloc;szone-&gt;basic_zone.destroy = (void *)szone_destroy;szone-&gt;basic_zone.batch_malloc = (void *)szone_batch_malloc;szone-&gt;basic_zone.batch_free = (void *)szone_batch_free;szone-&gt;basic_zone.introspect = (struct malloc_introspection_t *)&amp;szone_introspect;szone-&gt;basic_zone.memalign = (void *)szone_memalign;szone-&gt;basic_zone.free_definite_size = (void *)szone_free_definite_size;szone-&gt;basic_zone.pressure_relief = (void *)szone_pressure_relief;szone-&gt;basic_zone.claimed_address = (void *)szone_claimed_address; 其他使用 scalable_zone 分配内存的函数的方法也类似，所以大内存的分配，不管外部函数如何封装，最终都会调用到 malloc_logger 函数。所以我们可以用 fishhook 去 hook 这个函数，然后记录内存分配情况，结合一定的数据上报机制，上传到服务器，分析并修复。 // For logging VM allocation and deallocation, arg1 here// is the mach_port_name_t of the target task in which the// alloc or dealloc is occurring. For example, for mmap()// that would be mach_task_self(), but for a cross-task-capable// call such as mach_vm_map(), it is the target task. typedef void (malloc_logger_t)(uint32_t type, uintptr_t arg1, uintptr_t arg2, uintptr_t arg3, uintptr_t result, uint32_t num_hot_frames_to_skip); extern malloc_logger_t *__syscall_logger; 当 malloc_logger 和 __syscall_logger 函数指针不为空时，malloc/free、vm_allocate/vm_deallocate 等内存分配/释放通过这两个指针通知上层，这也是内存调试工具 malloc stack 的实现原理。有了这两个函数指针，我们很容易记录当前存活对象的内存分配信息（包括分配大小和分配堆栈）。分配堆栈可以用 backtrace 函数捕获，但捕获到的地址是虚拟内存地址，不能从符号表 DSYM 解析符号。所以还要记录每个 image 加载时的偏移 slide，这样 符号表地址 = 堆栈地址 - slide。 小 tips： ASLR（Address space layout randomization）：常见称呼为位址空间随机载入、位址空间配置随机化、位址空间布局随机化，是一种防止内存损坏漏洞被利用的计算机安全技术，通过随机放置进程关键数据区域的定址空间来放置攻击者能可靠地跳转到内存的特定位置来操作函数。现代作业系统一般都具备该机制。 函数地址 add: 函数真实的实现地址; 函数虚拟地址：vm_add; ASLR: slide 函数虚拟地址加载到进程内存的随机偏移量，每个 mach-o 的 slide 各不相同。vm_add + slide = add。也就是：*(base +offset)= imp。 由于腾讯也开源了自己的 OOM 定位方案- OOMDetector ，有了现成的轮子，那么用好就可以了，所以对于内存的监控思路就是找到系统给 App 的内存上限，然后当接近内存上限值的时候，dump 内存情况，组装基础数据信息成一个合格的上报数据，经过一定的数据上报策略到服务端，服务端消费数据，分析产生报表，客户端工程师根据报表分析问题。不同工程的数据以邮件、短信、企业微信等形式通知到该项目的 owner、开发者。（情况严重的会直接电话给开发者，并给主管跟进每一步的处理结果）。问题分析处理后要么发布新版本，要么 hot fix。 6. 开发阶段针对内存我们能做些什么 图片缩放 WWDC 2018 Session 416 - iOS Memory Deep Dive，处理图片缩放的时候直接使用 UIImage 会在解码时读取文件而占用一部分内存，还会生成中间位图 bitmap 消耗大量内存。而 ImageIO 不存在上述2种弊端，只会占用最终图片大小的内存 做了2组对比实验：给 App 显示一张图片 // 方法1: 19.6M UIImage *imageResult = [self scaleImage:[UIImage imageNamed:@”test”] newSize:CGSizeMake(self.view.frame.size.width, self.view.frame.size.height)]; self.imageView.image = imageResult; // 方法2: 14M NSData *data = UIImagePNGRepresentation([UIImage imageNamed:@”test”]); UIImage *imageResult = [self scaledImageWithData:data withSize:CGSizeMake(self.view.frame.size.width, self.view.frame.size.height) scale:3 orientation:UIImageOrientationUp]; self.imageView.image = imageResult; (UIImage *)scaleImage:(UIImage *)image newSize:(CGSize)newSize{ UIGraphicsBeginImageContextWithOptions(newSize, NO, 0); [image drawInRect:CGRectMake(0, 0, newSize.width, newSize.height)]; UIImage *newImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return newImage;} (UIImage *)scaledImageWithData:(NSData *)data withSize:(CGSize)size scale:(CGFloat)scale orientation:(UIImageOrientation)orientation{ CGFloat maxPixelSize = MAX(size.width, size.height); CGImageSourceRef sourceRef = CGImageSourceCreateWithData((__bridge CFDataRef)data, nil); NSDictionary *options = @{(__bridge id)kCGImageSourceCreateThumbnailFromImageAlways : (__bridge id)kCFBooleanTrue, (\\_\\_bridge id)kCGImageSourceThumbnailMaxPixelSize : \\[NSNumber numberWithFloat:maxPixelSize\\]&#125;; CGImageRef imageRef = CGImageSourceCreateThumbnailAtIndex(sourceRef, 0, (__bridge CFDictionaryRef)options); UIImage *resultImage = [UIImage imageWithCGImage:imageRef scale:scale orientation:orientation]; CGImageRelease(imageRef); CFRelease(sourceRef); return resultImage;} 可以看出使用 ImageIO 比使用 UIImage 直接缩放占用内存更低。 合理使用 autoreleasepool 我们知道 autoreleasepool 对象是在 RunLoop 结束时才释放。在 ARC 下，我们如果在不断申请内存，比如各种循环，那么我们就需要手动添加 autoreleasepool，避免短时间内内存猛涨发生 OOM。 对比实验 // 实验1 NSMutableArray *array = [NSMutableArray array]; for (NSInteger index = 0; index &lt; 10000000; index++) { NSString *indexStrng = [NSString stringWithFormat:@”%zd”, index]; NSString *resultString = [NSString stringWithFormat:@”%zd-%@”, index, indexStrng]; [array addObject:resultString]; } // 实验2 NSMutableArray *array = [NSMutableArray array]; for (NSInteger index = 0; index &lt; 10000000; index++) { @autoreleasepool { NSString *indexStrng = [NSString stringWithFormat:@”%zd”, index]; NSString *resultString = [NSString stringWithFormat:@”%zd-%@”, index, indexStrng]; [array addObject:resultString]; } } 实验1消耗内存 739.6M，实验2消耗内存 587M。 UIGraphicsBeginImageContext 和 UIGraphicsEndImageContext 必须成双出现，不然会造成 context 泄漏。另外 XCode 的 Analyze 也能扫出这类问题。 不管是打开网页，还是执行 js，都应该使用 WKWebView。UIWebView 会占用大量内存，从而导致 App 发生 OOM 的几率增加，而 WKWebView 是一个多进程组件，Network Loading 以及 UI Rendering 在其它进程中执行，比 UIWebView 占用更低的内存开销。 在做 SDK 或者 App，如果场景是缓存相关，尽量使用 NSCache 而不是 NSMutableDictionary。它是系统提供的专门处理缓存的类，NSCache 分配的内存是 Purgeable Memory，可以由系统自动释放。NSCache 与 NSPureableData 的结合使用可以让系统根据情况回收内存，也可以在内存清理时移除对象。 其他的开发习惯就不一一描述了，良好的开发习惯和代码意识是需要平时注意修炼的。 7. 现状及其改进在使用了一波业界优秀的的内存监控工具后发现了一些问题，比如 MLeaksFinder、OOMDetector、FBRetainCycleDetector等都有一些问题。比如 MLeaksFinder 因为单纯通过 VC 的 push、pop 等检测内存泄露的情况，会存在误报的情况。FBRetainCycleDetector 则因为对象深度优先遍历，会有一些性能问题，影响 App 性能。OOMDetector 因为没有合适的触发时机。 思路有2种： MLeaksFinder + FBRetainCycleDetector 结合提高准确性 借鉴头条的实现方案：基于内存快照技术的线上方案，我们称之为——线上 Memory Graph。（引用如下） 基于 Objective-C 对象引用关系找循环引用的方案，适用范围比较小，只能处理部分循环引用问题，而内存问题通常是复杂的，类似于内存堆积，Root Leak，C/C++层问题都无法解决。 基于分配堆栈信息聚类的方案需要常驻运行，对内存、CPU 等资源存在较大消耗，无法针对有内存问题的用户进行监控，只能广撒网，用户体验影响较大。同时，通过某些比较通用的堆栈分配的内存无法定位出实际的内存使用场景，对于循环引用等常见泄漏也无法分析。 核心原理是： 扫描进程中所有的 Dirty 内存，通过内存节点中保存的其他内存节点的地址值，建立起内存节点之间的引用关系的有向图。 全部的讲解可以看这里)。对于 Memory Graph 的实现细节感兴趣的可以看这篇文章 五、 App 网络监控移动网络环境一直很复杂，WIFI、2G、3G、4G、5G 等，用户使用 App 的过程中可能在这几种类型之间切换，这也是移动网络和传统网络间的一个区别，被称为「Connection Migration」。此外还存在 DNS 解析缓慢、失败率高、运营商劫持等问题。用户在使用 App 时因为某些原因导致体验很差，要想针对网络情况进行改善，必须有清晰的监控手段。 1. App 网络请求过程 App 发送一次网络请求一般会经历下面几个关键步骤： DNS 解析 Domain Name system，网络域名名称系统，本质上就是将域名和IP 地址 相互映射的一个分布式数据库，使人们更方便的访问互联网。首先会查询本地的 DNS 缓存，查找失败就去 DNS 服务器查询，这其中可能会经过非常多的节点，涉及到递归查询和迭代查询的过程。运营商可能不干人事：一种情况就是出现运营商劫持的现象，表现为你在 App 内访问某个网页的时候会看到和内容不相关的广告；另一种可能的情况就是把你的请求丢给非常远的基站去做 DNS 解析，导致我们 App 的 DNS 解析时间较长，App 网络效率低。一般做 HTTPDNS 方案去自行解决 DNS 的问题。 TCP 3次握手 关于 TCP 握手过程中为什么是3次握手而不是2次、4次，可以查看这篇文章。 TLS 握手 对于 HTTPS 请求还需要做 TLS 握手，也就是密钥协商的过程。 发送请求 连接建立好之后就可以发送 request，此时可以记录下 request start 时间 等待回应 等待服务器返回响应。这个时间主要取决于资源大小，也是网络请求过程中最为耗时的一个阶段。 返回响应 服务端返回响应给客户端，根据 HTTP header 信息中的状态码判断本次请求是否成功、是否走缓存、是否需要重定向。 2. 监控原理名称 说明 NSURLConnection 已经被废弃。用法简单 NSURLSession iOS7.0 推出，功能更强大 CFNetwork NSURL 的底层，纯 C 实现 iOS 网络框架层级关系如下： iOS 网络现状是由4层组成的：最底层的 BSD Sockets、SecureTransport；次级底层是 CFNetwork、NSURLSession、NSURLConnection、WebView 是用 Objective-C 实现的，且调用 CFNetwork；应用层框架 AFNetworking 基于 NSURLSession、NSURLConnection 实现。 目前业界对于网络监控主要有2种：一种是通过 NSURLProtocol 监控、一种是通过 Hook 来监控。下面介绍几种办法来监控网络请求，各有优缺点。 2.1 方案一：NSURLProtocol 监控 App 网络请求NSURLProtocol 作为上层接口，使用较为简单，但 NSURLProtocol 属于 URL Loading System 体系中。应用协议的支持程度有限，支持 FTP、HTTP、HTTPS 等几个应用层协议，对于其他的协议则无法监控，存在一定的局限性。如果监控底层网络库 CFNetwork 则没有这个限制。 对于 NSURLProtocol 的具体做法在这篇文章中讲过，继承抽象类并实现相应的方法，自定义去发起网络请求来实现监控的目的。 iOS 10 之后，NSURLSessionTaskDelegate 中增加了一个新的代理方法： /* * Sent when complete statistics information has been collected for the task. */ (void)URLSession:(NSURLSession *)session task:(NSURLSessionTask *)task didFinishCollectingMetrics:(NSURLSessionTaskMetrics *)metrics API_AVAILABLE(macosx(10.12), ios(10.0), watchos(3.0), tvos(10.0)); 可以从 NSURLSessionTaskMetrics 中获取到网络情况的各项指标。各项参数如下 @interface NSURLSessionTaskMetrics : NSObject /* * transactionMetrics array contains the metrics collected for every request/response transaction created during the task execution. */@property (copy, readonly) NSArray&lt;NSURLSessionTaskTransactionMetrics *&gt; *transactionMetrics; /* * Interval from the task creation time to the task completion time. * Task creation time is the time when the task was instantiated. * Task completion time is the time when the task is about to change its internal state to completed. */@property (copy, readonly) NSDateInterval *taskInterval; /* * redirectCount is the number of redirects that were recorded. */@property (assign, readonly) NSUInteger redirectCount; (instancetype)init API_DEPRECATED(“Not supported”, macos(10.12,10.15), ios(10.0,13.0), watchos(3.0,6.0), tvos(10.0,13.0)); (instancetype)new API_DEPRECATED(“Not supported”, macos(10.12,10.15), ios(10.0,13.0), watchos(3.0,6.0), tvos(10.0,13.0)); @end 其中：taskInterval 表示任务从创建到完成话费的总时间，任务的创建时间是任务被实例化时的时间，任务完成时间是任务的内部状态将要变为完成的时间；redirectCount 表示被重定向的次数；transactionMetrics 数组包含了任务执行过程中每个请求/响应事务中收集的指标，各项参数如下： /* * This class defines the performance metrics collected for a request/response transaction during the task execution. */ API_AVAILABLE(macosx(10.12), ios(10.0), watchos(3.0), tvos(10.0)) @interface NSURLSessionTaskTransactionMetrics : NSObject /* * Represents the transaction request. 请求事务 */ @property (copy, readonly) NSURLRequest *request; /* * Represents the transaction response. Can be nil if error occurred and no response was generated. 响应事务 */ @property (nullable, copy, readonly) NSURLResponse *response; /* * For all NSDate metrics below, if that aspect of the task could not be completed, then the corresponding “EndDate” metric will be nil. * For example, if a name lookup was started but the name lookup timed out, failed, or the client canceled the task before the name could be resolved -- then while domainLookupStartDate may be set, domainLookupEndDate will be nil along with all later metrics. */ /* * 客户端开始请求的时间，无论是从服务器还是从本地缓存中获取 * fetchStartDate returns the time when the user agent started fetching the resource, whether or not the resource was retrieved from the server or local resources. * * The following metrics will be set to nil, if a persistent connection was used or the resource was retrieved from local resources: * * domainLookupStartDate * domainLookupEndDate * connectStartDate * connectEndDate * secureConnectionStartDate * secureConnectionEndDate */ @property (nullable, copy, readonly) NSDate *fetchStartDate; /* * domainLookupStartDate returns the time immediately before the user agent started the name lookup for the resource. DNS 开始解析的时间 */ @property (nullable, copy, readonly) NSDate *domainLookupStartDate; /* * domainLookupEndDate returns the time after the name lookup was completed. DNS 解析完成的时间 */ @property (nullable, copy, readonly) NSDate *domainLookupEndDate; /* * connectStartDate is the time immediately before the user agent started establishing the connection to the server. * * For example, this would correspond to the time immediately before the user agent started trying to establish the TCP connection. 客户端与服务端开始建立 TCP 连接的时间 */ @property (nullable, copy, readonly) NSDate *connectStartDate; /* * If an encrypted connection was used, secureConnectionStartDate is the time immediately before the user agent started the security handshake to secure the current connection. HTTPS 的 TLS 握手开始的时间 * * For example, this would correspond to the time immediately before the user agent started the TLS handshake. * * If an encrypted connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSDate *secureConnectionStartDate; /* * If an encrypted connection was used, secureConnectionEndDate is the time immediately after the security handshake completed. HTTPS 的 TLS 握手结束的时间 * * If an encrypted connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSDate *secureConnectionEndDate; /* * connectEndDate is the time immediately after the user agent finished establishing the connection to the server, including completion of security-related and other handshakes. 客户端与服务器建立 TCP 连接完成的时间，包括 TLS 握手时间 */ @property (nullable, copy, readonly) NSDate *connectEndDate; /* * requestStartDate is the time immediately before the user agent started requesting the source, regardless of whether the resource was retrieved from the server or local resources. 客户端请求开始的时间，可以理解为开始传输 HTTP 请求的 header 的第一个字节时间 * * For example, this would correspond to the time immediately before the user agent sent an HTTP GET request. */ @property (nullable, copy, readonly) NSDate *requestStartDate; /* * requestEndDate is the time immediately after the user agent finished requesting the source, regardless of whether the resource was retrieved from the server or local resources. 客户端请求结束的时间，可以理解为 HTTP 请求的最后一个字节传输完成的时间 * * For example, this would correspond to the time immediately after the user agent finished sending the last byte of the request. */ @property (nullable, copy, readonly) NSDate *requestEndDate; /* * responseStartDate is the time immediately after the user agent received the first byte of the response from the server or from local resources. 客户端从服务端接收响应的第一个字节的时间 * * For example, this would correspond to the time immediately after the user agent received the first byte of an HTTP response. */ @property (nullable, copy, readonly) NSDate *responseStartDate; /* * responseEndDate is the time immediately after the user agent received the last byte of the resource. 客户端从服务端接收到最后一个请求的时间 */ @property (nullable, copy, readonly) NSDate *responseEndDate; /* * The network protocol used to fetch the resource, as identified by the ALPN Protocol ID Identification Sequence [RFC7301]. * E.g., h2, http/1.1, spdy/3.1. 网络协议名，比如 http/1.1, spdy/3.1 * * When a proxy is configured AND a tunnel connection is established, then this attribute returns the value for the tunneled protocol. * * For example: * If no proxy were used, and HTTP/2 was negotiated, then h2 would be returned. * If HTTP/1.1 were used to the proxy, and the tunneled connection was HTTP/2, then h2 would be returned. * If HTTP/1.1 were used to the proxy, and there were no tunnel, then http/1.1 would be returned. * */ @property (nullable, copy, readonly) NSString *networkProtocolName; /* * This property is set to YES if a proxy connection was used to fetch the resource. 该连接是否使用了代理 */ @property (assign, readonly, getter=isProxyConnection) BOOL proxyConnection; /* * This property is set to YES if a persistent connection was used to fetch the resource. 是否复用了现有连接 */ @property (assign, readonly, getter=isReusedConnection) BOOL reusedConnection; /* * Indicates whether the resource was loaded, pushed or retrieved from the local cache. 获取资源来源 */ @property (assign, readonly) NSURLSessionTaskMetricsResourceFetchType resourceFetchType; /* * countOfRequestHeaderBytesSent is the number of bytes transferred for request header. 请求头的字节数 */ @property (readonly) int64_t countOfRequestHeaderBytesSent API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * countOfRequestBodyBytesSent is the number of bytes transferred for request body. 请求体的字节数 * It includes protocol-specific framing, transfer encoding, and content encoding. */ @property (readonly) int64_t countOfRequestBodyBytesSent API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * countOfRequestBodyBytesBeforeEncoding is the size of upload body data, file, or stream. 上传体数据、文件、流的大小 */ @property (readonly) int64_t countOfRequestBodyBytesBeforeEncoding API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * countOfResponseHeaderBytesReceived is the number of bytes transferred for response header. 响应头的字节数 */ @property (readonly) int64_t countOfResponseHeaderBytesReceived API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * countOfResponseBodyBytesReceived is the number of bytes transferred for response body. 响应体的字节数 * It includes protocol-specific framing, transfer encoding, and content encoding. */ @property (readonly) int64_t countOfResponseBodyBytesReceived API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * countOfResponseBodyBytesAfterDecoding is the size of data delivered to your delegate or completion handler. 给代理方法或者完成后处理的回调的数据大小 */ @property (readonly) int64_t countOfResponseBodyBytesAfterDecoding API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * localAddress is the IP address string of the local interface for the connection. 当前连接下的本地接口 IP 地址 * * For multipath protocols, this is the local address of the initial flow. * * If a connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSString *localAddress API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * localPort is the port number of the local interface for the connection. 当前连接下的本地端口号 * * For multipath protocols, this is the local port of the initial flow. * * If a connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSNumber *localPort API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * remoteAddress is the IP address string of the remote interface for the connection. 当前连接下的远端 IP 地址 * * For multipath protocols, this is the remote address of the initial flow. * * If a connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSString *remoteAddress API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * remotePort is the port number of the remote interface for the connection. 当前连接下的远端端口号 * * For multipath protocols, this is the remote port of the initial flow. * * If a connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSNumber *remotePort API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * negotiatedTLSProtocolVersion is the TLS protocol version negotiated for the connection. 连接协商用的 TLS 协议版本号 * It is a 2-byte sequence in host byte order. * * Please refer to tls_protocol_version_t enum in Security/SecProtocolTypes.h * * If an encrypted connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSNumber *negotiatedTLSProtocolVersion API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * negotiatedTLSCipherSuite is the TLS cipher suite negotiated for the connection. 连接协商用的 TLS 密码套件 * It is a 2-byte sequence in host byte order. * * Please refer to tls_ciphersuite_t enum in Security/SecProtocolTypes.h * * If an encrypted connection was not used, this attribute is set to nil. */ @property (nullable, copy, readonly) NSNumber *negotiatedTLSCipherSuite API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * Whether the connection is established over a cellular interface. 是否是通过蜂窝网络建立的连接 */ @property (readonly, getter=isCellular) BOOL cellular API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * Whether the connection is established over an expensive interface. 是否通过昂贵的接口建立的连接 */ @property (readonly, getter=isExpensive) BOOL expensive API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * Whether the connection is established over a constrained interface. 是否通过受限接口建立的连接 */ @property (readonly, getter=isConstrained) BOOL constrained API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); /* * Whether a multipath protocol is successfully negotiated for the connection. 是否为了连接成功协商了多路径协议 */ @property (readonly, getter=isMultipath) BOOL multipath API_AVAILABLE(macos(10.15), ios(13.0), watchos(6.0), tvos(13.0)); - (instancetype)init API_DEPRECATED(&quot;Not supported&quot;, macos(10.12,10.15), ios(10.0,13.0), watchos(3.0,6.0), tvos(10.0,13.0)); + (instancetype)new API_DEPRECATED(&quot;Not supported&quot;, macos(10.12,10.15), ios(10.0,13.0), watchos(3.0,6.0), tvos(10.0,13.0)); @end 网络监控简单代码 // 监控基础信息@interface NetworkMonitorBaseDataModel : NSObject// 请求的 URL 地址@property (nonatomic, strong) NSString *requestUrl;//请求头@property (nonatomic, strong) NSArray *requestHeaders;//响应头@property (nonatomic, strong) NSArray *responseHeaders;//GET方法 的请求参数@property (nonatomic, strong) NSString *getRequestParams;//HTTP 方法, 比如 POST@property (nonatomic, strong) NSString *httpMethod;//协议名，如http1.0 / http1.1 / http2.0@property (nonatomic, strong) NSString *httpProtocol;//是否使用代理@property (nonatomic, assign) BOOL useProxy;//DNS解析后的 IP 地址@property (nonatomic, strong) NSString *ip;@end // 监控信息模型@interface NetworkMonitorDataModel : NetworkMonitorBaseDataModel//客户端发起请求的时间@property (nonatomic, assign) UInt64 requestDate;//客户端开始请求到开始dns解析的等待时间,单位ms@property (nonatomic, assign) int waitDNSTime;//DNS 解析耗时@property (nonatomic, assign) int dnsLookupTime;//tcp 三次握手耗时,单位ms@property (nonatomic, assign) int tcpTime;//ssl 握手耗时@property (nonatomic, assign) int sslTime;//一个完整请求的耗时,单位ms@property (nonatomic, assign) int requestTime;//http 响应码@property (nonatomic, assign) NSUInteger httpCode;//发送的字节数@property (nonatomic, assign) UInt64 sendBytes;//接收的字节数@property (nonatomic, assign) UInt64 receiveBytes; // 错误信息模型@interface NetworkMonitorErrorModel : NetworkMonitorBaseDataModel//错误码@property (nonatomic, assign) NSInteger errorCode;//错误次数@property (nonatomic, assign) NSUInteger errCount;//异常名@property (nonatomic, strong) NSString *exceptionName;//异常详情@property (nonatomic, strong) NSString *exceptionDetail;//异常堆栈@property (nonatomic, strong) NSString *stackTrace;@end // 继承自 NSURLProtocol 抽象类，实现响应方法，代理网络请求@interface CustomURLProtocol () &lt;NSURLSessionTaskDelegate&gt; @property (nonatomic, strong) NSURLSessionDataTask *dataTask;@property (nonatomic, strong) NSOperationQueue *sessionDelegateQueue;@property (nonatomic, strong) NetworkMonitorDataModel *dataModel;@property (nonatomic, strong) NetworkMonitorErrorModel *errModel; @end //使用NSURLSessionDataTask请求网络 (void)startLoading { NSURLSessionConfiguration *configuration = [NSURLSessionConfiguration defaultSessionConfiguration];NSURLSession \\*session = \\[NSURLSession sessionWithConfiguration:configuration delegate:self delegateQueue:nil\\]; NSURLSession *session = [NSURLSession sessionWithConfiguration:configuration delegate:self delegateQueue:nil];self.sessionDelegateQueue = \\[\\[NSOperationQueue alloc\\] init\\]; self.sessionDelegateQueue.maxConcurrentOperationCount = 1; self.sessionDelegateQueue.name = @”com.networkMonitor.session.queue”; self.dataTask = [session dataTaskWithRequest:self.request]; [self.dataTask resume];} #pragma mark - NSURLSessionTaskDelegate (void)URLSession:(NSURLSession *)session task:(NSURLSessionTask *)task didCompleteWithError:(NSError *)error { if (error) { \\[self.client URLProtocol:self didFailWithError:error\\]; } else { \\[self.client URLProtocolDidFinishLoading:self\\]; } if (error) { NSURLRequest \\*request = task.currentRequest; if (request) &#123; self.errModel.requestUrl = request.URL.absoluteString; self.errModel.httpMethod = request.HTTPMethod; self.errModel.requestParams = request.URL.query; &#125; self.errModel.errorCode = error.code; self.errModel.exceptionName = error.domain; self.errModel.exceptionDetail = error.description; // 上传 Network 数据到数据上报组件，数据上报会在 \\[打造功能强大、灵活可配置的数据上报组件\\](https://github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1%20-%20iOS/1.80.md) 讲 } self.dataTask = nil;} (void)URLSession:(NSURLSession *)session task:(NSURLSessionTask *)task didFinishCollectingMetrics:(NSURLSessionTaskMetrics *)metrics { if (@available(iOS 10.0, \\*) &amp;&amp; \\[metrics.transactionMetrics count\\] &gt; 0) &#123; \\[metrics.transactionMetrics enumerateObjectsUsingBlock:^(NSURLSessionTaskTransactionMetrics \\*\\_Nonnull obj, NSUInteger idx, BOOL \\*\\_Nonnull stop) &#123; if (obj.resourceFetchType == NSURLSessionTaskMetricsResourceFetchTypeNetworkLoad) &#123; if (obj.fetchStartDate) &#123; self.dataModel.requestDate = \\[obj.fetchStartDate timeIntervalSince1970\\] \\* 1000; &#125; if (obj.domainLookupStartDate &amp;&amp; obj.domainLookupEndDate) &#123; self.dataModel. waitDNSTime = ceil(\\[obj.domainLookupStartDate timeIntervalSinceDate:obj.fetchStartDate\\] \\* 1000); self.dataModel. dnsLookupTime = ceil(\\[obj.domainLookupEndDate timeIntervalSinceDate:obj.domainLookupStartDate\\] \\* 1000); &#125; if (obj.connectStartDate) &#123; if (obj.secureConnectionStartDate) &#123; self.dataModel. waitDNSTime = ceil(\\[obj.secureConnectionStartDate timeIntervalSinceDate:obj.connectStartDate\\] \\* 1000); &#125; else if (obj.connectEndDate) &#123; self.dataModel.tcpTime = ceil(\\[obj.connectEndDate timeIntervalSinceDate:obj.connectStartDate\\] \\* 1000); &#125; &#125; if (obj.secureConnectionEndDate &amp;&amp; obj.secureConnectionStartDate) &#123; self.dataModel.sslTime = ceil(\\[obj.secureConnectionEndDate timeIntervalSinceDate:obj.secureConnectionStartDate\\] \\* 1000); &#125; if (obj.fetchStartDate &amp;&amp; obj.responseEndDate) &#123; self.dataModel.requestTime = ceil(\\[obj.responseEndDate timeIntervalSinceDate:obj.fetchStartDate\\] \\* 1000); &#125; self.dataModel.httpProtocol = obj.networkProtocolName; NSHTTPURLResponse \\*response = (NSHTTPURLResponse \\*)obj.response; if (\\[response isKindOfClass:NSHTTPURLResponse.class\\]) &#123; self.dataModel.receiveBytes = response.expectedContentLength; &#125; if (\\[obj respondsToSelector:@selector(\\_remoteAddressAndPort)\\]) &#123; self.dataModel.ip = \\[obj valueForKey:@&quot;\\_remoteAddressAndPort&quot;\\]; &#125; if (\\[obj respondsToSelector:@selector(\\_requestHeaderBytesSent)\\]) &#123; self.dataModel.sendBytes = \\[\\[obj valueForKey:@&quot;\\_requestHeaderBytesSent&quot;\\] unsignedIntegerValue\\]; &#125; if (\\[obj respondsToSelector:@selector(\\_responseHeaderBytesReceived)\\]) &#123; self.dataModel.receiveBytes = \\[\\[obj valueForKey:@&quot;\\_responseHeaderBytesReceived&quot;\\] unsignedIntegerValue\\]; &#125; self.dataModel.requestUrl = \\[obj.request.URL absoluteString\\]; self.dataModel.httpMethod = obj.request.HTTPMethod; self.dataModel.useProxy = obj.isProxyConnection; &#125; &#125;\\]; // 上传 Network 数据到数据上报组件，数据上报会在 \\[打造功能强大、灵活可配置的数据上报组件\\](https://github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1%20-%20iOS/1.80.md) 讲 }} 2.2 方案二：NSURLProtocol 监控 App 网络请求之黑魔法篇文章上面 2.1 分析到了 NSURLSessionTaskMetrics 由于兼容性问题，对于网络监控来说似乎不太完美，但是自后在搜资料的时候看到了一篇文章。文章在分析 WebView 的网络监控的时候分析 Webkit 源码的时候发现了下面代码 #if !HAVE(TIMINGDATAOPTIONS)void setCollectsTimingData(){ static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ [NSURLConnection _setCollectsTimingData:YES]; … });}#endif 也就是说明 NSURLConnection 本身有一套 TimingData 的收集 API，只是没有暴露给开发者，苹果自己在用而已。在 runtime header 中找到了 NSURLConnection 的 _setCollectsTimingData: 、_timingData 2个 api（iOS8 以后可以使用）。 NSURLSession 在 iOS9 之前使用 _setCollectsTimingData: 就可以使用 TimingData 了。 注意： 因为是私有 API，所以在使用的时候注意混淆。比如 [[@&quot;_setC&quot; stringByAppendingString:@&quot;ollectsT&quot;] stringByAppendingString:@&quot;imingData:&quot;]。 不推荐私有 API，一般做 APM 的属于公共团队，你想想看虽然你做的 SDK 达到网络监控的目的了，但是万一给业务线的 App 上架造成了问题，那就得不偿失了。一般这种投机取巧，不是百分百确定的事情可以在玩具阶段使用。 @interface _NSURLConnectionProxy : DelegateProxy @end @implementation _NSURLConnectionProxy (BOOL)respondsToSelector:(SEL)aSelector{ if ([NSStringFromSelector(aSelector) isEqualToString:@”connectionDidFinishLoading:”]) { return YES; } return [self.target respondsToSelector:aSelector];} (void)forwardInvocation:(NSInvocation *)invocation{ [super forwardInvocation:invocation]; if ([NSStringFromSelector(invocation.selector) isEqualToString:@”connectionDidFinishLoading:”]) { \\_\\_unsafe\\_unretained NSURLConnection \\*conn; \\[invocation getArgument:&amp;conn atIndex:2\\]; SEL selector = NSSelectorFromString(\\[@&quot;\\_timin&quot; stringByAppendingString:@&quot;gData&quot;\\]); NSDictionary \\*timingData = \\[conn performSelector:selector\\]; \\[\\[NTDataKeeper shareInstance\\] trackTimingData:timingData request:conn.currentRequest\\]; }} @end @implementation NSURLConnection(tracker) (void)load{ static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ Class class = \\[self class\\]; SEL originalSelector = @selector(initWithRequest:delegate:); SEL swizzledSelector = @selector(swizzledInitWithRequest:delegate:); Method originalMethod = class\\_getInstanceMethod(class, originalSelector); Method swizzledMethod = class\\_getInstanceMethod(class, swizzledSelector); method\\_exchangeImplementations(originalMethod, swizzledMethod); NSString \\*selectorName = \\[\\[@&quot;\\_setC&quot; stringByAppendingString:@&quot;ollectsT&quot;\\] stringByAppendingString:@&quot;imingData:&quot;\\]; SEL selector = NSSelectorFromString(selectorName); \\[NSURLConnection performSelector:selector withObject:@(YES)\\]; });} (instancetype)swizzledInitWithRequest:(NSURLRequest *)request delegate:(id&lt;NSURLConnectionDelegate&gt;)delegate{ if (delegate) { \\_NSURLConnectionProxy \\*proxy = \\[\\[\\_NSURLConnectionProxy alloc\\] initWithTarget:delegate\\]; objc\\_setAssociatedObject(delegate ,@&quot;\\_NSURLConnectionProxy&quot; ,proxy, OBJC\\_ASSOCIATION\\_RETAIN\\_NONATOMIC); return \\[self swizzledInitWithRequest:request delegate:(id&lt;NSURLConnectionDelegate\\&gt;)proxy\\]; }else{ return \\[self swizzledInitWithRequest:request delegate:delegate\\]; }} @end 2.3 方案三：HookiOS 中 hook 技术有2类，一种是 NSProxy，一种是 method swizzling（isa swizzling） 2.3.1 方法一写 SDK 肯定不可能手动侵入业务代码（你没那个权限提交到线上代码 😂），所以不管是 APM 还是无痕埋点都是通过 Hook 的方式。 面向切面程序设计（Aspect-oriented Programming，AOP）是计算机科学中的一种程序设计范型，将横切关注点与业务主体进一步分离，以提高程序代码的模块化程度。在不修改源代码的情况下给程序动态增加功能。其核心思想是将业务逻辑（核心关注点，系统主要功能）与公共功能（横切关注点，比如日志系统）进行分离，降低复杂性，保持系统模块化程度、可维护性、可重用性。常被用在日志系统、性能统计、安全控制、事务处理、异常处理等场景下。 在 iOS 中 AOP 的实现是基于 Runtime 机制，目前由3种方式：Method Swizzling、NSProxy、FishHook（主要用用于 hook c 代码）。 文章上面 2.1 讨论了满足大多数的需求的场景，NSURLProtocol 监控了 NSURLConnection、NSURLSession 的网络请求，自身代理后可以发起网络请求并得到诸如请求开始时间、请求结束时间、header 信息等，但是无法得到非常详细的网络性能数据，比如 DNS 开始解析时间、DNS 解析用了多久、reponse 开始返回的时间、返回了多久等。 iOS10 之后 NSURLSessionTaskDelegate 增加了一个代理方法 - (void)URLSession:(NSURLSession *)session task:(NSURLSessionTask *)task didFinishCollectingMetrics:(NSURLSessionTaskMetrics *)metrics API_AVAILABLE(macosx(10.12), ios(10.0), watchos(3.0), tvos(10.0));，可以获取到精确的各项网络数据。但是具有兼容性。文章上面 2.2 讨论了从 Webkit 源码中得到的信息，通过私有方法 _setCollectsTimingData: 、_timingData 可以获取到 TimingData。 但是如果需要监全部的网络请求就不能满足需求了，查阅资料后发现了阿里百川有 APM 的解决方案，于是有了方案3，对于网络监控需要做如下的处理 可能对于 CFNetwork 比较陌生，可以看一下 CFNetwork 的层级和简单用法 CFNetwork 的基础是 CFSocket 和 CFStream。 CFSocket：Socket 是网络通信的底层基础，可以让2个 socket 端口互发数据，iOS 中最常用的 socket 抽象是 BSD socket。而 CFSocket 是 BSD socket 的 OC 包装，几乎实现了所有的 BSD 功能，此外加入了 RunLoop。 CFStream：提供了与设备无关的读写数据方法，使用它可以为内存、文件、网络（使用 socket）的数据建立流，使用 stream 可以不必将所有数据写入到内存中。CFStream 提供 API 对2种 CFType 对象提供抽象：CFReadStream、CFWriteStream。同时也是 CFHTTP、CFFTP 的基础。 简单 Demo - (void)testCFNetwork{ CFURLRef urlRef = CFURLCreateWithString(kCFAllocatorDefault, CFSTR(“https://httpbin.org/get&quot;), NULL); CFHTTPMessageRef httpMessageRef = CFHTTPMessageCreateRequest(kCFAllocatorDefault, CFSTR(“GET”), urlRef, kCFHTTPVersion1_1); CFRelease(urlRef); CFReadStreamRef readStream = CFReadStreamCreateForHTTPRequest(kCFAllocatorDefault, httpMessageRef); CFRelease(httpMessageRef); CFReadStreamScheduleWithRunLoop(readStream, CFRunLoopGetCurrent(), kCFRunLoopCommonModes); CFOptionFlags eventFlags = (kCFStreamEventHasBytesAvailable | kCFStreamEventErrorOccurred | kCFStreamEventEndEncountered); CFStreamClientContext context = &#123; 0, NULL, NULL, NULL, NULL &#125; ; // Assigns a client to a stream, which receives callbacks when certain events occur. CFReadStreamSetClient(readStream, eventFlags, CFNetworkRequestCallback, &amp;context); // Opens a stream for reading. CFReadStreamOpen(readStream); }// callbackvoid CFNetworkRequestCallback (CFReadStreamRef _Null_unspecified stream, CFStreamEventType type, void * _Null_unspecified clientCallBackInfo) { CFMutableDataRef responseBytes = CFDataCreateMutable(kCFAllocatorDefault, 0); CFIndex numberOfBytesRead = 0; do { UInt8 buffer[2014]; numberOfBytesRead = CFReadStreamRead(stream, buffer, sizeof(buffer)); if (numberOfBytesRead &gt; 0) { CFDataAppendBytes(responseBytes, buffer, numberOfBytesRead); } } while (numberOfBytesRead &gt; 0); CFHTTPMessageRef response = (CFHTTPMessageRef)CFReadStreamCopyProperty(stream, kCFStreamPropertyHTTPResponseHeader); if (responseBytes) &#123; if (response) &#123; CFHTTPMessageSetBody(response, responseBytes); &#125; CFRelease(responseBytes); &#125; // close and cleanup CFReadStreamClose(stream); CFReadStreamUnscheduleFromRunLoop(stream, CFRunLoopGetCurrent(), kCFRunLoopCommonModes); CFRelease(stream); // print response if (response) &#123; CFDataRef reponseBodyData = CFHTTPMessageCopyBody(response); CFRelease(response); printResponseData(reponseBodyData); CFRelease(reponseBodyData); &#125; } void printResponseData (CFDataRef responseData) { CFIndex dataLength = CFDataGetLength(responseData); UInt8 *bytes = (UInt8 *)malloc(dataLength); CFDataGetBytes(responseData, CFRangeMake(0, CFDataGetLength(responseData)), bytes); CFStringRef responseString = CFStringCreateWithBytes(kCFAllocatorDefault, bytes, dataLength, kCFStringEncodingUTF8, TRUE); CFShow(responseString); CFRelease(responseString); free(bytes);}// console{ “args”: {}, “headers”: { “Host”: “httpbin.org”, “User-Agent”: “Test/1 CFNetwork/1125.2 Darwin/19.3.0”, “X-Amzn-Trace-Id”: “Root=1-5e8980d0-581f3f44724c7140614c2564” }, “origin”: “183.159.122.102”, “url”: “https://httpbin.org/get&quot;} 我们知道 NSURLSession、NSURLConnection、CFNetwork 的使用都需要调用一堆方法进行设置然后需要设置代理对象，实现代理方法。所以针对这种情况进行监控首先想到的是使用 runtime hook 掉方法层级。但是针对设置的代理对象的代理方法没办法 hook，因为不知道代理对象是哪个类。所以想办法可以 hook 设置代理对象这个步骤，将代理对象替换成我们设计好的某个类，然后让这个类去实现 NSURLConnection、NSURLSession、CFNetwork 相关的代理方法。然后在这些方法的内部都去调用一下原代理对象的方法实现。所以我们的需求得以满足，我们在相应的方法里面可以拿到监控数据，比如请求开始时间、结束时间、状态码、内容大小等。 NSURLSession、NSURLConnection hook 如下。 业界有 APM 针对 CFNetwork 的方案，整理描述下： CFNetwork 是 c 语言实现的，要对 c 代码进行 hook 需要使用 Dynamic Loader Hook 库 - fishhook。 Dynamic Loader（dyld）通过更新 Mach-O 文件中保存的指针的方法来绑定符号。借用它可以在 Runtime 修改 C 函数调用的函数指针。fishhook 的实现原理：遍历 __DATA segment 里面 __nl_symbol_ptr 、__la_symbol_ptr 两个 section 里面的符号，通过 Indirect Symbol Table、Symbol Table 和 String Table 的配合，找到自己要替换的函数，达到 hook 的目的。 /* Returns the number of bytes read, or -1 if an error occurs preventing any bytes from being read, or 0 if the stream’s end was encountered. It is an error to try and read from a stream that hasn’t been opened first. This call will block until at least one byte is available; it will NOT block until the entire buffer can be filled. To avoid blocking, either poll using CFReadStreamHasBytesAvailable() or use the run loop and listen for the kCFStreamEventHasBytesAvailable event for notification of data available. */ CF_EXPORT CFIndex CFReadStreamRead(CFReadStreamRef _Null_unspecified stream, UInt8 * _Null_unspecified buffer, CFIndex bufferLength); CFNetwork 使用 CFReadStreamRef 来传递数据，使用回调函数的形式来接受服务器的响应。当回调函数受到 具体步骤及其关键代码如下，以 NSURLConnection 举例 因为要 Hook 挺多地方，所以写一个 method swizzling 的工具类 #import &lt;Foundation/Foundation.h&gt; NS_ASSUME_NONNULL_BEGIN @interface NSObject (hook) /** hook对象方法 @param originalSelector 需要hook的原始对象方法 @param swizzledSelector 需要替换的对象方法 */ (void)apm_swizzleMethod:(SEL)originalSelector swizzledSelector:(SEL)swizzledSelector; /**hook类方法 @param originalSelector 需要hook的原始类方法@param swizzledSelector 需要替换的类方法*/ (void)apm_swizzleClassMethod:(SEL)originalSelector swizzledSelector:(SEL)swizzledSelector; @end NS_ASSUME_NONNULL_END (void)apm_swizzleMethod:(SEL)originalSelector swizzledSelector:(SEL)swizzledSelector{ class_swizzleInstanceMethod(self, originalSelector, swizzledSelector);} (void)apm_swizzleClassMethod:(SEL)originalSelector swizzledSelector:(SEL)swizzledSelector{ //类方法实际上是储存在类对象的类(即元类)中，即类方法相当于元类的实例方法,所以只需要把元类传入，其他逻辑和交互实例方法一样。 Class class2 = object_getClass(self); class_swizzleInstanceMethod(class2, originalSelector, swizzledSelector);} void class_swizzleInstanceMethod(Class class, SEL originalSEL, SEL replacementSEL){ Method originMethod = class_getInstanceMethod(class, originalSEL); Method replaceMethod = class_getInstanceMethod(class, replacementSEL); if(class_addMethod(class, originalSEL, method_getImplementation(replaceMethod),method_getTypeEncoding(replaceMethod))) { class\\_replaceMethod(class,replacementSEL, method\\_getImplementation(originMethod), method\\_getTypeEncoding(originMethod)); }else { method\\_exchangeImplementations(originMethod, replaceMethod); }} 建立一个继承自 NSProxy 抽象类的类，实现相应方法。 #import &lt;Foundation/Foundation.h&gt; NS_ASSUME_NONNULL_BEGIN // 为 NSURLConnection、NSURLSession、CFNetwork 代理设置代理转发 @interface NetworkDelegateProxy : NSProxy (instancetype)setProxyForObject:(id)originalTarget withNewDelegate:(id)newDelegate; @end NS_ASSUME_NONNULL_END // .m@interface NetworkDelegateProxy () { id _originalTarget; id _NewDelegate;} @end @implementation NetworkDelegateProxy #pragma mark - life cycle (instancetype)sharedInstance { static NetworkDelegateProxy *_sharedInstance = nil; static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ \\_sharedInstance = \\[NetworkDelegateProxy alloc\\]; }); return _sharedInstance;} #pragma mark - public Method (instancetype)setProxyForObject:(id)originalTarget withNewDelegate:(id)newDelegate{ NetworkDelegateProxy *instance = [NetworkDelegateProxy sharedInstance]; instance-&gt;_originalTarget = originalTarget; instance-&gt;_NewDelegate = newDelegate; return instance;} (void)forwardInvocation:(NSInvocation *)invocation{ if ([_originalTarget respondsToSelector:invocation.selector]) { \\[invocation invokeWithTarget:\\_originalTarget\\]; \\[((NSURLSessionAndConnectionImplementor \\*)\\_NewDelegate) invoke:invocation\\]; }} (nullable NSMethodSignature *)methodSignatureForSelector:(SEL)sel{ return [_originalTarget methodSignatureForSelector:sel];} @end 创建一个对象，实现 NSURLConnection、NSURLSession、NSIuputStream 代理方法 // NetworkImplementor.m #pragma mark-NSURLConnectionDelegate (void)connection:(NSURLConnection *)connection didFailWithError:(NSError *)error { NSLog(@”%s”, __func__);} (nullable NSURLRequest *)connection:(NSURLConnection *)connection willSendRequest:(NSURLRequest *)request redirectResponse:(nullable NSURLResponse *)response { NSLog(@”%s”, __func__); return request;} #pragma mark-NSURLConnectionDataDelegate (void)connection:(NSURLConnection *)connection didReceiveResponse:(NSURLResponse *)response { NSLog(@”%s”, __func__);} (void)connection:(NSURLConnection *)connection didReceiveData:(NSData *)data { NSLog(@”%s”, __func__);} (void)connection:(NSURLConnection *)connection didSendBodyData:(NSInteger)bytesWrittentotalBytesWritten:(NSInteger)totalBytesWrittentotalBytesExpectedToWrite:(NSInteger)totalBytesExpectedToWrite { NSLog(@”%s”, __func__);} (void)connectionDidFinishLoading:(NSURLConnection *)connection { NSLog(@”%s”, __func__);} #pragma mark-NSURLConnectionDownloadDelegate (void)connection:(NSURLConnection *)connection didWriteData:(long long)bytesWritten totalBytesWritten:(long long)totalBytesWritten expectedTotalBytes:(long long) expectedTotalBytes { NSLog(@”%s”, __func__);} (void)connectionDidResumeDownloading:(NSURLConnection *)connection totalBytesWritten:(long long)totalBytesWritten expectedTotalBytes:(long long) expectedTotalBytes { NSLog(@”%s”, __func__);} (void)connectionDidFinishDownloading:(NSURLConnection *)connection destinationURL:(NSURL *) destinationURL { NSLog(@”%s”, __func__);}// 根据需求自己去写需要监控的数据项 给 NSURLConnection 添加 Category，专门设置 hook 代理对象、hook NSURLConnection 对象方法 // NSURLConnection+Monitor.m @implementation NSURLConnection (Monitor) (void)load{ static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ @autoreleasepool &#123; \\[\\[self class\\] apm\\_swizzleMethod:@selector(apm\\_initWithRequest:delegate:) swizzledSelector:@selector(initWithRequest: delegate:)\\]; &#125; });} (_Nonnull instancetype)apm_initWithRequest:(NSURLRequest *)request delegate:(nullable id)delegate{ /* 在设置 Delegate 的时候替换 delegate。 因为要在每个代理方法里面，监控数据，所以需要将代理方法都 hook 下 在原代理方法执行的时候，让新的代理对象里面，去执行方法的转发，*/NSString *traceId = @”traceId”;NSMutableURLRequest *rq = [request mutableCopy];NSString *preTraceId = [request.allHTTPHeaderFields valueForKey:@”head_key_traceid”];if (preTraceId) {// 调用 hook 之前的初始化方法，返回 NSURLConnectionreturn [self apm_initWithRequest:rq delegate:delegate];} else {[rq setValue:traceId forHTTPHeaderField:@”head_key_traceid”]; NSURLSessionAndConnectionImplementor *mockDelegate = [NSURLSessionAndConnectionImplementor new];[self registerDelegateMethod:@”connection:didFailWithError:” originalDelegate:delegate newDelegate:mockDelegate flag:”v@:@@”]; [self registerDelegateMethod:@”connection:didReceiveResponse:” originalDelegate:delegate newDelegate:mockDelegate flag:”v@:@@”];[self registerDelegateMethod:@”connection:didReceiveData:” originalDelegate:delegate newDelegate:mockDelegate flag:”v@:@@”];[self registerDelegateMethod:@”connection:didFailWithError:” originalDelegate:delegate newDelegate:mockDelegate flag:”v@:@@”]; [self registerDelegateMethod:@”connectionDidFinishLoading:” originalDelegate:delegate newDelegate:mockDelegate flag:”v@:@”];[self registerDelegateMethod:@”connection:willSendRequest:redirectResponse:” originalDelegate:delegate newDelegate:mockDelegate flag:”@@:@@”];delegate = [NetworkDelegateProxy setProxyForObject:delegate withNewDelegate:mockDelegate]; // 调用 hook 之前的初始化方法，返回 NSURLConnectionreturn [self apm_initWithRequest:rq delegate:delegate];}} (void)registerDelegateMethod:(NSString *)methodName originalDelegate:(id&lt;NSURLConnectionDelegate&gt;)originalDelegate newDelegate:(NSURLSessionAndConnectionImplementor *)newDelegate flag:(const char *)flag{ if ([originalDelegate respondsToSelector:NSSelectorFromString(methodName)]) { IMP originalMethodImp = class\\_getMethodImplementation(\\[originalDelegate class\\], NSSelectorFromString(methodName)); IMP newMethodImp = class\\_getMethodImplementation(\\[newDelegate class\\], NSSelectorFromString(methodName)); if (originalMethodImp != newMethodImp) &#123; \\[newDelegate registerSelector: methodName\\]; NSLog(@&quot;&quot;); &#125; } else { class\\_addMethod(\\[originalDelegate class\\], NSSelectorFromString(methodName), class\\_getMethodImplementation(\\[newDelegate class\\], NSSelectorFromString(methodName)), flag); }} @end 这样下来就是可以监控到网络信息了，然后将数据交给数据上报 SDK，按照下发的数据上报策略去上报数据。 2.3.2 方法二其实，针对上述的需求还有另一种方法一样可以达到目的，那就是 isa swizzling。 顺道说一句，上面针对 NSURLConnection、NSURLSession、NSInputStream 代理对象的 hook 之后，利用 NSProxy 实现代理对象方法的转发，有另一种方法可以实现，那就是 isa swizzling。 Method swizzling 原理 struct old_method { SEL method\\_name; char \\*method\\_types; IMP method\\_imp; }; method swizzling 改进版如下 Method originalMethod = class_getInstanceMethod(aClass, aSEL);IMP originalIMP = method_getImplementation(originalMethod);char *cd = method_getTypeEncoding(originalMethod);IMP newIMP = imp_implementationWithBlock(^(id self) { void (*tmp)(id self, SEL _cmd) = originalIMP; tmp(self, aSEL);});class_replaceMethod(aClass, aSEL, newIMP, cd); isa swizzling /// Represents an instance of a class. struct objc_object { Class \\_Nonnull isa OBJC\\_ISA\\_AVAILABILITY; }; /// A pointer to an instance of a class. typedef struct objc_object *id; 我们来分析一下为什么修改 isa 可以实现目的呢？ 写 APM 监控的人没办法确定业务代码 不可能为了方便监控 APM，写某些类，让业务线开发者别使用系统 NSURLSession、NSURLConnection 类 想想 KVO 的实现原理？结合上面的图 创建监控对象子类 重写子类中属性的 getter、seeter 将监控对象的 isa 指针指向新创建的子类 在子类的 getter、setter 中拦截值的变化，通知监控对象值的变化 监控完之后将监控对象的 isa 还原回去 按照这个思路，我们也可以对 NSURLConnection、NSURLSession 的 load 方法中动态创建子类，在子类中重写方法，比如 - (**nullable** **instancetype**)initWithRequest:(NSURLRequest *)request delegate:(**nullable** **id**)delegate startImmediately:(**BOOL**)startImmediately; ，然后将 NSURLSession、NSURLConnection 的 isa 指向动态创建的子类。在这些方法处理完之后还原本身的 isa 指针。 不过 isa swizzling 针对的还是 method swizzling，代理对象不确定，还是需要 NSProxy 进行动态处理。 至于如何修改 isa，我写一个简单的 Demo 来模拟 KVO - (void)lbpKVO_addObserver:(NSObject *)observer forKeyPath:(NSString *)keyPath options:(NSKeyValueObservingOptions)options context:(nullable void *)context { //生成自定义的名称 NSString *className = NSStringFromClass(self.class); NSString *currentClassName = [@”LBPKVONotifying_“ stringByAppendingString:className]; //1. runtime 生成类 Class myclass = objc_allocateClassPair(self.class, [currentClassName UTF8String], 0); // 生成后不能马上使用，必须先注册 objc_registerClassPair(myclass); //2. 重写 setter 方法 class\\_addMethod(myclass,@selector(say) , (IMP)say, &quot;v@:@&quot;); // class_addMethod(myclass,@selector(setName:) , (IMP)setName, “v@:@”); //3. 修改 isa object_setClass(self, myclass); //4. 将观察者保存到当前对象里面 objc\\_setAssociatedObject(self, &quot;observer&quot;, observer, OBJC\\_ASSOCIATION\\_ASSIGN); //5. 将传递的上下文绑定到当前对象里面 objc\\_setAssociatedObject(self, &quot;context&quot;, (\\_\\_bridge id \\_Nullable)(context), OBJC\\_ASSOCIATION\\_RETAIN); } void say(id self, SEL _cmd){ // 调用父类方法一 struct objc_super superclass = {self, [self superclass]}; ((void(*)(struct objc_super *,SEL))objc_msgSendSuper)(&amp;superclass,@selector(say)); NSLog(@”%s”, __func__);// 调用父类方法二// Class class = [self class];// object_setClass(self, class_getSuperclass(class));// objc_msgSend(self, @selector(say));} void setName (id self, SEL _cmd, NSString *name) { NSLog(@”come here”); //先切换到当前类的父类，然后发送消息 setName，然后切换当前子类 //1. 切换到父类 Class class = [self class]; object_setClass(self, class_getSuperclass(class)); //2. 调用父类的 setName 方法 objc_msgSend(self, @selector(setName:), name); //3. 调用观察 id observer = objc\\_getAssociatedObject(self, &quot;observer&quot;); id context = objc\\_getAssociatedObject(self, &quot;context&quot;); if (observer) &#123; objc\\_msgSend(observer, @selector(observeValueForKeyPath:ofObject:change:context:), @&quot;name&quot;, self, @&#123;@&quot;new&quot;: name, @&quot;kind&quot;: @1 &#125; , context); &#125; //4. 改回子类 object\\_setClass(self, class); } @end 2.4 方案四：监控 App 常见网络请求本着成本的原因，由于现在大多数的项目的网络能力都是通过 AFNetworking 完成的，所以本文的网络监控可以快速完成。 AFNetworking 在发起网络的时候会有相应的通知。AFNetworkingTaskDidResumeNotification 和 AFNetworkingTaskDidCompleteNotification。通过监听通知携带的参数获取网络情况信息。 self.didResumeObserver = [[NSNotificationCenter defaultCenter] addObserverForName:AFNetworkingTaskDidResumeNotification object:nil queue:self.queue usingBlock:^(NSNotification * _Nonnull note) { // 开始 __strong __typeof(weakSelf)strongSelf = weakSelf; NSURLSessionTask *task = note.object; NSString *requestId = [[NSUUID UUID] UUIDString]; task.apm_requestId = requestId; [strongSelf.networkRecoder recordStartRequestWithRequestID:requestId task:task];}]; self.didCompleteObserver = [[NSNotificationCenter defaultCenter] addObserverForName:AFNetworkingTaskDidCompleteNotification object:nil queue:self.queue usingBlock:^(NSNotification * _Nonnull note) { \\_\\_strong \\_\\_typeof(weakSelf)strongSelf = weakSelf; NSError \\*error = note.userInfo\\[AFNetworkingTaskDidCompleteErrorKey\\]; NSURLSessionTask \\*task = note.object; if (!error) &#123; // 成功 \\[strongSelf.networkRecoder recordFinishRequestWithRequestID:task.apmn\\_requestId task:task\\]; &#125; else &#123; // 失败 \\[strongSelf.networkRecoder recordResponseErrorWithRequestID:task.apmn\\_requestId task:task error:error\\]; &#125; }]; 在 networkRecoder 的方法里面去组装数据，交给数据上报组件，等到合适的时机策略去上报。 因为网络是一个异步的过程，所以当网络请求开始的时候需要为每个网络设置唯一标识，等到网络请求完成后再根据每个请求的标识，判断该网络耗时多久、是否成功等。所以措施是为 NSURLSessionTask 添加分类，通过 runtime 增加一个属性，也就是唯一标识。 这里插一嘴，为 Category 命名、以及内部的属性和方法命名的时候需要注意下。假如不注意会怎么样呢？假如你要为 NSString 类增加身份证号码中间位数隐藏的功能，那么写代码久了的老司机 A，为 NSString 增加了一个方法名，叫做 getMaskedIdCardNumber，但是他的需求是从 [9, 12] 这4位字符串隐藏掉。过了几天同事 B 也遇到了类似的需求，他也是一位老司机，为 NSString 增加了一个也叫 getMaskedIdCardNumber 的方法，但是他的需求是从 [8, 11] 这4位字符串隐藏，但是他引入工程后发现输出并不符合预期，为该方法写的单测没通过，他以为自己写错了截取方法，检查了几遍才发现工程引入了另一个 NSString 分类，里面的方法同名 😂 真坑。 下面的例子是 SDK，但是日常开发也是一样。 Category 类名：建议按照当前 SDK 名称的简写作为前缀，再加下划线，再加当前分类的功能，也就是类名+SDK名称简写_功能名称。比如当前 SDK 叫 JuhuaSuanAPM，那么该 NSURLSessionTask Category 名称就叫做 NSURLSessionTask+JuHuaSuanAPM_NetworkMonitor.h Category 属性名：建议按照当前 SDK 名称的简写作为前缀，再加下划线，再加属性名，也就是SDK名称简写_属性名称。比如 JuhuaSuanAPM_requestId` Category 方法名：建议按照当前 SDK 名称的简写作为前缀，再加下划线，再加方法名，也就是SDK名称简写_方法名称。比如 -(BOOL)JuhuaSuanAPM__isGzippedData 例子如下： #import &lt;Foundation/Foundation.h&gt; @interface NSURLSessionTask (JuhuaSuanAPM_NetworkMonitor) @property (nonatomic, copy) NSString* JuhuaSuanAPM_requestId; @end #import “NSURLSessionTask+JuHuaSuanAPM_NetworkMonitor.h”#import &lt;objc/runtime.h&gt; @implementation NSURLSessionTask (JuHuaSuanAPM_NetworkMonitor) (NSString*)JuhuaSuanAPM_requestId{ return objc_getAssociatedObject(self, _cmd);} (void)setJuhuaSuanAPM_requestId:(NSString*)requestId{ objc_setAssociatedObject(self, @selector(JuhuaSuanAPM_requestId), requestId, OBJC_ASSOCIATION_COPY_NONATOMIC);}@end 2.5 iOS 流量监控2.5.1 HTTP 请求、响应数据结构HTTP 请求报文结构 响应报文的结构 HTTP 报文是格式化的数据块，每条报文由三部分组成：对报文进行描述的起始行、包含属性的首部块、以及可选的包含数据的主体部分。 起始行和手部就是由行分隔符的 ASCII 文本，每行都以一个由2个字符组成的行终止序列作为结束（包括一个回车符、一个换行符） 实体的主体或者报文的主体是一个可选的数据块。与起始行和首部不同的是，主体中可以包含文本或者二进制数据，也可以为空。 HTTP 首部（也就是 Headers）总是应该以一个空行结束，即使没有实体部分。浏览器发送了一个空白行来通知服务器，它已经结束了该头信息的发送。 请求报文的格式 &lt;method&gt; &lt;request-URI&gt; &lt;version&gt;&lt;headers&gt; &lt;entity-body&gt; 响应报文的格式 下图是打开 Chrome 查看极课时间网页的请求信息。包括响应行、响应头、响应体等信息。 下图是在终端使用 curl 查看一个完整的请求和响应数据 我们都知道在 HTTP 通信中，响应数据会使用 gzip 或其他压缩方式压缩，用 NSURLProtocol 等方案监听，用 NSData 类型去计算分析流量等会造成数据的不精确，因为正常一个 HTTP 响应体的内容是使用 gzip 或其他压缩方式压缩的，所以使用 NSData 会偏大。 2.5.2 问题 Request 和 Response 不一定成对存在 比如网络断开、App 突然 Crash 等，所以 Request 和 Response 监控后不应该记录在一条记录里 请求流量计算方式不精确 主要原因有： 监控技术方案忽略了请求头和请求行部分的数据大小 监控技术方案忽略了 Cookie 部分的数据大小 监控技术方案在对请求体大小计算的时候直接使用 HTTPBody.length，导致不够精确 响应流量计算方式不精确 主要原因有： 监控技术方案忽略了响应头和响应行部分的数据大小 监控技术方案在对 body 部分的字节大小计算，因采用 exceptedContentLength 导致不够准确 监控技术方案忽略了响应体使用 gzip 压缩。真正的网络通信过程中，客户端在发起请求的请求头中 Accept-Encoding 字段代表客户端支持的数据压缩方式（表明客户端可以正常使用数据时支持的压缩方法），同样服务端根据客户端想要的压缩方式、服务端当前支持的压缩方式，最后处理数据，在响应头中Content-Encoding 字段表示当前服务器采用了什么压缩方式。 2.5.3 技术实现第五部分讲了网络拦截的各种原理和技术方案，这里拿 NSURLProtocol 来说实现流量监控（Hook 的方式）。从上述知道了我们需要什么样的，那么就逐步实现吧。 2.5.3.1 Request 部分 先利用网络监控方案将 NSURLProtocol 管理 App 的各种网络请求 在各个方法内部记录各项所需参数（NSURLProtocol 不能分析请求握手、挥手等数据大小和时间消耗，不过对于正常情况的接口流量分析足够了，最底层需要 Socket 层） @property(nonatomic, strong) NSURLConnection *internalConnection; @property(nonatomic, strong) NSURLResponse *internalResponse; @property(nonatomic, strong) NSMutableData *responseData; @property (nonatomic, strong) NSURLRequest *internalRequest; - (void)startLoading{ NSMutableURLRequest *mutableRequest = [[self request] mutableCopy]; self.internalConnection = [[NSURLConnection alloc] initWithRequest:mutableRequest delegate:self]; self.internalRequest = self.request;} (void)connection:(NSURLConnection *)connection didReceiveResponse:(NSURLResponse *)response{ [self.client URLProtocol:self didReceiveResponse:response cacheStoragePolicy:NSURLCacheStorageNotAllowed]; self.internalResponse = response;} (void)connection:(NSURLConnection *)connection didReceiveData:(NSData *)data{ [self.responseData appendData:data]; [self.client URLProtocol:self didLoadData:data];} Status Line 部分 NSURLResponse 没有 Status Line 等属性或者接口，HTTP Version 信息也没有，所以要想获取 Status Line 想办法转换到 CFNetwork 层试试看。发现有私有 API 可以实现。 思路：将 NSURLResponse 通过 _CFURLResponse 转换为 CFTypeRef，然后再将 CFTypeRef 转换为 CFHTTPMessageRef，再通过 CFHTTPMessageCopyResponseStatusLine 获取 CFHTTPMessageRef 的 Status Line 信息。 将读取 Status Line 的功能添加一个 NSURLResponse 的分类。 // NSURLResponse+apm_FetchStatusLineFromCFNetwork.h #import &lt;Foundation/Foundation.h&gt; NS_ASSUME_NONNULL_BEGIN @interface NSURLResponse (apm_FetchStatusLineFromCFNetwork) (NSString *)apm_fetchStatusLineFromCFNetwork; @end NS_ASSUME_NONNULL_END // NSURLResponse+apm_FetchStatusLineFromCFNetwork.m#import “NSURLResponse+apm_FetchStatusLineFromCFNetwork.h”#import &lt;dlfcn.h&gt; #define SuppressPerformSelectorLeakWarning(Stuff) \\do { \\_Pragma(“clang diagnostic push”) \\_Pragma(“clang diagnostic ignored \\“-Warc-performSelector-leaks\\“”) \\Stuff; \\_Pragma(“clang diagnostic pop”) \\} while (0) typedef CFHTTPMessageRef (*APMURLResponseFetchHTTPResponse)(CFURLRef response); @implementation NSURLResponse (apm_FetchStatusLineFromCFNetwork) (NSString *)apm_fetchStatusLineFromCFNetwork{NSString *statusLine = @””;NSString *funcName = @”CFURLResponseGetHTTPResponse”;APMURLResponseFetchHTTPResponse originalURLResponseFetchHTTPResponse = dlsym(RTLD_DEFAULT, [funcName UTF8String]); SEL getSelector = NSSelectorFromString(@”_CFURLResponse”);if ([self respondsToSelector:getSelector] &amp;&amp; NULL != originalURLResponseFetchHTTPResponse) { CFTypeRef cfResponse; SuppressPerformSelectorLeakWarning( cfResponse = CFBridgingRetain(\\[self performSelector:getSelector\\]); ); if (NULL != cfResponse) { CFHTTPMessageRef messageRef = originalURLResponseFetchHTTPResponse(cfResponse); statusLine = (\\_\\_bridge\\_transfer NSString \\*)CFHTTPMessageCopyResponseStatusLine(messageRef); CFRelease(cfResponse); }}return statusLine;} @end 将获取到的 Status Line 转换为 NSData，再计算大小 - (NSUInteger)apm_getLineLength { NSString *statusLineString = @””; if ([self isKindOfClass:[NSHTTPURLResponse class]]) { NSHTTPURLResponse \\*httpResponse = (NSHTTPURLResponse \\*)self; statusLineString = \\[self apm\\_fetchStatusLineFromCFNetwork\\]; } NSData *lineData = [statusLineString dataUsingEncoding:NSUTF8StringEncoding]; return lineData.length; } Header 部分 allHeaderFields 获取到 NSDictionary，然后按照 key: value 拼接成字符串，然后转换成 NSData 计算大小 注意：key: value key 后是有空格的，curl 或者 chrome Network 面板可以查看印证下。 - (NSUInteger)apm_getHeadersLength { NSUInteger headersLength = 0; if ([self isKindOfClass:[NSHTTPURLResponse class]]) { NSHTTPURLResponse \\*httpResponse = (NSHTTPURLResponse \\*)self; NSDictionary \\*headerFields = httpResponse.allHeaderFields; NSString \\*headerString = @&quot;&quot;; for (NSString \\*key in headerFields.allKeys) &#123; headerString = \\[headerStr stringByAppendingString:key\\]; headheaderStringerStr = \\[headerString stringByAppendingString:@&quot;: &quot;\\]; if (\\[headerFields objectForKey:key\\]) &#123; headerString = \\[headerString stringByAppendingString:headerFields\\[key\\]\\]; &#125; headerString = \\[headerString stringByAppendingString:@&quot;\\\\n&quot;\\]; &#125; NSData \\*headerData = \\[headerString dataUsingEncoding:NSUTF8StringEncoding\\]; headersLength = headerData.length; } return headersLength; } Body 部分 Body 大小的计算不能直接使用 excepectedContentLength，官方文档说明了其不准确性，只可以作为参考。或者 allHeaderFields 中的 Content-Length 值也是不够准确的。 /*! @abstract Returns the expected content length of the receiver. @discussion Some protocol implementations report a content length as part of delivering load metadata, but not all protocols guarantee the amount of data that will be delivered in actuality. Hence, this method returns an expected amount. Clients should use this value as an advisory, and should be prepared to deal with either more or less data. @result The expected content length of the receiver, or -1 if there is no expectation that can be arrived at regarding expected content length. */ @property (readonly) long long expectedContentLength; HTTP 1.1 版本规定，如果存在 Transfer-Encoding: chunked，则在 header 中不能有 Content-Length，有也会被忽视。 在 HTTP 1.0及之前版本中，content-length 字段可有可无 在 HTTP 1.1及之后版本。如果是 keep alive，则 Content-Length 和 chunked 必然是二选一。若是非keep alive，则和 HTTP 1.0一样。Content-Length 可有可无。 什么是 Transfer-Encoding: chunked 数据以一系列分块的形式进行发送 Content-Length 首部在这种情况下不被发送. 在每一个分块的开头需要添加当前分块的长度, 以十六进制的形式表示，后面紧跟着 \\r\\n , 之后是分块本身, 后面也是 \\r\\n ，终止块是一个常规的分块, 不同之处在于其长度为0. 我们之前拿 NSMutableData 记录了数据，所以我们可以在 stopLoading 方法中计算出 Body 大小。步骤如下： 在 didReceiveData 中不断添加 data - (void)connection:(NSURLConnection *)connection didReceiveData:(NSData *)data{ [self.responseData appendData:data]; [self.client URLProtocol:self didLoadData:data];} 在 stopLoading 方法中拿到 allHeaderFields 字典，获取 Content-Encoding key 的值，如果是 gzip，则在 stopLoading 中将 NSData 处理为 gzip 压缩后的数据，再计算大小。（gzip 相关功能可以使用这个工具） 需要额外计算一个空白行的长度 - (void)stopLoadi { \\[self.internalConnection cancel\\]; HCTNetworkTrafficModel \\*model = \\[\\[HCTNetworkTrafficModel alloc\\] init\\]; model.path = self.request.URL.path; model.host = self.request.URL.host; model.type = DMNetworkTrafficDataTypeResponse; model.lineLength = \\[self.internalResponse apm\\_getStatusLineLength\\]; model.headerLength = \\[self.internalResponse apm\\_getHeadersLength\\]; model.emptyLineLength = \\[self.internalResponse apm\\_getEmptyLineLength\\]; if (\\[self.dm\\_response isKindOfClass:\\[NSHTTPURLResponse class\\]\\]) &#123; NSHTTPURLResponse \\*httpResponse = (NSHTTPURLResponse \\*)self.dm\\_response; NSData \\*data = self.dm\\_data; if (\\[\\[httpResponse.allHeaderFields objectForKey:@&quot;Content-Encoding&quot;\\] isEqualToString:@&quot;gzip&quot;\\]) &#123; data = \\[self.dm\\_data gzippedData\\]; &#125; model.bodyLength = data.length; &#125; model.length = model.lineLength + model.headerLength + model.bodyLength + model.emptyLineLength; NSDictionary \\*networkTrafficDictionary = \\[model convertToDictionary\\]; \\[\\[HermesClient sharedInstance\\] sendWithType:APMMonitorNetworkTrafficType meta:networkTrafficDictionary payload:nil\\]; } 2.5.3.2 Resquest 部分 先利用网络监控方案将 NSURLProtocol 管理 App 的各种网络请求 在各个方法内部记录各项所需参数（NSURLProtocol 不能分析请求握手、挥手等数据大小和时间消耗，不过对于正常情况的接口流量分析足够了，最底层需要 Socket 层） @property(nonatomic, strong) NSURLConnection *internalConnection; @property(nonatomic, strong) NSURLResponse *internalResponse; @property(nonatomic, strong) NSMutableData *responseData; @property (nonatomic, strong) NSURLRequest *internalRequest; - (void)startLoading{ NSMutableURLRequest *mutableRequest = [[self request] mutableCopy]; self.internalConnection = [[NSURLConnection alloc] initWithRequest:mutableRequest delegate:self]; self.internalRequest = self.request;} (void)connection:(NSURLConnection *)connection didReceiveResponse:(NSURLResponse *)response{ [self.client URLProtocol:self didReceiveResponse:response cacheStoragePolicy:NSURLCacheStorageNotAllowed]; self.internalResponse = response;} (void)connection:(NSURLConnection *)connection didReceiveData:(NSData *)data{ [self.responseData appendData:data]; [self.client URLProtocol:self didLoadData:data];} Status Line 部分 对于 NSURLRequest 没有像 NSURLResponse 一样的方法找到 StatusLine。所以兜底方案是自己根据 Status Line 的结构，自己手动构造一个。结构为：协议版本号+空格+状态码+空格+状态文本+换行 为 NSURLRequest 添加一个专门获取 Status Line 的分类。 // NSURLResquest+apm_FetchStatusLineFromCFNetwork.m (NSUInteger)apm_fetchStatusLineLength{NSString *statusLineString = [NSString stringWithFormat:@”%@ %@ %@\\n”, self.HTTPMethod, self.URL.path, @”HTTP/1.1”];NSData *statusLineData = [statusLineString dataUsingEncoding:NSUTF8StringEncoding];return statusLineData.length;} Header 部分 一个 HTTP 请求会先构建判断是否存在缓存，然后进行 DNS 域名解析以获取请求域名的服务器 IP 地址。如果请求协议是 HTTPS，那么还需要建立 TLS 连接。接下来就是利用 IP 地址和服务器建立 TCP 连接。连接建立之后，浏览器端会构建请求行、请求头等信息，并把和该域名相关的 Cookie 等数据附加到请求头中，然后向服务器发送构建的请求信息。 所以一个网络监控不考虑 cookie 😂，借用王多鱼的一句话「那不完犊子了吗」。 看过一些文章说 NSURLRequest 不能完整获取到请求头信息。其实问题不大， 几个信息获取不完全也没办法。衡量监控方案本身就是看接口在不同版本或者某些情况下数据消耗是否异常，WebView 资源请求是否过大，类似于控制变量法的思想。 所以获取到 NSURLRequest 的 allHeaderFields 后，加上 cookie 信息，计算完整的 Header 大小 // NSURLResquest+apm_FetchHeaderWithCookies.m (NSUInteger)apm_fetchHeaderLengthWithCookie{ NSDictionary *headerFields = self.allHTTPHeaderFields; NSDictionary *cookiesHeader = [self apm_fetchCookies]; if (cookiesHeader.count) { NSMutableDictionary \\*headerDictionaryWithCookies = \\[NSMutableDictionary dictionaryWithDictionary:headerFields\\]; \\[headerDictionaryWithCookies addEntriesFromDictionary:cookiesHeader\\]; headerFields = \\[headerDictionaryWithCookies copy\\]; } NSString *headerString = @””; for (NSString *key in headerFields.allKeys) { headerString = \\[headerString stringByAppendingString:key\\]; headerString = \\[headerString stringByAppendingString:@&quot;: &quot;\\]; if (\\[headerFields objectForKey:key\\]) &#123; headerString = \\[headerString stringByAppendingString:headerFields\\[key\\]\\]; &#125; headerString = \\[headerString stringByAppendingString:@&quot;\\\\n&quot;\\]; } NSData *headerData = [headerString dataUsingEncoding:NSUTF8StringEncoding]; headersLength = headerData.length; return headerString;} (NSDictionary *)apm_fetchCookies{ NSDictionary *cookiesHeaderDictionary; NSHTTPCookieStorage *cookieStorage = [NSHTTPCookieStorage sharedHTTPCookieStorage]; NSArray&lt;NSHTTPCookie *&gt; *cookies = [cookieStorage cookiesForURL:self.URL]; if (cookies.count) { cookiesHeaderDictionary = \\[NSHTTPCookie requestHeaderFieldsWithCookies:cookies\\]; } return cookiesHeaderDictionary;} Body 部分 NSURLConnection 的 HTTPBody 有可能获取不到，问题类似于 WebView 上 ajax 等情况。所以可以通过 HTTPBodyStream 读取 stream 来计算 body 大小. - (NSUInteger)apm_fetchRequestBody { NSDictionary \\*headerFields = self.allHTTPHeaderFields; NSUInteger bodyLength = \\[self.HTTPBody length\\]; if (\\[headerFields objectForKey:@&quot;Content-Encoding&quot;\\]) &#123; NSData \\*bodyData; if (self.HTTPBody == nil) &#123; uint8\\_t d\\[1024\\] = &#123;0&#125;; NSInputStream \\*stream = self.HTTPBodyStream; NSMutableData \\*data = \\[\\[NSMutableData alloc\\] init\\]; \\[stream open\\]; while (\\[stream hasBytesAvailable\\]) &#123; NSInteger len = \\[stream read:d maxLength:1024\\]; if (len &gt; 0 &amp;&amp; stream.streamError == nil) &#123; \\[data appendBytes:(void \\*)d length:len\\]; &#125; &#125; bodyData = \\[data copy\\]; \\[stream close\\]; &#125; else &#123; bodyData = self.HTTPBody; &#125; bodyLength = \\[\\[bodyData gzippedData\\] length\\]; &#125; return bodyLength; } 在 - (NSURLRequest *)connection:(NSURLConnection *)connection willSendRequest:(NSURLRequest *)request redirectResponse:(NSURLResponse *)response 方法中将数据上报会在 打造功能强大、灵活可配置的数据上报组件 讲 -(NSURLRequest *)connection:(NSURLConnection *)connection willSendRequest:(NSURLRequest *)request redirectResponse:(NSURLResponse *)response { if (response != nil) &#123; self.internalResponse = response; \\[self.client URLProtocol:self wasRedirectedToRequest:request redirectResponse:response\\]; &#125; HCTNetworkTrafficModel \\*model = \\[\\[HCTNetworkTrafficModel alloc\\] init\\]; model.path = request.URL.path; model.host = request.URL.host; model.type = DMNetworkTrafficDataTypeRequest; model.lineLength = \\[connection.currentRequest dgm\\_getLineLength\\]; model.headerLength = \\[connection.currentRequest dgm\\_getHeadersLengthWithCookie\\]; model.bodyLength = \\[connection.currentRequest dgm\\_getBodyLength\\]; model.emptyLineLength = \\[self.internalResponse apm\\_getEmptyLineLength\\]; model.length = model.lineLength + model.headerLength + model.bodyLength + model.emptyLineLength; NSDictionary \\*networkTrafficDictionary = \\[model convertToDictionary\\]; \\[\\[HermesClient sharedInstance\\] sendWithType:APMMonitorNetworkTrafficType meta:networkTrafficDictionary payload:nil\\]; return request; } 六、 电量消耗移动设备上电量一直是比较敏感的问题，如果用户在某款 App 的时候发现耗电量严重、手机发热严重，那么用户很大可能会马上卸载这款 App。所以需要在开发阶段关心耗电量问题。 一般来说遇到耗电量较大，我们立马会想到是不是使用了定位、是不是使用了频繁网络请求、是不是不断循环做某件事情？ 开发阶段基本没啥问题，我们可以结合 Instrucments 里的 Energy Log 工具来定位问题。但是线上问题就需要代码去监控耗电量，可以作为 APM 的能力之一。 1. 如何获取电量在 iOS 中，IOKit 是一个私有框架，用来获取硬件和设备的详细信息，也是硬件和内核服务通信的底层框架。所以我们可以通过 IOKit 来获取硬件信息，从而获取到电量信息。步骤如下： 首先在苹果开放源代码 opensource 中找到 IOPowerSources.h、IOPSKeys.h。在 Xcode 的 Package Contents 里面找到 IOKit.framework。 路径为 /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS.sdk/System/Library/Frameworks/IOKit.framework 然后将 IOPowerSources.h、IOPSKeys.h、IOKit.framework 导入项目工程 设置 UIDevice 的 batteryMonitoringEnabled 为 true 获取到的耗电量精确度为 1% 2. 定位问题通常我们通过 Instrucments 里的 Energy Log 解决了很多问题后，App 上线了，线上的耗电量解决就需要使用 APM 来解决了。耗电地方可能是二方库、三方库，也可能是某个同事的代码。 思路是：在检测到耗电后，先找到有问题的线程，然后堆栈 dump，还原案发现场。 在上面部分我们知道了线程信息的结构， thread_basic_info 中有个记录 CPU 使用率百分比的字段 cpu_usage。所以我们可以通过遍历当前线程，判断哪个线程的 CPU 使用率较高，从而找出有问题的线程。然后再 dump 堆栈，从而定位到发生耗电量的代码。详细请看 3.2 部分。 - (double)fetchBatteryCostUsage{ // returns a blob of power source information in an opaque CFTypeRef CFTypeRef blob = IOPSCopyPowerSourcesInfo(); // returns a CFArray of power source handles, each of type CFTypeRef CFArrayRef sources = IOPSCopyPowerSourcesList(blob); CFDictionaryRef pSource = NULL; const void *psValue; // returns the number of values currently in an array int numOfSources = CFArrayGetCount(sources); // error in CFArrayGetCount if (numOfSources == 0) { NSLog(@”Error in CFArrayGetCount”); return -1.0f; } // calculating the remaining energy for (int i=0; i&lt;numOfSources; i++) &#123; // returns a CFDictionary with readable information about the specific power source pSource = IOPSGetPowerSourceDescription(blob, CFArrayGetValueAtIndex(sources, i)); if (!pSource) &#123; NSLog(@&quot;Error in IOPSGetPowerSourceDescription&quot;); return \\-1.0f; &#125; psValue = (CFStringRef) CFDictionaryGetValue(pSource, CFSTR(kIOPSNameKey)); int curCapacity = 0; int maxCapacity = 0; double percentage; psValue = CFDictionaryGetValue(pSource, CFSTR(kIOPSCurrentCapacityKey)); CFNumberGetValue((CFNumberRef)psValue, kCFNumberSInt32Type, &amp;curCapacity); psValue = CFDictionaryGetValue(pSource, CFSTR(kIOPSMaxCapacityKey)); CFNumberGetValue((CFNumberRef)psValue, kCFNumberSInt32Type, &amp;maxCapacity); percentage = ((double) curCapacity / (double) maxCapacity \\* 100.0f); NSLog(@&quot;curCapacity : %d / maxCapacity: %d , percentage: %.1f &quot;, curCapacity, maxCapacity, percentage); return percentage; &#125; return \\-1.0f; } 3. 开发阶段针对电量消耗我们能做什么CPU 密集运算是耗电量主要原因。所以我们对 CPU 的使用需要精打细算。尽量避免让 CPU 做无用功。对于大量数据的复杂运算，可以借助服务器的能力、GPU 的能力。如果方案设计必须是在 CPU 上完成数据的运算，则可以利用 GCD 技术，使用 dispatch_block_create_with_qos_class(&lt;#dispatch_block_flags_t flags#&gt;, dispatch_qos_class_t qos_class, &lt;#int relative_priority#&gt;, &lt;#^(void)block#&gt;)() 并指定 队列的 qos 为 QOS_CLASS_UTILITY。将任务提交到这个队列的 block 中，在 QOS_CLASS_UTILITY 模式下，系统针对大量数据的计算，做了电量优化 除了 CPU 大量运算，I/O 操作也是耗电主要原因。业界常见方案都是将「碎片化的数据写入磁盘存储」这个操作延后，先在内存中聚合吗，然后再进行磁盘存储。碎片化数据先聚合，在内存中进行存储的机制，iOS 提供 NSCache 这个对象。 NSCache 是线程安全的，NSCache 会在达到达预设的缓存空间的条件时清理缓存，此时会触发 - (**void**)cache:(NSCache *)cache willEvictObject:(**id**)obj; 方法回调，在该方法内部对数据进行 I/O 操作，达到将聚合的数据 I/O 延后的目的。I/O 次数少了，对电量的消耗也就减少了。 NSCache 的使用可以查看 SDWebImage 这个图片加载框架。在图片读取缓存处理时，没直接读取硬盘文件（I/O），而是使用系统的 NSCache。 - (nullable UIImage *)imageFromMemoryCacheForKey:(nullable NSString *)key { return [self.memoryCache objectForKey:key];} (nullable UIImage *)imageFromDiskCacheForKey:(nullable NSString *)key { UIImage *diskImage = [self diskImageForKey:key]; if (diskImage &amp;&amp; self.config.shouldCacheImagesInMemory) { NSUInteger cost = diskImage.sd\\_memoryCost; \\[self.memoryCache setObject:diskImage forKey:key cost:cost\\]; } return diskImage;} 可以看到主要逻辑是先从磁盘中读取图片，如果配置允许开启内存缓存，则将图片保存到 NSCache 中，使用的时候也是从 NSCache 中读取图片。NSCache 的 totalCostLimit、countLimit 属性， - (void)setObject:(ObjectType)obj forKey:(KeyType)key cost:(NSUInteger)g; 方法用来设置缓存条件。所以我们写磁盘、内存的文件操作时可以借鉴该策略，以优化耗电量。 七、 Crash 监控1. 异常相关知识回顾1.1 Mach 层对异常的处理Mach 在消息传递基础上实现了一套独特的异常处理方法。Mach 异常处理在设计时考虑到： 带有一致的语义的单一异常处理设施：Mach 只提供一个异常处理机制用于处理所有类型的异常（包括用户定义的异常、平台无关的异常以及平台特定的异常）。根据异常类型进行分组，具体的平台可以定义具体的子类型。 清晰和简洁：异常处理的接口依赖于 Mach 已有的具有良好定义的消息和端口架构，因此非常优雅（不会影响效率）。这就允许调试器和外部处理程序的拓展-甚至在理论上还支持拓展基于网络的异常处理。 在 Mach 中，异常是通过内核中的基础设施-消息传递机制处理的。一个异常并不比一条消息复杂多少，异常由出错的线程或者任务（通过 msg_send()） 抛出，然后由一个处理程序通过 msg_recv()）捕捉。处理程序可以处理异常，也可以清楚异常（将异常标记为已完成并继续），还可以决定终止线程。 Mach 的异常处理模型和其他的异常处理模型不同，其他模型的异常处理程序运行在出错的线程上下文中，而 Mach 的异常处理程序在不同的上下文中运行异常处理程序，出错的线程向预先指定好的异常端口发送消息，然后等待应答。每一个任务都可以注册一个异常处理端口，这个异常处理端口会对该任务中的所有线程生效。此外，每个线程都可以通过 thread_set_exception_ports(&lt;#thread_act_t thread#&gt;, &lt;#exception_mask_t exception_mask#&gt;, &lt;#mach_port_t new_port#&gt;, &lt;#exception_behavior_t behavior#&gt;, &lt;#thread_state_flavor_t new_flavor#&gt;) 注册自己的异常处理端口。通常情况下，任务和线程的异常端口都是 NULL，也就是异常不会被处理，而一旦创建异常端口，这些端口就像系统中的其他端口一样，可以转交给其他任务或者其他主机。（有了端口，就可以使用 UDP 协议，通过网络能力让其他的主机上应用程序处理异常）。 发生异常时，首先尝试将异常抛给线程的异常端口，然后尝试抛给任务的异常端口，最后再抛给主机的异常端口（即主机注册的默认端口）。如果没有一个端口返回 KERN_SUCCESS，那么整个任务将被终止。也就是 Mach 不提供异常处理逻辑，只提供传递异常通知的框架。 异常首先是由处理器陷阱引发的。为了处理陷阱，每一个现代的内核都会安插陷阱处理程序。这些底层函数是由内核的汇编部分安插的。 1.2 BSD 层对异常的处理BSD 层是用户态主要使用的 XUN 接口，这一层展示了一个符合 POSIX 标准的接口。开发者可以使用 UNIX 系统的一切功能，但不需要了解 Mach 层的细节实现。 Mach 已经通过异常机制提供了底层的陷进处理，而 BSD 则在异常机制之上构建了信号处理机制。硬件产生的信号被 Mach 层捕捉，然后转换为对应的 UNIX 信号，为了维护一个统一的机制，操作系统和用户产生的信号首先被转换为 Mach 异常，然后再转换为信号。 Mach 异常都在 host 层被 ux_exception 转换为相应的 unix 信号，并通过 threadsignal 将信号投递到出错的线程。 2. Crash 收集方式iOS 系统自带的 Apples`s Crash Reporter 在设置中记录 Crash 日志，我们先观察下 Crash 日志 Incident Identifier: 7FA6736D-09E8-47A1-95EC-76C4522BDE1ACrashReporter Key: 4e2d36419259f14413c3229e8b7235bcc74847f3Hardware Model: iPhone7,1Process: APMMonitorExample [3608]Path: /var/containers/Bundle/Application/9518A4F4-59B7-44E9-BDDA-9FBEE8CA18E5/APMMonitorExample.app/APMMonitorExampleIdentifier: com.Wacai.APMMonitorExampleVersion: 1.0 (1)Code Type: ARM-64Parent Process: ? [1] Date/Time: 2017-01-03 11:43:03.000 +0800OS Version: iOS 10.2 (14C92)Report Version: 104 Exception Type: EXC_CRASH (SIGABRT)Exception Codes: 0x00000000 at 0x0000000000000000Crashed Thread: 0 Application Specific Information:*** Terminating app due to uncaught exception ‘NSInvalidArgumentException’, reason: ‘-[__NSSingleObjectArrayI objectForKey:]: unrecognized selector sent to instance 0x174015060’ Thread 0 Crashed:0 CoreFoundation 0x0000000188f291b8 0x188df9000 + 1245624 ( + 124)1 libobjc.A.dylib 0x000000018796055c 0x187958000 + 34140 (objc_exception_throw + 56)2 CoreFoundation 0x0000000188f30268 0x188df9000 + 1274472 ( + 140)3 CoreFoundation 0x0000000188f2d270 0x188df9000 + 1262192 ( + 916)4 CoreFoundation 0x0000000188e2680c 0x188df9000 + 186380 (_CF_forwarding_prep_0 + 92)5 APMMonitorExample 0x000000010004c618 0x100044000 + 34328 (-[MakeCrashHandler throwUncaughtNSException] + 80) 会发现，Crash 日志中 Exception Type 项由2部分组成：Mach 异常 + Unix 信号。 所以 Exception Type: EXC_CRASH (SIGABRT) 表示：Mach 层发生了 EXC_CRASH 异常，在 host 层被转换为 SIGABRT 信号投递到出错的线程。 问题： 捕获 Mach 层异常、注册 Unix 信号处理都可以捕获 Crash，这两种方式如何选择？ 答： 优选 Mach 层异常拦截。根据上面 1.2 中的描述我们知道 Mach 层异常处理时机更早些，假如 Mach 层异常处理程序让进程退出，这样 Unix 信号永远不会发生了。 业界关于崩溃日志的收集开源项目很多，著名的有： KSCrash、plcrashreporter，提供一条龙服务的 Bugly、友盟等。我们一般使用开源项目在此基础上开发成符合公司内部需求的 bug 收集工具。一番对比后选择 KSCrash。为什么选择 KSCrash 不在本文重点。 KSCrash 功能齐全，可以捕获如下类型的 Crash Mach kernel exceptions Fatal signals C++ exceptions Objective-C exceptions Main thread deadlock (experimental) Custom crashes (e.g. from scripting languages) 所以分析 iOS 端的 Crash 收集方案也就是分析 KSCrash 的 Crash 监控实现原理。 2.1. Mach 层异常处理大体思路是：先创建一个异常处理端口，为该端口申请权限，再设置异常端口、新建一个内核线程，在该线程内循环等待异常。但是为了防止自己注册的 Mach 层异常处理抢占了其他 SDK、或者业务线开发者设置的逻辑，我们需要在最开始保存其他的异常处理端口，等逻辑执行完后将异常处理交给其他的端口内的逻辑处理。收集到 Crash 信息后组装数据，写入 json 文件。 流程图如下： 对于 Mach 异常捕获，可以注册一个异常端口，该端口负责对当前任务的所有线程进行监听。 下面来看看关键代码: 注册 Mach 层异常监听代码 static bool installExceptionHandler(){ KSLOG_DEBUG(“Installing mach exception handler.”); bool attributes\\_created = false; pthread\\_attr\\_t attr; kern\\_return\\_t kr; int error; // 拿到当前进程 const task\\_t thisTask = mach\\_task\\_self(); exception\\_mask\\_t mask = EXC\\_MASK\\_BAD\\_ACCESS | EXC\\_MASK\\_BAD\\_INSTRUCTION | EXC\\_MASK\\_ARITHMETIC | EXC\\_MASK\\_SOFTWARE | EXC\\_MASK\\_BREAKPOINT; KSLOG\\_DEBUG(&quot;Backing up original exception ports.&quot;); // 获取该 Task 上的注册好的异常端口 kr = task\\_get\\_exception\\_ports(thisTask, mask, g\\_previousExceptionPorts.masks, &amp;g\\_previousExceptionPorts.count, g\\_previousExceptionPorts.ports, g\\_previousExceptionPorts.behaviors, g\\_previousExceptionPorts.flavors); // 获取失败走 failed 逻辑 if(kr != KERN\\_SUCCESS) &#123; KSLOG\\_ERROR(&quot;task\\_get\\_exception\\_ports: %s&quot;, mach\\_error\\_string(kr)); goto failed; &#125; // KSCrash 的异常为空则走执行逻辑 if(g\\_exceptionPort == MACH\\_PORT\\_NULL) &#123; KSLOG\\_DEBUG(&quot;Allocating new port with receive rights.&quot;); // 申请异常处理端口 kr = mach\\_port\\_allocate(thisTask, MACH\\_PORT\\_RIGHT\\_RECEIVE, &amp;g\\_exceptionPort); if(kr != KERN\\_SUCCESS) &#123; KSLOG\\_ERROR(&quot;mach\\_port\\_allocate: %s&quot;, mach\\_error\\_string(kr)); goto failed; &#125; KSLOG\\_DEBUG(&quot;Adding send rights to port.&quot;); // 为异常处理端口申请权限：MACH\\_MSG\\_TYPE\\_MAKE\\_SEND kr = mach\\_port\\_insert\\_right(thisTask, g\\_exceptionPort, g\\_exceptionPort, MACH\\_MSG\\_TYPE\\_MAKE\\_SEND); if(kr != KERN\\_SUCCESS) &#123; KSLOG\\_ERROR(&quot;mach\\_port\\_insert\\_right: %s&quot;, mach\\_error\\_string(kr)); goto failed; &#125; &#125; KSLOG\\_DEBUG(&quot;Installing port as exception handler.&quot;); // 为该 Task 设置异常处理端口 kr = task\\_set\\_exception\\_ports(thisTask, mask, g\\_exceptionPort, EXCEPTION\\_DEFAULT, THREAD\\_STATE\\_NONE); if(kr != KERN\\_SUCCESS) &#123; KSLOG\\_ERROR(&quot;task\\_set\\_exception\\_ports: %s&quot;, mach\\_error\\_string(kr)); goto failed; &#125; KSLOG\\_DEBUG(&quot;Creating secondary exception thread (suspended).&quot;); pthread\\_attr\\_init(&amp;attr); attributes\\_created = true; pthread\\_attr\\_setdetachstate(&amp;attr, PTHREAD\\_CREATE\\_DETACHED); // 设置监控线程 error = pthread\\_create(&amp;g\\_secondaryPThread, &amp;attr, &amp;handleExceptions, kThreadSecondary); if(error != 0) &#123; KSLOG\\_ERROR(&quot;pthread\\_create\\_suspended\\_np: %s&quot;, strerror(error)); goto failed; &#125; // 转换为 Mach 内核线程 g\\_secondaryMachThread = pthread\\_mach\\_thread\\_np(g\\_secondaryPThread); ksmc\\_addReservedThread(g\\_secondaryMachThread); KSLOG\\_DEBUG(&quot;Creating primary exception thread.&quot;); error = pthread\\_create(&amp;g\\_primaryPThread, &amp;attr, &amp;handleExceptions, kThreadPrimary); if(error != 0) &#123; KSLOG\\_ERROR(&quot;pthread\\_create: %s&quot;, strerror(error)); goto failed; &#125; pthread\\_attr\\_destroy(&amp;attr); g\\_primaryMachThread = pthread\\_mach\\_thread\\_np(g\\_primaryPThread); ksmc\\_addReservedThread(g\\_primaryMachThread); KSLOG\\_DEBUG(&quot;Mach exception handler installed.&quot;); return true; failed: KSLOG_DEBUG(“Failed to install mach exception handler.”); if(attributes_created) { pthread_attr_destroy(&amp;attr); } // 还原之前的异常注册端口，将控制权还原 uninstallExceptionHandler(); return false;} 处理异常的逻辑、组装崩溃信息 /** Our exception handler thread routine. * Wait for an exception message, uninstall our exception port, record the * exception information, and write a report. */static void* handleExceptions(void* const userData){ MachExceptionMessage exceptionMessage = 0; MachReplyMessage replyMessage = 0; char* eventID = g_primaryEventID; const char\\* threadName = (const char\\*) userData; pthread\\_setname\\_np(threadName); if(threadName == kThreadSecondary) &#123; KSLOG\\_DEBUG(&quot;This is the secondary thread. Suspending.&quot;); thread\\_suspend((thread\\_t)ksthread\\_self()); eventID = g\\_secondaryEventID; &#125; // 循环读取注册好的异常端口信息 for(;;) &#123; KSLOG\\_DEBUG(&quot;Waiting for mach exception&quot;); // Wait for a message. kern\\_return\\_t kr = mach\\_msg(&amp;exceptionMessage.header, MACH\\_RCV\\_MSG, 0, sizeof(exceptionMessage), g\\_exceptionPort, MACH\\_MSG\\_TIMEOUT\\_NONE, MACH\\_PORT\\_NULL); // 获取到信息后则代表发生了 Mach 层异常，跳出 for 循环，组装数据 if(kr == KERN\\_SUCCESS) &#123; break; &#125; // Loop and try again on failure. KSLOG\\_ERROR(&quot;mach\\_msg: %s&quot;, mach\\_error\\_string(kr)); &#125; KSLOG\\_DEBUG(&quot;Trapped mach exception code 0x%x, subcode 0x%x&quot;, exceptionMessage.code\\[0\\], exceptionMessage.code\\[1\\]); if(g\\_isEnabled) &#123; // 挂起所有线程 ksmc\\_suspendEnvironment(); g\\_isHandlingCrash = true; // 通知发生了异常 kscm\\_notifyFatalExceptionCaptured(true); KSLOG\\_DEBUG(&quot;Exception handler is installed. Continuing exception handling.&quot;); // Switch to the secondary thread if necessary, or uninstall the handler // to avoid a death loop. if(ksthread\\_self() == g\\_primaryMachThread) &#123; KSLOG\\_DEBUG(&quot;This is the primary exception thread. Activating secondary thread.&quot;); // TODO: This was put here to avoid a freeze. Does secondary thread ever fire? restoreExceptionPorts(); if(thread_resume(g_secondaryMachThread) != KERN_SUCCESS) { KSLOG_DEBUG(“Could not activate secondary thread. Restoring original exception ports.”); } } else { KSLOG_DEBUG(“This is the secondary exception thread. Restoring original exception ports.”);// restoreExceptionPorts(); } // Fill out crash information // 组装异常所需要的方案现场信息 KSLOG\\_DEBUG(&quot;Fetching machine state.&quot;); KSMC\\_NEW\\_CONTEXT(machineContext); KSCrash\\_MonitorContext\\* crashContext = &amp;g\\_monitorContext; crashContext-&gt;offendingMachineContext = machineContext; kssc\\_initCursor(&amp;g\\_stackCursor, NULL, NULL); if(ksmc\\_getContextForThread(exceptionMessage.thread.name, machineContext, true)) &#123; kssc\\_initWithMachineContext(&amp;g\\_stackCursor, 100, machineContext); KSLOG\\_TRACE(&quot;Fault address 0x%x, instruction address 0x%x&quot;, kscpu\\_faultAddress(machineContext), kscpu\\_instructionAddress(machineContext)); if(exceptionMessage.exception == EXC\\_BAD\\_ACCESS) &#123; crashContext-&gt;faultAddress = kscpu\\_faultAddress(machineContext); &#125; else &#123; crashContext-&gt;faultAddress = kscpu\\_instructionAddress(machineContext); &#125; &#125; KSLOG\\_DEBUG(&quot;Filling out context.&quot;); crashContext-&gt;crashType = KSCrashMonitorTypeMachException; crashContext-&gt;eventID = eventID; crashContext-&gt;registersAreValid = true; crashContext-&gt;mach.type = exceptionMessage.exception; crashContext-&gt;mach.code = exceptionMessage.code\\[0\\]; crashContext-&gt;mach.subcode = exceptionMessage.code\\[1\\]; if(crashContext-&gt;mach.code == KERN\\_PROTECTION\\_FAILURE &amp;&amp; crashContext-&gt;isStackOverflow) &#123; // A stack overflow should return KERN\\_INVALID\\_ADDRESS, but // when a stack blasts through the guard pages at the top of the stack, // it generates KERN\\_PROTECTION\\_FAILURE. Correct for this. crashContext-&gt;mach.code = KERN\\_INVALID\\_ADDRESS; &#125; crashContext-&gt;signal.signum = signalForMachException(crashContext-&gt;mach.type, crashContext-&gt;mach.code); crashContext-&gt;stackCursor = &amp;g\\_stackCursor; kscm\\_handleException(crashContext); KSLOG\\_DEBUG(&quot;Crash handling complete. Restoring original handlers.&quot;); g\\_isHandlingCrash = false; ksmc\\_resumeEnvironment(); &#125; KSLOG\\_DEBUG(&quot;Replying to mach exception message.&quot;); // Send a reply saying &quot;I didn&#39;t handle this exception&quot;. replyMessage.header = exceptionMessage.header; replyMessage.NDR = exceptionMessage.NDR; replyMessage.returnCode = KERN\\_FAILURE; mach\\_msg(&amp;replyMessage.header, MACH\\_SEND\\_MSG, sizeof(replyMessage), 0, MACH\\_PORT\\_NULL, MACH\\_MSG\\_TIMEOUT\\_NONE, MACH\\_PORT\\_NULL); return NULL; } 还原异常处理端口，转移控制权 /** Restore the original mach exception ports. */static void restoreExceptionPorts(void){ KSLOG_DEBUG(“Restoring original exception ports.”); if(g_previousExceptionPorts.count == 0) { KSLOG_DEBUG(“Original exception ports were already restored.”); return; } const task\\_t thisTask = mach\\_task\\_self(); kern\\_return\\_t kr; // Reinstall old exception ports. // for 循环去除保存好的在 KSCrash 之前注册好的异常端口，将每个端口注册回去 for(mach\\_msg\\_type\\_number\\_t i = 0; i &lt; g\\_previousExceptionPorts.count; i++) &#123; KSLOG\\_TRACE(&quot;Restoring port index %d&quot;, i); kr = task\\_set\\_exception\\_ports(thisTask, g\\_previousExceptionPorts.masks\\[i\\], g\\_previousExceptionPorts.ports\\[i\\], g\\_previousExceptionPorts.behaviors\\[i\\], g\\_previousExceptionPorts.flavors\\[i\\]); if(kr != KERN\\_SUCCESS) &#123; KSLOG\\_ERROR(&quot;task\\_set\\_exception\\_ports: %s&quot;, mach\\_error\\_string(kr)); &#125; &#125; KSLOG\\_DEBUG(&quot;Exception ports restored.&quot;); g\\_previousExceptionPorts.count = 0; } 2.2. Signal 异常处理对于 Mach 异常，操作系统会将其转换为对应的 Unix 信号，所以开发者可以通过注册 signanHandler 的方式来处理。 KSCrash 在这里的处理逻辑如下图： 看一下关键代码: 设置信号处理函数 static bool installSignalHandler(){ KSLOG_DEBUG(“Installing signal handler.”); #if KSCRASH_HAS_SIGNAL_STACK // 在堆上分配一块内存， if(g_signalStack.ss_size == 0) { KSLOG_DEBUG(“Allocating signal stack area.”); g_signalStack.ss_size = SIGSTKSZ; g_signalStack.ss_sp = malloc(g_signalStack.ss_size); } // 信号处理函数的栈挪到堆中，而不和进程共用一块栈区 // sigaltstack() 函数，该函数的第 1 个参数 sigstack 是一个 stack_t 结构的指针，该结构存储了一个“可替换信号栈” 的位置及属性信息。第 2 个参数 old_sigstack 也是一个 stack_t 类型指针，它用来返回上一次建立的“可替换信号栈”的信息(如果有的话) KSLOG_DEBUG(“Setting signal stack area.”); // sigaltstack 第一个参数为创建的新的可替换信号栈，第二个参数可以设置为NULL，如果不为NULL的话，将会将旧的可替换信号栈的信息保存在里面。函数成功返回0，失败返回-1. if(sigaltstack(&amp;g_signalStack, NULL) != 0) { KSLOG_ERROR(“signalstack: %s”, strerror(errno)); goto failed; }#endif const int\\* fatalSignals = kssignal\\_fatalSignals(); int fatalSignalsCount = kssignal\\_numFatalSignals(); if(g\\_previousSignalHandlers == NULL) &#123; KSLOG\\_DEBUG(&quot;Allocating memory to store previous signal handlers.&quot;); g\\_previousSignalHandlers = malloc(sizeof(\\*g\\_previousSignalHandlers) \\* (unsigned)fatalSignalsCount); &#125; // 设置信号处理函数 sigaction 的第二个参数，类型为 sigaction 结构体 struct sigaction action = &#123;&#123;0&#125;&#125;; // sa\\_flags 成员设立 SA\\_ONSTACK 标志，该标志告诉内核信号处理函数的栈帧就在“可替换信号栈”上建立。 action.sa\\_flags = SA\\_SIGINFO | SA\\_ONSTACK; #if KSCRASH_HOST_APPLE &amp;&amp; defined(__LP64__) action.sa_flags |= SA_64REGSET;#endif sigemptyset(&amp;action.sa_mask); action.sa_sigaction = &handleSignal; // 遍历需要处理的信号数组 for(int i = 0; i &lt; fatalSignalsCount; i++) &#123; // 将每个信号的处理函数绑定到上面声明的 action 去，另外用 g\\_previousSignalHandlers 保存当前信号的处理函数 KSLOG\\_DEBUG(&quot;Assigning handler for signal %d&quot;, fatalSignals\\[i\\]); if(sigaction(fatalSignals\\[i\\], &amp;action, &amp;g\\_previousSignalHandlers\\[i\\]) != 0) &#123; char sigNameBuff\\[30\\]; const char\\* sigName = kssignal\\_signalName(fatalSignals\\[i\\]); if(sigName == NULL) &#123; snprintf(sigNameBuff, sizeof(sigNameBuff), &quot;%d&quot;, fatalSignals\\[i\\]); sigName = sigNameBuff; &#125; KSLOG\\_ERROR(&quot;sigaction (%s): %s&quot;, sigName, strerror(errno)); // Try to reverse the damage for(i--;i &gt;= 0; i--) &#123; sigaction(fatalSignals\\[i\\], &amp;g\\_previousSignalHandlers\\[i\\], NULL); &#125; goto failed; &#125; &#125; KSLOG\\_DEBUG(&quot;Signal handlers installed.&quot;); return true; failed: KSLOG_DEBUG(“Failed to install signal handlers.”); return false;} 信号处理时记录线程等上下文信息 static void handleSignal(int sigNum, siginfo_t* signalInfo, void* userContext){ KSLOG_DEBUG(“Trapped signal %d”, sigNum); if(g_isEnabled) { ksmc_suspendEnvironment(); kscm_notifyFatalExceptionCaptured(false); KSLOG\\_DEBUG(&quot;Filling out context.&quot;); KSMC\\_NEW\\_CONTEXT(machineContext); ksmc\\_getContextForSignal(userContext, machineContext); kssc\\_initWithMachineContext(&amp;g\\_stackCursor, 100, machineContext); // 记录信号处理时的上下文信息 KSCrash\\_MonitorContext\\* crashContext = &amp;g\\_monitorContext; memset(crashContext, 0, sizeof(\\*crashContext)); crashContext-&gt;crashType = KSCrashMonitorTypeSignal; crashContext-&gt;eventID = g\\_eventID; crashContext-&gt;offendingMachineContext = machineContext; crashContext-&gt;registersAreValid = true; crashContext-&gt;faultAddress = (uintptr\\_t)signalInfo-&gt;si\\_addr; crashContext-&gt;signal.userContext = userContext; crashContext-&gt;signal.signum = signalInfo-&gt;si\\_signo; crashContext-&gt;signal.sigcode = signalInfo-&gt;si\\_code; crashContext-&gt;stackCursor = &amp;g\\_stackCursor; kscm\\_handleException(crashContext); ksmc\\_resumeEnvironment(); &#125; KSLOG\\_DEBUG(&quot;Re-raising signal for regular handlers to catch.&quot;); // This is technically not allowed, but it works in OSX and iOS. raise(sigNum); } KSCrash 信号处理后还原之前的信号处理权限 static void uninstallSignalHandler(void){ KSLOG_DEBUG(“Uninstalling signal handlers.”); const int\\* fatalSignals = kssignal\\_fatalSignals(); int fatalSignalsCount = kssignal\\_numFatalSignals(); // 遍历需要处理信号数组，将之前的信号处理函数还原 for(int i = 0; i &lt; fatalSignalsCount; i++) &#123; KSLOG\\_DEBUG(&quot;Restoring original handler for signal %d&quot;, fatalSignals\\[i\\]); sigaction(fatalSignals\\[i\\], &amp;g\\_previousSignalHandlers\\[i\\], NULL); &#125; KSLOG\\_DEBUG(&quot;Signal handlers uninstalled.&quot;); } 说明： 先从堆上分配一块内存区域，被称为“可替换信号栈”，目的是将信号处理函数的栈干掉，用堆上的内存区域代替，而不和进程共用一块栈区。 为什么这么做？一个进程可能有 n 个线程，每个线程都有自己的任务，假如某个线程执行出错，这样就会导致整个进程的崩溃。所以为了信号处理函数正常运行，需要为信号处理函数设置单独的运行空间。另一种情况是递归函数将系统默认的栈空间用尽了，但是信号处理函数使用的栈是它实现在堆中分配的空间，而不是系统默认的栈，所以它仍旧可以正常工作。 int sigaltstack(const stack_t * __restrict, stack_t * __restrict) 函数的二个参数都是 stack_t 结构的指针，存储了可替换信号栈的信息（栈的起始地址、栈的长度、状态）。第1个参数该结构存储了一个“可替换信号栈” 的位置及属性信息。第 2 个参数用来返回上一次建立的“可替换信号栈”的信息(如果有的话)。 _STRUCT_SIGALTSTACK { void \\*ss\\_sp; /\\* signal stack base \\*/ \\_\\_darwin\\_size\\_t ss\\_size; /\\* signal stack length \\*/ int ss\\_flags; /\\* SA\\_DISABLE and/or SA\\_ONSTACK \\*/ }; typedef _STRUCT_SIGALTSTACK stack_t; /* [???] signal stack */ 新创建的可替换信号栈，ss_flags 必须设置为 0。系统定义了 SIGSTKSZ 常量，可满足绝大多可替换信号栈的需求。 /* * Structure used in sigaltstack call. */ #define SS_ONSTACK 0x0001 /* take signal on signal stack */#define SS_DISABLE 0x0004 /* disable taking signals on alternate stack */#define MINSIGSTKSZ 32768 /* (32K)minimum allowable stack */#define SIGSTKSZ 131072 /* (128K)recommended stack size */ sigaltstack 系统调用通知内核“可替换信号栈”已经建立。 ss_flags 为 SS_ONSTACK 时，表示进程当前正在“可替换信号栈”中执行，如果此时试图去建立一个新的“可替换信号栈”，那么会遇到 EPERM (禁止该动作) 的错误；为 SS_DISABLE 说明当前没有已建立的“可替换信号栈”，禁止建立“可替换信号栈”。 int sigaction(int, const struct sigaction * __restrict, struct sigaction * __restrict); 第一个函数表示需要处理的信号值，但不能是 SIGKILL 和 SIGSTOP ，这两个信号的处理函数不允许用户重写，因为它们给超级用户提供了终止程序的方法（ SIGKILL and SIGSTOP cannot be caught, blocked, or ignored）； 第二个和第三个参数是一个 sigaction 结构体。如果第二个参数不为空则代表将其指向信号处理函数，第三个参数不为空，则将之前的信号处理函数保存到该指针中。如果第二个参数为空，第三个参数不为空，则可以获取当前的信号处理函数。 /* * Signal vector “template” used in sigaction call. */ struct sigaction { union \\_\\_sigaction\\_u \\_\\_sigaction\\_u; /\\* signal handler \\*/ sigset\\_t sa\\_mask; /\\* signal mask to apply \\*/ int sa\\_flags; /\\* see signal options below \\*/ }; sigaction 函数的 sa_flags 参数需要设置 SA_ONSTACK 标志，告诉内核信号处理函数的栈帧就在“可替换信号栈”上建立。 2.3. C++ 异常处理c++ 异常处理的实现是依靠了标准库的 std::set_terminate(CPPExceptionTerminate) 函数。 iOS 工程中某些功能的实现可能使用了C、C++等。假如抛出 C++ 异常，如果该异常可以被转换为 NSException，则走 OC 异常捕获机制，如果不能转换，则继续走 C++ 异常流程，也就是 default_terminate_handler。这个 C++ 异常的默认 terminate 函数内部调用 abort_message 函数，最后触发了一个 abort 调用，系统产生一个 SIGABRT 信号。 在系统抛出 C++ 异常后，加一层 try...catch... 来判断该异常是否可以转换为 NSException，再重新抛出的C++异常。此时异常的现场堆栈已经消失，所以上层通过捕获 SIGABRT 信号是无法还原发生异常时的场景，即异常堆栈缺失。 为什么？try...catch... 语句内部会调用 __cxa_rethrow() 抛出异常，__cxa_rethrow() 内部又会调用 unwind，unwind 可以简单理解为函数调用的逆调用，主要用来清理函数调用过程中每个函数生成的局部变量，一直到最外层的 catch 语句所在的函数，并把控制移交给 catch 语句，这就是C++异常的堆栈消失原因。 static void setEnabled(bool isEnabled) { if(isEnabled != g_isEnabled) { g_isEnabled = isEnabled; if(isEnabled) { initialize(); ksid\\_generate(g\\_eventID); g\\_originalTerminateHandler = std::set\\_terminate(CPPExceptionTerminate); &#125; else &#123; std::set\\_terminate(g\\_originalTerminateHandler); &#125; g\\_captureNextStackTrace = isEnabled; &#125; } static void initialize() { static bool isInitialized = false; if(!isInitialized) { isInitialized = true; kssc_initCursor(&amp;g_stackCursor, NULL, NULL); }} void kssc_initCursor(KSStackCursor *cursor, void (*resetCursor)(KSStackCursor*), bool (*advanceCursor)(KSStackCursor*)) { cursor-&gt;symbolicate = kssymbolicator_symbolicate; cursor-&gt;advanceCursor = advanceCursor != NULL ? advanceCursor : g_advanceCursor; cursor-&gt;resetCursor = resetCursor != NULL ? resetCursor : kssc_resetCursor; cursor-&gt;resetCursor(cursor);} static void CPPExceptionTerminate(void) { ksmc_suspendEnvironment(); KSLOG_DEBUG(“Trapped c++ exception”); const char* name = NULL; std::type_info* tinfo = __cxxabiv1::__cxa_current_exception_type(); if(tinfo != NULL) { name = tinfo-&gt;name(); } if(name == NULL || strcmp(name, &quot;NSException&quot;) != 0) &#123; kscm\\_notifyFatalExceptionCaptured(false); KSCrash\\_MonitorContext\\* crashContext = &amp;g\\_monitorContext; memset(crashContext, 0, sizeof(\\*crashContext)); char descriptionBuff\\[DESCRIPTION\\_BUFFER\\_LENGTH\\]; const char\\* description = descriptionBuff; descriptionBuff\\[0\\] = 0; KSLOG\\_DEBUG(&quot;Discovering what kind of exception was thrown.&quot;); g\\_captureNextStackTrace = false; try &#123; throw; &#125; catch(std::exception&amp; exc) &#123; strncpy(descriptionBuff, exc.what(), sizeof(descriptionBuff)); &#125; #define CATCH_VALUE(TYPE, PRINTFTYPE) \\catch(TYPE value)\\{ \\ snprintf(descriptionBuff, sizeof(descriptionBuff), “%” #PRINTFTYPE, value); \\} CATCH_VALUE(char, d) CATCH_VALUE(short, d) CATCH_VALUE(int, d) CATCH_VALUE(long, ld) CATCH_VALUE(long long, lld) CATCH_VALUE(unsigned char, u) CATCH_VALUE(unsigned short, u) CATCH_VALUE(unsigned int, u) CATCH_VALUE(unsigned long, lu) CATCH_VALUE(unsigned long long, llu) CATCH_VALUE(float, f) CATCH_VALUE(double, f) CATCH_VALUE(long double, Lf) CATCH_VALUE(char*, s) catch(…) { description = NULL; } g_captureNextStackTrace = g_isEnabled; // TODO: Should this be done here? Maybe better in the exception handler? KSMC\\_NEW\\_CONTEXT(machineContext); ksmc\\_getContextForThread(ksthread\\_self(), machineContext, true); KSLOG\\_DEBUG(&quot;Filling out context.&quot;); crashContext-&gt;crashType = KSCrashMonitorTypeCPPException; crashContext-&gt;eventID = g\\_eventID; crashContext-&gt;registersAreValid = false; crashContext-&gt;stackCursor = &amp;g\\_stackCursor; crashContext-&gt;CPPException.name = name; crashContext-&gt;exceptionName = name; crashContext-&gt;crashReason = description; crashContext-&gt;offendingMachineContext = machineContext; kscm\\_handleException(crashContext); &#125; else &#123; KSLOG\\_DEBUG(&quot;Detected NSException. Letting the current NSException handler deal with it.&quot;); &#125; ksmc\\_resumeEnvironment(); KSLOG\\_DEBUG(&quot;Calling original terminate handler.&quot;); g\\_originalTerminateHandler(); } 2.4. Objective-C 异常处理对于 OC 层面的 NSException 异常处理较为容易，可以通过注册 NSUncaughtExceptionHandler 来捕获异常信息，通过 NSException 参数来做 Crash 信息的收集，交给数据上报组件。 static void setEnabled(bool isEnabled) { if(isEnabled != g_isEnabled) { g_isEnabled = isEnabled; if(isEnabled) { KSLOG_DEBUG(@”Backing up original handler.”); // 记录之前的 OC 异常处理函数 g_previousUncaughtExceptionHandler = NSGetUncaughtExceptionHandler(); KSLOG\\_DEBUG(@&quot;Setting new handler.&quot;); // 设置新的 OC 异常处理函数 NSSetUncaughtExceptionHandler(&amp;handleException); KSCrash.sharedInstance.uncaughtExceptionHandler = &amp;handleException; &#125; else &#123; KSLOG\\_DEBUG(@&quot;Restoring original handler.&quot;); NSSetUncaughtExceptionHandler(g\\_previousUncaughtExceptionHandler); &#125; &#125; } 2.5. 主线程死锁主线程死锁的检测和 ANR 的检测有些类似 创建一个线程，在线程运行方法中用 do...while... 循环处理逻辑，加了 autorelease 避免内存过高 有一个 awaitingResponse 属性和 watchdogPulse 方法。watchdogPulse 主要逻辑为设置 awaitingResponse 为 YES，切换到主线程中，设置 awaitingResponse 为 NO， - (void) watchdogPulse { \\_\\_block id blockSelf = self; self.awaitingResponse = YES; dispatch\\_async(dispatch\\_get\\_main\\_queue(), ^ &#123; \\[blockSelf watchdogAnswer\\]; &#125;); } 线程的执行方法里面不断循环，等待设置的 g_watchdogInterval 后判断 awaitingResponse 的属性值是不是初始状态的值，否则判断为死锁 - (void) runMonitor { BOOL cancelled = NO; do &#123; // Only do a watchdog check if the watchdog interval is &gt; 0. // If the interval is &lt;= 0, just idle until the user changes it. @autoreleasepool &#123; NSTimeInterval sleepInterval = g\\_watchdogInterval; BOOL runWatchdogCheck = sleepInterval &gt; 0; if(!runWatchdogCheck) &#123; sleepInterval = kIdleInterval; &#125; \\[NSThread sleepForTimeInterval:sleepInterval\\]; cancelled = self.monitorThread.isCancelled; if(!cancelled &amp;&amp; runWatchdogCheck) &#123; if(self.awaitingResponse) &#123; \\[self handleDeadlock\\]; &#125; else &#123; \\[self watchdogPulse\\]; &#125; &#125; &#125; &#125; while (!cancelled); } 2.6 Crash 的生成与保存2.6.1 Crash 日志的生成逻辑上面的部分讲过了 iOS 应用开发中的各种 crash 监控逻辑，接下来就应该分析下 crash 捕获后如何将 crash 信息记录下来，也就是保存到应用沙盒中。 拿主线程死锁这种 crash 举例子，看看 KSCrash 是如何记录 crash 信息的。 // KSCrashMonitor_Deadlock.m (void) handleDeadlock{ ksmc_suspendEnvironment(); kscm_notifyFatalExceptionCaptured(false); KSMC_NEW_CONTEXT(machineContext); ksmc_getContextForThread(g_mainQueueThread, machineContext, false); KSStackCursor stackCursor; kssc_initWithMachineContext(&amp;stackCursor, 100, machineContext); char eventID[37]; ksid_generate(eventID); KSLOG_DEBUG(@”Filling out context.”); KSCrash_MonitorContext* crashContext = &amp;g_monitorContext; memset(crashContext, 0, sizeof(*crashContext)); crashContext-&gt;crashType = KSCrashMonitorTypeMainThreadDeadlock; crashContext-&gt;eventID = eventID; crashContext-&gt;registersAreValid = false; crashContext-&gt;offendingMachineContext = machineContext; crashContext-&gt;stackCursor = &stackCursor; kscm_handleException(crashContext); ksmc_resumeEnvironment(); KSLOG_DEBUG(@”Calling abort()”); abort();} 其他几个 crash 也是一样，异常信息经过包装交给 kscm_handleException() 函数处理。可以看到这个函数被其他几种 crash 捕获后所调用。 /** Start general exception processing. * * @oaram context Contextual information about the exception. */void kscm_handleException(struct KSCrash_MonitorContext* context){ context-&gt;requiresAsyncSafety = g_requiresAsyncSafety; if(g_crashedDuringExceptionHandling) { context-&gt;crashedDuringCrashHandling = true; } for(int i = 0; i &lt; g_monitorsCount; i++) { Monitor* monitor = &amp;g_monitors[i]; // 判断当前的 crash 监控是开启状态 if(isMonitorEnabled(monitor)) { // 针对每种 crash 类型做一些额外的补充信息 addContextualInfoToEvent(monitor, context); } } // 真正处理 crash 信息，保存 json 格式的 crash 信息 g_onExceptionEvent(context); if(g\\_handlingFatalException &amp;&amp; !g\\_crashedDuringExceptionHandling) &#123; KSLOG\\_DEBUG(&quot;Exception is fatal. Restoring original handlers.&quot;); kscm\\_setActiveMonitors(KSCrashMonitorTypeNone); &#125; } g_onExceptionEvent 是一个 block，声明为 static void (*g_onExceptionEvent)(struct KSCrash_MonitorContext* monitorContext); 在 KSCrashMonitor.c 中被赋值 void kscm_setEventCallback(void (*onEvent)(struct KSCrash_MonitorContext* monitorContext)){ g_onExceptionEvent = onEvent;} kscm_setEventCallback() 函数在 KSCrashC.c 文件中被调用 KSCrashMonitorType kscrash_install(const char* appName, const char* const installPath){ KSLOG_DEBUG(“Installing crash reporter.”); if(g\\_installed) &#123; KSLOG\\_DEBUG(&quot;Crash reporter already installed.&quot;); return g\\_monitoring; &#125; g\\_installed = 1; char path\\[KSFU\\_MAX\\_PATH\\_LENGTH\\]; snprintf(path, sizeof(path), &quot;%s/Reports&quot;, installPath); ksfu\\_makePath(path); kscrs\\_initialize(appName, path); snprintf(path, sizeof(path), &quot;%s/Data&quot;, installPath); ksfu\\_makePath(path); snprintf(path, sizeof(path), &quot;%s/Data/CrashState.json&quot;, installPath); kscrashstate\\_initialize(path); snprintf(g\\_consoleLogPath, sizeof(g\\_consoleLogPath), &quot;%s/Data/ConsoleLog.txt&quot;, installPath); if(g\\_shouldPrintPreviousLog) &#123; printPreviousLog(g\\_consoleLogPath); &#125; kslog\\_setLogFilename(g\\_consoleLogPath, true); ksccd\\_init(60); // 设置 crash 发生时的 callback 函数 kscm\\_setEventCallback(onCrash); KSCrashMonitorType monitors = kscrash\\_setMonitoring(g\\_monitoring); KSLOG\\_DEBUG(&quot;Installation complete.&quot;); return monitors; } /** Called when a crash occurs. * * This function gets passed as a callback to a crash handler. */static void onCrash(struct KSCrash_MonitorContext* monitorContext){ KSLOG_DEBUG(“Updating application state to note crash.”); kscrashstate_notifyAppCrash(); monitorContext-&gt;consoleLogPath = g_shouldAddConsoleLogToReport ? g_consoleLogPath : NULL; // 正在处理 crash 的时候，发生了再次 crash if(monitorContext-&gt;crashedDuringCrashHandling) &#123; kscrashreport\\_writeRecrashReport(monitorContext, g\\_lastCrashReportFilePath); &#125; else &#123; // 1. 先根据当前时间创建新的 crash 的文件路径 char crashReportFilePath\\[KSFU\\_MAX\\_PATH\\_LENGTH\\]; kscrs\\_getNextCrashReportPath(crashReportFilePath); // 2. 将新生成的文件路径保存到 g\\_lastCrashReportFilePath strncpy(g\\_lastCrashReportFilePath, crashReportFilePath, sizeof(g\\_lastCrashReportFilePath)); // 3. 将新生成的文件路径传入函数进行 crash 写入 kscrashreport\\_writeStandardReport(monitorContext, crashReportFilePath); &#125; } 接下来的函数就是具体的日志写入文件的实现。2个函数做的事情相似，都是格式化为 json 形式并写入文件。区别在于 crash 写入时如果再次发生 crash， 则走简易版的写入逻辑 kscrashreport_writeRecrashReport()，否则走标准的写入逻辑 kscrashreport_writeStandardReport()。 bool ksfu_openBufferedWriter(KSBufferedWriter* writer, const char* const path, char* writeBuffer, int writeBufferLength){ writer-&gt;buffer = writeBuffer; writer-&gt;bufferLength = writeBufferLength; writer-&gt;position = 0; /* open() 的第二个参数描述的是文件操作的权限 #define O_RDONLY 0x0000 open for reading only #define O_WRONLY 0x0001 open for writing only #define O_RDWR 0x0002 open for reading and writing #define O_ACCMODE 0x0003 mask for above mode #define O\\_CREAT 0x0200 create if nonexistant #define O\\_TRUNC 0x0400 truncate to zero length #define O\\_EXCL 0x0800 error if already exists 0755：即用户具有读/写/执行权限，组用户和其它用户具有读写权限； 0644：即用户具有读写权限，组用户和其它用户具有只读权限； 成功则返回文件描述符，若出现则返回 -1 \\*/ writer-&gt;fd = open(path, O\\_RDWR | O\\_CREAT | O\\_EXCL, 0644); if(writer-&gt;fd &lt; 0) &#123; KSLOG\\_ERROR(&quot;Could not open crash report file %s: %s&quot;, path, strerror(errno)); return false; &#125; return true; } /** * Write a standard crash report to a file. * * @param monitorContext Contextual information about the crash and environment. * The caller must fill this out before passing it in. * * @param path The file to write to. */void kscrashreport_writeStandardReport(const struct KSCrash_MonitorContext* const monitorContext, const char* path){ KSLOG_INFO(“Writing crash report to %s”, path); char writeBuffer[1024]; KSBufferedWriter bufferedWriter; if(!ksfu\\_openBufferedWriter(&amp;bufferedWriter, path, writeBuffer, sizeof(writeBuffer))) &#123; return; &#125; ksccd\\_freeze(); KSJSONEncodeContext jsonContext; jsonContext.userData = &amp;bufferedWriter; KSCrashReportWriter concreteWriter; KSCrashReportWriter\\* writer = &amp;concreteWriter; prepareReportWriter(writer, &amp;jsonContext); ksjson\\_beginEncode(getJsonContext(writer), true, addJSONData, &amp;bufferedWriter); writer-&gt;beginObject(writer, KSCrashField\\_Report); &#123; writeReportInfo(writer, KSCrashField\\_Report, KSCrashReportType\\_Standard, monitorContext-&gt;eventID, monitorContext-&gt;System.processName); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writeBinaryImages(writer, KSCrashField\\_BinaryImages); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writeProcessState(writer, KSCrashField\\_ProcessState, monitorContext); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writeSystemInfo(writer, KSCrashField\\_System, monitorContext); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writer-&gt;beginObject(writer, KSCrashField\\_Crash); &#123; writeError(writer, KSCrashField\\_Error, monitorContext); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writeAllThreads(writer, KSCrashField\\_Threads, monitorContext, g\\_introspectionRules.enabled); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); &#125; writer-&gt;endContainer(writer); if(g\\_userInfoJSON != NULL) &#123; addJSONElement(writer, KSCrashField\\_User, g\\_userInfoJSON, false); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); &#125; else &#123; writer-&gt;beginObject(writer, KSCrashField\\_User); &#125; if(g\\_userSectionWriteCallback != NULL) &#123; ksfu\\_flushBufferedWriter(&amp;bufferedWriter); g\\_userSectionWriteCallback(writer); &#125; writer-&gt;endContainer(writer); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writeDebugInfo(writer, KSCrashField\\_Debug, monitorContext); &#125; writer-&gt;endContainer(writer); ksjson\\_endEncode(getJsonContext(writer)); ksfu\\_closeBufferedWriter(&amp;bufferedWriter); ksccd\\_unfreeze(); } /** Write a minimal crash report to a file. * * @param monitorContext Contextual information about the crash and environment. * The caller must fill this out before passing it in. * * @param path The file to write to. */void kscrashreport_writeRecrashReport(const struct KSCrash_MonitorContext* const monitorContext, const char* path){ char writeBuffer[1024]; KSBufferedWriter bufferedWriter; static char tempPath[KSFU_MAX_PATH_LENGTH]; // 将传递过来的上份 crash report 文件名路径（/var/mobile/Containers/Data/Application/******/Library/Caches/KSCrash/Test/Reports/Test-report-******.json）修改为去掉 .json ，加上 .old 成为新的文件路径 /var/mobile/Containers/Data/Application/******/Library/Caches/KSCrash/Test/Reports/Test-report-******.old strncpy(tempPath, path, sizeof(tempPath) - 10); strncpy(tempPath + strlen(tempPath) - 5, &quot;.old&quot;, 5); KSLOG\\_INFO(&quot;Writing recrash report to %s&quot;, path); if(rename(path, tempPath) &lt; 0) &#123; KSLOG\\_ERROR(&quot;Could not rename %s to %s: %s&quot;, path, tempPath, strerror(errno)); &#125; // 根据传入路径来打开内存写入需要的文件 if(!ksfu\\_openBufferedWriter(&amp;bufferedWriter, path, writeBuffer, sizeof(writeBuffer))) &#123; return; &#125; ksccd\\_freeze(); // json 解析的 c 代码 KSJSONEncodeContext jsonContext; jsonContext.userData = &amp;bufferedWriter; KSCrashReportWriter concreteWriter; KSCrashReportWriter\\* writer = &amp;concreteWriter; prepareReportWriter(writer, &amp;jsonContext); ksjson\\_beginEncode(getJsonContext(writer), true, addJSONData, &amp;bufferedWriter); writer-&gt;beginObject(writer, KSCrashField\\_Report); &#123; writeRecrash(writer, KSCrashField\\_RecrashReport, tempPath); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); if(remove(tempPath) &lt; 0) &#123; KSLOG\\_ERROR(&quot;Could not remove %s: %s&quot;, tempPath, strerror(errno)); &#125; writeReportInfo(writer, KSCrashField\\_Report, KSCrashReportType\\_Minimal, monitorContext-&gt;eventID, monitorContext-&gt;System.processName); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); writer-&gt;beginObject(writer, KSCrashField\\_Crash); &#123; writeError(writer, KSCrashField\\_Error, monitorContext); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); int threadIndex = ksmc\\_indexOfThread(monitorContext-&gt;offendingMachineContext, ksmc\\_getThreadFromContext(monitorContext-&gt;offendingMachineContext)); writeThread(writer, KSCrashField\\_CrashedThread, monitorContext, monitorContext-&gt;offendingMachineContext, threadIndex, false); ksfu\\_flushBufferedWriter(&amp;bufferedWriter); &#125; writer-&gt;endContainer(writer); &#125; writer-&gt;endContainer(writer); ksjson\\_endEncode(getJsonContext(writer)); ksfu\\_closeBufferedWriter(&amp;bufferedWriter); ksccd\\_unfreeze(); } 2.6.2 Crash 日志的读取逻辑当前 App 在 Crash 之后，KSCrash 将数据保存到 App 沙盒目录下，App 下次启动后我们读取存储的 crash 文件，然后处理数据并上传。 App 启动后函数调用： [KSCrashInstallation sendAllReportsWithCompletion:] -&gt; [KSCrash sendAllReportsWithCompletion:] -&gt; [KSCrash allReports] -&gt; [KSCrash reportWithIntID:] -&gt;[KSCrash loadCrashReportJSONWithID:] -&gt; kscrs_readReport 在 sendAllReportsWithCompletion 里读取沙盒里的Crash 数据。 // 先通过读取文件夹，遍历文件夹内的文件数量来判断 crash 报告的个数static int getReportCount(){ int count = 0; DIR* dir = opendir(g_reportsPath); if(dir == NULL) { KSLOG_ERROR(“Could not open directory %s”, g_reportsPath); goto done; } struct dirent* ent; while((ent = readdir(dir)) != NULL) { if(getReportIDFromFilename(ent-&gt;d_name) &gt; 0) { count++; } } done: if(dir != NULL) { closedir(dir); } return count;} // 通过 crash 文件个数、文件夹信息去遍历，一次获取到文件名（文件名的最后一部分就是 reportID），拿到 reportID 再去读取 crash 报告内的文件内容，写入数组 (NSArray*) allReports{ int reportCount = kscrash_getReportCount(); int64_t reportIDs[reportCount]; reportCount = kscrash_getReportIDs(reportIDs, reportCount); NSMutableArray* reports = [NSMutableArray arrayWithCapacity:(NSUInteger)reportCount]; for(int i = 0; i &lt; reportCount; i++) { NSDictionary\\* report = \\[self reportWithIntID:reportIDs\\[i\\]\\]; if(report != nil) &#123; \\[reports addObject:report\\]; &#125; } return reports;} // 根据 reportID 找到 crash 信息 (NSDictionary*) reportWithIntID:(int64_t) reportID{ NSData* jsonData = [self loadCrashReportJSONWithID:reportID]; if(jsonData == nil) { return nil; } NSError* error = nil; NSMutableDictionary* crashReport = [KSJSONCodec decode:jsonData options:KSJSONDecodeOptionIgnoreNullInArray | KSJSONDecodeOptionIgnoreNullInObject | KSJSONDecodeOptionKeepPartialObject error:&amp;error\\]; if(error != nil) { KSLOG\\_ERROR(@&quot;Encountered error loading crash report %&quot; PRIx64 &quot;: %@&quot;, reportID, error); } if(crashReport == nil) { KSLOG\\_ERROR(@&quot;Could not load crash report&quot;); return nil; } [self doctorReport:crashReport]; return crashReport;} // reportID 读取 crash 内容并转换为 NSData 类型 (NSData*) loadCrashReportJSONWithID:(int64_t) reportID{ char* report = kscrash_readReport(reportID); if(report != NULL) { return \\[NSData dataWithBytesNoCopy:report length:strlen(report) freeWhenDone:YES\\]; } return nil;} // reportID 读取 crash 数据到 char 类型char* kscrash_readReport(int64_t reportID){ if(reportID &lt;= 0) { KSLOG_ERROR(“Report ID was %” PRIx64, reportID); return NULL; } char\\* rawReport = kscrs\\_readReport(reportID); if(rawReport == NULL) &#123; KSLOG\\_ERROR(&quot;Failed to load report ID %&quot; PRIx64, reportID); return NULL; &#125; char\\* fixedReport = kscrf\\_fixupCrashReport(rawReport); if(fixedReport == NULL) &#123; KSLOG\\_ERROR(&quot;Failed to fixup report ID %&quot; PRIx64, reportID); &#125; free(rawReport); return fixedReport; } // 多线程加锁，通过 reportID 执行 c 函数 getCrashReportPathByID，将路径设置到 path 上。然后执行 ksfu_readEntireFile 读取 crash 信息到 resultchar* kscrs_readReport(int64_t reportID){ pthread_mutex_lock(&amp;g_mutex); char path[KSCRS_MAX_PATH_LENGTH]; getCrashReportPathByID(reportID, path); char* result; ksfu_readEntireFile(path, &amp;result, NULL, 2000000); pthread_mutex_unlock(&amp;g_mutex); return result;} int kscrash_getReportIDs(int64_t* reportIDs, int count){ return kscrs_getReportIDs(reportIDs, count);} int kscrs_getReportIDs(int64_t* reportIDs, int count){ pthread_mutex_lock(&amp;g_mutex); count = getReportIDs(reportIDs, count); pthread_mutex_unlock(&amp;g_mutex); return count;}// 循环读取文件夹内容，根据 ent-&gt;d_name 调用 getReportIDFromFilename 函数，来获取 reportID，循环内部填充数组static int getReportIDs(int64_t* reportIDs, int count){ int index = 0; DIR* dir = opendir(g_reportsPath); if(dir == NULL) { KSLOG_ERROR(“Could not open directory %s”, g_reportsPath); goto done; } struct dirent\\* ent; while((ent = readdir(dir)) != NULL &amp;&amp; index &lt; count) &#123; int64\\_t reportID = getReportIDFromFilename(ent-&gt;d\\_name); if(reportID &gt; 0) &#123; reportIDs\\[index++\\] = reportID; &#125; &#125; qsort(reportIDs, (unsigned)count, sizeof(reportIDs\\[0\\]), compareInt64); done: if(dir != NULL) { closedir(dir); } return index;} // sprintf(参数1， 格式2) 函数将格式2的值返回到参数1上，然后执行 sscanf(参数1， 参数2， 参数3)，函数将字符串参数1的内容，按照参数2的格式，写入到参数3上。crash 文件命名为 “App名称-report-reportID.json”static int64_t getReportIDFromFilename(const char* filename){ char scanFormat[100]; sprintf(scanFormat, “%s-report-%%” PRIx64 “.json”, g_appName); int64\\_t reportID = 0; sscanf(filename, scanFormat, &amp;reportID); return reportID; } 2.7 前端 js 相关的 Crash 的监控2.7.1 JavascriptCore 异常监控这部分简单粗暴，直接通过 JSContext 对象的 exceptionHandler 属性来监控，比如下面的代码 jsContext.exceptionHandler = ^(JSContext *context, JSValue *exception) { // 处理 jscore 相关的异常信息}; 2.7.2 h5 页面异常监控当 h5 页面内的 Javascript 运行异常时会 window 对象会触发 ErrorEvent 接口的 error 事件，并执行 window.onerror()。 window.onerror = function (msg, url, lineNumber, columnNumber, error) { // 处理异常信息}; 2.7.3 React Native 异常监控小实验：下图是写了一个 RN Demo 工程，在 Debug Text 控件上加了事件监听代码，内部人为触发 crash &lt;Text style={styles.sectionTitle} onPress={()=&gt;{1+qw;}}&gt;Debug&lt;/Text&gt; 对比组1： 条件： iOS 项目 debug 模式。在 RN 端增加了异常处理的代码。 模拟器点击 command + d 调出面板，选择 Debug，打开 Chrome 浏览器， Mac 下快捷键 Command + Option + J 打开调试面板，就可以像调试 React 一样调试 RN 代码了。 查看到 crash stack 后点击可以跳转到 sourceMap 的地方。 Tips：RN 项目打 Release 包 在项目根目录下创建文件夹（ release_iOS），作为资源的输出文件夹 在终端切换到工程目录，然后执行下面的代码 react-native bundle –entry-file index.js –platform ios –dev false –bundle-output release_ios/main.jsbundle –assets-dest release_iOS –sourcemap-output release_ios/index.ios.map; 将 release_iOS 文件夹内的 .jsbundle 和 assets 文件夹内容拖入到 iOS 工程中即可 对比组2： 条件：iOS 项目 release 模式。在 RN 端不增加异常处理代码 操作：运行 iOS 工程，点击按钮模拟 crash 现象：iOS 项目奔溃。截图以及日志如下 2020-06-22 22:26:03.318 [info][tid:main][RCTRootView.m:294] Running application todos ({ initialProps = { }; rootTag = 1;})2020-06-22 22:26:03.490 [info][tid:com.facebook.react.JavaScript] Running “todos” with {“rootTag”:1,”initialProps”:{}}2020-06-22 22:27:38.673 [error][tid:com.facebook.react.JavaScript] ReferenceError: Can’t find variable: qw2020-06-22 22:27:38.675 [fatal][tid:com.facebook.react.ExceptionsManagerQueue] Unhandled JS Exception: ReferenceError: Can’t find variable: qw2020-06-22 22:27:38.691300+0800 todos[16790:314161] *** Terminating app due to uncaught exception ‘RCTFatalException: Unhandled JS Exception: ReferenceError: Can’t find variable: qw’, reason: ‘Unhandled JS Exception: ReferenceError: Can’t find variable: qw, stack:onPress@397:1821@203:3896_performSideEffectsForTransition@210:9689_performSideEffectsForTransition@(null):(null)_receiveSignal@210:8425_receiveSignal@(null):(null)touchableHandleResponderRelease@210:5671touchableHandleResponderRelease@(null):(null)onResponderRelease@203:3006b@97:1125S@97:1268w@97:1322R@97:1617M@97:2401forEach@(null):(null)U@97:2201@97:13818Pe@97:90199Re@97:13478Ie@97:13664receiveTouches@97:14448value@27:3544@27:840value@27:2798value@27:812value@(null):(null)‘*** First throw call stack:( 0 CoreFoundation 0x00007fff23e3cf0e __exceptionPreprocess + 350 1 libobjc.A.dylib 0x00007fff50ba89b2 objc_exception_throw + 48 2 todos 0x00000001017b0510 RCTFormatError + 0 3 todos 0x000000010182d8ca -[RCTExceptionsManager reportFatal:stack:exceptionId:suppressRedBox:] + 503 4 todos 0x000000010182e34e -[RCTExceptionsManager reportException:] + 1658 5 CoreFoundation 0x00007fff23e43e8c __invoking___ + 140 6 CoreFoundation 0x00007fff23e41071 -[NSInvocation invoke] + 321 7 CoreFoundation 0x00007fff23e41344 -[NSInvocation invokeWithTarget:] + 68 8 todos 0x00000001017e07fa -[RCTModuleMethod invokeWithBridge:module:arguments:] + 578 9 todos 0x00000001017e2a84 _ZN8facebook5reactL11invokeInnerEP9RCTBridgeP13RCTModuleDatajRKN5folly7dynamicE + 246 10 todos 0x00000001017e280c ___ZN8facebook5react15RCTNativeModule6invokeEjON5folly7dynamicEi_block_invoke + 78 11 libdispatch.dylib 0x00000001025b5f11 _dispatch_call_block_and_release + 12 12 libdispatch.dylib 0x00000001025b6e8e _dispatch_client_callout + 8 13 libdispatch.dylib 0x00000001025bd6fd _dispatch_lane_serial_drain + 788 14 libdispatch.dylib 0x00000001025be28f _dispatch_lane_invoke + 422 15 libdispatch.dylib 0x00000001025c9b65 _dispatch_workloop_worker_thread + 719 16 libsystem_pthread.dylib 0x00007fff51c08a3d _pthread_wqthread + 290 17 libsystem_pthread.dylib 0x00007fff51c07b77 start_wqthread + 15)libc++abi.dylib: terminating with uncaught exception of type NSException(lldb) Tips：如何在 RN release 模式下调试（看到 js 侧的 console 信息） 在 AppDelegate.m 中引入 #import &lt;React/RCTLog.h&gt; 在 - (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions 中加入 RCTSetLogThreshold(RCTLogLevelTrace); 对比组3： 条件：iOS 项目 release 模式。在 RN 端增加异常处理代码。 global.ErrorUtils.setGlobalHandler((e) =&gt; { console.log(e); let message = { name: e.name, message: e.message, stack: e.stack }; axios.get(‘http://192.168.1.100:8888/test.php&#39;, { params: { ‘message’: JSON.stringify(message) } }).then(function (response) { console.log(response) }).catch(function (error) { console.log(error) });}, true) 操作：运行 iOS 工程，点击按钮模拟 crash。 现象：iOS 项目不奔溃。日志信息如下，对比 bundle 包中的 js。 结论： 在 RN 项目中，如果发生了 crash 则会在 Native 侧有相应体现。如果 RN 侧写了 crash 捕获的代码，则 Native 侧不会奔溃。如果 RN 侧的 crash 没有捕获，则 Native 直接奔溃。 RN 项目写了 crash 监控，监控后将堆栈信息打印出来发现对应的 js 信息是经过 webpack 处理的，crash 分析难度很大。所以我们针对 RN 的 crash 需要在 RN 侧写监控代码，监控后需要上报，此外针对监控后的信息需要写专门的 crash 信息还原给你，也就是 sourceMap 解析。 2.7.3.1 js 逻辑错误写过 RN 的人都知道在 DEBUG 模式下 js 代码有问题则会产生红屏，在 RELEASE 模式下则会白屏或者闪退，为了体验和质量把控需要做异常监控。 在看 RN 源码时候发现了 ErrorUtils，看代码可以设置处理错误信息。 /** * Copyright (c) Facebook, Inc. and its affiliates. * * This source code is licensed under the MIT license found in the * LICENSE file in the root directory of this source tree. * * @format * @flow strict * @polyfill */ let _inGuard = 0; type ErrorHandler = (error: mixed, isFatal: boolean) =&gt; void;type Fn&lt;Args, Return&gt; = (…Args) =&gt; Return; /** * This is the error handler that is called when we encounter an exception * when loading a module. This will report any errors encountered before * ExceptionsManager is configured. */let _globalHandler: ErrorHandler = function onError( e: mixed, isFatal: boolean, ) { throw e;}; /** * The particular require runtime that we are using looks for a global * `ErrorUtils` object and if it exists, then it requires modules with the * error handler specified via ErrorUtils.setGlobalHandler by calling the * require function with applyWithGuard. Since the require module is loaded * before any of the modules, this ErrorUtils must be defined (and the handler * set) globally before requiring anything. */const ErrorUtils = { setGlobalHandler(fun: ErrorHandler): void { _globalHandler = fun; }, getGlobalHandler(): ErrorHandler { return _globalHandler; }, reportError(error: mixed): void { _globalHandler &amp;&amp; _globalHandler(error, false); }, reportFatalError(error: mixed): void { // NOTE: This has an untyped call site in Metro. _globalHandler &amp;&amp; _globalHandler(error, true); }, applyWithGuard&lt;TArgs: $ReadOnlyArray, TOut&gt;( fun: Fn&lt;TArgs, TOut&gt;, context?: ?mixed, args?: ?TArgs, // Unused, but some code synced from www sets it to null. unused_onError?: null, // Some callers pass a name here, which we ignore. unused_name?: ?string, ): ?TOut { try { _inGuard++; // $FlowFixMe: TODO T48204745 (1) apply(context, null) is fine. (2) array -&gt; rest array should work return fun.apply(context, args); } catch (e) { ErrorUtils.reportError(e); } finally { _inGuard–; } return null; }, applyWithGuardIfNeeded&lt;TArgs: $ReadOnlyArray, TOut&gt;( fun: Fn&lt;TArgs, TOut&gt;, context?: ?mixed, args?: ?TArgs, ): ?TOut { if (ErrorUtils.inGuard()) { // $FlowFixMe: TODO T48204745 (1) apply(context, null) is fine. (2) array -&gt; rest array should work return fun.apply(context, args); } else { ErrorUtils.applyWithGuard(fun, context, args); } return null; }, inGuard(): boolean { return !!_inGuard; }, guard&lt;TArgs: $ReadOnlyArray, TOut&gt;( fun: Fn&lt;TArgs, TOut&gt;, name?: ?string, context?: ?mixed, ): ?(…TArgs) =&gt; ?TOut { // TODO: (moti) T48204753 Make sure this warning is never hit and remove it - types // should be sufficient. if (typeof fun !== ‘function’) { console.warn(‘A function must be passed to ErrorUtils.guard, got ‘, fun); return null; } const guardName = name ?? fun.name ?? ‘‘; function guarded(…args: TArgs): ?TOut { return ErrorUtils.applyWithGuard( fun, context ?? this, args, null, guardName, ); } return guarded; },}; global.ErrorUtils = ErrorUtils; export type ErrorUtilsT = typeof ErrorUtils; 所以 RN 的异常可以使用 global.ErrorUtils 来设置错误处理。举个例子 global.ErrorUtils.setGlobalHandler(e =&gt; { // e.name e.message e.stack}, true); 2.7.3.2 组件问题其实对于 RN 的 crash 处理还有个需要注意的就是 React Error Boundaries。详细资料 过去，组件内的 JavaScript 错误会导致 React 的内部状态被破坏，并且在下一次渲染时 产生 可能无法追踪的 错误。这些错误基本上是由较早的其他代码（非 React 组件代码）错误引起的，但 React 并没有提供一种在组件中优雅处理这些错误的方式，也无法从错误中恢复。 部分 UI 的 JavaScript 错误不应该导致整个应用崩溃，为了解决这个问题，React 16 引入了一个新的概念 —— 错误边界。 错误边界是一种 React 组件，这种组件可以捕获并打印发生在其子组件树任何位置的 JavaScript 错误，并且，它会渲染出备用 UI，而不是渲染那些崩溃了的子组件树。错误边界在渲染期间、生命周期方法和整个组件树的构造函数中捕获错误。 它能捕获子组件生命周期函数中的异常，包括构造函数（constructor）和 render 函数 而不能捕获以下异常： Event handlers（事件处理函数） Asynchronous code（异步代码，如setTimeout、promise等） Server side rendering（服务端渲染） Errors thrown in the error boundary itself (rather than its children)（异常边界组件本身抛出的异常） 所以可以通过异常边界组件捕获组件生命周期内的所有异常然后渲染兜底组件 ，防止 App crash，提高用户体验。也可引导用户反馈问题，方便问题的排查和修复 至此 RN 的 crash 分为2种，分别是 js 逻辑错误、组件 js 错误，都已经被监控处理了。接下来就看看如何从工程化层面解决这些问题 2.7.4 RN Crash 还原SourceMap 文件对于前端日志的解析至关重要，SourceMap 文件中各个参数和如何计算的步骤都在里面有写，可以查看这篇文章。 有了 SourceMap 文件，借助于 mozilla 的 source-map 项目，可以很好的还原 RN 的 crash 日志。 我写了个 NodeJS 脚本，代码如下 var fs = require(‘fs’);var sourceMap = require(‘source-map’);var arguments = process.argv.splice(2); function parseJSError(aLine, aColumn) { fs.readFile(‘./index.ios.map’, ‘utf8’, function (err, data) { const whatever = sourceMap.SourceMapConsumer.with(data, null, consumer =&gt; { // 读取 crash 日志的行号、列号 let parseData = consumer.originalPositionFor({ line: parseInt(aLine), column: parseInt(aColumn) }); // 输出到控制台 console.log(parseData); // 输出到文件中 fs.writeFileSync(‘./parsed.txt’, JSON.stringify(parseData) + ‘\\n’, ‘utf8’, function(err) { if(err) { console.log(err); } }); }); });} var line = arguments[0];var column = arguments[1];parseJSError(line, column); 接下来做个实验，还是上述的 todos 项目。 在 Text 的点击事件上模拟 crash &lt;Text style={styles.sectionTitle} onPress={()=&gt;{1+qw;}}&gt;Debug&lt;/Text&gt; 将 RN 项目打 bundle 包、产出 sourceMap 文件。执行命令, react-native bundle –entry-file index.js –platform android –dev false –bundle-output release_ios/main.jsbundle –assets-dest release_iOS –sourcemap-output release_ios/index.android.map; 因为高频使用，所以给 iterm2 增加 alias 别名设置，修改 .zshrc 文件 alias RNRelease=’react-native bundle –entry-file index.js –platform ios –dev false –bundle-output release_ios/main.jsbundle –assets-dest release_iOS –sourcemap-output release_ios/index.ios.map;’ # RN 打 Release 包 将 js bundle 和图片资源拷贝到 Xcode 工程中 点击模拟 crash，将日志下面的行号和列号拷贝，在 Node 项目下，执行下面命令 node index.js 397 1822 拿脚本解析好的行号、列号、文件信息去和源代码文件比较，结果很正确。 2.7.5 SourceMap 解析系统设计目的：通过平台可以将 RN 项目线上 crash 可以还原到具体的文件、代码行数、代码列数。可以看到具体的代码，可以看到 RN stack trace、提供源文件下载功能。 打包系统下管理的服务器： 生产环境下打包才生成 source map 文件 存储打包前的所有文件（install） 开发产品侧 RN 分析界面。点击收集到的 RN crash，在详情页可以看到具体的文件、代码行数、代码列数。可以看到具体的代码，可以看到 RN stack trace、Native stack trace。（具体技术实现上面讲过了） 由于 souece map 文件较大，RN 解析过长虽然不久，但是是对计算资源的消耗，所以需要设计高效读取方式 SourceMap 在 iOS、Android 模式下不一样，所以 SoureceMap 存储需要区分 os。 3. KSCrash 的使用包装然后再封装自己的 Crash 处理逻辑。比如要做的事情就是： 继承自 KSCrashInstallation 这个抽象类，设置初始化工作（抽象类比如 NSURLProtocol 必须继承后使用），实现抽象类中的 sink 方法。 /** * Crash system installation which handles backend-specific details. * * Only one installation can be installed at a time. * * This is an abstract class. */ @interface KSCrashInstallation : NSObject #import “APMCrashInstallation.h”#import &lt;KSCrash/KSCrashInstallation+Private.h&gt;#import “APMCrashReporterSink.h” @implementation APMCrashInstallation (instancetype)sharedInstance { static APMCrashInstallation *sharedInstance = nil; static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ sharedInstance = \\[\\[APMCrashInstallation alloc\\] init\\]; }); return sharedInstance;} (id)init { return [super initWithRequiredProperties: nil];} (id)sink { APMCrashReporterSink *sink = [[APMCrashReporterSink alloc] init]; return [sink defaultCrashReportFilterSetAppleFmt];} @end sink 方法内部的 APMCrashReporterSink 类，遵循了 KSCrashReportFilter 协议，声明了公有方法 defaultCrashReportFilterSetAppleFmt // .h #import &lt;Foundation/Foundation.h&gt; #import &lt;KSCrash/KSCrashReportFilter.h&gt; @interface APMCrashReporterSink : NSObject&lt;KSCrashReportFilter&gt; (id ) defaultCrashReportFilterSetAppleFmt; @end // .m#pragma mark - public Method (id ) defaultCrashReportFilterSetAppleFmt{ return [KSCrashReportFilterPipeline filterWithFilters: \\[APMCrashReportFilterAppleFmt filterWithReportStyle:KSAppleReportStyleSymbolicatedSideBySide\\], self, nil\\]; } 其中 defaultCrashReportFilterSetAppleFmt 方法内部返回了一个 KSCrashReportFilterPipeline 类方法 filterWithFilters 的结果。 APMCrashReportFilterAppleFmt 是一个继承自 KSCrashReportFilterAppleFmt 的类，遵循了 KSCrashReportFilter 协议。协议方法允许开发者处理 Crash 的数据格式。 /** Filter the specified reports. * * @param reports The reports to process. * @param onCompletion Block to call when processing is complete. */ (void) filterReports:(NSArray*) reports onCompletion:(KSCrashReportFilterCompletion) onCompletion; #import &lt;KSCrash/KSCrashReportFilterAppleFmt.h&gt; @interface APMCrashReportFilterAppleFmt : KSCrashReportFilterAppleFmt&lt;KSCrashReportFilter&gt; @end // .m (void) filterReports:(NSArray*)reports onCompletion:(KSCrashReportFilterCompletion)onCompletion{ NSMutableArray* filteredReports = [NSMutableArray arrayWithCapacity:[reports count]]; for(NSDictionary *report in reports){if(\\[self majorVersion:report\\] == kExpectedMajorVersion)&#123; id monitorInfo = \\[self generateMonitorInfoFromCrashReport:report\\]; if(monitorInfo != nil)&#123; \\[filteredReports addObject:monitorInfo\\]; &#125; &#125; } kscrash_callCompletion(onCompletion, filteredReports, YES, nil);} /** @brief 获取Crash JSON中的crash时间、mach name、signal name和apple report */ (NSDictionary *)generateMonitorInfoFromCrashReport:(NSDictionary *)crashReport{ NSDictionary *infoReport = [crashReport objectForKey:@”report”]; // … id appleReport = [self toAppleFormat:crashReport]; NSMutableDictionary *info = [NSMutableDictionary dictionary]; [info setValue:crashTime forKey:@”crashTime”]; [info setValue:appleReport forKey:@”appleReport”]; [info setValue:userException forKey:@”userException”]; [info setValue:userInfo forKey:@”custom”]; return [info copy];} /** * A pipeline of filters. Reports get passed through each subfilter in order. * * Input: Depends on what’s in the pipeline. * Output: Depends on what’s in the pipeline. */@interface KSCrashReportFilterPipeline : NSObject &lt;KSCrashReportFilter&gt; APM 能力中为 Crash 模块设置一个启动器。启动器内部设置 KSCrash 的初始化工作，以及触发 Crash 时候监控所需数据的组装。比如：SESSION_ID、App 启动时间、App 名称、崩溃时间、App 版本号、当前页面信息等基础信息。 /** C Function to call during a crash report to give the callee an opportunity to * add to the report. NULL = ignore. * * WARNING: Only call async-safe functions from this function! DO NOT call * Objective-C methods!!! */ @property(atomic,readwrite,assign) KSReportWriteCallback onCrash; + (instancetype)sharedInstance{ static APMCrashMonitor *_sharedManager = nil; static dispatch_once_t onceToken; dispatch_once(&amp;onceToken, ^{ _sharedManager = [[APMCrashMonitor alloc] init]; }); return _sharedManager;} #pragma mark - public Method (void)startMonitor{ APMMLog(@”crash monitor started”); #ifdef DEBUG BOOL _trackingCrashOnDebug = [APMMonitorConfig sharedInstance].trackingCrashOnDebug; if (_trackingCrashOnDebug) { [self installKSCrash]; }#else [self installKSCrash];#endif} #pragma mark - private method static void onCrash(const KSCrashReportWriter* writer){ NSString *sessionId = [NSString stringWithFormat:@”\\“%@\\“”, ***]]; writer-&gt;addJSONElement(writer, “SESSION_ID”, [sessionId UTF8String], true); NSString \\*appLaunchTime = \\*\\*\\*; writer-&gt;addJSONElement(writer, &quot;USER\\_APP\\_START\\_DATE&quot;, \\[\\[NSString stringWithFormat:@&quot;\\\\&quot;%@\\\\&quot;&quot;, appLaunchTime\\] UTF8String\\], true); // ... } (void)installKSCrash{ [[APMCrashInstallation sharedInstance] install]; [[APMCrashInstallation sharedInstance] sendAllReportsWithCompletion:nil]; [APMCrashInstallation sharedInstance].onCrash = onCrash; dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(5.f * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{ \\_isCanAddCrashCount = NO; });} 在 installKSCrash 方法中调用了 [[APMCrashInstallation sharedInstance] sendAllReportsWithCompletion: nil]，内部实现如下 - (void) sendAllReportsWithCompletion:(KSCrashReportFilterCompletion) onCompletion{ NSError* error = [self validateProperties]; if(error != nil) { if(onCompletion != nil) { onCompletion(nil, NO, error); } return; } id&lt;KSCrashReportFilter&gt; sink = \\[self sink\\]; if(sink == nil) &#123; onCompletion(nil, NO, \\[NSError errorWithDomain:\\[\\[self class\\] description\\] code:0 description:@&quot;Sink was nil (subclasses must implement method \\\\&quot;sink\\\\&quot;)&quot;\\]); return; &#125; sink = \\[KSCrashReportFilterPipeline filterWithFilters:self.prependedFilters, sink, nil\\]; KSCrash\\* handler = \\[KSCrash sharedInstance\\]; handler.sink = sink; \\[handler sendAllReportsWithCompletion:onCompletion\\]; } 方法内部将 KSCrashInstallation 的 sink 赋值给 KSCrash 对象。 内部还是调用了 KSCrash 的 sendAllReportsWithCompletion 方法，实现如下 - (void) sendAllReportsWithCompletion:(KSCrashReportFilterCompletion) onCompletion{ NSArray* reports = [self allReports]; KSLOG\\_INFO(@&quot;Sending %d crash reports&quot;, \\[reports count\\]); \\[self sendReports:reports onCompletion:^(NSArray\\* filteredReports, BOOL completed, NSError\\* error) &#123; KSLOG\\_DEBUG(@&quot;Process finished with completion: %d&quot;, completed); if(error != nil) &#123; KSLOG\\_ERROR(@&quot;Failed to send reports: %@&quot;, error); &#125; if((self.deleteBehaviorAfterSendAll == KSCDeleteOnSucess &amp;&amp; completed) || self.deleteBehaviorAfterSendAll == KSCDeleteAlways) &#123; kscrash\\_deleteAllReports(); &#125; kscrash\\_callCompletion(onCompletion, filteredReports, completed, error); &#125;\\]; } 该方法内部调用了对象方法 sendReports: onCompletion:，如下所示 - (void) sendReports:(NSArray*) reports onCompletion:(KSCrashReportFilterCompletion) onCompletion{ if([reports count] == 0) { kscrash_callCompletion(onCompletion, reports, YES, nil); return; } if(self.sink == nil) &#123; kscrash\\_callCompletion(onCompletion, reports, NO, \\[NSError errorWithDomain:\\[\\[self class\\] description\\] code:0 description:@&quot;No sink set. Crash reports not sent.&quot;\\]); return; &#125; \\[self.sink filterReports:reports onCompletion:^(NSArray\\* filteredReports, BOOL completed, NSError\\* error) &#123; kscrash\\_callCompletion(onCompletion, filteredReports, completed, error); &#125;\\]; } 方法内部的 [self.sink filterReports: onCompletion: ] 实现其实就是 APMCrashInstallation 中设置的 sink getter 方法，内部返回了 APMCrashReporterSink 对象的 defaultCrashReportFilterSetAppleFmt 方法的返回值。内部实现如下 - (id ) defaultCrashReportFilterSetAppleFmt{ return [KSCrashReportFilterPipeline filterWithFilters: [APMCrashReportFilterAppleFmt filterWithReportStyle:KSAppleReportStyleSymbolicatedSideBySide], self, nil];} 可以看到这个函数内部设置了多个 filters，其中一个就是 self，也就是 APMCrashReporterSink 对象，所以上面的 [self.sink filterReports: onCompletion:] ，也就是调用 APMCrashReporterSink 内的数据处理方法。完了之后通过 kscrash_callCompletion(onCompletion, reports, YES, nil); 告诉 KSCrash 本地保存的 Crash 日志已经处理完毕，可以删除了。 - (void)filterReports:(NSArray *)reports onCompletion:(KSCrashReportFilterCompletion)onCompletion{ for (NSDictionary *report in reports) { // 处理 Crash 数据，将数据交给统一的数据上报组件处理… } kscrash_callCompletion(onCompletion, reports, YES, nil);} 至此，概括下 KSCrash 做的事情，提供各种 crash 的监控能力，在 crash 后将进程信息、基本信息、异常信息、线程信息等用 c 高效转换为 json 写入文件，App 下次启动后读取本地的 crash 文件夹中的 crash 日志，让开发者可以自定义 key、value 然后去上报日志到 APM 系统，然后删除本地 crash 文件夹中的日志。 4. 符号化应用 crash 之后，系统会生成一份崩溃日志，存储在设置中，应用的运行状态、调用堆栈、所处线程等信息会记录在日志中。但是这些日志是地址，并不可读，所以需要进行符号化还原。 4.1 .DSYM 文件.DSYM （debugging symbol）文件是保存十六进制函数地址映射信息的中转文件，调试信息（symbols）都包含在该文件中。Xcode 工程每次编译运行都会生成新的 .DSYM 文���。默认情况下 debug 模式时不生成 .DSYM ，可以在 Build Settings -&gt; Build Options -&gt; Debug Information Format 后将值 DWARF 修改为 DWARF with DSYM File，这样再次编译运行就可以生成 .DSYM 文件。 所以每次 App 打包的时候都需要保存每个版本的 .DSYM 文件。 .DSYM 文件中包含 DWARF 信息，打开文件的包内容 Test.app.DSYM/Contents/Resources/DWARF/Test 保存的就是 DWARF 文件。 .DSYM 文件是从 Mach-O 文件中抽取调试信息而得到的文件目录，发布的时候为了安全，会把调试信息存储在单独的文件，.DSYM 其实是一个文件目录，结构如下： 4.2 DWARF 文件 DWARF is a debugging file format used by many compilers and debuggers to support source level debugging. It addresses the requirements of a number of procedural languages, such as C, C++, and Fortran, and is designed to be extensible to other languages. DWARF is architecture independent and applicable to any processor or operating system. It is widely used on Unix, Linux and other operating systems, as well as in stand-alone environments. DWARF 是一种调试文件格式，它被许多编译器和调试器所广泛使用以支持源代码级别的调试。它满足许多过程语言（C、C++、Fortran）的需求，它被设计为支持拓展到其他语言。DWARF 是架构独立的，适用于其他任何的处理器和操作系统。被广泛使用在 Unix、Linux 和其他的操作系统上，以及独立环境上。 DWARF 全称是 Debugging With Arbitrary Record Formats，是一种使用属性化记录格式的调试文件。 DWARF 是可执行程序与源代码关系的一个紧凑表示。 大多数现代编程语言都是块结构：每个实体（一个类、一个函数）被包含在另一个实体中。一个 c 程序，每个文件可能包含多个数据定义、多个变量、多个函数，所以 DWARF 遵循这个模型，也是块结构。DWARF 里基本的描述项是调试信息项 DIE（Debugging Information Entry）。一个 DIE 有一个标签，表示这个 DIE 描述了什么以及一个填入了细节并进一步描述该项的属性列表（类比 html、xml 结构）。一个 DIE（除了最顶层的）被一个父 DIE 包含，可能存在兄弟 DIE 或者子 DIE，属性可能包含各种值：常量（比如一个函数名），变量（比如一个函数的起始地址），或对另一个DIE的引用（比如一个函数的返回值类型）。 DWARF 文件中的数据如下： 数据列 信息说明 .debug_loc 在 DW_AT_location 属性中使用的位置列表 .debug_macinfo 宏信息 .debug_pubnames 全局对象和函数的查找表 .debug_pubtypes 全局类型的查找表 .debug_ranges 在 DW_AT_ranges 属性中使用的地址范围 .debug_str 在 .debug_info 中使用的字符串表 .debug_types 类型描述 常用的标记与属性如下： 数据列 信息说明 DW_TAG_class_type 表示类名称和类型信息 DW_TAG_structure_type 表示结构名称和类型信息 DW_TAG_union_type 表示联合名称和类型信息 DW_TAG_enumeration_type 表示枚举名称和类型信息 DW_TAG_typedef 表示 typedef 的名称和类型信息 DW_TAG_array_type 表示数组名称和类型信息 DW_TAG_subrange_type 表示数组的大小信息 DW_TAG_inheritance 表示继承的类名称和类型信息 DW_TAG_member 表示类的成员 DW_TAG_subprogram 表示函数的名称信息 DW_TAG_formal_parameter 表示函数的参数信息 DW_TAG_name 表示名称字符串 DW_TAG_type 表示类型信息 DW_TAG_artifical 在创建时由编译程序设置 DW_TAG_sibling 表示兄弟位置信息 DW_TAG_data_memver_location 表示位置信息 DW_TAG_virtuality 在虚拟时设置 简单看一个 DWARF 的例子：将测试工程的 .DSYM 文件夹下的 DWARF 文件用下面命令解析 dwarfdump -F –debug-info Test.app.DSYM/Contents/Resources/DWARF/Test &gt; debug-info.txt 打开如下 Test.app.DSYM/Contents/Resources/DWARF/Test: file format Mach-O arm64 .debug_info contents:0x00000000: Compile Unit: length = 0x0000004f version = 0x0004 abbr_offset = 0x0000 addr_size = 0x08 (next unit at 0x00000053) 0x0000000b: DW_TAG_compile_unit DW_AT_producer [DW_FORM_strp] (“Apple clang version 11.0.3 (clang-1103.0.32.62)”) DW_AT_language [DW_FORM_data2] (DW_LANG_ObjC) DW_AT_name [DW_FORM_strp] (“_Builtin_stddef_max_align_t”) DW_AT_stmt_list [DW_FORM_sec_offset] (0x00000000) DW_AT_comp_dir [DW_FORM_strp] (“/Users/lbp/Desktop/Test”) DW_AT_APPLE_major_runtime_vers [DW_FORM_data1] (0x02) DW_AT_GNU_dwo_id [DW_FORM_data8] (0x392b5344d415340c) 0x00000027: DW_TAG_module DW_AT_name [DW_FORM_strp] (“_Builtin_stddef_max_align_t”) DW_AT_LLVM_config_macros [DW_FORM_strp] (“\\“-DDEBUG=1\\“ \\“-DOBJC_OLD_DISPATCH_PROTOTYPES=1\\“”) DW_AT_LLVM_include_path [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/11.0.3/include”) DW_AT_LLVM_isysroot [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk”) 0x00000038: DW_TAG_typedef DW_AT_type [DW_FORM_ref4] (0x0000004b “long double”) DW_AT_name [DW_FORM_strp] (“max_align_t”) DW_AT_decl_file [DW_FORM_data1] (“/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/11.0.3/include/__stddef_max_align_t.h”) DW_AT_decl_line [DW_FORM_data1] (16) 0x00000043: DW_TAG_imported_declaration DW_AT_decl_file [DW_FORM_data1] (“/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/11.0.3/include/__stddef_max_align_t.h”) DW_AT_decl_line [DW_FORM_data1] (27) DW_AT_import [DW_FORM_ref_addr] (0x0000000000000027) 0x0000004a: NULL 0x0000004b: DW_TAG_base_type DW_AT_name [DW_FORM_strp] (“long double”) DW_AT_encoding [DW_FORM_data1] (DW_ATE_float) DW_AT_byte_size [DW_FORM_data1] (0x08) 0x00000052: NULL0x00000053: Compile Unit: length = 0x000183dc version = 0x0004 abbr_offset = 0x0000 addr_size = 0x08 (next unit at 0x00018433) 0x0000005e: DW_TAG_compile_unit DW_AT_producer [DW_FORM_strp] (“Apple clang version 11.0.3 (clang-1103.0.32.62)”) DW_AT_language [DW_FORM_data2] (DW_LANG_ObjC) DW_AT_name [DW_FORM_strp] (“Darwin”) DW_AT_stmt_list [DW_FORM_sec_offset] (0x000000a7) DW_AT_comp_dir [DW_FORM_strp] (“/Users/lbp/Desktop/Test”) DW_AT_APPLE_major_runtime_vers [DW_FORM_data1] (0x02) DW_AT_GNU_dwo_id [DW_FORM_data8] (0xa4a1d339379e18a5) 0x0000007a: DW_TAG_module DW_AT_name [DW_FORM_strp] (“Darwin”) DW_AT_LLVM_config_macros [DW_FORM_strp] (“\\“-DDEBUG=1\\“ \\“-DOBJC_OLD_DISPATCH_PROTOTYPES=1\\“”) DW_AT_LLVM_include_path [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include”) DW_AT_LLVM_isysroot [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk”) 0x0000008b: DW_TAG_module DW_AT_name [DW_FORM_strp] (“C”) DW_AT_LLVM_config_macros [DW_FORM_strp] (“\\“-DDEBUG=1\\“ \\“-DOBJC_OLD_DISPATCH_PROTOTYPES=1\\“”) DW_AT_LLVM_include_path [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include”) DW_AT_LLVM_isysroot [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk”) 0x0000009c: DW_TAG_module DW_AT_name [DW_FORM_strp] (“fenv”) DW_AT_LLVM_config_macros [DW_FORM_strp] (“\\“-DDEBUG=1\\“ \\“-DOBJC_OLD_DISPATCH_PROTOTYPES=1\\“”) DW_AT_LLVM_include_path [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include”) DW_AT_LLVM_isysroot [DW_FORM_strp] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk”) 0x000000ad: DW_TAG_enumeration_type DW_AT_type [DW_FORM_ref4] (0x00017276 “unsigned int”) DW_AT_byte_size [DW_FORM_data1] (0x04) DW_AT_decl_file [DW_FORM_data1] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/fenv.h”) DW_AT_decl_line [DW_FORM_data1] (154) 0x000000b5: DW_TAG_enumerator DW_AT_name [DW_FORM_strp] (“__fpcr_trap_invalid”) DW_AT_const_value [DW_FORM_udata] (256) 0x000000bc: DW_TAG_enumerator DW_AT_name [DW_FORM_strp] (“__fpcr_trap_divbyzero”) DW_AT_const_value [DW_FORM_udata] (512) 0x000000c3: DW_TAG_enumerator DW_AT_name [DW_FORM_strp] (“__fpcr_trap_overflow”) DW_AT_const_value [DW_FORM_udata] (1024) 0x000000ca: DW_TAG_enumerator DW_AT_name [DW_FORM_strp] (“__fpcr_trap_underflow”)// ……0x000466ee: DW_TAG_subprogram DW_AT_name [DW_FORM_strp] (“CFBridgingRetain”) DW_AT_decl_file [DW_FORM_data1] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObject.h”) DW_AT_decl_line [DW_FORM_data1] (105) DW_AT_prototyped [DW_FORM_flag_present] (true) DW_AT_type [DW_FORM_ref_addr] (0x0000000000019155 “CFTypeRef”) DW_AT_inline [DW_FORM_data1] (DW_INL_inlined) 0x000466fa: DW_TAG_formal_parameter DW_AT_name [DW_FORM_strp] (“X”) DW_AT_decl_file [DW_FORM_data1] (“/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/System/Library/Frameworks/Foundation.framework/Headers/NSObject.h”) DW_AT_decl_line [DW_FORM_data1] (105) DW_AT_type [DW_FORM_ref4] (0x00046706 “id”) 0x00046705: NULL 0x00046706: DW_TAG_typedef DW_AT_type [DW_FORM_ref4] (0x00046711 “objc_object*“) DW_AT_name [DW_FORM_strp] (“id”) DW_AT_decl_file [DW_FORM_data1] (“/Users/lbp/Desktop/Test/Test/NetworkAPM/NSURLResponse+apm_FetchStatusLineFromCFNetwork.m”) DW_AT_decl_line [DW_FORM_data1] (44) 0x00046711: DW_TAG_pointer_type DW_AT_type [DW_FORM_ref4] (0x00046716 “objc_object”) 0x00046716: DW_TAG_structure_type DW_AT_name [DW_FORM_strp] (“objc_object”) DW_AT_byte_size [DW_FORM_data1] (0x00) 0x0004671c: DW_TAG_member DW_AT_name [DW_FORM_strp] (“isa”) DW_AT_type [DW_FORM_ref4] (0x00046727 “objc_class*“) DW_AT_data_member_location [DW_FORM_data1] (0x00)// …… 这里就不粘贴全部内容了（太长了）。可以看到 DIE 包含了函数开始地址、结束地址、函数名、文件名、所在行数，对于给定的地址，找到函数开始地址、结束地址之间包含该地址的 DIE，则可以还原函数名和文件名信息。 debug_line 可以还原文件行数等信息 dwarfdump -F –debug-line Test.app.DSYM/Contents/Resources/DWARF/Test &gt; debug-inline.txt 贴部分信息 Test.app.DSYM/Contents/Resources/DWARF/Test: file format Mach-O arm64 .debug_line contents:debug_line[0x00000000]Line table prologue: total_length: 0x000000a3 version: 4 prologue_length: 0x0000009a min_inst_length: 1max_ops_per_inst: 1 default_is_stmt: 1 line_base: -5 line_range: 14 opcode_base: 13standard_opcode_lengths[DW_LNS_copy] = 0standard_opcode_lengths[DW_LNS_advance_pc] = 1standard_opcode_lengths[DW_LNS_advance_line] = 1standard_opcode_lengths[DW_LNS_set_file] = 1standard_opcode_lengths[DW_LNS_set_column] = 1standard_opcode_lengths[DW_LNS_negate_stmt] = 0standard_opcode_lengths[DW_LNS_set_basic_block] = 0standard_opcode_lengths[DW_LNS_const_add_pc] = 0standard_opcode_lengths[DW_LNS_fixed_advance_pc] = 1standard_opcode_lengths[DW_LNS_set_prologue_end] = 0standard_opcode_lengths[DW_LNS_set_epilogue_begin] = 0standard_opcode_lengths[DW_LNS_set_isa] = 1include_directories[ 1] = “/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/11.0.3/include”file_names[ 1]: name: “__stddef_max_align_t.h” dir_index: 1 mod_time: 0x00000000 length: 0x00000000 Address Line Column File ISA Discriminator Flags 0x0000000000000000 1 0 1 0 0 is_stmt end_sequencedebug_line[0x000000a7]Line table prologue: total_length: 0x0000230a version: 4 prologue_length: 0x00002301 min_inst_length: 1max_ops_per_inst: 1 default_is_stmt: 1 line_base: -5 line_range: 14 opcode_base: 13standard_opcode_lengths[DW_LNS_copy] = 0standard_opcode_lengths[DW_LNS_advance_pc] = 1standard_opcode_lengths[DW_LNS_advance_line] = 1standard_opcode_lengths[DW_LNS_set_file] = 1standard_opcode_lengths[DW_LNS_set_column] = 1standard_opcode_lengths[DW_LNS_negate_stmt] = 0standard_opcode_lengths[DW_LNS_set_basic_block] = 0standard_opcode_lengths[DW_LNS_const_add_pc] = 0standard_opcode_lengths[DW_LNS_fixed_advance_pc] = 1standard_opcode_lengths[DW_LNS_set_prologue_end] = 0standard_opcode_lengths[DW_LNS_set_epilogue_begin] = 0standard_opcode_lengths[DW_LNS_set_isa] = 1include_directories[ 1] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include”include_directories[ 2] = “/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/11.0.3/include”include_directories[ 3] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/sys”include_directories[ 4] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/mach”include_directories[ 5] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/libkern”include_directories[ 6] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/architecture”include_directories[ 7] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/sys/_types”include_directories[ 8] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/_types”include_directories[ 9] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/arm”include_directories[ 10] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/sys/_pthread”include_directories[ 11] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/mach/arm”include_directories[ 12] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/libkern/arm”include_directories[ 13] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/uuid”include_directories[ 14] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/netinet”include_directories[ 15] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/netinet6”include_directories[ 16] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/net”include_directories[ 17] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/pthread”include_directories[ 18] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/mach_debug”include_directories[ 19] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/os”include_directories[ 20] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/malloc”include_directories[ 21] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/bsm”include_directories[ 22] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/machine”include_directories[ 23] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/mach/machine”include_directories[ 24] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/secure”include_directories[ 25] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/xlocale”include_directories[ 26] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/arpa”file_names[ 1]: name: “fenv.h” dir_index: 1 mod_time: 0x00000000 length: 0x00000000file_names[ 2]: name: “stdatomic.h” dir_index: 2 mod_time: 0x00000000 length: 0x00000000file_names[ 3]: name: “wait.h” dir_index: 3 mod_time: 0x00000000 length: 0x00000000// ……Address Line Column File ISA Discriminator Flags 0x000000010000b588 14 0 2 0 0 is_stmt0x000000010000b5b4 16 5 2 0 0 is_stmt prologue_end0x000000010000b5d0 17 11 2 0 0 is_stmt0x000000010000b5d4 0 0 2 0 00x000000010000b5d8 17 5 2 0 00x000000010000b5dc 17 11 2 0 00x000000010000b5e8 18 1 2 0 0 is_stmt0x000000010000b608 20 0 2 0 0 is_stmt0x000000010000b61c 22 5 2 0 0 is_stmt prologue_end0x000000010000b628 23 5 2 0 0 is_stmt0x000000010000b644 24 1 2 0 0 is_stmt0x000000010000b650 15 0 1 0 0 is_stmt0x000000010000b65c 15 41 1 0 0 is_stmt prologue_end0x000000010000b66c 11 0 2 0 0 is_stmt0x000000010000b680 11 17 2 0 0 is_stmt prologue_end0x000000010000b6a4 11 17 2 0 0 is_stmt end_sequencedebug_line[0x0000def9]Line table prologue: total_length: 0x0000015a version: 4 prologue_length: 0x000000eb min_inst_length: 1max_ops_per_inst: 1 default_is_stmt: 1 line_base: -5 line_range: 14 opcode_base: 13standard_opcode_lengths[DW_LNS_copy] = 0standard_opcode_lengths[DW_LNS_advance_pc] = 1standard_opcode_lengths[DW_LNS_advance_line] = 1standard_opcode_lengths[DW_LNS_set_file] = 1standard_opcode_lengths[DW_LNS_set_column] = 1standard_opcode_lengths[DW_LNS_negate_stmt] = 0standard_opcode_lengths[DW_LNS_set_basic_block] = 0standard_opcode_lengths[DW_LNS_const_add_pc] = 0standard_opcode_lengths[DW_LNS_fixed_advance_pc] = 1standard_opcode_lengths[DW_LNS_set_prologue_end] = 0standard_opcode_lengths[DW_LNS_set_epilogue_begin] = 0standard_opcode_lengths[DW_LNS_set_isa] = 1include_directories[ 1] = “Test”include_directories[ 2] = “Test/NetworkAPM”include_directories[ 3] = “/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS13.5.sdk/usr/include/objc”file_names[ 1]: name: “AppDelegate.h” dir_index: 1 mod_time: 0x00000000 length: 0x00000000file_names[ 2]: name: “JMWebResourceURLProtocol.h” dir_index: 2 mod_time: 0x00000000 length: 0x00000000file_names[ 3]: name: “AppDelegate.m” dir_index: 1 mod_time: 0x00000000 length: 0x00000000file_names[ 4]: name: “objc.h” dir_index: 3 mod_time: 0x00000000 length: 0x00000000// …… 可以看到 debug_line 里包含了每个代码地址对应的行数。上面贴了 AppDelegate 的部分。 4.3 symbols 在链接中，我们将函数和变量统称为符合（Symbol），函数名或变量名就是符号名（Symbol Name），我们可以将符号看成是链接中的粘合剂，整个链接过程正是基于符号才能正确完成的。 上述文字来自《程序员的自我修养》。所以符号就是函数、变量、类的统称。 按照类型划分，符号可以分为三类： 全局符号：目标文件外可见的符号，可以被其他目标文件所引用，或者需要其他目标文件定义 局部符号：只在目标文件内可见的符号，指只在目标文件内可见的函数和变量 调试符号：包括行号信息的调试符号信息，行号信息记录了函数和变量对应的文件和文件行号。 符号表（Symbol Table）：是内存地址与函数名、文件名、行号的映射表。每个定义的符号都有一个对应的值得，叫做符号值（Symbol Value），对于变量和函数来说，符号值就是地址，符号表组成如下 &lt;起始地址&gt; &lt;结束地址&gt; &lt;函数&gt; [&lt;文件名：行号&gt;] 4.4 如何获取地址？image 加载的时候会进行相对基地址进行重定位，并且每次加载的基地址都不一样，函数栈 frame 的地址是重定位后的绝对地址，我们要的是重定位前的相对地址。 Binary Images 拿测试工程的 crash 日志举例子，打开贴部分 Binary Images 内容 // …Binary Images:0x102fe0000 - 0x102ff3fff Test arm64 &lt;37eaa57df2523d95969e47a9a1d69ce5&gt; /var/containers/Bundle/Application/643F0DFE-A710-4136-A278-A89D780B7208/Test.app/Test0x1030e0000 - 0x1030ebfff libobjc-trampolines.dylib arm64 &lt;181f3aa866d93165ac54344385ac6e1d&gt; /usr/lib/libobjc-trampolines.dylib0x103204000 - 0x103267fff dyld arm64 &lt;6f1c86b640a3352a8529bca213946dd5&gt; /usr/lib/dyld0x189a78000 - 0x189a8efff libsystem_trace.dylib arm64 /usr/lib/system/libsystem_trace.dylib// … 可以看到 Crash 日志的 Binary Images 包含每个 Image 的加载开始地址、结束地址、image 名称、arm 架构、uuid、image 路径。 crash 日志中的信息 Last Exception Backtrace:// …5 Test 0x102fe592c -[ViewController testMonitorCrash] + 22828 (ViewController.mm:58) Binary Images:0x102fe0000 - 0x102ff3fff Test arm64 &lt;37eaa57df2523d95969e47a9a1d69ce5&gt; /var/containers/Bundle/Application/643F0DFE-A710-4136-A278-A89D780B7208/Test.app/Test 所以 frame 5 的相对地址为 0x102fe592c - 0x102fe0000 。再使用 命令可以还原符号信息。 使用 atos 来解析，0x102fe0000 为 image 加载的开始地址，0x102fe592c 为 frame 需要还原的地址。 atos -o Test.app.DSYM/Contents/Resources/DWARF/Test-arch arm64 -l 0x102fe0000 0x102fe592c 4.5 UUID crash 文件的 UUID grep –after-context=2 “Binary Images:” *.crash Test 5-28-20, 7-47 PM.crash:Binary Images:Test 5-28-20, 7-47 PM.crash-0x102fe0000 - 0x102ff3fff Test arm64 &lt;37eaa57df2523d95969e47a9a1d69ce5&gt; /var/containers/Bundle/Application/643F0DFE-A710-4136-A278-A89D780B7208/Test.app/TestTest 5-28-20, 7-47 PM.crash-0x1030e0000 - 0x1030ebfff libobjc-trampolines.dylib arm64 &lt;181f3aa866d93165ac54344385ac6e1d&gt; /usr/lib/libobjc-trampolines.dylib–Test.crash:Binary Images:Test.crash-0x102fe0000 - 0x102ff3fff Test arm64 &lt;37eaa57df2523d95969e47a9a1d69ce5&gt; /var/containers/Bundle/Application/643F0DFE-A710-4136-A278-A89D780B7208/Test.app/TestTest.crash-0x1030e0000 - 0x1030ebfff libobjc-trampolines.dylib arm64 &lt;181f3aa866d93165ac54344385ac6e1d&gt; /usr/lib/libobjc-trampolines.dylib Test App 的 UUID 为 37eaa57df2523d95969e47a9a1d69ce5. .DSYM 文件的 UUID dwarfdump –uuid Test.app.DSYM 结果为 UUID: 37EAA57D-F252-3D95-969E-47A9A1D69CE5 (arm64) Test.app.DSYM/Contents/Resources/DWARF/Test app 的 UUID dwarfdump –uuid Test.app/Test 结果为 UUID: 37EAA57D-F252-3D95-969E-47A9A1D69CE5 (arm64) Test.app/Test 4.6 符号化（解析 Crash 日志）上述篇幅分析了如何捕获各种类型的 crash，App 在用户手中我们通过技术手段可以获取 crash 案发现场信息并结合一定的机制去上报，但是这种堆栈是十六进制的地址，无法定位问题，所以需要做符号化处理。 上面也说明了.DSYM 文件 的作用，通过符号地址结合 DSYM 文件来还原文件名、所在行、函数名，这个过程叫符号化。但是 .DSYM 文件必须和 crash log 文件的 bundle id、version 严格对应。 获取 Crash 日志可以通过 Xcode -&gt; Window -&gt; Devices and Simulators 选择对应设备，找到 Crash 日志文件，根据时间和 App 名称定位。 app 和 .DSYM 文件可以通过打包的产物得到，路径为 ~/Library/Developer/Xcode/Archives。 解析方法一般有2种： 使用 symbolicatecrash symbolicatecrash 是 Xcode 自带的 crash 日志分析工具，先确定所在路径，在终端执行下面的命令 find /Applications/Xcode.app -name symbolicatecrash -type f 会返回几个路径，找到 iPhoneSimulator.platform 所在那一行 /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/Library/PrivateFrameworks/DVTFoundation.framework/symbolicatecrash 将 symbolicatecrash 拷贝到指定文件夹下（保存了 app、DSYM、crash 文件的文件夹） 执行命令 ./symbolicatecrash Test.crash Test.DSYM &gt; Test.crash 第一次做这事儿应该会报错 Error: &quot;DEVELOPER_DIR&quot; is not defined at ./symbolicatecrash line 69.，解决方案：在终端执行下面命令 export DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer 使用 atos 区别于 symbolicatecrash，atos 较为灵活，只要 .crash 和 .DSYM 或者 .crash 和 .app 文件对应即可。 用法如下，-l 最后跟得是符号地址 xcrun atos -o Test.app.DSYM/Contents/Resources/DWARF/Test -arch armv7 -l 0x1023c592c 也可以解析 .app 文件（不存在 .DSYM 文件），其中xxx为段地址，xx为偏移地址 atos -arch architecture -o binary -l xxx xx 因为我们的 App 可能有很多，每个 App 在用户手中可能是不同的版本，所以在 APM 拦截之后需要符号化的时候需要将 crash 文件和 .DSYM 文件一一对应，才能正确符号化，对应的原则就是 UUID 一致。 4.7 系统库符号化解析我们每次真机连接 Xcode 运行程序，会提示等待，其实系统为了堆栈解析，都会把当前版本的系统符号库自动导入到 /Users/你自己的用户名/Library/Developer/Xcode/iOS DeviceSupport 目录下安装了一大堆系统库的符号化文件。你可以访问下面目录看看 /Users/你自己的用户名/Library/Developer/Xcode/iOS DeviceSupport/ 5. 服务端处理5.1 ELK 日志系统业界设计日志监控系统一般会采用基于 ELK 技术。ELK 是 Elasticsearch、Logstash、Kibana 三个开源框架缩写。Elasticsearch 是一个分布式、通过 Restful 方式进行交互的近实时搜索的平台框架。Logstash 是一个中央数据流引擎，用于从不同目标（文件/数据存储/MQ）收集不同格式的数据，经过过滤后支持输出到不同目的地（文件/MQ/Redis/ElasticsSearch/Kafka）。Kibana 可以将 Elasticserarch 的数据通过友好的页面展示出来，提供可视化分析功能。所以 ELK 可以搭建一个高效、企业级的日志分析系统。 早期单体应用时代，几乎应用的所有功能都在一台机器上运行，出了问题，运维人员打开终端输入命令直接查看系统日志，进而定位问题、解决问题。随着系统的功能越来越复杂，用户体量越来越大，单体应用几乎很难满足需求，所以技术架构迭代了，通过水平拓展来支持庞大的用户量，将单体应用进行拆分为多个应用，每个应用采用集群方式部署，负载均衡控制调度，假如某个子模块发生问题，去找这台服务器上终端找日志分析吗？显然台落后，所以日志管理平台便应运而生。通过 Logstash 去收集分析每台服务器的日志文件，然后按照定义的正则模版过滤后传输到 Kafka 或 Redis，然后由另一个 Logstash 从 Kafka 或 Redis 上读取日志存储到 ES 中创建索引，最后通过 Kibana 进行可视化分析。此外可以将收集到的数据进行数据分析，做更进一步的维护和决策。 上图展示了一个 ELK 的日志架构图。简单说明下： Logstash 和 ES 之前存在一个 Kafka 层，因为 Logstash 是架设在数据资源服务器上，将收集到的数据进行实时过滤，过滤需要消耗时间和内存，所以存在 Kafka，起到了数据缓冲存储作用，因为 Kafka 具备非常出色的读写性能。 再一步就是 Logstash 从 Kafka 里面进行读取数据，将数据过滤、处理，将结果传输到 ES 这个设计不但性能好、耦合低，还具备可拓展性。比如可以从 n 个不同的 Logstash 上读取传输到 n 个 Kafka 上，再由 n 个 Logstash 过滤处理。日志来源可以是 m 个，比如 App 日志、Tomcat 日志、Nginx 日志等等 下图贴一个 Elasticsearch 社区分享的一个 “Elastic APM 动手实战”主题的内容截图。 5.2 服务侧Crash log 统一入库 Kibana 时是没有符号化的，所以需要符号化处理，以方便定位问题、crash 产生报表和后续处理。 所以整个流程就是：客户端 APM SDK 收集 crash log -&gt; Kafka 存储 -&gt; Mac 机执行定时任务符号化 -&gt; 数据回传 Kafka -&gt; 产品侧（显示端）对数据进行分类、报表、报警等操作。 因为公司的产品线有多条，相应的 App 有多个，用户使用的 App 版本也各不相同，所以 crash 日志分析必须要有正确的 .DSYM 文件，那么多 App 的不同版本，自动化就变得非常重要了。 自动化有2种手段，规模小一点的公司或者图省事，可以在 Xcode中 添加 runScript 脚本代码来自动在 release 模式下上传DSYM）。 因为我们大前端有一套体系，可以同时管理 iOS SDK、iOS App、Android SDK、Android App、Node、React、React Native 工程项目的初始化、依赖管理、构建（持续集成、Unit Test、Lint、统跳检测）、测试、打包、部署、动态能力（热更新、统跳路由下发）等能力于一身。可以基于各个阶段做能力的插入，所以可以在打包系统中，当调用打包后在打包机上传 .DSYM 文件到七牛云存储（规则可以是以 AppName + Version 为 key，value 为 .DSYM 文件）。 现在很多架构设计都是微服务，至于为什么选微服务，不在本文范畴。所以 crash 日志的符号化被设计为一个微服务。架构图如下 说明： Symbolication Service 作为整个监控系统的一个组成部分，是专注于 crash report 符号化的微服务。 接收来自任务调度框架的包含预处理过的 crash report 和 DSYM index 的请求，从七牛拉取对应的 DSYM，对 crash report 做符号化解析，计算 hash，并将 hash 响应给「数据处理和任务调度框架」。 接收来自 APM 管理系统的包含原始 crash report 和 DSYM index 的请求，从七牛拉取对应的 DSYM，对crash report 做符号化解析，并将符号化的 crash report 响应给 APM 管理系统。 脚手架 cli 有个能力就是调用打包系统的打包构建能力，会根据项目的特点，选择合适的打包机（打包平台是维护了多个打包任务，不同任务根据特点被派发到不同的打包机上，任务详情页可以看到依赖的下载、编译、运行过程等，打包好的产物包括二进制包、下载二维码等等） 其中符号化服务是大前端背景下大前端团队的产物，所以是 NodeJS 实现的（单线程，所以为了提高机器利用率，就要开启多进程能力）。iOS 的符号化机器是 双核的 Mac mini，这就需要做实验测评到底需要开启几个 worker 进程做符号化服务。结果是双进程处理 crash log，比单进程效率高近一倍，而四进程比双进程效率提升不明显，符合双核 mac mini 的特点。所以开启两个 worker 进程做符号化处理。 下图是完整设计图 简单说明下，符号化流程是一个主从模式，一台 master 机，多个 slave 机，master 机读取 .DSYM 和 crash 结果的 cache。「数据处理和任务调度框架」调度符号化服务（内部2个 symbolocate worker）同时从七牛云上获取 .DSYM 文件。 系统架构图如下 八、 APM 小结 通常来说各个端的监控能力是不太一致的，技术实现细节也不统一。所以在技术方案评审的时候需要将监控能力对齐统一。每个能力在各个端的数据字段必须对齐（字段个数、名称、数据类型和精度），因为 APM 本身是一个闭环，监控了之后需符号化解析、数据整理，进行产品化开发、最后需要监控大盘展示等 一些 crash 或者 ANR 等根据等级需要邮件、短信、企业内容通信工具告知干系人，之后快速发布版本、hot fix 等。 监控的各个能力需要做成可配置，灵活开启关闭。 监控数据需要做内存到文件的写入处理，需要注意策略。监控数据需要存储数据库，数据库大小、设计规则等。存入数据库后如何上报，上报机制等会在另一篇文章讲：打造一个通用、可配置的数据上报 SDK 尽量在技术评审后，将各端的技术实现写进文档中，同步给相关人员。比如 ANR 的实现 /* android 端 根据设备分级，一般超过 300ms 视为一次卡顿 hook 系统 loop，在消息处理前后插桩，用以计算每条消息的时长 开启另外线程 dump 堆栈，处理结束后关闭 */ new ExceptionProcessor().init(this, new Runnable() { @Override public void run() &#123; //监测卡顿 try &#123; ProxyPrinter proxyPrinter = new ProxyPrinter(PerformanceMonitor.this); Looper.getMainLooper().setMessageLogging(proxyPrinter); mWeakPrinter = new WeakReference&lt;ProxyPrinter&gt;(proxyPrinter); &#125; catch (FileNotFoundException e) &#123; &#125; &#125; &#125;) /* iOS 端 子线程通过 ping 主线程来确认主线程当前是否卡顿。 卡顿阈值设置为 300ms，超过阈值时认为卡顿。 卡顿时获取主线程的堆栈，并存储上传。 */ (void) main() { while (self.cancle == NO) { self.isMainThreadBlocked = YES; dispatch\\_async(dispatch\\_get\\_main\\_queue(), ^&#123; self.isMainThreadBlocked = YES; \\[self.semaphore singal\\]; &#125;); \\[Thread sleep:300\\]; if (self.isMainThreadBlocked) &#123; \\[self handleMainThreadBlock\\]; &#125; \\[self.semaphore wait\\]; }} 整个 APM 的架构图如下 说明： 埋点 SDK，通过 sessionId 来关联日志数据 APM 技术方案本身是随着技术手段、分析需求不断调整升级的。上图的几个结构示意图是早期几个版本的，目前使用的是在此基础上进行了升级和结构调整，提几个关键词：Hermes、Flink SQL、InfluxDB。","categories":[{"name":"监控","slug":"监控","permalink":"http://zhangyu.info/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"监控","slug":"监控","permalink":"http://zhangyu.info/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"企业完成云转型的成功之道：云成本优化管理","slug":"企业完成云转型的成功之道-云成本优化管理","date":"2022-04-22T16:00:00.000Z","updated":"2022-04-23T14:47:42.471Z","comments":true,"path":"2022/04/23/企业完成云转型的成功之道-云成本优化管理/","link":"","permalink":"http://zhangyu.info/2022/04/23/%E4%BC%81%E4%B8%9A%E5%AE%8C%E6%88%90%E4%BA%91%E8%BD%AC%E5%9E%8B%E7%9A%84%E6%88%90%E5%8A%9F%E4%B9%8B%E9%81%93-%E4%BA%91%E6%88%90%E6%9C%AC%E4%BC%98%E5%8C%96%E7%AE%A1%E7%90%86/","excerpt":"","text":"https://zhuanlan.zhihu.com/p/498641381 近年来，各行各业的企业的云采用率迅速增加，预计向云的迁移将继续加速，以实现快速的敏捷性以及以最少的资本支出获得规模和弹性。即用即付定价带来的成本优势也是推动云采用的关键驱动因素之一。 然而，随着企业将更多的工作负载（业务应用程序和数据）转移到云上，在缺乏明确定义的云成本管理策略的情况下，它们的成本优势会迅速消失。 为了快速迁移到云计算上，企业通常采用直接迁移策略来迁移云，但会导致与本地部署相同的低效率。这些效率低下的表现形式可能是内存、计算或存储容量过剩。如果在迁移过程中或迁移之后没有针对云计算进行架构和优化，则基本成本要素保持不变，不会提供成本优势，并且在许多情况下会增加成本。 此外，很多时候会使用旧的预算和预测实践来管理云上的成本。云财务管理必须区别对待，它需要实时跟踪、准确预测，并在发生任何事件时立即采取行动。不能像以前那样将其留作每月或每季度的审查。 因此，有效的云成本管理是所有云转型过程中的基本要素。 1、控制云成本 – 框架和策略 云采用被认为是按使用付费的模式。但是，其提供的所有服务都是收费的，无论它们是否已被充分利用。对云服务的无纪律、不受监管和不受监控的使用会导致云预算激增，企业难以控制其不断增长的云支出。有许多公司在迁移到云时大大超过其本地基础设施成本。 云成本管理需要从整体角度严格监控和控制云成本。这可以通过定义和实施清晰的云成本管理框架来管理云经济来实现。该框架应允许企业了解和基线化云计算需求，提供对云服务相关支出的可见性、优化使用的能力和工具、实施建议、与利益相关者的成本透明度以及向业务线（LoB）回收成本的机制。 2、云成本管理框架 以下是应考虑的云成本管理框架的核心组件。 1）预算和控制 定义、分配和管理分配给部门或项目的预算。此功能是计划和控制资源利用率，跟踪预算与实际运行成本并针对变化采取行动。这将为企业提供可预测的成本消耗预算。 2）基线和优化 为云计算设定合适的初始规模以基线化成本并定期重复以进一步优化。 3）监控和分析 了解使用的云资源，以便对其进行有效管理。应审查当前和过去的消费、非标准、未使用或未优化使用的云服务等详细信息并采取行动。应该对云使用模式和成本趋势进行分析，以便对 LoB 和产品组合进行精细预算和预测。另一个有助于云成本管理的方面是基于事件的干预定义和自动化。 4）治理和标准化 每个角色的企业范围内基于策略的访问和权限。标准化云基础设施配置，例如创建已批准的虚拟机配置、内置安全性、网络设置等的预定义模板，开发人员可以配置这些模板以提高生产力和自动化。 使用元数据设置自动警报机制，当云服务使用量超过预定义级别时通知管理员或通知未使用的资源、未充分利用的资源和自动响应的标签。 建立与不同环境的运行时间相关的治理（例如，在不使用时可以作为候选关闭的开发/测试环境），定期审查与 CSP 的计费协议，并根据预期的工作负载变化重新协商。 5）成本透明度 为不同的 LoB 和部门带来云使用成本的可见性和透明度。使用元数据和资源标签对 LoB 和部门的云使用情况进行跟踪和基于计量的显示和计费。 3、云成本优化策略 以下是可用于优化云费用的一些策略和最佳实践： 1）正确调整内存、计算、存储和其他资源的大小 很多时候，特别是如果一个企业采用了一种提升和转移的方法来迁移到云，基础设施资源就会被过度配置。由于按使用付费实际上是按订单付费，因此最初正确调整大小并定期进行审查和重新调整大小是必不可少的。这消除了过度配置或次优化使用的机会。 2）消除未使用的资源和服务 识别云设置中未使用的资源并删除它们是一项关键策略。这通常发生在为临时目的创建服务器然后被遗忘时。同样，无法删除附加到虚拟机实例的服务——诸如块级存储卷或静态公共 IP 地址之类的服务，过时的快照，即使实例已经终止，仍然会产生成本。 识别和消除未使用的服务将降低成本。 3）使用正确的服务和生命周期策略进行存储 云存储服务的定价因使用模式而异。根据业务需求选择合适的云服务。根据预期的使用模式和延迟要求，使用存储生命周期策略将内容移动到正确的存储桶。 4）安排可用时间 设置不同环境的运行时间，尤其是非生产实例。例如，识别不需要全天候运行的虚拟机并设置最具成本效益的动态停止和启动计划。 5）使用预留实例，现货定价 估算云服务的计划使用量并购买预留实例。预留实例具有折扣价，可以显著降低成本。这种方法更适合具有长期承诺的和具有相对较低可变性的应用程序的企业。 但是，如果不能准确地估计，这可能会导致总成本增加（由于使用不足）。为此，对历史使用模式的分析和推断至关重要。同样，使用虚拟机的现货定价可以获得显著的成本效益。这最适合容错和无状态应用程序，例如大数据和分析、高性能和高吞吐量计算、机器学习和人工智能应用程序。 6）架构优化 无论云服务提供商如何，为云构建解决方案并选择正确的服务对于从云中获得最大价值至关重要。使用 IaaS 和云原生 PaaS 服务的正确组合可以降低成本，因为它们是基于使用的收费模式。例如，从虚拟机上的数据库迁移到完全托管的弹性数据库即服务。 此外，建立企业标准并确保在云上加入的所有应用程序都遵循优化的架构。此外，定期评估并避免解决方案中所有成本低效的架构元素，例如，最小化来自云的数据出口。 7）使用容器 容器提供了一种在同一虚拟机上以隔离方式运行多个应用程序的轻量级和可移植方式。与传统的虚拟机托管相比，它们能够以更高的单位硬件密度运行应用程序。如果操作正确，这可以降低总体计算成本。此外，容器还具有敏捷性、跨环境简化部署和可移植性的优势。 8）AI/ML 的训练环境 在大型数据集上完成 AI/ML 训练需要大量计算，并且重复执行以微调和提高模型的准确性。在公共云上重复执行此操作可能会变得昂贵，尤其是对于大型数据集。其中一种方法是在内部设置 AI/ML 培训基础设施，并在公共云上运行经过培训的模型。这样可以更好地控制与培训相关的成本。 有些供应商提供专用硬件 (GPU) 和软件的捆绑解决方案，以在本地设置 AI/ML 培训基础设施。关于 AI/ML 培训设置的基础设施位置的决策应在考虑业务需求和成本的情况下整体完成。 9）使用工具监控使用情况、重新基线并时常调整 使用工具查看云费用。所有云供应商都提供云原生服务，许多第三方供应商提供解决方案来组织、预算、跟踪、监控、报告和优化云成本。使用资源标记和元数据来查看和跟踪资源，以获得使用情况、成本跟踪和缓解的实时情况。这需要主动管理而不是被动管理，因为每次延误都要花费金钱。 4、结论 云成本管理可以将云成本智能嵌入到企业的工作中。这样，云成本管理将成为云转型不可或缺的一部分，实现优化成本的框架应该嵌入到云采用生命周期中。有了适当的防范措施，它将推动一种注重成本的文化，并将成为目标运营模式的重要组成部分。","categories":[{"name":"架构","slug":"架构","permalink":"http://zhangyu.info/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://zhangyu.info/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"谈谈如何设计好网站的URL","slug":"谈谈如何设计好网站的URL","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T01:49:09.960Z","comments":true,"path":"2022/04/12/谈谈如何设计好网站的URL/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E8%B0%88%E8%B0%88%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%A5%BD%E7%BD%91%E7%AB%99%E7%9A%84URL/","excerpt":"","text":"来源: IT架构师联盟 URL设计，这是一个非常重要但是往往容易给忽略的部分，也比较少架构师会去关注或者重视。在整个系统架构中，有时候一个好的URL设计对整个系统会起到一个好的作用。 URI和URL及URN URL大家都比较熟悉，其他两个词就比较陌生了。URI、URL和URN是识别、定位和命名互联网上的资源的标准途径。1989年Tim Berners-Lee发明了互联网（World Wide Web）。WWW被认为是全球互连的实际的和抽象的资源的集合–它按需求提供信息实体–通过互联网访问。实际的资源的范围从文件到人，抽象的资源包括数据库查询。 因为要通过多样的方式识别资源（人的名字可能相同，然而计算机文件只能通过唯一的路径名称组合访问），所以需要标准的识别WWW资源的途径。为了满足这种需要，Tim Berners-Lee引入了标准的识别、定位和命名的途径：URI、URL和URN。 URI：Uniform Resource Identifier，统一资源标识符 URL：Uniform Resource Locator，统一资源定位符 URN：Uniform Resource Name，统一资源名称 在这个体系中的URI、URL和URN是彼此关联的。URI的范畴位于体系的顶层，URL和URN的范畴位于体系的底层。这种排列显示URL和URN都是URI的子范畴。 URI可被视为定位符（URL），名称（URN）或两者兼备。统一资源名（URN）如同一个人的名称，而统一资源定位符（URL）代表一个人的住址。换言之，URN定义某事物的身份，而URL提供查找该事物的方法。 用于标志唯一书目的ISBN系统是一个典型的URN使用范例。例如，ISBN 0-486-27557-4无二义性地标志出莎士比亚的戏剧《罗密欧与朱丽叶》的某一特定版本。为获得该资源并阅读该书，人们需要它的位置，也就是一个URL地址。在类Unix操作系统中，一个典型的URL地址可能是一个文件目录，例如file:///home/username/RomeoAndJuliet.pdf。该URL标志出存储于本地硬盘中的电子书文件。因此，URL和URN有着互补的作用。 URL URL是Internet上用来描述信息资源的字符串，主要用在各种WWW客户程序和服务器程序上。采用URL可以用一种统一的格式来描述各种信息资源，包括文件、服务器的地址和目录等。 URL的格式由下列三部分组成： 协议（或称为服务方式） 存有该资源的主机IP地址（有时也包括端口号） 主机资源的具体地址。如目录和文件名等 第一部分和第二部分之间用”：//”符号隔开，第二部分和第三部分用”/”符号隔开。第一部分和第二部分是不可缺少的，第三部分有时可以省略。 目前最大的缺点是当信息资源的存放地点发生变化时，必须对URL作相应的改变。因此人们正在研究新的信息资源表示方法。 URN URN（统一资源名称）是标准格式的URI，指的是资源而不指定其位置或是否存在。这个例子来自RFC3986:urn:oasis:names:specification:docbook:dtd:xml:4.1.2 URN与URL之间仍存在重大区别： URN对一项资源予以持久性的识别。 URL主要为资源标识路径。路径可能随着时间的推移发生改变，原因有二：首先，可于特定URL获取的资源可能发生改变（互联网上的内容变更非常频繁）。另外，资源可能被移至其它定位，也可能同时出现在多个定位。因此，URL往往并不具有唯一性或持久性。 因此，URL与URN的设计相似，宗旨不同。 URN的设计旨在确保与现行标准标识符系统（例如ISSN或任何其它新标准系统）具有互操作性。因此，URN拥有与标识符系统挂钩的命名空间，有别于其它持久性标识符系统（例如DOI前缀并不代表任何标识符系统，而是指明提供标识符的相关组织) 。URL与URI则完全不考虑传统标识符系统。因此，一项期刊的URN基于其ISSN号，URN:ISSN 命名空间则适用ISSN相关规则。相反的，期刊主页的URL往往与ISSN号无关。 URN的结构 URN采用URI语法。 URN:NID:NSS 因此，URN至少包括三个部分： URN：URN首先标注URN方案Scheme NID：于IANA（互联网号码分配机构）注册的命名空间标识符 NSS：NSS（命名空间特定字符串）予以精确标识 ISSN与URN ISSN是最早接受URN方案的书目标识符，以期通过标准方式在互联网上进行使用和表达。每一个ISSN号都可根据以下语法以URN形式予以表达： URN:ISSN:xxxx-xxxx urn:issn:xxxx-xxxx UrN:IsSn:xxxx-xxxx xxxx-xxxx即转化为URN的ISSN号，例如：urn:issn:1234-1231 建议将URN:ISSN记录于可获取的网络资源（例如在线发布的报刊等）的嵌入元数据中。以HTML文件为例，URN:ISSN应录入于HEAD部分： META NAME=”Identifier” SCHEME=”URN:ISSN” CONTENT=”1234-1231” URI URI是以某种统一的（标准化的）方式标识资源的简单字符串，一般由三部分组成： 访问资源的命名机制 存放资源的主机名 资源自身的名称，由路径表示 统一资源标志符URI就是在某一规则下能把一个资源独一无二地标识出来。URL是URI的子集。 典型情况下，这种字符串以scheme开头，语法如下： 有的URI指向一个资源的内部。这种URI以”#”结束，并跟着一个anchor标志符（称为片断标志符）。 相对URI不包含任何命名规范信息。它的路径通常指同一台机器上的资源。相对URI可能含有相对路径（如：“..”表示上一层路径），还可以包含片断标志符。 URI的常见问题 难以输入，URI不必要的冗长 莫明其妙的大写字母 不常见的标点符号 在纸介质上显示很困难，一些字符在纸上打印出来不容易辨认 主机和端口的问题除了scheme-specific部分，domain和port也可能给用户带来困惑。 不成熟的技术：Data URI Data URI是由RFC 2397定义的一种把小文件直接嵌入文档的方案。通过如下语法就可以把小文件变成指定编码直接嵌入到页面中： data:[][;base64], MIME-type：指定嵌入数据的MIME。其形式是[type]/[subtype]; parameter，比如png图片对应的MIME是image/png。parameter可以用來指定附加的信息，更多情況下是用于指定text/plain和text/htm等的文字编码方式的charset参数。默认是text/plain;charset=US-ASCII。 base64：声明后面的数据的编码是base64的，否则数据必须要用百分号编码（即对内容进行urlencode）。 Data URI的优点： 减少HTTP请求数，没有了TCP连接消耗和同一域名下浏览器的并发数限制。 对于小文件会降低带宽。虽然编码后数据量会增加，但是却减少了http头，当http头的数据量大于文件编码的增量，那么就会降低带宽。 对于HTTPS站点，HTTPS和HTTP混用会有安全提示，而HTTPS相对于HTTP来讲开销要大更多，所以Data URI在这方面的优势更明显。 可以把整个多媒体页面保存为一个文件。 Data URI的缺点： 无法被重复利用，同一个文档应用多次同一个内容，则需要重复多次，数据量大量增加，增加了下载时间。 无法被独自缓存，所以其包含文档重新加载时，它也要重新加载。 客户端需要重新解码和显示，增加了点消耗。不支持数据压缩，base64编码会增加1/3大小，而urlencode后数据量会增加更多。 不利于安全软件的过滤，同时也存在一定的安全隐患。 转换工具：Data URI Generator 优秀的URI不会改变 什么样的URI称得上优秀的URI？优秀的URI就是不需要改变的URI。是什么迫使URI做出改变？不改变的是URI：改变的是人。理论上人们没有什么原因去改变URI，但是在实际运行中却存在着成百上千的原因。从理论上讲，域名的所有者拥有该域名下的所有URI。理论上您域名下的所有URI是完全由您控制的，所以你是可以按照自己喜欢的方式使URI变得稳定。迫使一个文件地址消失的原因是域名过期或服务器没有在继续运行。这就是为了会有这么多改来改去的链接。其中的一部分是由于缺乏远见导致的。下面是您可以听到的一些原因。 为了使网站更好，我们刚改版了网站。 我们有大量过期的、保密的、无效的文档需要进行区分。 我们发现不得不移动文件。 我们曾经使用的是一个CGI脚本，现在我们使用的是一个二进制程序。 我不认为URI是需要持久的，需要持久的那是URN。 当你在服务器上修改了URI，你不会知道还有多少人会使用旧的URI。他们可能把你的链接发布到了其他网站上，他们可能把你的链接存为了书签。他们可能把你的URI告诉了别人。当一些人点击链接，但是发现链接无法打开的时候，他们就会对网站拥有者失去信心。他们会因为不能完成自己想要的目标而沮丧。 我该怎么去设计URI? 使一个URI可以持续2年、20年、200年，这是一个网站管理员的责任。这需要思想、组织和承诺。一般来说URI改变时因为文档里的一些信息发生了改变，这和URI的设计至关重要的。文件的创建日期这是不会改变的。这对分离旧的系统和新的系统非常有用。这能很好的让你开始设计一个URI。即使这个文件会被多次修改，但是他还是只会有一个创建日期。唯一例外的是一个网页是故意“最新”的，例如频道的首页。 http://www.example.com/money/moneydaily/latest/ 此URI不需要日期的主要原因是此页面时不断更新的，如果你需要者页面的存档，存档地址可以是 http://www.example.com/money/moneydaily/1998/981212.moneyonline.html （这个URI看上去不错，除了”98″和“.html”有些多余） 哪些信息需要被抛弃？ 在使用日期以后，把任何信息放入URI都有可能带来问题。 作者的名字：著作权可能会因为版本的修改而改变，比如团队里的某些人离开使事务被转手。 标题：这个是非常棘手的，他总是现在看起来非常合适，但是过些时间久需要改变。 状态：如”old”,”new”,”latest”等，文件很可能会改变状态。 访问权限：一个文件的访问权限可能会因为情况而改变，不要将文档放在”public”、”team”下。 文件扩展名：即使是”.html”也有可能会改变。 程序机制：如”cgi”和”exec” 磁盘名称：这个也有用使用的！ 按文章主题进行分类 这是非常危险的操作。通常情况下，你URI中的文档分类是按你正在进行的工作进行区分的。这就可能带来隐患，你从事的领域可能会在今后发生变化。在W3C，我们期望吧“MarkUp”修改为“Markup”，后来又期望修改为“HTML”，我们不能保证现在的命名在以后是否适用。 按主题分类这是一个非常理想的分类方案，包括把整个互联网进行分类一样，这是一个非常不错的解决办法，但是从长远看存在着严重的缺陷。每个人对语言中的每个聚类的主题词都有不同的理解，网络之间的主题关系，并不是像树型那么简单。事实上，当你在你的URI中绑定分类时，未来你很有可能去改变这个分类，到时候URI就需要跟随着改变。 在URI中使用主题进行分类的一个原因是你需要一个名称作为URI的一个部分来组织内容，比如内容细分，通常来说在日期存在（日期在左边）的情况下还是非常安全的，1998/pics可以理解为，我们在1998年的照片。而不是照片中1998年我们在做什么。 不要忘记你的域名 请记住，这不仅适用于URI“路径”，同时也使用与服务器名称。在域名中无论代表公司，或文件状态，或访问级别，或者安全级别划分，要非常非常小心，特别是在使用多个域名访问一个文件的时候，不要忘记，你可以在服务器端使用重定向。 URL即UI 尽管APP和小程序在替代WEB网站。但WEB网站最终难以被完全替代。未来的很多年，URL还将成为用户界面的一部分。所以一个可用性好的网站需要： 一个容易记忆及拼写的域名 简短的URL网址 容易输入的URL 可视化的URL结构 用户可以删除URL最后的一层到达上级目录 URL保持不变 用户不需要像服务器一样了解每个URL，事实上更多的人是通过大概的印象访问网站的： 很多人会在没有访问网站前猜测网站的域名，所以你最好使用公司名称或品牌作为域名 更多的人更喜欢记住网站名而不是将网站添加到书签，所以最好注册一个易于拼写的域名 在用户将你的URL通过邮件告知别人时，保证你的URL要少于78个字符，因为如果URL过程可能会造成换行 如果很多人通过输入那么最好使用简短的URL 不要再URL中大下写混用，因为很多人会忽视大小写，但是部分服务器不会 在服务器上进行拼写检查来减少由于拼写错误造成的问题 来自第三网站的外链对流量很重要，所以在生成你的URL的时候要考虑到易于传播。 保证所有的URL都是持续可以访问的，且链向的页面不做改变。 不要把文件的URL改来改去， 保证同一个文件前后只有一个URL。 很多人考虑是否.COM域名是否要比.CN（国别域名）要好？是的，很多人已经习惯了域名以.COM结尾。这是由于早期由美国人开发的浏览器会自动补全.COM（现在苹果的iPad上默认也有.COM按钮），基于这种情况，我的建议是： 如果网站是英文的或国际性最好使用.com域名 如果网站使用的是其他语言可以使用国别域名代替 如果网站内容是区域性的，无所谓使用哪种域名 国别域名相比.COM域名主要优势是，还有很多可以注册的简短的、易记的域名。 从长远的发展来看，需要大量的名称（按照人类语言习惯）来识别世界上每个实体。新的顶级域名会不断出现，但由于旧的用户习惯，旧的浏览器或软件还是会存在很多年，所以好的域名还是会影响很多年。 怎样设计一个好的URL URL是网站UI的一部分，因此，可用的网站应该满足这些URL要求： 简单，好记的域名 简短（short）的URL 容易录入的URI URL能反应站点的结构 URL是可以被用户猜测和hack的（也鼓励用户如此） 永久链接 记住下面四句话，你就知道应该设计什么样的URL了。 URL应当是用户友好的 URL应当是可读的 URL应当是可预测的 URL应当是统一的 网址根目录（level section）是非常珍贵的 对于任何一个URL而言，它最用价值的方面是在他的根目录（level section），我的观点是她必须在你写任何代码前确定下来，他会确定你网站最后是怎么组织起来的。当你想建立新的站点的时候，一定要想好哪些根目录的网址是需要保留的。 命名空间是一个非常有用的拓展网址方案 命名空间是一个建立容易记忆的良好网址结构的方案。那命名空间是什么意思呢？下面是一个例子： https://github.com/pallets/flask/issues 在上面的URL中，pallets/flask是命名空间。为什么这个是有用的？因为任何跟在命名空间后面的部分都将成为level section。在可以在任何 / 后面跟上/issues或/wiki来生成页面。 为了命名空间的通用性，保持命名空间的简洁，不要将内容加在前面或后面，类似/feature/ / 或/ / /feature. 查询字符串对排序和过滤非常的有用 网站都有一些查询字符串，很多网站使用多个查询字符串。他们通常使用同一的模式来对页面或内容进行排序或过滤（sort=alpha&amp;dir=desc），他们可以是URL更加简单和易记。 需要记住的是，在URL上没有带任何查询字符串时需要显示一个不同的页面。 非ASCII字符出现在网址中 非 ASCII字符不但难以输入，而且还难以记忆。URL是为人设计的，不是为搜索引擎设计的。在URL中堆砌关键词的手法，并不罕见，比如下面的URL：这样的URL在Google 2003年修改算法前对SEO很有效，但是一些SEO教程上现在还是叫你将关键词写入URL。他们错了，忽略他们。 除此之外，你还需要记住以下两点： 下划线很不好，请在URL中使用中划线。 在URL中使用一些短的、通俗的词，如果一段URL中有中划线会特殊的字符，那它可能有些长。 URL是为人使用的，也是为人设计的。 一个URL就是一个协议 一个URL是一个协议，你需要让他保存做够长的时间。一旦有人点击了你的URL，他们就是和你签署有了一个协议，他们期望下次再打开这个网址的时候看到同样的内容。在你的URL公布出去以后，不要轻易的去修改它，如果你真的迫不得已要去修改它，那么请对原来的URL做跳转。 任何页面都需要有个URL 在理想的情况下，每个单独的页面都需要一个URL，这个URL在复制到别的浏览器的时候要还可以访问。事实上这样做是完全不可能的，直到新的HTML5浏览器历史记录Javascript API的出现，这里有两种方法： onReplaceState：这个方法取代了浏览器历史记录中的URL，使URL留下后退按钮。 onPushState：这个方法能push一个新的URL到浏览器历史记录，用来更换浏览器中的历史堆栈。 这两个新的方法可以改变浏览器中的访问历史，有了这个新的特征，我们需要为页面设计后退页面。在使用前需要问自己：这个动作是否需要产生新的内容或用不同的方法显示相同的内容。 生成新的内容：你应该使用onPushState（如分页链接） 用不同的方法显示相同的内容：你应该使用onReplaceState（如排序了过滤） 通过自己的判断，想想你需要实现怎样的效果。 链接需要看上去像个链接 很多生成链接的方法如、，如果你点击它们它们会打开新的页面，当你将鼠标放在标签时，你的浏览器状态栏就会告诉你URL地址是什么。在使用onReplaceState和onPushState时不要破坏这样的规则。 POST后的网站需要转向 过去很多开发人员喜欢生成不能再次使用的URL，这种URL也称为Post-specific URLs，当你提交一个表单的时候你不会发现地址栏中的URL会发生任何变化，当你将复制URL重新打开后却得到一个错误的页面。这样的URL本身没有任何错误，他们的主要作用是进行重定向和在API中使用，并不应该给用户使用。 一定要短 为了URI能被方便的录入，写下，拼写和记忆，URI 要尽可能的短，根据w3c 提供的参考数据，一个URI的长度最好不要超过80个字节（这并非一个技术限制，经验和统计提供的数据），包括schema和host,port等。 大小写策略 URI的大小写策略要适当，要么全部小写，要么首字母大写，应避免混乱的大小写组合，在Unix 世界，文件路径队大小写是敏感的，而在Windows 世界，则不对大小写敏感。 允许URL管理URL映射 管理员可以重新组织服务器上的文件系统结构，而无需改动URI，这就需要URI和真实的服务器文件系统结构之间有一个映射机制。而不是生硬的对应。这种映射机制可以通过如下技术手段实现： Aliases，别名，Apache上的目录别名，IIS上的虚拟目录 Symbolic links，符号链接，Unix世界的符号链接 Table or database of mappings，数据库映射，URI和文件系统结构的对应关系存储在数据库中。 标准的重定向 管理员可以简单的通过修改HTTP 状态代码来实现服务器文件系统结构变更之后的URL兼容，可以利用的HTTP Status Code有： 301 Moved Permanently ([RFC2616] section 10.3.2) 302 Found (undefined redirect scheme, [RFC2616] Section 10.3.3) Temporary Redirect ([RFC2616] Section 10.3.8) 用独立的URI 技术无关的URI 提供动态内容服务时，应使用技术无关的URI。即URI不暴露服务器端使用的脚本语言，平台引擎，而这些语言，平台，引擎的变化也不会导致URI的变更。因此，sevelet,cgi-bin之类的单词不应该出现在URI 中。 提供静态内容服务时，应当隐去文件的扩展名取而代之的技术是content-negotiation, proxy, 和URI mapping 身份标志和Session机制 使用标准的身份认证机制，而不是每个用户一个特定的URI 使用标准的Session机制，而不是把Session ID放在URI中使用 内容变更时使用标准转向： 对变更的内容使用标准的重定向 对删除的资源使用 HTTP 410 提供索引代理：索引策略 Content-Location Content-MD5 提供适当的缓存信息： 缓存相关的HTTP头 缓存策略 缓存生成内容HTTP HEAD和HTTP GET 总结 URI 是Web UI 的一部分，应当像对待网站Logo 和公司品牌一样对待它 URI 是网站和普通用户之间的唯一接口，应当像对待你的商务电话号码一样对待它 URL中井号的作用 井号在URL中指定的是页面中的一个位置。井号作为页面定位符出现在URL中，浏览器读取这个URL后，会自动将位置滚动至指定区域。 井号后面的数据不会发送到HTTP请求中。井号后面的参数是针对浏览器起作用的而不是服务器端。 任何位于井号后面的字符都是位置标识符。不管第一个井号后面跟的是什么参数，只要是在井号后面的参数一律看成是位置标识符。比如这样一个链接（http://example.com/?color=#fff&amp;;shape=circle），后面跟的参数是颜色和形状，但是服务器却并不能理解URL中的含义。服务器接收到的只是：http://example.com/?color= 改变井号后面的参数不会触发页面的重新加载但是会留下一个历史记录。仅改变井号后面的内容，只会使浏览器滚动到相应的位置，并不会重现加载页面。浏览器并不会去重新请求页面，但是此操作会在浏览器的历史记录中添加一次记录，即你可以通过返回按钮回到上次的位置。这个特性对Ajax来说特别的有用，可以通过设置不同井号值，来表示不同的访问状态，并返回不同的内容给用户。 可以通过javascript使用location.hash来改变井号后面的值。window.location.hash这个属性可以对URL中的井号参数进行修改，基于这个原理，我们可以在不重载页面的前提下创造一条新的访问记录。除此之外，HTML 5新增的onhashchange事件，当#值发生变化时，就会触发这个事件。 Googlebot对井号的过滤机制。默认情况下Google在索引页面的时候会忽略井号后面的参数，同时也不会去执行页面中的javascript。然而谷歌为了支持对Ajax生成内容的索引，定义了如果在URL中使用“#!”，则Google会自动将其后面的内容转成查询字符串_escaped_fragment_的值。比如最新的twitter URL：http://twitter.com/#!/username，Google会自动请求http://twitter.com/?\\_escaped\\_fragment\\_=/username来获取Ajax内容。 实例：Flickr API 中的URL规则 Flickr API中的URL规则非常值得我们学习，下面就来揭开Flickr URL的神秘面纱。 Flickr图片地址 URL主要有下面三类： http://farm{farm-id}.static.flickr.com/{server-id}/{id}\\_{secret}.jpg http://farm{farm-id}.static.flickr.com/{server-id}/{id}\\_{secret}\\_\\[mstzb\\].jpg http://farm{farm-id}.static.flickr.com/{server-id}/{id}\\_{o-secret}\\_o.(jpg|gif|png) 尺寸字母后缀说明： s small square,小正方形,75×75 t thumbnail,缩微图,最长边为100 m small 小图,最长边为240 – medium,中图,最长边为500 z medium 640,中等尺寸640,最长边为640 b large,大图,最长边为1024 o original image,原始图片,可能是jpg,或是png,或是gif 注意：原始图片有些不同，他们有自己的密钥，在返回数据中被称为originalsecret，除此之外还包含原始图片格式，被称为originalformat。这些值都会在向API请求原始图片时返回。 以下为图片地址URL示例： http://farm1.static.flickr.com/2/1418878\\_1e92283336\\_m.jpg farm-id: 1 server-id: 2 photo-id: 1418878 secret: 1e92283336 size: m Flickr网页地址URL 个人档案及相片页面的URL使用NSID（带@ 符号的数字）或自定义URL（需要设置），可以通过请求flickr.people.getInfo获取自定义URL。不管用户是否设置自定义URL，NSID一直是有效的。所以你可以使用用户ID来进行所有的请求。 你可以非常轻松的创建个人档案、影集、所有照片、个人相片或影集的URL： http://www.flickr.com/people/{user-id}/ – profile http://www.flickr.com/photos/{user-id}/ – photostream http://www.flickr.com/photos/{user-id}/{photo-id} – individual photo http://www.flickr.com/photos/{user-id}/sets/ – all photosets http://www.flickr.com/photos/{user-id}/sets/{photoset-id} – single photoset 同样还可以构建其他页面，比如用户在登录的情况，可以让他们链向 http://www.flickr.com/photos/me/\\* 或 http://www.flickr.com/people/me/\\* ，將使用其自己的ID 取代「me」。 链接示例： http://www.flickr.com/photos/12037949754@N01/ http://www.flickr.com/photos/12037949754@N01/155761353/ http://www.flickr.com/photos/12037949754@N01/sets/ http://www.flickr.com/photos/12037949754@N01/sets/72157594162136485/ 短网址服务 Flickr 针对上传的图片提供短网址服务。Flickr上每张相片均拥有经数学计算的简短URL：http://flic.kr/p/{base58-photo-id} 利用Base58将数字和字母进行组合对照片ID进行压缩。Base58和base62[0-9a-zA-Z]差不多，只是为了更加利于辨认，删除了容易混淆的0, O, I,和 l。","categories":[{"name":"优化","slug":"优化","permalink":"http://zhangyu.info/categories/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"优化","slug":"优化","permalink":"http://zhangyu.info/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"敏捷教练第01课-敏捷教练和ScrumMaster基本功四部曲","slug":"敏捷教练第01课-敏捷教练和ScrumMaster基本功四部曲","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T01:53:52.858Z","comments":true,"path":"2022/04/12/敏捷教练第01课-敏捷教练和ScrumMaster基本功四部曲/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC01%E8%AF%BE-%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E5%92%8CScrumMaster%E5%9F%BA%E6%9C%AC%E5%8A%9F%E5%9B%9B%E9%83%A8%E6%9B%B2/","excerpt":"","text":"课程概述敏捷教练是一个职业。Scrum Master 和敏捷教练是同一职业的不同阶段。当一个人能带好一个 Scrum 团队时，他是一个 Scrum Master。当他能带各种不同类型的团队，并持续追求更好，他就是一个敏捷教练。 Scrum Master 职责的范围和边界相对确定，敏捷教练职责的范围和边界相对不确定。但从学习的角度，他们所需要的基本功是一致的。本课程中对这两个角色，在大多数时候不太区分。鉴于这两个角色既有相似处又有区别，大家在使用时对这两个名称的理解上又有变异，所以课程的名称中就把这两个名称并称，以求相对准确地表达这个课程所要服务的角色。就算是您所采用的敏捷方法不是 Scrum，依然可以从本课程中受益。 如同任何其他职业，敏捷教练有它的技能，也需要并且能够通过练习达到精通。我们可以通过四部曲的结构理解敏捷教练这个职业及其技能： 目的：任何一个职业，都有它存在的目的。这个目的包括职业产生的背景，工作的环境，以及所承担的职责。 储备：即敏捷教练所必备的基础知识。 技巧：即如何运用基础知识履行职责。 实战：即在一个典型完整的工作周期中，如何利用储备和技巧取得成功。 本章会介绍： 敏捷教练这个职业产生的背景 敏捷教练的工作环境 敏捷教练的职责 体系化的参考书目 敏捷教练职业产生背景 “追求更好”旅途的守护者 敏捷方式可以追溯到1620年弗朗西斯·培根（Francis Bacon）科学方法的发源时期。更合理一点的起点可能是在20世纪30年代，那时候贝尔实验室的物理学家和统计学家沃特·阿曼德·休哈特（Walter A. Shewhart）开始使用计划-执行-学习-调整（PDSA）循环对产品和过程进行改善。 休哈特把这种反复渐进的开发过程教给了他的学员戴明（W.Edwards Deming），后者在二次大战后的日本大量使用了该方法。戴明将 PDSA 改造为 PDCA。丰田公司雇用了戴明来培训公司中数百名经理，并在他的经验之上创立了著名的丰田生产体系——这也是如今精益思想的最初由来。这种反复渐进的方式对于20世纪50年代的 X-15 超音速飞机的制造也是贡献巨大。 丰田模式的关键，以及使丰田有杰出表现的原因并不是任何个别要素，而是一个由各要素组成的 4P 体系： 长期理念（philosophy）：重视着眼于长期的思维，公司高层注重为顾客及社会创造与提升价值，这个目的主导了该公司的长期方法——建立学习型组织，投资于人员、产品与工厂，以及绝不松懈地坚持质量，以适应环境的变迁，成为高效的组织。 正确的流程（process）：正确的流程方能产生优异成果，流程是以低成本，高安全性与高昂的士气达成最佳质量的关键。 借助员工与合作伙伴（people and partner）的发展，为组织创造价值：丰田公司管理层的看法是，他们打造的是“人”，不是汽车。尊重员工的智慧和能力，并不断激励他们做得更好。 持续解决根本问题（problems）是组织型学习的驱动力：丰田模式的最高境界是组织型学习，丰田的持续学习制度重心在于辨识问题的根源，并预防问题的发生，持续改善。 此体系必须每天以贯彻一致的态度实行，而非只是一阵旋风。这个体系成功的秘诀是，经理即教练。培养深谙公司理念的领袖，使他们能教导其他员工。这是我们今天思考敏捷教练职责的最重要参照物。丰田的 4P 模式，也能帮助我们从根本上去思考什么是敏捷。 大野耐一是将丰田生产方式体系化的重要人物。大野耐一退休后，与其弟子创建了 NPS（New Production System），为其他企业服务。精益教练诞生，教练与经理分离。这也预示着在今天敏捷教练和管理者通常是分离的职位。 Scrum 的另一根植于日本的基础，是1986年野中郁次郎和竹内弘高在哈佛商业评论上发表的名为《新的新产品开发游戏》的文章。通过研究那些比竞争者更快发布新产品的制造商们，比如富士-施乐的复印机，本田的摩托车引擎，佳能的照相机，定义了以团队为基础的新的产品设计和研发过程。这种过程不是通常在产品开发中的“接力赛”——一组专家完成产品部分功能并将项目传递到下一组专家手中。这种方式被野中郁次郎和竹内弘高称作为“橄榄球”方式，“团队试图作为一个整体完成所有任务，将球传来传去。” 在1993年，Jeff Sutherland 面临一项似乎是不可能完成的挑战：Easel 是一家软件公司，需要在半年之内开发一款新产品来替代它的传统产品。Jeff Sutherland 通晓很多方法，比如快速应用程序研发，面向对象设计，PDSA 循环，专案工作等等。他希望在公司总部建立一个类似于专案工作的文化氛围，将组织分割和合并的好处结合起来。他开始学习任何和提高组织效率相关的知识。通过阅读上百篇研究报告和顶尖的产品管理专家面谈，他脑海中逐渐有了一些有煽动力的想法。 这中间有一个想法来自于贝尔实验室的关于 Borland Quattro Pro 团队的文章。该文章主张，每天短的团队会议能显著增加团队效率。而 Jeff Sutherland 的核心概念则来自于竹内弘高和野中郁次郎的“橄榄球”方式，虽然该方法更关注制造过程而不是软件开发过程。通过借鉴哈佛商业评论文章中的关键想法和进行一些特别的试验，Jeff Sutherland 创建了一种新的软件开发方法，归功于橄榄球带来的灵感，Sutherland 将这种方法称为“争球”（Scrum）。Scrum 方式最后确保了他准时完成了似乎不可能的任务，也没有超出预算，程序漏洞比之前版本还要少很多。Sutherland 随后就长时间和 Ken Schwaber 对该方法进行长期研究，并在1995年两人首次在公众面前发布 Scrum 的方法。 在2001年，17位自称“有组织的无政府主义者”在美国犹他州的雪鸟滑雪场会面，分享他们的想法。Jeff Sutherland 和其它 Scrum 的先驱也在其中。参与者们分享了互相竞争的几种方式：极限编程（XP）、水晶方法、自适应软件开发（ASD）、特性驱动开发（FDD）、动态系统开发方法（DSDM）。所有这些方式都是“轻量版”的框架，因为这些方法使用更少、更简单的规则来适应快速变化的环境。不少与会者都觉得“轻量”这个术语挺适用的。 虽然与会者不能在方法上达成一致，但是他们还是为这个运动取了个名字：敏捷。这个词是一位参与者提出的，他当时正在读《敏捷竞争者和虚拟组织：给客户更多的策略》一书。书中列举了100家公司的例子——包括 ABB， 联邦快递，波音，博士和哈雷戴维森，这些公司正在创建适应动荡市场的新方法。有了这个名字，参与者达成一致，发布了“敏捷软件开发宣言”，该宣言中突出了每个人都同意的4个关键价值。稍后在会议中，以及之后的几个月中，他们发展了12个原则，被称为“敏捷宣言背后的原则”。 从2001年开始，所有的开发框架，以及与之匹配的价值观和原则就被称作为敏捷技术。 同时，敏捷方法继续演化。在20世纪80年代后期和90年代前期，MIT 的研究学者们开始研究日本的制造体系，特别是丰田生产体系。他们借用了名词“精益”来描述改善效率的这套体系，包括消除浪费（muda)， 减少波动（mura）和降低负荷（muri）。虽然精益方法并没有在雪鸟会议上被表述成敏捷方法，但是精益和看板软件开发系统在后来被并入敏捷系统。在开始时候，一些纯粹的敏捷主义者拒绝承认精益方法。 但是精益宣传该方法能关注客户协作，最终更多的敏捷践行者开始接受精益，看板，还有混合方法（比如 Scrumban 和 Lean scrum），作为敏捷价值和原则合法的应用。 这些新方法论的创始人们是精通技术的管理者，和管理者中的思想者。敏捷宣言的17位创始人，是敏捷思想的传道者，可以被认为是最早的敏捷教练。他们所创造的这些方法的本质，不是一些死板的规定，而是在追求“更好”的旅途中，作为承载“更好”的载体。这些方法论的落地，以及作为这些方法论内在精神的追求“更好”，不会自动发生。 一种可能的逻辑是，由管理者来承担落实新方法论的责任。管理者可以转型为教练，重拾作为精益鼻祖的丰田的精神。对于管理者无法承担教练职责但又想追随敏捷潮流的组织，则需要专职的敏捷教练。 敏捷教练工作的环境守破离的概念来自日本，大致可以理解为遵守、突破和脱离。这个概念在敏捷界被广泛运用，含义也会有所变迁。下面这个关于组织所处阶段的守破离，来自于 Scrum 之父 Jeff Sutherland。 组织的守的状态 CEO 没有敏捷思维。以命令和控制的文化为主。 依据传统的管理层级结构产生项目组。 即使采用敏捷，也是跟风，流于形式，无法深入。 在这种状态之下的效率提升通常只能做到20%~30% 组织的破的状态 CEO 改变管理者的角色。教练和支持的文化浮现。 管理者教导团队自组织和自管理。管理者成为领导者。 领导者为团队提供有挑战的排好优先级的目标。 消除组织债，创建可行的商业和组织计划，提供团队所需的资源。 识别和移除障碍，消除浪费和技术债，确保团队速率最大化。 确保产品负责人对交付的价值负责。 确保 Scrum Master 对流程改善和团队快乐负责。 确保团队对质量提升和技术债修复负责。 团队依据排好优先级的产品列表自我形成。 领导者在组织内驱动不同技能的虚拟实践社区，为组织提供能力建设。 领导者按需重构组织。 在生产力方面会取得200%~400%的提升。 示例公司：Spotify，SAP，Salesforce，Microsoft。 组织的离的状态 层级仍然存在，但主要是为技能培养服务。 团队自组织负责产品方向和组织重构。 领导者支持团队所需的技能。 群游使组织在压力之下更强壮。 产生500%~1000%的生产力提升。 示例公司：Valve，Zappos，Morning Star，Gore，Grindr。 这三种状态，跟建国方略中的军政、训政和宪政暗合，可参照理解。 瓶颈通常在瓶子的上部，一个公司最大的瓶颈是 CEO。作为一个敏捷教练，针对所处的组织形态，可以采取运用敏捷基本功加上变通的方法来开展工作。 至于团队，也会有三种形态。 无组织团队 从团队绩效方面看，是相对不高和不稳定的，时好时坏。迭代计划预测的靠谱度较差，速度也不高。 从团队动态和互动看，呈现出一种各自为政的状态，沟通不畅，合作困难。从会议看，目的不明确，流程不清晰，效率低，参与者沮丧。 自运转团队 从团队绩效看，呈现出相对稳定的状态，迭代目标承诺靠谱度较好，迭代目标基本能完成。 从团队动态和互动看，团队成员目标一致，有良好的沟通合作，在各项活动中，团队成员都能主动参与。会议的目的和流程清晰，没有 Scrum Master 的情况下，会议也能按照打磨好的流程自动运转起来。 自组织团队 从团队绩效看，在稳定的基础上，呈现出阶段性的持续提升，生产率和质量不断提高。 从团队动态和互动看，团队有更多高质量的互动，团队除了关心共同的目标，还关心持续改善和从根本上解决问题。呈现出上文中所说的丰田 4P 的一些特征。 敏捷教练所要做的，就是把团队从无组织状态带到自运转状态，再进一步带到自组织状态。这个使命的履行，本课程中敏捷教练和 ScrumMaster 的基本功可以帮到您。 敏捷教练的职责：流程与人两手抓在设计本课程之前，针对一部分敏捷从业人员和经历者做了一个小调查，想了解他们对 Scrum Master 职责的理解。这个调查虽然样本较小，不具备统计意义，但依然可以帮助我们了解跟我们处在同样角色的人对这个问题怎么看。调查结果如下： 精通管理规则，精通业务梳理，极强的沟通协作能力，技术熟练，懂业务管理。 敏捷教练确保 Scrum 被正确的运用和贯彻，同时保护团队和引导新的想法落地。 Scrum Master 是牧羊犬的作用，让团队在一个迭代中不受打扰，同时他应该对敏捷的流程、理念有深入的了解，具有较强的管理能力。 引导团队进行效率的提升，通过各种工具的导入，来实现项目目标。但是，究竟是否要像传统团队一样，也要引导团队进行项目交付，并解决依赖问题，这个要商榷。 保证团队资源，保证各个角色及职责协作，解决团队开发中的障碍，协调解决沟通问题，保证开发过程按计划进行。 指导 Scrum 小组成员理解为什么、知道如何参与 Scrum 实践的每一个环节，把控好 Scrum 实践的产出，为整个小组的 Scrum 迭代/计划结果负责。 基于对 Scrum 角色的了解，以及对项目和资源的认识，帮助 stakeholder 决定最佳的按照 Agile 路线来实施项目的教练。 培训和指导团队践行敏捷实践；关注项目的度量数据，及时带领团队调整，加速或稳步前进；关注成员的状态，激励督促团队前进；带领和辅导团队按照敏捷和精益的方式做事，打造优秀自组织团队。 牧羊犬守护团队，流程；教练，培训，引导团队，PO，相关人知识，技能；推动过程改进，促进变革；提升团队，组织效能。 在敏捷团队中推进敏捷开发模式和流程，是团队的组织者，保证团队资源，协调内外部关系，解决出现的问题。 帮助团队进行敏捷实践落地，梳理流程，减少外部干扰，鼓舞士气，提高团队作战能力。提高工作效率。 传播敏捷思想，指导团队，指导 PO，组织敏捷会议，排除团队干扰。 指导团队按 Scrum 方式运转，传播 Scrum 思想，指导敏捷实践，提高效率。 保证团队资源完全可被利用并且全部是高产出的。保证各个角色及职责的良好协作。解决团队开发中的障碍。做为团队和外部的接口，屏蔽外界对团队成员的干扰。保证开发过程按计划进行，组织 Daily Scrum，Sprint Review and Sprint Planning meetings。 Scrum Master 是 Sprint 的负责人，Sprint 做得好不好的终责者。负责计划，执行 Sprint，并使团队团结及有自主创造能力。 搭建 team 架构，分配各个角色成员，开展 scrum 常规的事项，并让敏捷的理念深入人心。帮助团队更好的按照 Scrum 框架有效的运行，对团队遇到的问题和障碍提供帮助，协助扫清研发过程中的障碍，打造高效能的团队。 组织项目团队，承诺项目开发，回顾项目过程，总结项目经验教训，组织每日站会，制定 Sprint 计划。 Facilitate everything and eventually retire，留下一个自组织团队，悄然离去，深藏功与名。激励团队，coach，team lead，life tutor。 Scrum Master 应该是作为团队初步接触敏捷时作为流程与套路教授和规范。在团队逐步成熟后，Scrum Master 的职责可以旁落，而专职 Scrum Master 可以取消。 那么敏捷教练的职责到底是什么呢？ 《敏捷教练》一书的作者之一，瑞秋·戴维斯（Rachel Davies）对敏捷教练的观点： 概括地说，敏捷教练帮助团队在工作中应用敏捷实践，从而帮助团队发展的更健壮。接受这些变化需要时间，所以没法通过“点到即止”的方法立即让它们生效。你需要与团队长时间呆在一起，并帮他们，让他们更加关注工作流程、关注如何更有效地协作。你对团队的目标是在你离开后，让他们能“自我指导”并且擅长应用敏捷。这样不会限制敏捷教练向组织引进敏捷，以及建立新敏捷团队。 &lt; Coaching Agile Teams &gt; 的作者 Lyssa Adkins 对敏捷教练的观点： 敏捷教练确实非常重要，因为现在有许多人在运用一堆蹩脚的敏捷工作方式。即使运用了，它们只是更快地产出了平庸的结果，我知道，那并不是他们运用所谓“敏捷”工作方式的主要意图。我认为教练是帮助团队取得惊人成果所不可或缺的组成部分，因为所有的成果都是人互相交互所产生的。敏捷框架中没有说明如何处理人与人交互的部分。为了使敏捷框架良好运作，它当然会提供可让其正常运行的结构和容器。但是在敏捷框架之外，还有很多事情要做，还有很多东西需要带给团队，针对不同的规则，需要给团队很多建议——如冲突管理、敏捷促进、教导及指引人、专业指导等等。 本文给出的敏捷教练的职责定义是： 贯彻一种工作方式，包括精益、敏捷和系统思考。 打造自组织团队，特别是要面对人（包括自我与他人）这个最复杂的实体。 以此来消除浪费，增加价值，达到组织的目标。 要履行这些职责，需要理解敏捷，这是本课程基本功的储备部分；要能够在组织中用敏捷影响他人，这是基本功中技能的部分；要体会真实环境中的敏捷运用，这是本课程基本功中的实战部分。 体系化的参考书目 敏捷是敏捷教练的代码 敏捷的历史是一场不断追求更好的历史，在这个过程中，先行者们为我们留下了众多可供参考和让我们无须重新发明轮子的书籍。 本节以类库、框架、架构，和编辑、编译、链接、运行的视角解析敏捷和敏捷教练，以及如何运用先行者们留下的书籍。 敏捷是一种代码，2001年2月，17人在美国犹他州的雪鸟滑雪场，解码和发明了这门语言，并贡献了敏捷基础类库。 敏捷基础类库 Kent Beck 等的 （《敏捷宣言》）。 敏捷框架类库 Jeff Sutherland &amp; Ken Schwaber Kent Beck （《拥抱变化：解析极限编程》） Mike Cohn （《用户故事与敏捷方法》）（《敏捷估算与规划》） David Anderson （《看板方法》） Mary Poppendieck （《精益软件开发》） Craig Larman 的 Large Scale Scrum Dean Leffingwell 的 SAFe 敏捷扩展类库 野中郁次郎和竹内宏高《新的新产品开发游戏》《场理论》 Henrik Kniberg （《硝烟中的 Scrum 和 XP 》） Kenny Rubin （《Scrum 精髓》） Jeff Patton （《用户故事地图》） Mitch Lacey （《Scrum 实战指南》） Ken Schwaber （《Scrum 敏捷项目管理》） Mike Cohn （《Scrum 敏捷软件开发》） Eric Ries （《精益创业》） Ellen Gottesdiener Jezz Humble （《持续交付》） 《戴明14条》 艾永亮《腾讯之道》 何勉《精益产品开发原则、方法与实施》 精益类库 大野耐一 《丰田生产方式》《现场管理》 新乡重夫《从 IE 的角度看丰田生产方式》 James Womack （《改变世界的机器》）（《精益思想》） Jeffery Liker （《丰田模式》）系列 John Shook （A3 报告）（价值流图） 引导与心理学类库 NLP 神经语言程式 世界咖啡 六顶思考帽 管理与变革类库 Chip &amp; Dan Heath （《瞬变》） Jurgen Appelo （《管理3.0》） Jurgen Appelo （《变革管理3.0》） Daniel Pink （《驱动力》） （《重新定义公司》） 敏捷模式类库 Scrum 本身就是个模式 《用户故事地图》也是模式 Linda Rising 本课程中的 Scrum 子模式，例如故事泳道、一人天任务、随机一分钟项目经理。 敏捷教练方法类库 Lyssa Adkins 修身类库 《大学》《中庸》《论语》《孟子》 《王阳明全集》 《心经》《金刚经》 《圣经》 阿德勒《超越自卑》 《人生五大问题》 斯蒂芬柯维《七个习惯》 《红与黑》 《基督山伯爵》 《悲惨世界》 《百年孤独》 《活着》 《常识》 而在设计敏捷工作方法的架构时，可以基于上面提到的敏捷框架中的一个或多个。可以使用的思维线索有： 软件开发的阶段：概念，机会，策略，需求，方案，计划，实施，验证，部署，维护，退役。 PDCA 5W2H 在做敏捷工作方法的实施时, 第一步是需求： 与关键人员交流，了解问题与目标 这一步要放下敏捷的代码，倾听了解问题与目标本身。 第二步是制定解决方案： 根据现状，参考敏捷方法，制定关键举措。 使用类库和框架，制定架构。 敏捷工作方法的编码就是用上面的各种类库和框架，生成适合组织和团队的可执行的敏捷方法，包括架构和详细实现。执行的环境是团队中每个人的大脑。 编辑，是把方案细化的过程： 把敏捷方法动作化，做好剧本。无剧本，不操作。 为每一次谈话做好充分准备。 编译，是与团队中所有人交流的过程，使所有人理解敏捷方法： 可以是讨论，针对某个具体变化的方案与执行。 可以是培训，介绍整体或某个环节的工作方法。 可以是一对一交流，让方法切实而不只是形式上发生。 链接，是处理与现状和与已有工作方法的冲突： 分析问题，解决问题。 调整“代码” 运行，是新方法的执行： 落实每一个动作，并检查调整。 编辑、编译、链接、运行会反复多次进行，跟程序员写代码没有区别。 本文作者： &#106;&#111;&#110;&#105;&#x40;&#x65;&#102;&#98;&#105;&#122;&#46;&#x6f;&#x72;&#103;本文链接： https://github.com/efbiz/2018/05/14/敏捷教练第01课-敏捷教练和 ScrumMaster 基本功四部曲/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第02课-储备-Scrum精要","slug":"敏捷教练第02课-储备-Scrum精要","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T01:57:08.352Z","comments":true,"path":"2022/04/12/敏捷教练第02课-储备-Scrum精要/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC02%E8%AF%BE-%E5%82%A8%E5%A4%87-Scrum%E7%B2%BE%E8%A6%81/","excerpt":"","text":"介绍 Scrum 的书虽然还没有达到汗牛充栋的程度，但已经是著作等身了——所有著作加起来能够等同于一个人的身高了。本文并不是对 Scrum 理论的简单重复，而是立意能做到两点： 涵盖 Scrum 中所有重要的概念。 所介绍的方法达到说明书的程度，拿过去就能用。 敏捷产生的历史背景首先简要谈一下敏捷产生的历史背景，以及由 Scrum 及其众多兄弟方法共同抽象出的敏捷宣言。 敏捷产生的历史背景，在于世界变化越来越快。人们不断产生更多更新的需求，技术因此不断进步，两者交相辉映，使得变化越来越快。 以通信行业为例，从 1G 到 5G 呈现出一种升级越来越快的状态。 1986年，作为 1G 标志的使用模拟信号的世界上第一套移动通信系统在美国芝加哥诞生。 1995年，诺基亚崛起，进入数字调制的 2G 时代。 2009年左右，CDMA 大行其道，进入数据传输速率更高的 3G 时代。 2013年左右，伴随移动互联网，移动通信进入网速更高的 4G 时代。 最近一两年，随着 AR、VR、车联网、物联网的诞生，5G 的商用化指日可待。 在这种变化越来越快的环境之下，传统的软件开发方法不再奏效。敏捷先驱们开始探索一些新的方法，对丰田生产方式、组织模式等进行了大量学习，发明了 Scrum、XP 等各种方法论。2001年，新方法论的创始人们聚首一堂，总结了各家方法论的共同点，提出了敏捷软件开发宣言。 敏捷宣言有4个价值观和12个原则，它们也是 Scrum 的基础。对4个价值观要能够背诵下来，对12个原则也要熟悉，以便达到遇到实践情况时能容易对照的目的。我们为您精制了手绘版的敏捷宣言价值观和原则，可以打印张贴备查。 敏捷软件开发宣言 我们一直在实践中探寻更好的软件开发方法，身体力行的同时也帮助他人。由此我们建立了如下价值观： 个体和互动 &gt; 流程和工具 工作的软件 &gt; 详尽的文档 客户合作 &gt; 合同谈判 响应变化 &gt; 遵循计划 也就是说，尽管右项有其价值，我们更重视左项的价值。 敏捷宣言遵循的原则 我们最重要的目标，是通过持续不断地及早交付有价值的软件使客户满意。 欣然面对需求变化，即使在开发后期也一样。为了客户的竞争优势，敏捷过程掌控变化。 经常地交付可工作的软件，相隔几星期或一两个月，倾向于采取较短的周期。 业务人员和开发人员必须相互合作，项目中的每一天都不例外。 激发个体的斗志，以他们为核心搭建项目。提供所需的环境和支援，辅以信任，从而达成目标。 不论团队内外，传递信息效果最好效率也最高的方式是面对面的交谈。 可工作的软件是进度的首要度量标准。 敏捷过程倡导可持续开发。责任人、开发人员和用户要能够共同维持其步调稳定延续。 坚持不懈地追求技术卓越和良好设计，敏捷能力由此增强。 以简洁为本，它是极力减少不必要工作量的艺术。 最好的架构、需求和设计出自自组织团队。 团队定期地反思如何能提高成效，并依此调整自身的举止表现。 Scrum 方法论 我们力求把方法论介绍到可操作的程度，从方便理解和记忆的角度，Scrum 方法论可以被概括为3355 3个角色 PO（产品负责人） SM（Scrum Master） 团队成员。 3个物件 Product Backlog（产品列表） Sprint Backlog（迭代列表） Product Increment（产品增量） 5个会议 Product Backlog Grooming （产品列表精化） Sprint Planning（迭代计划会） Daily Stand（每日站会） Spring Review （迭代评审会） Sprint Retrospective（迭代回顾会） 5个价值观勇气，承诺，专注，开放，尊重。 Scrum 由上述四种要素及背后的规则粘合起来。 3个角色各有担当又通力合作。 3个简单的物件统摄产品层面与迭代层面的交付物。 5个会议贯通产品层面与迭代层面的计划与执行活动。 5个价值观作为方法论的一部分，体现了 Scrum 以价值观为方法论的特色。 3个角色的职责 Scrum Master 的职责Scrum Master 的职责最难讲得清楚。有一个思路是参照卡诺模型。日本教授把产品需求分为三类： 必备型需求：这类需求没有满足，客户不会购买这个产品。 多多益善型需求：这类需求的实现程度与客户付钱的愿望呈线性关系。 喜出望外型需求：这类需求是区别于竞争产品的分水岭，客户愿意付出溢价。 按照这个思路，我们可以把 Scrum Master 的职责分为三类： 必备型职责：协助管理 Scrum 的3个物件和5个会议。 多多益善型职责：与各方沟通和协调问题解决。 喜出望外型职责：系统思考，发现流程和团队工作中的改善点，并推动改善。 产品负责人职责管好产品列表。理解了什么是好的产品列表，也就理解了产品负责人的职责。后面会讲产品列表。 团队职责与传统团队职责不太一样的主要有两点： 跨职能：个人不一定是全能的，但团队要是全能的，具备把产品列表变成产品增量的全部技能。团队成员之间，不受角色和头衔的制约，只要具备能力，每个人都可以认领所有任务。 自组织：团队自行决定自己的工作方式，只要团队有共识。原则上是全员参与估算和计划，全员进行项目进度的监控和调整。 在现实的团队中，有专职人员，也可能有浮动人员，不管是专职人员还是浮动人员，几个共同的基础是：自动化与及时化，每个人都做好本职工作，彼此之间良好配合；每个人都理解团队的产品目标和迭代目标；每个人都了解团队的工作方式和节拍。 浮动人员的类别 一类浮动人员，例如架构师和设计师，有全局影响，但又不是全职参与。有两种变通的参与方式，一是跟专职人员一样，参加 Scrum 会议，二是在团队中指定他的影子，与他密切协作保证他能及时贡献到团队的目标。 二类浮动人员，如固定在两个团队之间共享的测试人员。 减少共享的人数，尽量把测试人员固定在其中一个团队；由有能力多任务的资深人员在两个团队之间浮动领任务，以缓解对其他人员的共享需要；在极端情况下，才让（1）中的人员也在两个团队之间浮动。 三类浮动人员，如在一段时间之内有部分时间花在该 Scrum 团队的人员。与一类浮动人员相比，这类人员的全局影响相对小，更多是因为技能或资源问题而导致的安排。其变通的参与方式与一类浮动人员类似。 四类浮动人员，如尚不能独立工作的新员工。变通的方法是由其导师协助领取任务。 团队之要 在 Scrum Master 的引导和辅导下理解 Scrum 框架，特别是从事情角度的严密的 PDCA 循环，和从人的角度的紧密合作。 以严密的纪律性使 Scrum 能良好运转，达成业务上的目标，并收获高效快乐的团队。 在纪律的支撑之下，技术上精益求精，更好地发挥创造性，包括在技术领域，并适当参与产品探索领域。 Team 在 Scrum 中的活动 梳理 计划 执行 每日检查和适应 迭代检查和适应（评审与回顾） Team 特征 自组织：自组织不能来自命令与控制，而是简单规则支持下的群游。 跨职能团队：多样性，跨职能，不同背景，不同的思考角度，造就更好的产出，更快更好的解决方案、更棒的创新。 一专多能 火枪手精神（互助） 宽带宽沟通（沟通带宽递减：面对面 &gt; 电话 &gt; 即时消息 &gt; 邮件） 透明 团队大小5~9人 专注与承诺 可持续步伐 长期稳定的团队 3个物件 产品列表（Product Backlog）产品列表（Product Backlog）是产品列表项（Product Backlog Item，简称PBI）的列表。PBI 包括特性、故障、技术工作和知识学习。 好的产品列表要满足 DEEP 原则： Detailed Appropriately 细节得当。越是马上要做的 PBI，越是要有足够的细节。很久以后才做的，可以粗略一点。 Emergent 涌现式的。PBI 可以根据实际情况随时插入。 Estimated 有估算的。同样是近期要做的 PBI，估算要较精细，可以采用费波纳契序列的故事点估算，即1，2，3，5，8 …对于远期要做的 PBI，估算可以粗略，可以采用粗略的T恤尺寸估算，即 XS，S，M，L，XL 等。 Prioritized 排好优先级的。近期要发布版本中的 PBI 的优先级要明确排列，中期的可粗略排列，远期的可不必排列。 迭代列表迭代列表由从产品列表中选出当前迭代要完成的 PBI，及由 PBI 分解产生的任务构成。对于任务的估算，可以采用天或小时估算，也可以不估算。采用哪种方式，以团队能够做出靠谱的迭代承诺，以及在迭代工作的每一天方便监测趋势为标准。 产品增量产品增量是一个迭代结束时，输出的用户可用的新功能。产品增量要达到潜在可交付状态，即如果用户需要，可以快速部署给用户使用。 5个价值观 5个价值观的落实与否，是 Scrum 团队形成的重要标志。 对于5个价值观的运用，可以由团队一起讨论，每个价值观分别意味着什么，并进而把价值观转化为可执行的团队规范。利用迭代回顾会议，审视团队规范的执行情况。 5个会议 产品列表精化会目的：准备好。准备好的意思是，经过精化，PBI 达到可估算可计划和可执行的状态。开发人员可以对之进行开发工作了。 流程：主要是围绕 DEEP 标准 细节得当。产品负责人讲解每个 PBI，达到团队成员理解需求的程度。涌现式。在精化的过程中，根据产品负责人与团队的互动，可能会产生新的 PBI。估算。在产品负责人讲完每个 PBI 时，团队可以用估算纸牌进行估算。通过纸牌对话，也可以保证每位团队成员都理解了需求。优先级。优先级主要由产品负责人排列，但团队的估算和实现的难易程度，也会影响产品负责人重新考虑优先级的排列。 辅助物件：为了保证产品列表精化达到准备好的标准，可以制定一个叫做准备好的定义（Definition of Ready，简称DoR）的检查列表。DoR 列表示例如下： 业务价值清晰。 团队了解需求细节，能够做出是否能完成的决定。 依赖被清楚地识别和管理，没有妨碍完成 PBI 的依赖。 团队具备完成 PBI 的技能。 PBI 被估算，并且足够小，能够装到一个迭代里。 验收标准清晰和可测试。 性能指标清晰和可测试。 团队知道在完成后如何演示。 迭代计划会目的：在计划会结束时，给出靠谱的迭代承诺。流程： 产品负责人建议迭代目标和要完成的 PBI。 团队把 PBI 分解成任务。 团队决定是在计划会上就把所有任务分配到个人，还是在迭代过程中动态分配。分配的方式是团队成员认领。要不要分配的标准是，团队是否能对迭代计划进行承诺。 评估计划的工作量与团队容量是否平衡。 从迭代计划中提炼出迭代目标，把 PBI 粘合在一起，把团队团结在一起。 团队对迭代目标和计划进行承诺。 辅助物件：为了对完成有统一的标准，需要完成的定义（Definition of Done，简称DoD）检查列表。DoD 检查列表示例如下： 设计有评审。 代码完成，包括：代码有重构，代码符合标准，有注释，有签入，有评审。 用户文档更新。 测试完成，包括：单元测试，集成测试，回归测试，平台测试，语言测试等。 零已知故障。 验收测试完成。 部署到生产环境。 可以用 A1 纸和便利贴辅助计划会议： Scrum 的两个要点是：人的有效参与，做事的有效轨道。这个计划会的设计，是以有效的轨道辅助人的主动参与。 贴出一张 A1 大白纸。 左边第一列是故事，由 PO 用同一种颜色的便利贴书写和表达。字要大，用白板笔写，保证不用站得很近也能看清楚。故障，新特性或任何要交付的事情统称故事。故事右边，用另一种颜色的便利贴列出任务。任务是为完成故事所要做的事，由团队书写。可以写上任务的执行人与估算。 整个会议，一次围绕一个焦点，即当前讨论的故事。以故事为单位流动起来。 PO 的注意事项：清晰讲述。随着会议的焦点流动，把故事讲得让团队明白。 SM 的注意事项：适度引导。控制焦点与流动，一个故事充分讨论完并分解成任务，再进行下一个。保持紧凑的节奏和整体时间盒。 团队注意事项：主动参与。一是对故事不清楚的主动提问，而是主动参与任务分解、估算、认领。 全部故事讨论完和分解成任务后，统计每人工作量，看工作量与容量是否平衡，个人之间工作是否能置换以达到每个人的工作相对均衡。最后是团队决定是否能对迭代计划和目标承诺。 每日站会目的：围绕目标同步进度，掌握对于完成目标的趋势。流程： 准时开始。每天固定时间和地点。 每人回答三个问题：为了帮助团队达到迭代目标，昨天完成了什么，今天打算完成什么，遇到了什么障碍。 总结趋势和风险。 15分钟之内结束。 站会中细颗粒度的协作： 利用站会，促进细颗粒度协作。 故事和任务拉动按优先级进行。 需要协作的任务，其所有者勇于发起协作请求。 被请求者以协作的任务先于本人可独立承担的任务进行。 在回顾会议中明确讨论该模式，并贯彻。 模式可以由任何一人发现。 关于站会中的发散讨论： 站会中发散讨论的度以全部团队成员觉得爽为标准。 15分钟时间盒不必严格遵守。 Scrum Master 需要对站会之后团队成员的日程有了解，以便判断站会延长一点是否产生影响。 Scrum Master 可以观察对于发散讨论是否全部或大部分团队成员沉浸其中，如果是，暂不打断。 如果出现较多分神现象，打断讨论，并提议会后安排讨论。 或者根据站会的剩余时间，询问团队，这种发散的讨论是否会影响团队成员的后续日程。 按以上原则，打断可以由任何一人提出。 在回顾会议明确探讨这种情景中的模式。 迭代评审会目的：了解过去一个迭代完成了什么，并对下一步做出预测。流程： 产品负责人邀请客户和干系人参与。 团队总结过去一个迭代的成就和克服的挑战。 团队演示完成的产品，获得反馈。 产品负责人分享来自用户和市场的信息，预测调整发布计划，预测下一迭代的内容。 迭代回顾会目的：团队建设，发掘和计划改善。流程： 基调：真诚和有效。排除顾虑，真诚表达。提出有效的问题，落实有效的方案。 白板上写两个栏目：感谢，改善。 每人（包括 PO 和 Scrum Master）用 Post It 写出要感谢的人，每张 Post It 写一个，数量不限。写完贴在白板。 每人（包括 PO 和 Scrum Master）用 Post It 写一个最痛的改善点，只写一个就好。写完贴出来。 Scrum Master 无需太多发言，只需起一个指针的作用。先从感谢的纸条开始，一张一张拿出来问是谁写的，写的人面向要感谢的人表达感谢，不能太空洞，要谈一下感谢的内容。 然后转向改善栏目，Scrum Master 同样不要多说话，一张一张拿起纸条，让写的人讲，其他人可以参与讨论，这个环节焦点放在问题澄清，而不是解决方案，Scrum Master 对这一点要有一定把控。 每张纸条讲完后，所有人当场举手或竖大拇指，表达的是认为这个问题是否要尽快即在下一迭代解决。 全部问题澄清后，全体针对要解决的问题，讨论方案。Scrum Master 关注一下讨论的流动情况，既不要太乱，也不要冷场。极端情况下，可以点名让大家逐一发言，但尽量不用这一招。 产生的方案，形成改善 Backlog。Scrum Master 要跟踪起来，可以在日常或站会中跟踪落实情况。 Scrum Master 要观察团队互动中的交互情况，如果有分歧点，就是改善点。比如说在 Demo 中，PO 对验收标准的理解与团队不一致。这就是需要改善的地方。改善的讨论和进行，可以在日常与相关人员讨论，也可以放到回顾会议。 除了团队的回顾会议，还可以有一对一的回顾会议： 一对一 Retrospective 是对团队 Retrospective 的鼓励和驯化。是为了帮助打磨团队 Retrospective。 一对一 Retrospective 是对团队 Retrospective 的补充。即使团队 Retrospective 已经搞得很好了，也还需要一对一 Retrospective。 一对一 Retrospective 可以由 Scrum Master 发起，也可以由任何人向任何人发起。 一对一 Retrospective 的目的，是加强人与人之间的连接，传递改善的信念，和计划和执行改善。 一对一 Retrospective 的边界，是围绕改善的基调，就与团队项目工作相关的事进行讨论。 一对一 Retrospective 的框架，可以包含探询交流对象对工作方式的反馈、探询痛点和关注的问题，和以 Scrum 实践和角色要求为基准、以观察到的行为为依据向交流对象提供的反馈。还可以包含不同团队之间的经验传递、桥梁和延展。 如果希望痛点和问题的探询更封闭一点，可以分解为几个角度：就团队项目工作的上下文而言，您的目标和期望的理想状态是什么？与现状的差距是什么？流程上有什么问题，或有什么妨碍理想状态的达到？团队合作方面呢？团队工作绩效和质量呢？任何其他方面？ 这个框架的运用要灵活。人的主动参与重于规则。如果人能主动参与改善事项的发掘、计划和行动，框架就可以放下。 Scrum Master 日常有力的观察是 Retrospective 的重要输入。 各个角色的普适标准：专业、尊重、坚持。 改变的第一原则：一切改变基于自愿。改善的用意是改善系统，不是改变个人。 最后用十论 Scrum 就是知行合一对 Scrum 作个小结： 人人知行合一：人人计划，人人行动。 时时知行合一：时时计划，时时行动。 团队知行合一：团队决定，团队行动。 敏捷知行合一：快速决定，快速行动。 需求知行合一：接近客户，掌握需求。 支柱知行合一：检验是知，适应是行。 完成知行合一：定义完成，共识目标。 透明知行合一：高度透明，流畅过程。 纪律知行合一：自我纪律，助长能力。 美德知行合一：积极主动，集思广益。 本文作者： &#106;&#111;&#x6e;&#105;&#x40;&#x65;&#102;&#98;&#x69;&#122;&#x2e;&#111;&#114;&#103;本文链接： https://github.com/efbiz/2018/06/03/敏捷教练第02课-储备Scrum 精要/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第03课-储备-用户故事精要","slug":"敏捷教练第03课-储备-用户故事精要","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T01:56:56.764Z","comments":true,"path":"2022/04/12/敏捷教练第03课-储备-用户故事精要/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC03%E8%AF%BE-%E5%82%A8%E5%A4%87-%E7%94%A8%E6%88%B7%E6%95%85%E4%BA%8B%E7%B2%BE%E8%A6%81/","excerpt":"","text":"本节课程我们主要解决三个问题：为什么要有用户故事？用户故事是什么，它们具有哪些属性、内涵和特征？在产品开发的全流程中怎么使用用户故事？ 理解用户故事理解用户故事，从两个问题开始。 用户想要达到的目标是什么？ 达到这个目标的成本是多少？ 为了回答这两个问题，我们需要把大目标分解为小目标，排优先级和估算。 用户故事是这个过程中使用的基本单元载体。比用户故事大或者模糊的可以叫做史诗，主题或者只是一个产品点子。 关于上面两个根本问题，“@左耳朵”有很好的描述： 亚马逊里挑战产品和运营的方法论是 T-shirt Size Estimation。就是用 T 恤的尺寸来做评估。你要做这个东西可以，但是产品和运营必须得拿出证据说明你要做的这个东西是什么样尺寸的。 比如说 XXL 可以带来一百万人民币或一百万用户受益，XL 就是 50 万，L 是 25 万，这样砍下去，然后这些需求给到技术团队，这边技术团队也会做个评估。XXL 是要六个月做完，XL 是三个月，L 是一个月，M 是两周，S 是一周。这样两边一对。如果说业务影响力比较高，而且技术实施比较轻，这就是小而美，那就是最高优先级；如果是反过来，技术这边要花好大的力气，业务影响力又不是特别高，那就坚定不移地把它砍掉；如果两个都是中等，那就是自由行事；如果两个都是 XXL 的话，那能不能需求这边降低一个维度。比如将 XXL 降到 XL，我这边可以降好几个档次，两三个星期就可以实现出来，这样可能会是一个比较好的方式。 产品愿景是用户故事的统摄产品愿景要回答三个问题： Who：它的用户是谁？ What：它是什么？一是要有清晰的画面感，二是与已有产品不同。 Why：它意味着什么？有什么价值和目的？ 两个产品愿景的例子： 我们要在10年的时间里，把一个人送到月亮上，还要让他回来。 把1000首歌装在口袋里。 产品愿景可以从最初的一个点子开始，慢慢打磨成熟。而在产品愿景打磨的过程中，把大而模糊的点子敲碎成清晰而可执行的小块，就是用户故事。产品愿景与用户故事同生同长，用户故事全部做完了，产品愿景就实现了。 用户建模，是用户故事的起点用户建模可以考虑用户的两点： Pain 痛点：用户在产品目标领域目前有什么痛点？ Gain 收益点：我们的产品能给用户带来什么收益？ 用户角色建模，要以用户为中心。建模的 BSCR 步骤： Brainstorm：产品探索团队头脑风暴出用户角色。 Sort：对头脑风暴出的用户角色快速分组和识别层级。 Consolidate：对每一角色逐一讨论，整合角色。 Refine：提炼角色。识别用户角色的： 用户角色分组可以考虑用户的： 使用频次 领域知识 IT 知识 使用目标 用户建模的其他技术： 典型用户画像。针对一种角色，虚构出一个人物，描述他的行为特征，使用场景，核心诉求。还可以给出画像。打印出来放在团队可见的地方，让团队在开发产品时对用户的需求能感同身受。 极端用户。想象一个极端用户会有什么诉求，以产生新的灵感。这方面不用太用力。 用户故事 用户故事是从用户的角度描述功能。用户故事的 WWW 及格式 Who：谁要使用这个功能？ What：需要完成什么样的功能？ Why：这个功能带来的价值是什么？ 用户故事的格式 英文：As &lt; a user &gt; ，I want &lt; to do something &gt;， so that &lt; I can achieve some purpose &gt;. 中文：作为 &lt; 一个用户 &gt;，我想要 &lt; 做什么 &gt;，以便 &lt; 达成什么目的 &gt;。 例子： 作为一个求职者，我想要发布简历，以便用人单位找到我。 现存世界上第一张用户故事卡，from @张克强。 用户故事的 AC（Acceptance Criteria）验收标准 验收标准也是迭代结束时 Demo 的标准 更多代表外部质量 简洁与完备之间平衡 包含成功路径和失败路径 对前述故事的验收标准例子： 我想要发布 Word 简历 我想要发布 PDF 简历 发前能预览 发后能检查 可以设置公开或私密状态 失败有原因提示 搜集故事的方式 用户访谈：不设定立场，问开放式问题和背景无关问题。 问卷调查：问卷可简单直接，比如了解用户使用软件功能的频率。 直接观察用户如何使用产品。 产品探索团队使用故事编写工作坊，头脑风暴，输出简单原型和用户故事。 用户故事的一些重要特征 一目了然格式一致的表达。 用户故事有两个部分：描述部分和验收标准。描述部分的常用格式是：As &lt;用户&gt;，I want &lt;功能&gt;，so that &lt;目的&gt;。验收标准可以有多条，常用的格式是：Given &lt;前提条件&gt;，When &lt;动作&gt;，Then &lt;结果&gt;。 鼓励推迟细节，只需有足够的信息以使项目前行。 我们不需要一次性把项目的所有需求搞清楚，我们只需要在迭代之前把一个迭代要做的事搞清楚即可，而以用户故事作为基本单元，可以支持这种开发模式。 用户故事以合适的颗粒度，方便理解，方便排优先级，保证重要的事先做。随着时间的推移，不重要的事也许就不需要了。 用户故事鼓励通过交谈了解细节。强调对话而不是书面沟通。 用户故事的验收标准保证成果可以被审核验证。 以用户故事为载体，促进结构化沟通，使谈话有落地点。 从业务角度描述，可以同时被业务人员和开发人员理解。 用户故事的大小适合估算和做计划。颗粒度适合迭代开发。 支持随机应变的开发，检视和适应。 鼓励各方参与交流，传播隐性知识。 用户故事的 3C，鼓励通过交谈了解细节 Card 卡片：用户故事通常写在卡片上，描述用户想要达到的目标，突出对用户有价值。 Conversation 对话：书面文档是一种低带宽沟通，表达信息的能力受限，接受信息的体验受限，信息传递能力受限。面对面交流是高宽带沟通，利用检验，适应和反馈促进沟通的有效性。用户故事模式鼓励对话。无形的对话为有形的卡片补充细节。 Confirmation 验收标准：无形的对话再次落实到有形的验收标准，并成为故事的一部分。 排优先级要考虑的要素 大部分用户对该功能的渴望程度。 少部分重要用户对该功能的渴望程度。 故事之间的关系，是否有开发的先后顺序。 技术实现的难度。 成本高低。 关于优先级的两个模型 MoSCoW 莫斯科定律：Must have 必须有；Should have 要有；Could have 可以有；Woudn’t have 不会有。 Kano 模型：没有不行的功能，线性功能，尖叫功能。 用户故事的 INVEST 准则 Independent：用户故事之间是独立的。如果不独立，可能是划分的方法有问题，可重新划分。 Negotiable：用户故事是可讨论的。使用卡片是为了提醒对话。讨论的细节会变成测试。 Valuable：用户故事对用户有价值。有条件的话，让用户写或确认用户故事。 Estimatable：用户故事是可估算的。不可估算的原因可能是：缺乏领域知识（多与客户讨论，学习对方的语言，了解对方的想法，事实往往比想象的简单），缺乏技术知识（可以把故事分为两部分：探针实验 spike，和实际开发），故事太大(分拆)。 Size appropriately：大小适中。故事的合适大小取决于团队，容量和使用的技术。 需要分割的故事，可能是复合故事（按 CRUD 分拆），或复杂故事（可以分探针实验 spike 和实际开发两个故事，与其他故事一起排排优先级，然后在不同的迭代完成探针实验 spike 和实际开发）。需要合并的故事，包括若干小 bug，或小改动。 Testable：用户故事是可测试的。故事的描述需使用具体的量化的描述，而不是绝对的含糊的描述。 其中有价值和可估算是第一级标准，对应着本文开始时提到的产品开发所要回答的两个根本问题。另外四个标准更多是为了方便迭代开发。 用户故事的一些其他原则 从目标开始，大目标分解成小目标。 切蛋糕式划分故事，竖切而不是横切。 故事的描述要封闭，有完成感。故事针对一个场景，一个画面，一次完整的动作。 约束也可以作为一个故事。 故事的冰山法则：近期要开发的要划分的细而清晰，远期开发的可粗而模糊。 文档也可以作为故事。 故事中要包含用户角色，一个故事只包含一种用户。 为了表述简单，故事使用主动语态。 有可能的话，让用户编写故事。 故事不用编号。可提炼出作为标识的关键词以方便讨论时引用。 一定不要忘记故事的意图。 避免镀金，开发不需要的功能。团队公开透明的工作方式可以帮助这一点。 非功能需求可以作为故事。 缺陷可以作为故事。 故事分级 Epic 史诗：一个很大的故事。 Theme 主题：安放一组类似故事的篮子。 Story 故事：业务角度。 Task 任务：技术角度，一个故事可分解为多个任务。 用户故事分解大故事拆成小故事，9种不同切分方法。 按工作流程步骤切分 是否可以先完成工作流程的头尾部分，再添加中间步骤去完善该故事呢？ 按操作切分 是否可以把不同操作切分成独立的故事呢？（比如是有关“管理”或“配置”某物） 按不同业务规则切分 是否可以先完成业务规则的一个子集，后续再添加其他规则呢？（比如故事中有没有范围型词语像是“灵活的日期”来暗示着多种变化呢？） 按不同类型数据切分 是否可以先处理一种类型的数据，后续再处理其他类型的数据呢？ 按实现先后依赖切分 该故事的实现背后是否依赖另一个流程的数据输入？ 按照体验质量切分 是否可以先完成一个低体验质量的故事，然后再提高体验水平？ 按不同界面切分 是否能先简化复杂界面，先完成一个简单版本？如果多个界面获取相同类型数据，是否可以先从一个界面处理数据，后续再增添其他界面呢？ 按简单/复杂切分 如果简单的核心就能提供大部分价值，是否可以先完成简单核心，再通过后续的故事来扩充它呢？ 延迟性能优化 是否可以先使其工作，后续再满足非功能性需求呢？（当复杂主要来自非功能性需求时） 绝对估算与相对估算绝对估算：按天或小时。 相对估算： 按倍增数列：1，2，4，8，16，32，64 按斐波那契数列：1，2，3，5，8，13，20，40，100 按T恤衫尺寸：XS，S，M，L，XL 估算纸牌 产品负责人讲一个故事 团队提问澄清 团队成员独立出牌估算，互不干扰，喊统一翻牌时才翻牌 出到最小点数和最大点数的人讲讲话 其他人可参与对话 第二轮出牌 对同一个故事不超过三轮出牌 客户参与、产品探索和产品交付的全流程 第一，产品负责人收集需求。利用前文讲到的客户团队和需求收集方法。需求来自客户和干系人，也来自产品负责人的思考。需求以业务角度为主，也考虑技术角度。需产品负责人与技术领域的架构师等密切协作完成需求的梳理。第二，产品探索团队利用故事写作研讨会对用户建模，编写用户故事，放入待办列表，排序，估算，制定发布计划。好的故事估算方法 允许团队随着对产品了解的加深随时改变想法。 要能适用于史诗和小故事。 估算精度随故事变大而变小。 估算快速。 能帮助提供进度和剩余工作的有用信息。 要能做到不精确也没事。 可以用来制定发布计划。 团队集体估算。 故事估算的单位可采用故事点或者理想天。 具体的估算方法 产品负责人拿起一个故事，进行讲解，团队成员提问澄清。 团队成员用斐波那契序列的纸牌出牌估算，出到点数最大和最小的人说明，其他人可参与讨论。独立出牌估算，翻牌前不要把估算讲出来，也不要受他人估算的影响。 大多数故事的估算会在一到两轮出牌后收敛。最多也不要超过三轮出牌。 还可用冒泡分桶法进行快速估算。把故事按从小到大从左到友排成一排，从冒泡法排到大家都认可为止。然后分组分到不同的桶里。桶的大小排列也是斐波那契序列。 每个迭代能完成的故事点数是速率。速率是一个靠谱的指标的依据是：故事的估算值与实际大小的偏差呈正态分布。每个迭代选取的故事的正负偏移可以彼此抵消。因此故事估算的精度不会影响速率指标的可靠性。速率指标可靠的前提是：在项目过程中始终采用一致化的估算方法，迭代之间的故事没有交叉重叠，项目过程中没有发生影响速率的大的异常。 速率和估算的几个注意事项 不要拿速率来比团队。 分解后小故事估算的总合不一定等于大故事的估算。 任务工时的总和不需要跟故事点成正比。 结对编程不影响对故事的估算方法。 做到一半的故事不能计入速率。没有完全完成的事情是不靠谱的。鼓励一件流。迭代完成后不要修订速率。 发布计划的制定 选定迭代长度。 预测速率。 产品的总故事点除以速率，即得出需要多少个迭代完成。 速率预测可采用 历史值 执行一轮看看速率是多少 猜测 可视化发布计划的方法 墙 电子表格 甘特图 第三，与客户确认发布计划。第四，与交付团队梳理故事，达到准备好的状态（DoR - Definition of Ready）。故事要有清晰的验收标准，优先级和估算。团队成员有充分的理解。第五，迭代计划，分解任务，生成迭代待办列表。 在迭代计划会上 选取本迭代要完成的故事。 把业务故事分解为技术任务。分解也是做设计。不能技能的人员可以在同一个故事中合作。 团队做出承诺。 第六，每日工作，按故事和任务的优先级执行。 测量和监控速率的方法 迭代故事点燃尽图。 迭代工时燃机图。 每迭代计划速率和实际速率趋势图。 每迭代计划和实际速率累计图。 这些图表只是团队监测趋势的工具。团队要在每天的工作中检视和适应以保证能达到完成迭代目标的趋势。 第七，迭代验收和演示。最好能有客户参与，以获得反馈。在 Scrum 中，用户故事运转在两条轨道上。一条是产品级别的产品列表精化，既帮助产品的长远规划，也为迭代的开始做好准备。另一条是迭代级别，故事经过精化准备好之后，经由迭代计划会进入迭代，经由每一天的检查与适应，在迭代评审会议上展现为产品增量和可工作的软件，在经过迭代回顾会议探讨下一个迭代如何做得更好。 本文作者： &#x6a;&#111;&#x6e;&#105;&#64;&#101;&#102;&#x62;&#105;&#122;&#x2e;&#111;&#x72;&#x67;本文链接： https://github.com/efbiz/2018/05/14/敏捷教练第03课-储备-用户故事精要/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第04课-储备-scrum的20个子模式","slug":"敏捷教练第04课-储备-scrum的20个子模式","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:00:31.712Z","comments":true,"path":"2022/04/12/敏捷教练第04课-储备-scrum的20个子模式/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC04%E8%AF%BE-%E5%82%A8%E5%A4%87-scrum%E7%9A%8420%E4%B8%AA%E5%AD%90%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"自克里斯托弗亚历山大发明建筑模式，经由设计模式，发展到组织模式。模式是一种经过验证的、经过分类的、可以被反复重用的、场景化、定式化的经验总结。模式思维是理性和感性思维的中和，是人固有的一种能力。模式的好处是省力。敏捷管理，也存在一些这样的模式。本文介绍的模式可分为三大类。第一类是 Scrum 本身存在的模式，第二类是在 Scrum 活动中探索出来的模式，第三类是 Scrum 反模式。 Scrum 中存在的模式 这部分模式可以帮助我们更好地理解和实施 Scrum，以及评估和改进 Scrum 的实施，也能帮助我们理解 Scrum 的发明。 模式名称：Backlog解决的问题： 在本质上偏于不确定性的项目中需要知道下一步做什么。需要一种组织方法使得在项目的任何阶段都易于知道下一步要做什么。甘特图方法事先定义任务和锁定时间，无法满足不确定性环境下的项目管理要求。完全不管也不是一种好方法。 解决方案： 用 Backlog 组织工作。 Backlog 是一个排好优先级的列表。高优先级的工作先做。 Backlog 的总合是产品，Backlog 中工作的完成意味着产品愿景的实现。Backlog 是动态管理的，以便完成的产品是合适的、有竞争力和有用的。 Backlog 的内容来源很多。可以来自市场、销售、技术、开发部门和客户。 只有一个人对 Backlog 排序。这个人对产品愿景的实现负责。 根据市场需要和组织预算，一个或多个 Scrum 团队工作于一个 Backlog。团队从 Backlog 中选择一个迭代可以完成的工作项。 团队在一个迭代中选择的工作要有一定的内聚度，构成迭代目标。团队在一个迭代内的工作，是为了完成迭代目标。 团队把工作分解为任务，以便完成迭代目标。 Backlog 这个简单的物件，上溯产品愿景，下达具体的工作项管理。 达到的效果： 项目中的所有工作根据客户需要和团队能力动态排序。 模式名称：Sprint解决的问题： 工作中涉及探索、创造与测试，而不是简单重复的机械式工作。 需要优化沟通和信息共享。 需要在一个时间盒里承诺完成来自 Backlog 的一定的工作量。 团队需要无打扰地工作，管理层和客户需要看到进展。 预先计划和命令与控制的方式行不通。 解决方案： 给开发人员空间进行创造性的工作，学习探索，做实际的工作，免受外部干扰，运用机会和洞见来优化工作方式。 同时显示真正的进展让管理者和干系人有信心。 以短周期，小团队，来承载 Backlog 中的一段工作，并产生可交付的产品。 聚焦，而不是微管理。焦点有不同层级的颗粒度，迭代层级，是团队和干系人共同关注的焦点。迭代之下的焦点和颗粒度，如每天的工作，更多由团队关注就可以了。 在一个 Sprint 中，外部干扰被排除。团队内外及干系人对这一点有共识。 团队在过程中可调整工作方式。团队以最好的方式，使用他们的技能经验和创造力，专注于手上的工作，产生可见、可用和可展示的交付。 创建安全的时间盒，并向客户和干系人做出可信的承诺。 是团队主动进行的承诺，和对创造性的工作和工作方式的优化。 达到的效果： 所有参与者高度有效而又各有侧重的所有权，包括团队、干系人和客户。 最大可能完成计划的工作。在每一个周期结束的时候，干系人可以影响和调整后续迭代的计划。因而使得长期计划具有极大的灵活性。 每日站会的高度可见性，可以帮助发现浪费并很快回归真正的工作。 为了能够选出迭代的工作，强制产品负责人更好地对 Backlog 排优先级。 不适用于需要高度指导的人。 模式名称：每日站会解决的问题： 需要一个方法来控制经验主义和不可预测的流程，例如软件开发，科研，艺术项目和创新设计等。 对于需要探索、创造性和测试的工作，精确估算是困难的。 过渡计划不会增加可预测性，只会浪费时间。太过详细和锁定细节日程的计划也难以执行。 太过频繁的日常跟踪浪费时间，还会打扰开发人员。 太多跟踪也无助于计划的完成。 解决方案： 团队每天碰面15分钟，回答三个问题：昨天完成了什么，今天打算完成什么，遇到了什么困难。 每天发生在同一时间同一地点，强化团队状态、问题和计划的社会化。 是大家主动参与的一种目标、趋势和风险追踪。 达到的效果： 增加紧迫感。 提升知识分享，社会化，外化，内化和组合知识（参见野中郁次郎的知识管理、场理论和 SECI 模型）。技术专家成为社群资产。 鼓励沟通和诚实。 提升归属感。 转变文化。通过透明，提升大家对目标的关注和主动参与，进而打磨流程，促进从根本上解决问题，建立学习型组织。 高度可见的项目状态。 高度可见的个人生产力。通过同事压力促进卓越。 更少因阻碍造成的浪费。阻碍被及时发现和解决。 更少因等待造成的浪费。通过细颗粒度的协作减少等待。 提升团队内的社交。建立快乐高效的团队。 Scrum 活动中探索出来的模式 这部分的模式是作者在 Scrum 的使用中探索出来的模式，可以根据团队情况试用并调整。 模式名称：团队生物钟解决的问题： 使团队成员更能感受团队工作的节律。 使团队成员之间的合作更有节拍。 使团队从良好惯性中受益。 解决方案： 团队生物钟由团队共同讨论制定和调整。 团队生物钟有两种，每天级别的和迭代级别的。 在每天级别的团队生物钟里，大概包括以下四种时间。 每日站会时间，15分钟。 站会后讨论时间，大概1小时。不是全员参与，话题和人按需设置。 团队核心时间，大家都能找到彼此且欢迎骚扰的时间，可以选取每天的两个时间段。 私密时间，即不欢迎打扰的时间，个人可以集中精力做一下需要专注的事。 迭代级别的团队生物钟也大概包括四种时间。 日常工作时间。 Scrum 的五个会议时间。 Scrum 之外的会议时间，比如知识分享，代码评审等。 发布的准备时间和发布时间。 迭代级别及每日级别的各种生物钟要形成节律。 产生的效果： 各项活动更能准时高效开展。 团队成员有专注的时间做专注的事情。 工作的节律感带来更高的员工满意度。 好习惯帮助减少团队活动和任务间的切换成本，产生效益。 模式名称：会议议程精确到十分钟。解决的问题： 会议目的不明确，流程不清晰，执行不聚焦，效率低。 解决方案： Scrum Master 提前收集议题，设计议程，尽量切割到以十分钟为颗粒度。 确实不能切割的，可以以20分钟或30分钟为颗粒度。对于不能切割的，也可以寻找一些切割的角度，帮助会议的焦点有效流动，即所有人能同时有效地关注一个点，当一个点关注完，流动到下一个点。 在执行每一议程前声明时间盒。 执行时可以根据情况调整，并为下一次议程设计提供经验。 达到的效果： 会议整体效率更高。 在某个小议程，目标更明确，团队更专注。 通过提升会议的效率与效能，提升团队满意度，更好地完成团队目标。 模式名称：产品列表业务精化会与技术精化会解决的问题： 如果只讨论业务，不讨论技术方案，无法进行估算。因此到了计划会上也无法做出靠谱的计划和承诺。 在同一个精化会上既讨论业务也讨论技术，时间不够，而且团队的准备度也不足。 把技术方案的制定留到会后，可能受其他任务的挤压，无法确保能制定出。 解决方案： 把精化会分解为两个，业务精化会和技术精化会。 在业务精化会上，集中精力梳理业务需求。 识别出需要提前制定技术方案的故事和可以直接带到计划会上的故事。 对于需要制定技术方案的故事，指定专人调查，并在稍后的时间举行技术精化会，产生技术方案。 两个精化会的时间都固化。 产生的效果： 业务精化会聚焦于业务，主题清晰，效率高。 技术精化会专注于技术方案的制定。 因此到计划会时可以制定靠谱的计划和承诺。 整体工作过程更加有条不紊，团队对完成迭代目标更有信心。 模式名称：一人天任务解决的问题： 培养当日事当日毕。 帮助发现障碍。 形成流动。 解决方案： 尽量把任务的平均大小向一人天靠拢。 理解这个模式的目的，而不是照搬形式。 任务可以在计划会产生，也可以在每日站会动态产生。 达到的效果： 更好的流动。 更好的风险识别和管理。 模式名称：故事 X 人矩阵解决的问题： 清晰呈现不同人员在同一个故事上的合作。 清晰呈现每人当前手上的任务数。 解决方案： 在白板 To Do 列中，按故事带任务两级呈现，故事从上到下反映优先级，任务从左到右反映大致的先后顺序。 在 Doing 列中，自左到右按人排列。故事与人形成矩阵。 每日站会进行的顺序是按故事泳道，故事驱动。 每个故事涉及到的人讲解和拉动任务。 达到的效果： 更清晰的合作呈现。 更清晰的多任务呈现。 更好的故事和任务流动。 模式名称：随机一分钟项目经理解决的问题： 确保在每日站会中，每人都关心团队目标。 确保在每日站会中，每人都认真听他人说什么。 并以此确保隐性知识传递，团队成员互相学习。 解决方案： 在每日站会最后，以随机方法产生一分钟项目经理。 一分钟项目经理做五点说明。 我们的迭代目标是什么。 趋势是什么，是否可控。 主要风险和障碍是什么，如何管理。 团队士气如何。 邀请全体团队成员对完成迭代目标的信心点赞，以创造一个再次思考风险的契机。并宣布散会。 达到的效果： 站会中团队成员更好的投入。 更好的风险和趋势管理，更好的目标完成。 更好的团队凝聚力。 模式名称：Scrum 风险管理分类解决的问题： 风险分类不清晰，难以确定合适的负责人。 风险分类与角色职责没有匹配。 解决方案： 风险分为三类：与产品和客户相关的，与技术相关的，与社会化相关的。 产品和技术相关的，由产品负责人解决。 技术相关的，由团队解决。 社会化相关的，由 Scrum Master 解决。 风险管理可视化。 达到的效果： 更好的风险分配。 风险管理与角色职责匹配。 有意识的风险管理。 模式名称：知识分享基金解决的问题： 给分享者一点激励，更多代表的是团队的认可。 让所有人，不管是分享者还是参与者，都成为分享的主动拥有者。 解决方案： 由团队或个人众筹知识分享基金，不用太多。 每迭代固定设一个知识分享时间，分享内容不限，只要对团队或项目有帮助。 分享者主动提出分享话题。 如多人都想分享，由多位分享者协调分享时间。 每次分享结束时，给分享者一个小礼物。 选取每次分享的重要照片，形成团队分享相册。 达成的效果： 更好的知识传播。 更好的团队成员之间的连接。 模式名称：同事赞扬解决的问题： 提供一个团队成员互相感谢的机会，把内心的感谢表达出来。 以此加深彼此了解。 并形成一个更有凝聚力的团队。 解决方案： 在回顾会议开始时，增加一个同事赞扬环节。 每人以 Post-It 写下要感谢的人，并把所有 Post-It 放在一起。 Scrum Master 随机抽取一张，让写的人面向要感谢的人表达感谢，包括感谢的原因。 依次过完所有 Post-It。 达到的效果： 更高的士气，更好的团队凝聚力。 更高的产出。 模式名称：卓越驱动的迭代回顾会议解决的问题： 迭代回顾会议缺乏结构与效率。 迭代回顾会议缺乏有利于长期提升的制度化。 解决方案： 团队制定一组卓越指标，例如跨职能团队、价值流、团队工作。卓越指标代表团队为了完成产品目标和迭代目标所最看重的东西。 针对每一指标，制定一系列子项或检查项。 定期更新指标定义。 在迭代回顾会上，把卓越指标打印张贴。 针对每一指标，集体评估团队做得好的地方和需要改善的地方。产生改善措施。 产生的效果： 更系统化结构化制度化的回顾。 更持续的改进。 模式名称：同事评估解决的问题： 在传统的由经理进行的评估中，多数员工是不满意的。 解决方案： 在每个迭代结束的时候，分配给每个团队成员等量的代币。 规则只有一条，就是要把代币送给其他团队成员，自己不能留。至于全部送给一个人，还是平均送给每个人，是当事人自己的决定。 根据组织的形态，可以有两种方式：只有结果公开，和结果和过程（即谁送给谁多少）都透明。 根据组织的形态，决定代币的用途，如只是兑换小礼品，还是兑换奖金。 是否要使用，如何使用，使用的程度，需获得组织和团队的共识。 达到的效果： 同事评估更能反映一个人的真实表现。 Scrum 反模式 这部分是在 Scrum 的使用中应该避免的模式。 产品负责人的反模式 产品负责人在迭代中大部分时间缺席，不能回答团队的问题。 在迭代计划之后，改变故事的范围或验收标准。 对于无法实现或不再有效的验收标准，缺乏改变的灵活度。 产品负责人不能及时对完成的故事提供反馈。 误用取消迭代的权力。 在迭代目标不再有效时，也不取消迭代。 开发团队的反模式 没有 WIP（进行中的工作项）限制。 在遇到阻塞时，开始其他任务，使自己保持忙碌。 任务板不能保持更新。 工作于没有显示在任务板的工作。 镀金，增加不必要的工作。 Scrum Master 的反模式 允许管理者或干系人在迭代中打扰团队，让团队从事与迭代目标无关的事情或会议。 不能支持需要帮助的团队成员。 允许微管理，允许产品负责人向团队分配任务。 没有回顾会议。 Scrum 团队的反模式 有人没有询问团队就把任务加到迭代列表。 不产生交付的迭代。 做不是产品负责人认为当前应该做的工作。 没有紧迫感。 新人加入时，没有接轨计划。 可变的迭代长度。 管理者的反模式 在紧急情况下放弃 Scrum。 经常在团队之间重新分配团队成员。 不经产品负责人，直接向团队分配特定任务。 干系人反模式 偷偷向团队加入任务。 把每件事都当成紧急的。 打扰团队。 模式是一种语言，语言承载思想。Scrum 中的模式承载了“更好”在不同场景中的实现。下一章探讨精益体系，看作为敏捷起源的精益如何在敏捷中发挥作用。 本文作者： &#106;&#x6f;&#110;&#105;&#64;&#x65;&#102;&#x62;&#x69;&#122;&#46;&#x6f;&#x72;&#x67;本文链接： https://github.com/efbiz/2018/05/14/敏捷教练第04课-储备-Scrum 的20个子模式/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第05课-储备-精益体系精要","slug":"敏捷教练第05课-储备-精益体系精要","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:00:21.759Z","comments":true,"path":"2022/04/12/敏捷教练第05课-储备-精益体系精要/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC05%E8%AF%BE-%E5%82%A8%E5%A4%87-%E7%B2%BE%E7%9B%8A%E4%BD%93%E7%B3%BB%E7%B2%BE%E8%A6%81/","excerpt":"","text":"精益是敏捷的重要来源，敏捷对精益作了继承和发扬。 精益的体系浩繁，本文按4＊2的结构进行介绍，即从思想、方法、模式和工具四个层面对精益进行介绍，并在四个层面分别谈及在敏捷中的体现和运用。最后用三纲八目的结构，总结精益中最重要的三个思想及八种落实的方法。 精益学问体系有四个层面： 思想：是大脑，是思维。 方法论：是在宽泛领域看事情的眼睛。是复眼。 解决方案（模式）：是对特定场景问题的总是有效的解决方法。 工具：是在狭窄领域看事情的眼睛。是单眼。 思想就思想而言，涉及到的人包括： 丰田佐吉、丰田喜一郎、大野耐一作为创始祖师。 詹姆斯沃迈克作为研究第一人。 杰弗瑞莱克作为另树一帜的研究人。 思想的应用者： Scrum 的创始人 Jeff Sutherland &amp; Ken Schwaber。 Lean Software Development 的创始人 Mary Poppendieck。 Kanban 方法的创始人 David Anderson。 LeSS 的创始人 Craig Larman。 SAFe 的创始人 Dean Leffingwell。 在方法论领域的精益大师： 约翰舒克，特别是在 A3 报告和价值流图方面把相关知识显式化的功劳。 精益思想精益学问体系的思想，无法简单描述，就脉络来说，大致三个： 丰田屋或精益屋，经由以大野耐一为代表的创始祖师，和后续发展。 詹姆斯沃迈克的精益思想。 杰弗瑞莱克的丰田 4P。 丰田佐吉、丰田喜一郎和大野耐一等人关于丰田生产方式的思想，反映在丰田屋模型中。屋顶代表的是通过最佳质量、最低成本、最短生产周期和消除浪费来为客户创造价值。底座代表的是管理层对创造价值和消除浪费的长期承诺和支持。两个支柱是自动化和及时化。自动化是关于个人自动自发。及时化是关于团队合作。 渡边昭捷丰田之道2001版的丰田屋模型中，两个支柱演变为持续改善和尊重人。新支柱从概念上来讲，适用的范围更广泛，但也存在丢失旧支柱包含的精神的风险。 Craig Larman 写了一本 &lt; Lean Primer &gt;，将精益介绍到软件界，并使之成为其 LeSS 的重要思想基础。Craig 在书中也总结了一个精益屋，跟丰田本身的屋子模型基本相似，只是在术语上采用了软件开发人员更容易明白的术语。 LeSS 的对手 SAFe 也以精益作为基础。SAFe 的创始人在 SAFe 体系中也使用了屋子模型。这个屋子的屋顶是价值，底座是领导力，跟前几个屋子模型一致。不同的是，支柱变成了四个，在尊重人和持续改善之外，增加了流动和创新。流动和创新也是丰田模式中本有的概念。 精益软件开发的创始人 Mary Poppendieck 从精益中提炼了7个原则，应用于软件开发中：消除浪费，增加反馈，延迟承诺，尽快交付，内建质量，赋能团队和全局优化。 精益软件开发中的一部分思想来自于詹姆斯沃迈克总结的精益思想，这也是精益思想的另一个山头。沃迈克作为精益思想第一人，是把精益介绍给全球的主要桥梁。沃迈克经过对丰田的研究，提出了精益思想五原则：价值，价值流，流动，拉动和尽善尽美。 精益中重要思想“流动”的历史，则可以追溯到1574年亨利三世在威尼斯造船时采用的连续流，经由1799年埃里惠特尼的可互换件，1902年丰田佐吉的自动化，1910年亨利福特的流水线，1950年戴明的统计过程控制，发展到后来的丰田生产方式。 精益思想的第三个高耸的山头是杰弗瑞莱克的丰田模式和 4P 模型。杰弗瑞莱克对丰田的研究，不次于詹姆斯沃迈克，被称为最懂丰田的人。 杰弗瑞莱克丰田 4P 模型及14条原则Philosophy 理念 原则1. 管理决策以长期理念为基础，即使因此牺牲短期财务目标也在所不惜。 Process 流程 原则2. 建立连续的作业流程以使问题浮现。 原则3. 使用拉动式生产方式以避免生产过剩。 原则4. 使工作负荷平均（生产均衡化）。 原则5. 建立立即暂停以解决问题、从一开始就重视质量控制的文化。 原则6. 工作的标准化是持续改善与授权员工的基础。 原则7.通过可视化管理使问题无所隐藏 。 People and Partners 员工与合作伙伴 原则8.使用可靠且已经充分测试的技术以协助员工及生产流程。 原则9.培养深谙公司理念的领袖，使他们能教导其他员工。 原则10. 培养与发展信奉公司理念的杰出人才与团队。 原则11. 重视合作伙伴与供应商，激励并助其改善 。 Problem Solving 问题解决 原则12.亲临现场，彻底了解情况（现地现物）。 原则13.制定决策时要稳健，穷尽所有的选择，并征得一致意见；实施决策时要迅速 。 原则14.通过不断省思与持续改善以成为一个学习型组织。 莱克的书中，不提精益，重回本名：丰田模式。沃迈克在后来的一本书，《十年观察》中采用了目的、流程、人的框架，也暗合了莱克的表述。 丰田模式的4P模型和14条原则，揭示了一种真北标准。 具备真北标准的企业具备三个特征： 特征一、具备正义的目的。正义的目的有两层含义，一是以客户价值为中心，二是以正义和不作恶的方式达成目的。 海底捞的使命：服务至上、顾客至上，以创新为核心，提倡个性化的特色服务，致力于为顾客提供愉悦的用餐服务。 丰田父子传承的使命： 丰田佐吉临终前，将丰田喜一郎叫到眼前，给他留下了作为父亲的最后一句话：“我搞织布机，你搞汽车，你要和我一样，通过发明创造为国效力。” 谷歌不作恶使命： 组织全球信息，使人人皆可访问和使用。不作恶口号的提出来自员工。由阿米特帕特尔于1999年提出，或者是由保罗·布克海特在2000年或是2001年初有关企业价值观的会议上提出。这个口号被创始人采纳和推广。Google 2004年的首次公开募股的招股书（又名“S-1”），（Google创始人的一封信，后来被称为“不作恶的宣言”）：“不要作恶。我们坚信，作为一个为世界做好事的公司，从长远来看，我们会得到更好的回馈-即使我们放弃一些短期收益。” 口号最重要不是拿来说和挂在墙上，而是相信自己的口号和拿来使用。在做产品时，如果会伤害用户的利益，虽然有短期收益，也可以根据这一价值观来否决，而且这个否决可以来自任何一个人。 特征二、全员主动参与海底捞在管理上，倡导双手改变命运的价值观，为员工创建公平公正的工作环境，实施人性化和亲情化管理理模式，提升员工价值。 每一个工会会员都必须明白一个基本道理，我们不是在执行公司命令去关心员工，而是真正意识到我们都是人，每个人都需要关心与被关心，而这个关心基于一种信念，那就是人生而平等。 海底捞的内刊上，有两行让人印象深刻的字：倡双手改变命运之理，树公司公平公正之风。在海底捞，员工可以享受一个特权：基层服务员可以享有打折、换菜甚至免单的权利，只要事后口头说明即可。 关于海底捞被人广为称道的细节服务，发圈、眼镜布等，最初只是一个个自发的想法。包丹袋就是这个想法的代表，这是一个防止顾客手机被溅湿的塑封袋子。由于是一名叫包丹的员工提出这个创意的，即用员工的名字命名。“这种命名的方式既能实现他的价值，也是对他的尊重，很多员工都有很多不错的创意，要给他们提供机会。”当包丹袋在其他店也开始使用时，这些店会给这位员工交纳一定的费用。 海底捞这种开放的平台还体现在培养员工的兴趣爱好上。一名员工在和外国顾客交流时，说起了流利的英语，随后公司为此举行了一次英语竞赛，并为优胜者请来了外语老师。“让员工能够发挥自己的特长，从而在工作中获得乐趣，使工作变得更有价值”。 大野耐一：“没有人喜欢自己只是螺丝钉，工作一成不变，只是听命行事，不知道为何而忙，丰田做的事很简单，就是真正给员工思考的空间，引导出他们的智慧。员工奉献宝贵的时间给公司，如果不妥善运用他们的智慧，才是浪费。” 丰田汽车提出“创意工夫提案制度”，对每个员工建议设置500日元到20万日元不等的奖金，优秀建言者的头像会被永久贴上丰田公司的“光荣走廊”。结果，丰田公司在40年间收到了超过2000万个提案，其中99%被采纳。 丰田的创意提案制度强调领导者的参与性和问题的精细化：首先，领导者要对员工进行培训，告诉他们什么是真正的问题；其次，提出的问题具有较强的可行性，员工不需要面对“怎样增支减收”之类的宏观问题，而是具体到“机器之间隔几米能使操作者少走路”“左手应该拿工具还是拿加工半成品”的实操问题；最后，员工不参与工资、考评等领域的建言，以免引起争论与攻击。 谷歌认为，员工应该都成为创意精英（Smart Creatives），才能够使得这个组织产生赋能的效应。 有一个周五下班前，拉里发现了某产品中一个严重的问题。他没有告诉任何人，而是写下了问题，及其影响。拉里把它贴在茶水间，就回家了。 周一早上五点钟，拉里和相关人员就收到一封邮件。邮件不是简单的附和创始人的想法，或是对要解决这个问题的不痛不痒虚张声势的呼吁。相反，邮件中包含了对这一问题根源的详细分析，及对多种方案中最优方案的选择，还有对这一方案的具体执行。而且，还提出了进一步想法，这一想法成了后来一个重大业务的基础。 邮件的发出人只是在周五下班前偶然看到了拉里的纸条，并且从组织架构上来说，他不属于出问题的这个产品。 这个故事的重点不在于是否提倡加班，工作是否需要计划。而在于： 每个人都明白公司的当务之急和价值取向。 一种不急赏不惩罚不嫉妒的文化：这件事做成了，也不会有马上的奖赏。失败了，也不会被惩罚。成功了，也不会被嫉妒。 这个例子，将谷歌文化的力量彰显的淋漓尽致。如果一个企业支持员工有发言权，那么持相同观点的人就会被吸引来，子曰：近者悦，远者来。在企业成立之初就认真考虑和确定你希望的企业文化，是明智之举。创始人是企业文化的源头，创始人为实现大计而物色并信赖的团队，是企业文化的最佳载体。问一下你的团队：我们重视什么？我们的信念是什么？我们想成为怎样的企业？ 特征三、形成一个具有持久生命力的系统。勉强给企业真北标准下一个定义，大致是：企业有一套理念，这个理念把客户，员工，企业发展，甚至造福社会等各种要素组合在一起，充分考虑各方面的福祉，持续优化形成卓越运作，达到企业的基业长青和个人的幸福。 这样一种管理理念的主要思想是：管理决策的制定以最终的目的为前提，眼光放长远，有一套处理问题的步骤，通过培养员工来为企业增值，并且认同持续不断解决根本问题会促进整体学习的观念。 如果把上述各个流派的精益思想做个总结，可以概括为以下几点： 强调目的和价值 强调领导者的作用 强调对人的尊重 强调流程 强调问题解决和持续改善 所有这些是精益的要点，也是敏捷的要点，而且所有这些点要有机结合，而不是孤立存在。企业要想通过敏捷转型获得竞争优势，需要对精益体系有完整的理解。这些都是精益和敏捷的元认知，敏捷教练可协助管理者理解。 精益方法基础的重要基本概念什么是价值？ 客户愿意为之付钱。 是一种产品或服务的形态发生改变的过程。 这种改变必须无浪费地做对。 什么是问题？ 标准没有达到。 旧的标准达到了，又提出了新的标准。 标准没有稳定地达到，有时能达到，有时不能达到。 什么是浪费？TIM WOODS 八大浪费： Transportation 运输的浪费。 Inventory 库存的浪费。 Motion 移动的浪费。 Waiting 等待的浪费。 Over production 过度生产。 Over processing 过度加工。 Defect 缺陷也是一种浪费。 Skill unused 未使用的技能。 从更高的视野看什么是浪费 3M： Muda 无驮，就是一般所说的浪费。 Muri 无理，比如说人或设备过载。 Mura 无稳，比如说有时忙有时闲。 浪费管理的基本思路，跟把大象请出冰箱一样简单。分两步： 识别浪费。 消除浪费。 这些关于价值、浪费与问题的思想，都是非常深刻的认识，在敏捷当中也是同样适用的。敏捷教练可以把这些思想，运用到问题解决和持续改善中。 ###把思想、方法论、解决方案（模式）、工具组合在一起： 定义价值。可以使用： 增值工作 八大浪费 3M 观察整个价值流。可以使用：价值流图 让增值步骤流动。可以使用： 目视管理 5S 防呆法 节拍时间 标准工作 连续流 单元式布局 产线均衡 快速换模 全员生产维护 分层流程审计 让客户拉动。可以使用：看板 持续重复前面的步骤。可以使用： 反思 经验分享 这个价值流分析，可以直接运用到软件开发的管理中，所使用的具体工具可以变通。David Anderson （《看板方法》）的重要基础就是价值流。 精益方法论方法论是解决问题的方法，重点介绍 TBP 或 A3 报告。 TBPTBP：Toyota Business Practice，中文可以叫丰田问题解决法，是A3报告的一种逻辑框架。A3 报告参看在丰田工作过的精益大师 John Shook 的 &lt; Manage to Learn &gt;。 TBP 体现了质量大师戴明的 PDCA。美国人一般把这个方法叫做 Practical Problem Solving，即实践的问题解决法。 TPB 中的基本意识 客户至上 经常自问自答“为什么” 当事者意识 可视化 根据现场和事实判断 彻底地思考和执行 把握速度和时机 诚实正直 全员参与 PDCA/TBP/A3/Practical Problem Sovling 为什么这么好？依然可以用 4P 解释： Purpose：从目标出发。 Process：是一步一步紧密相扣的闭环和螺旋式上升的问题解决方法。 People：平等积极参与。 Problem Solving：彻底的问题解决。 A3 在丰田就是一种尚方宝剑，可以穿越等级和部门墙。TBP 八步详解PDCA 扩展为丰田八步问题解决法： Plan 计划 第一步：澄清问题。 第二步：分解问题。 第三步：设定目标。 第四步：分析根源。 第五步：制定措施。 Do 执行 第六步：贯彻措施。 Check 检查 第七步：评估结果。 Act 行动 第八步：标准化。 下面我们来逐一解释一下。 第一步：澄清问题。 通过问 5W2H，形成清晰简洁脚踏实地的问题描述。注意这里的 5W2H 是从现象级来问，不是问原因。原因在后面，不要着急，慢就是快。 问题是什么？ 它是在哪里发生的？ 何时发生的？ 谁会受到影响？ 为什么这是一个问题? 影响有多大？ 发生的频率如何？ 第二步：分解问题。 可以画一个流程图，按流程分。 可以按影响因素的重要程度分，参考二八原理和柏拉图。 第三步：设定目标。 重新回顾什么是问题：理想状态是什么，现状是什么，理想状态与现状的差距就是目标。目标的设定可以是更高目标，或稳定化的目标。 第四步：分析根源。 可以采用鱼骨图，五个为什，主效应图等。 第五步：制定对策。 对症下药。 第六步：执行对策。 坚持，不妥协。 第七步：评估效果。 可以采用控制图，箱体图（控制图的变种）等。 第八步：形成标准。 在敏捷当中，可以把 A3 方法作为一个问题解决工具来使用，以打造彻底解决问题，持续改善的学习型文化。 其他问题解决方法的简要描述如下： DMAIC 方法：用于复杂问题 Define 定义 Measure 度量 Analyze 分析 Improve 改善 Control 控制 DMADV 方法：用于创新问题 Define 定义 Measure 度量 Analyze 分析 Design 设计 Verify 验证 8D 方法：用于质量问题 D1：定义问题 D2：建立团队 D3：抑制问题 D4：调查与根源识别 D5：纠正问题的措施 D6：预防问题的措施 D7：实施与评估 D8：可持续性评估 精益工具工具是对事物的相对简单的抽象。工具可以被方法论调用。 精益工具主要有： 流图 Flow Chart 柏拉图 鱼骨图 五个为什么 控制图 箱体图 主效应图 特别说一下五个为什么。问为什么的三个角度： 为什么故障会发生？ 为什么故障没有被检测到？ 系统中有什么漏洞？ 精益工具可以在敏捷软件开发中有选择的使用。 精益解决方案（模式） 模式是方法论的产物，是针对特定场景的解决方案。 5S Sort 分类 Set in order 排序 Shine 清扫 Standardize 标准化 Sustain 持续 目视化管理的三个层次 Visual order 可视化顺序。 Visual display 可视化展示。 Visual control 可视化控制。 创建一件流的步骤 化身为物。 让它流动。 持续流动。 标准化的原因： 不稳定制造了浪费，并且阻碍了改善。 标准化在改善循环中的地位 标准化-〉暴露问题-〉解决问题-〉实施新方法 节拍时间 有效工作时间/需求数量。 标准工作三要素 周期时间 工作顺序 在制品数量 防呆法 例如，能够使用下拉菜单的，就不要让用户填写。 快速换模 持续构建是快速换模思想的体现。 精益当中的解决方案或模式，并不能直接运用到软件开发中，但仍然可以给我们很多启示。 重温一下精益知识体系的四层：思想，方法，工具，模式。这种思维体系本身也与我们的敏捷教练基本功相一致：我们要掌握敏捷的核心思想，通过教练方法，调动教练工具，来生成和普及 Scrum 模式。 精益三纲八目 最后，按照精益三纲八目的结构做一下总结。 精益当中最重要的三条思想，成为精益三纲： 从内在的生存动机来说：适者生存，持续改善，更高标准 从客户价值来说：专注价值，消除浪费 从方法来说：身临现场，科学方法，快速反馈 敏捷转型，需要企业上下对这三点有一致的认知。 而实现这三条思想的具体方法，可归类为八种，成为八目： 因业果关系：偏因的鱼骨图，五个为什么，柏拉图；偏果的关系图，箱体图，主效应图，控制图；偏解决方案的流图。 结构化上升：PDCA，A3，DMAIC，DMADV 可视化：价值流图 简化：5S，防呆法，单元式布局，快速换模，全面生产维护，逐层过程审核，纸芝居 流：价值流图，3M，八大浪费，一件流，看板，反思，分享 均衡化：3M，产线均衡 标准化：标准化，标准作业 稳定化：节拍时间 这八类方法的每一类，都体现着一种智慧。大部分都可以运用到敏捷中。 本文提供了两种理解精益的思路，思想-方法-工具-模式和三纲八目。这是对精益知识体系的完整概括。其中很多可以通过常识思考和理解运用。更深入的了解，参看第一章推荐的书目。 本文作者： joni@efbiz.org 本文链接： https://github.com/efbiz/2018/05/14/敏捷教练第05课-储备-精益体系精要/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第08课-技巧-敏捷教练的六脉神剑（下）","slug":"敏捷教练第08课-技巧-敏捷教练的六脉神剑（下）","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:07:19.557Z","comments":true,"path":"2022/04/12/敏捷教练第08课-技巧-敏捷教练的六脉神剑（下）/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC08%E8%AF%BE-%E6%8A%80%E5%B7%A7-%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%9A%84%E5%85%AD%E8%84%89%E7%A5%9E%E5%89%91%EF%BC%88%E4%B8%8B%EF%BC%89/","excerpt":"","text":"敏捷教练的六脉神剑针对敏捷团队的工作方式提供了六个不同的教练角度，前三剑针对的是更基本的场景，后三剑针对的是更特定的场景。敏捷教练需要对六个角度谙熟于胸，并针对不同的场景混合运用。大多数情况下，场景是可预测的，可以以六脉神剑为基础，制定自己的脚本和台词。在每一次的教练行为中，目的是第一位的，首先是确定目的，然后才是确定方法。除了每一次教练行为的设计，还要对团队完整工作周期的教练计划进行设计。敏捷基础与六脉神剑是设计的基石。 对六脉神剑还可以做另一角度的分类理解，即主动型和应对型。主动型即主动主导的行为，应对型即当一个情况发生时的应对行为。从这个角度看，指导、协助、讲授和协作指挥更多是主动型的行为，问题解决和冲突导航更多是应对型的行为。但主动的行为也可能是由场景触发，而应对的行为也需要有预案和主动探询。主动的行为更多是前馈，应对的行为更多是反馈。主动的行为更多发生在正式的仪式中，应对的行为则更多在日常随机发生。主动和应对都需要计划和练习。 这一章的使用方法跟上一章的使用方法一样，提供的是凝聚了前人智慧的检查列表，可以作为我们大脑的扩展。使用时，首先是事先学习，设想在教练工作中可能遇到的场景，针对每一场景可以有什么样的预案。第二是，在每一活动前，根据这些检查列表，制定自己的剧本或脚本，以便在活动中使用。第三是事后回顾，在每一活动之后，再来阅读这些检查列表，什么事自己做得好，什么事还可以做得更好。经过反复演练这三步使用，把这些方法内化成自己的方法，并能进行扩展和创新。 第四剑：问题解决 敏捷问题解决规则 一个问题引起了你的注意，或者你觉察到了一个问题。这个问题可能来自观察，也可能来自团队的反馈。 暂停一下，思考该问题，把问题看清楚。问题的真相和本质是什么。 团队拥有对问题的最多理解，并且对问题的解决负有责任，教练的责任是协助团队，把团队纳入问题解决和变成问题解决者。 允许团队应对解决或者不应对。解决问题以及是否制定计划和采取行动是团队的决定和承诺。 传授解决问题的框架，让团队成为解决问题者，是教练的目的。 发现与寻找问题 要抵制立即解决问题的诱惑。除非人的基本尊严受到侵犯，对于其他情况，并不需要立即处理。 识别是否是真实的问题。不为虚拟的问题进行虚拟的讨论。 流程层面的问题：我们在敏捷方面做得如何？可以采用敏捷成熟度或健康度检查列表，推荐 Mike Cohn 的敏捷成熟度模型和 Spotify 的敏捷健康度检查。成熟度或健康检查中，对问题的评估只是个触发，团队的对话才是最有价值的部分。 质量和绩效问题：团队如何可以做得更好？通过制定完成的定义，并在每个计划会上完善，在每个评审会上检查，来持续提升质量。 团队的动态问题：团队如何可以变成一个更好的团队？ 扩展：团队动态问题之艾伦-布劳恩的团队动态调查 在团队每天的交互中有多少幽默的成分？ 在出现困难和高压时，团队最初的行为是什么？ 团队成员出现抵触情绪有多么频繁？ 当团队成员有抵触情绪时，他们之间多久会进行一次充分的讨论？ 基于团队规范，在平常的交互中团队成员做出妥协有多频繁？ 任何一位团队成员向任何其他一位团队成员能提供什么样的反馈意见？ 任何一位团队成员向任何其他一位团队成员真正提出了什么样的反馈意见？ 一位团队成员与另一位团队成员讨论你的绩效和行为，而不是直接向你或者在第三方在场的情况下提供反馈意见，可能性有多大。 就个人的职业目标而言，你从团队中得到了多大程度的支持？ 在处理一个工作问题时，需要承认有问题并需要请求团队成员帮助，这种情况有多少？ 当你向团队分享个人信息时，你感受到攻击的可能性有多大？ 当一个问题可能引发团队内部的冲突或争论时，团队有多大可能性把这个问题带到团队中讨论？ 当一个问题可能有很多个冲突点时，你有多大可能把这个问题带到团队中讨论？ 如果一个工作项可能引发很多不同的冲突，你把它带到了团队中，并且团队达成了可行的共识，这种可能性有多大？ 在过去两个工作日中，你能举一个例子说明你在团队中感受到温暖和包容吗？ 在过去两个工作日中，你能举一个例子说明你在团队中感受到被轻视或排斥吗？ 团队在多大程度上让你觉得你对自己的工作是负责的？ 扩展：团队动态问题之 BART（边界、权力、角色、任务）分析角色 在你的敏捷框架中，所有正式定义的角色都有人员各就其位吗？ 是否所有正式定义的角色都在角色边界之内运作良好？ 是否有人承担的正式角色多于一个？ 如果团队增加了额外的角色，这些角色的描述是否充分和清晰？ 任务 团队成员对于他们的团队目标是否清晰？ 是否有为了完成团队目标所需的所有不同任务？ 人们从先前相似的情况中引入了什么样的历史和过去的经验？ 权力 对于每一个角色，它的权力是否被清晰地制定，并被所有人理解和支持？ 团队成员是否在适当地行使权力？ 边界 人们是否在被赋予的敏捷角色的权力边界内工作？ 在迭代期间，团队成员如何赋予另一个团队成员获取任务的权力？ 在团队中各种各样势力的边界的什么？ 清楚地看待与思考问题 留到第二天解决：让脑子放轻松，看看第二天起床的时候是否已经有了答案，也许一些更深入的事情或者重现敏捷本质的事情会出现。 向自己提问：如果在这个世界上我可以做任何事情，这个事情是什么？这里的风险是什么？如果这个情况已经很好地解决了，解决后的情况会是什么样的？ 与另一位教练结队：从一位旁观者的角度得到建议，或者一些挑战性的问题。 直奔源头：复习敏捷宣言及其背后的十二条原则。 解决问题 直接解决它：说出你看到的症状，抛出你的假设，询问团队，对于这个问题，他们想要做什么。 重申敏捷的含义：例如，对于每个会议的目的和流程，重回 Scrum 指南。 揭示体系本身：把团队比作一个生态体系而不是机器，通过观察和探究揭示这个系统本身，并向团队分享。 善用回顾：让团队以不同的角度思考他们一起工作得如何。 增加一种启示方法：例如，增加一面痛苦墙，让团队把遇到的痛苦记录下来。 第五剑：冲突领航冲突中敏捷教练的角色 敏捷团队要长期在一起，冲突解决尤为重要。 教练需要帮助团队排除冲突，提供解决冲突的方法、指南和框架。 教练需认真决定是否要、何时及如何介入冲突管理。 冲突的五个级别： 第一级，解决问题。人们有不同的意见，也许误解已经出现，目标与价值的冲突也可能存在，并且团队对于这种冲突的氛围感到焦虑。在这一级里，团队关注问题解决，信息分享和协作流畅，语言是开放和基于事实的。 第二级，争执。自我保护变得跟解决问题一样重要。团队成员之间产生距离感，壁垒在强化。个人保护胜过协作，语言是戒备的但允许解释。 第三级，争辩。在第三级，目标就是赢得胜利。多个问题汇聚成更大的问题，在这个存在误解的环境中出现派系。问题和人成了同义词，人们已经不能只对事不对人。胜利重于解决，语言中包含个人攻击。 第四级，圣战。人们相信，唯一的选择就是让对手退出。保护自己的族群成了焦点，语言呈现出意识形态的特征，而不是针对具体的问题和实事。 第五级，世界大战。毁灭是这一级的战斗宣言，胜利已经不够了，对手必须输。很少或没有语言交流，唯一的目的是摧毁对方。 确定团队的冲突级别 聆听抱怨：带着同情心去聆听，接受抱怨者所说的事情，让抱怨者知道你在关心他，并正在花时间了解冲突的方方面面。 感受活力：大家是在充满活力地协作？还是彼此梳理漠视？团队的精气神是积极向上，还是消极冷漠？ 关注语言：通过大家说什么和如何说，判断团队的冲突级别。 扩展：冲突级别与语言 第一级，团队成员开放且建设性地参与冲突。语言例如：我听到你说了，但我认为你忘记了一个事实；我知道你的想法，但我不同意，因为。。。 第二级，对话变得偏于自我保护。语言例如：是的，我是弄坏了这个构建，但在我们团队中有比这更严重的问题。 第三级，扭曲的语言出现了。例如：他们总是走捷径，他经常控制每一次对话。 第四级，变得更加主观。例如：他们永远不会改变，我们是对的。 第五级，充满了斗争。例如：要么是我们，要么是你们，我们必须赢。 应对冲突 首先，什么也不做。在决定干预之前，花一些时间观察团队的行为，看他们在处理冲突上是否有进步。 分析和应对，考虑以下问题：冲突的级别是什么？问题是什么？作为 A 方，我该如何回应？作为 B 方，我该如何回应？有哪些分解冲突的选择？如果有什么我应该做的话，那会是什么？ 使用架构：使用敏捷的精髓如原则、价值观和角色定义来排除冲突。平衡任务导向和人际关系导向。 揭示模型：揭示冲突层次模型，邀请团队加入对话，探讨如何降低冲突级别。 扩展：冲突应对模式 第一级，解决问题。协作寻求双赢，了解每位团队成员的想法，达成共识。 第二级，争执。允许对方解决问题，任何事情以心理安全为基础。 第三级，争辩。关系比问题本身更重要，先接受对方的观点。对于可以分解的问题，进行交涉。收集数据，建立事实。 第四级，圣战。再次构建安全框架，居间传递想法，降低冲突级别。 第五级，世界大战。做任何必要的事情防止大家受到伤害。 化解团队成员之间的抱怨 第一，建议抱怨者跟被抱怨者直接提及顾虑和感受。 第二，跟抱怨者一起找被抱怨者交流。 第三，征得同意，居间转告抱怨。 如果这三招都不灵的话，最后一招是：不再把它作为一个问题。 面对无法解决的冲突 敏捷团队是亲密的。增加团队成员之间的良性互动，而不是关注于解开并解决问题。优秀团队中积极交互与消极交互的比例是三比一到五比一。 避免误解形成。团队学习聆听和互相关注。 确认允诺和共识。确认所有的声音都被听到，并显式确认允诺和共识。 利用共享愿景：尽管团队通常是由经理们创建并通告团队的，团队在一起也没什么共同梦想，但团队依然可以围绕产品和项目生成共同愿景。 关于冲突的最后招式 了解冲突框架，选择应对方式，处理抱怨和增加团队的积极性，这些技术自身并不能解决冲突。 只有这些技能和思维方式在团队的行动中得到体现时，神奇才会发生。 保持对话的基础存在，冲突才有被解决的可能。 第六剑：协作指挥 从合作到协作 合作的效果是各个部分之和。而协作是整体大于个体之和。 对于不需要创造性的活动，可以只合作不协作。 协作是以合作为基础，但增加了产生创造性、突破性和意想不到结果的重要元素。 在协作中，每个人都会在他人想法的基础上思考，敢于分享和评议，让那些更好的、个人无法独创的想法浮现。 在协作之前先形成合作，每个人集中精力演奏好自己的乐器。 教练作为协作指挥者，加入沉默的人员一方，消除高声音者的优势，鼓励每个人变得更自信。 让团队通过不断练习，达到不需要指挥者的境界。 培养协作个体 传授他们合作技巧：建立响应能力，沉默即暗示同意，运用否决权，寻找其它方案，反思有意识意图与无意识意图，带着同情心说出真相。 引导他们提前准备：在协作活动时提前做好准备，例如，在每日站会时身心都准时到达。 鼓励他们表现自我：培养自我价值，但排除虚荣心。 建立协作态度：对自己的生活环境负责，而不是把责任推给他人；不设防，而不是有保留地进行响应；坦然响应，而不是在需要响应时感到威胁；尝试双赢，而不是防御；寻找解决方法，而不是泾渭分明反应强烈；劝说而不是责备；坚定而不是对抗；既考虑长期也考虑短期；对他人观点感兴趣；欢迎反馈；把冲突看成人类环境的天然成分；平静而直接地谈论艰难的问题；为自己的行为后果负责；持续寻求更深层次的理解；传达关切的态度；寻求卓越；聆听。 培养团队的协作肌肉 说出不能说的话：敢于暴露弱点，遇到困难时可以求助。 建立而不是打压想法：对过失展示容忍，制止运用惩罚，讨论和接纳不同的观点产生合力。 听所有的声音：提升安静的成员并抑制占优势的成员。 培养协作中的亲密关系：团队成员能够感知其他人对新素材的理解程度，迅速介入澄清，使团队级别的理解更快地达成。 一起认真做游戏：例如计划纸牌。 不断提醒他们直到会用。 揭示协作的真谛 要创新，协作不是唯一的方法，但它是最直接的方法。 协作发生在当下，只有当你这样做时，它才存在。 要协作，必须了解你和你的协作同伴为汇聚带来了什么。 对你的工作充满热情是协作的前提。 如果你遇到一个问题，需要别人做出改变，那么你尚未真正了解你的问题。 本文作者： joni@efbiz.org 本文链接： https://github.com/efbiz/2018/05/23/敏捷教练第08课-技巧-敏捷教练的六脉神剑（下）","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第09课-技巧-敏捷教练的提升三式","slug":"敏捷教练第09课-技巧-敏捷教练的提升三式","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:19:55.574Z","comments":true,"path":"2022/04/12/敏捷教练第09课-技巧-敏捷教练的提升三式/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC09%E8%AF%BE-%E6%8A%80%E5%B7%A7-%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%9A%84%E6%8F%90%E5%8D%87%E4%B8%89%E5%BC%8F/","excerpt":"","text":"在经历了四种心法和六脉神剑的修炼之后，敏捷教练的路还要往前走。本文介绍敏捷教练的提升三式： 成败模式 技能清单 教练之旅 提升第一式：成败模式本节介绍敏捷教练失败的种种陷阱，和成功的许多要素。注意在教练过程中的错误模式，并有意识地选择不掉进它的陷阱里。越少掉进失败模式的陷阱，你就越有时间注意到你在指导过程中出现的好的事情，即成功模式。 敏捷教练的失败模式 侦探。花不多不少的时间观察团队，然后便生成并带着下次回顾会议的话题，消失在黑暗中。 海鸥。猛然扎进站会中，用善意的观察和建议冲击整个团队，然后便飞走。 武断者。经常表达各种观点，固执己见，不惜放弃指导团队进行有意义的讨论所需要的客观性。 行政人员。通过充当不必要的会议后勤、访问权限申请和其他行政事务的中间人来削弱团队的自主权。 轴心。充当团队成员间所有交流的中心，并进行任务层面的协调。 蝴蝶。在一个个团队周围掠过，停留的时间仅够传授一滴慧珠或提出一个富于哲理的问题。 专家。深入地进入团队工作的细节中，以致因为树太多而不见森林。 唠叨者。好心地提醒团队开始站立会议、更新状态墙、按时完成承诺的任务等。 失败模式的来源 失败模式起源于教练的自负或持续的局部关注。 当以我为主的想法无节制地蔓延时，就会很容易地转化为以我为中心的状况。 以我为中心，就不能给团队留出足够的空间来领会可能发生的变化、提出新的观点，让他们意识到能够真正做到多好。 当敏捷教练指导多个团队或因为其他情况分散精力时，通常就会出现持续局部关注的现象。 从失败模式中恢复 方法很简单：用信任取代担心。 要对团队成员寄予信任，相信他们真正知道去做正确的事情。如果他们失败了，也会从中吸取教训并变得更强大。 敏捷方法的框架中有各种内置的机制来帮助你拥有信任感，因为他们鼓励和容许犯错误。固定时长的短迭代确保大家不会失利得太远或者造成影响深远的后果。 关注团队中真正发生的事情和试图发生的事情。 信任加关注就是好的教练方法的基础。 培养觉知。让头脑中的噪音沉寂，以便能够思考，和可以进行清楚的观察。为自己的头脑腾出一些空间，有意识地对自己进行调整，以了解团队所需的东西而不是自己内心发生的东西。 保持好奇。对团队正在进行的事产生好奇，并产生清晰理解。 拓展视角。在一个更大的时间轴和更宽的框架内看团队的当前状况，团队的缺陷就只是一片有趣多变的环境中闪现的一个略带瑕疵的小点。回到团队的共享愿景宣言：同饮同甘共苦之水。精神振奋，让不良片刻远离。 结对合作。当你感觉到失败模式在控制你时，与同侪结对。他们协助你重申目标，牢记于心，把以自我为中心的想法抛到一边，聚焦于你的关注，并准备用信任的心态进行指导。 练习成功。成功需要练习。自我提炼或借用他人的成功模式，不断练习，直到你对它们的感觉比失败模式还要自然。 敏捷教练的成功模式 魔术师：问这样的问题，看看什么东西在那里，但刚才看不见。 儿童型：诚恳又惊讶地问为什么，并对生活和其中的一切有永不满足的好奇心。 长耳朵：听所有的东西，但不响应所有听到的，以给别人发展的空间。 好质问者：以轻松有趣和稍微失衡的方式，把别人从自满中摇醒。 大智若愚者：提出粗浅的问题来启发大家。 蔓延的葡萄藤：通过团队基本感觉不到的小步移动，无情地将团队一点一点拉回敏捷的核心。 梦想家：勇敢地说出未来可能创造出来的东西。 扩音器：确保所有的声音都被听到，特别是被压抑的声音。 提升第二式：技能清单敏捷教练之路没有终点。敏捷教练只是在不断学习并把各种新的技能汇集到我们的指导过程中，为我们团队的辉煌而努力。本节提供了一些线路标志来帮助你在敏捷教练之旅中确定方位。这些标志符即敏捷指导的技能和行为，可表明你仍然处于通向好的敏捷教练的诸多途径之上。 技能之缓慢地灌输敏捷实践 帮助团队从敏捷实践中收到预期的收益。 在考虑产品构造方案，决定其如何相互协作时，团队能够坚持从敏捷宣言价值观和原则出发。 在改变敏捷实践时，确保敏捷宣言和检查调整环路完整无缺，并视之为团队至关重要的法宝。 技能之启动敏捷团队 懂得启动团队的目标，并尝试用各种方法和活动来达到这些目标。 目标包括计划和执行启动活动，并根据这些活动的结果调整后续活动的执行方法。 需要知道如何实施团队启动才既符合敏捷特征又对团队有价值。 技能之一对一指导团队成员 能够轻松自如地进行一对一指导，并且被指导的成员感受到自身的变化。 认识到每个成员处于敏捷转型的什么阶段。 激发每个人为了成为优秀的敏捷开发人员而愿意采取行动。 技能之指导整个团队 把自己想象成敏捷的清道夫、领头羊、管家、质量和绩效的看护者。 确保团队在一段时间内只专注于一个迭代目标。 密切关注团队的日常交流，确保他们是在真正协作并朝最简单的方向努力。 指出某个破坏性行为，使得这种行为下次出现时，其他人也有勇气指出来。 当团队忘记他们的共享愿景时，进行提醒。 帮助团队朝健康的敏捷团队方向前进，最终产生他们引以为荣的结果。 技能之指导产品负责人 指导产品负责人与团队交互。鼓励产品负责人以正面的方式与团队交互，约束那些损害团队自组织的行为。 指导产品负责人实践商业价值驱动的思维方法。确保团队只做那些用来创建优秀产品的事情。 指导产品负责人创建、整理和使用产品列表。 指导产品负责人帮助团队排出障碍。 指导产品负责人管理干系人。产品负责人不断与干系人一道工作来了解他们的需求，将它们转化为唯一的明确的声音。 技能之指导团队外部干系人 与项目发起人、经理以及团队之外的干系人进行指导性交谈。 为他们与团队可能发生的有用和有害的交互制定法规，帮助干系人了解他们如何能够最好地支持团队的动力和结果。 教会他们如何利用敏捷来仅构造最本质和最有价值的东西以实现竞争优势。 技能之在变化中指导团队 帮助团队走出来自变化或处境艰难的失望，提出新的计划来恢复团队的技能和活力。 当团队前进时，总会有事情把他们击退回来。 通过运用敏捷原则、实践和价值观，给团队指明重新站稳脚跟的方法。 技能之激发通向高绩效的途径 指导团队取得越来越高的绩效。 在团队中激发出来一条通道，让团队把通往高绩效的旅途掌握在自己手中。 为团队中的每个人，团队整体，他们构造的产品以及公司带来成果。 技能之接受团队比你更好的想法 愿意让你的决定和观点屈从于团队的观点和决定。 让团队掌握产生想法和决定的方法。 走向自我管理，和对想法和决定的彻底执行。 技能之自我掌控 自己的行动是为了给团队带来所需的东西。 不是为了自己的需要。 为了团队而存在。 技能之成为敏捷价值观和原则的楷模 被团队看作称职和成功的敏捷人员。 让团队从你在各种情况下的处事方法和与他们相处的方式中看到敏捷的价值。 让他们通过你的示范学会了如何很好地运用敏捷。 让他们学习更深入的技能使自己能更完全地协作，并产生令人惊异的结果。 技能之驾驭冲突 学会并运用至少一种冲突导航模式来帮助团队跨越冲突。 在冲突未解决时能让相关方友善相处。 有意识地选择时机和方式介入团队冲突，包括有意识地选择让冲突存在而不加干涉。 最终达到让团队能完全由他们自己驾驭冲突。 技能之不断学习和成长 给自己灌输一种永不知足的学习渴望，以及见证团队和公司蓬勃发展的愿望。 把新发现的知识融入到指导过程中，并在指导过程中注意自己技能的增长。 腾出时间来学习新的技能，体验新的观念。 不断反思自己的指导能力。 技能之分享回馈 分享自己在尝试和折磨过程中所获得的宝贵教训，及学到的新方法。 参加敏捷社区和敏捷会议，在各种讨论中贡献自己的观点。 在会议中提交和分享话题。 敏捷教练绩效度量标准 不要驱使团队取得结果，不要命令式指导他人的工作。 要含蓄地领导，创建一个自然的环境让团队交付好的结果，而不需要任何人来驱使他们。 不要控制团队的工作来保证预测的准确性。 要放开手脚让团队来完成他们选择的工作，并且支持他们对自己承诺的结果负责。 不要遵从公司的规则。 要发现当公司的规则限制了价值交付时，就挑战公司的规则。 不要立即把问题提升到管理层。 要与相关人员一起研究问题，直到问题完全解决，并继续向前进。 不要偏爱已经验证的并且安全的选择。 要给团队营造安全感来试验、失败并吸取教训。 不要按计划交付产品。 要允许团队根据他们的变化和逐步精确的计划来交付产品。把交付商业价值作为唯一的度量。 不要遵从经受了时间考验的策略和过程。 要培养创造性和提高团队能力，把每种情形当成全新的情形并提升产生全新结果的可能性，即使在熟悉的领域里。 不要照本宣科地实施敏捷。 要知道在什么时候照本宣科是最好的方法，什么时候需要舍弃最强有力的敏捷表述来换取再困境下至少一点点的改善。 敏捷教练每周价值清单（示例）： 帮助产品负责人和产品发起人保持一致，让他们给团队的指示与产品愿景一致。 制作团队交付的东西对软件操作的影响的视图。 帮助 PMO 采用敏捷团队的发布计划来创建他的总体进度表。 指导 PO 和 Scrum Master 创建产品列表。 启动一个新的团队。 说服项目级变更管理团队与敏捷团队一道工作，而不是下达最后期限。 让敏捷经理意识到自己以前太强势。 敏捷教练对一次指导的回顾（示例）： 团队发生了重大变化并被公司领导认可。经理们说，如果没有敏捷，他们很难按时交付。 培养了强有力的 Scrum Master。 两个团队成功地完成了产品负责人的更替。 团队不断认识到浪费。 影响新团队采用敏捷。 帮助运营团队采用敏捷更快完成操作性工作。 憎恨敏捷的团队愿意尝试敏捷。 正向影响了三四个 PO 和 Scrum Master。 选用了新工具协助回顾会议和团队协作。 收到了正面反馈和肯定。 有能力指导管理层。 需要改善：帮助高层领导知道如何发掘敏捷团队的能力来更好地交付和应对变化。 需要改善：协助新 PO 和 Scrum Master 与组织墙抗争。 需要改善：让高层领导接受指导。 敏捷教练绩效考评 影响力：当你与团队交互并提出一个有洞察力或强有力的问题时，团队会提出更好的想法或转向行动吗？ 一对一指导时，注意他几天或几周后的变化。 为你做得好的事情欢欣鼓舞，客观面对使你或他人失望的地方。 只有你自己知道什么时候你做到了合格的敏捷教练。 提升第三式：教练之旅每个敏捷教练都是沿着自己设计的旅途在前进。本节给出了八个敏捷教练的故事。他们的背景、经验和视角迥异，但他们热爱敏捷指导的原因都是，敏捷方法既体现了天然的工作方式，又能交付切实的商业结果。 作者 Rachel Davies 的旅途：发现与倡导 从开发人员：接到需求，再设计，再开发，再测试。当项目没有按时交付时，每个人的辛苦工作成果就被丢弃了。 到开发经理：指挥团队工作。开始寻找一种尊重人同时也能使他们交付产品的方法。 到 XP 开发者：结对编程，团队工作，测试驱动，每周都能交付软件。 到敏捷教练：让团队驱动流程，以平和的步调工作。对于无意于敏捷的团队，需要理解变革需要时间，敏捷教练需要耐心，从简单的东西开始。随着时间的推移，花时间去听团队的想法并帮助他们找出可能采取的行动，而不是指挥他们去做什么。在与团队的互动和回顾中播下变化的种子，在种子变成绿芽之前需要时间，在结果之前不断施肥。 技术培训师丹的旅途：守破离进阶的反思 守：参加认证 Scrum Master 培训，按照书本上的方式做事。 破：敏捷团队的行动就像创业公司一样，都是通过观察来学习，对工作采用经验主义的做法。 离：Scrum 是关于角色、权力和边界的定义。意识到有更多的东西要学。 的作者 Lyssa Adkins 的旅途：弥补过失 从项目经理：按计划做事情。项目一个接一个地按时、按量、按预算交付，但没有一个交付能够使客户满意。一长串的人们为这些项目伤害了他们的生活，因为他们的工作时间远远超出人的正常期望。 到 Scrum Master：教授、辅导、协助、沉思、提高。大家在工作中专业、快速、具有质量头脑。团队真的知道如何正确地做要做的事。团队中最羞怯的成员开始大声说笑，并因为他的才气被大家认可。 到工作/生活教练。学会了如何让团队对他们的日程负责。带给他们新的思考方法，给他们的生活带来欣喜的变化。 到敏捷教练：团队确实知道什么是最好的，我只是帮助他们了解他们自己知道的。 认证 Scrum 培训师马丁·科恩的旅途：个人回顾 从层级管理者：让下属执行我的方案。 到 Scrum Master：我知道什么是最好的，教给他们做。 到学习情商：在一个新的层次上理解人们的动机、表达和能动性。需要重视每个人自身，尊重他们的观点，更多地了解他们的个人目标和信仰。聆听和理解大家的感觉和想法，通过提问题来帮助所有人理解事情的缘由。尊重团队过去的经验，帮助他们开发他们的优势，建立起敏捷团队的观念和意识。 到敏捷教练：让敏捷的好处永远伴随。帮助建立一个环境让每个团队成员充分发挥自己的才干，并创建非凡的方案来真正满足业务的需要。 凯西的旅途：学会指导 从项目经理：你们开始编程吧，我去看看他们要的是什么。擅长动员团队，带领团队走向成功，与管理层沟通，使管理层满意。 到 Scrum Master：发现团队难以管理，对 Scrum 会议没有兴趣。 到生活教练：学习主动倾听，强有力地提问，增强意识，管理进度及责任感。采用指导风格而不是管理风格。 到敏捷教练：指导团队作为一个专一的实体来驶向成功。帮助团队实现自管理。多提问而不是多给建议，承认团队具有成功所需要的知识，维持积极的前景和方法。观察团队成员如何行动、说话与交互，让他们发现哪种方法可行。介绍工具和技术来帮助他们，每次会问他们是否愿意采用这种技术，并遵从他们的决定。 敏捷教练 Glen Wang 的旅途：寻求合理的管理 从工程师：一线工作人员看不到大局，每天的工作被微指挥微管理，等级制度造成各种弊病，开始思考合理的组织和应该的管理是什么样子。 到经理：开始在可控制的范围内自发地实践自组织自管理，设定团队愿景，让团队成员参与计划和管理，自由领取工作。 到 Scrum Master：Scrum 就是要找的方法，运用 Scrum 之后，看见了更好的团队。团队产出更多，也更快乐。 到敏捷教练：精通精益和敏捷体系，系统思考，注重内在品质和教练方法。致力于打造理想的组织。 敏捷教练虎头锤的旅途：保持探索之心 从工程师：从瀑布到 CMMI，从学习敏捷理论，并参与到“四不像”的敏捷实践，总觉得摸到了敏捷的门，但是进不了敏捷的“道”。 到 CSM：CSM 的培训被 Spotify 模式惊艳，开始尝试在公司内部进行敏捷培训，通过准备培训教材和多次培训，探索和总结过往的经验教训。 到进入采用 Spotify 模式的公司：真正理解了敏捷怎样融入日常工作，如何用透明度传递信任。发现了可视化的种种好处，所以从绘画小白踏上了视觉引导之旅。 您的旅途：未解之谜，意义之旅 ? 你曾经做过什么？学到什么？什么东西被你发现了并融入到你的敏捷指导方法中？ 指导敏捷团队对你来说，有什么重要意义？ 你以前在哪里，准备去哪里，有哪些让你走到今天的重要指导事件？ 你前面的路是什么，你下一个要到达的顶峰是什么？ 把你的旅途分享给他人，既能获得支持，也是给他人的启发和礼物。 本文作者： joni@efbiz.org 本文链接： https://github.com/efbiz/2018/05/23/敏捷教练第09课-技巧-敏捷教练的提升三式/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第10课-技巧-持续改善和系统思考方法","slug":"敏捷教练第10课-技巧-持续改善和系统思考方法","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:21:06.002Z","comments":true,"path":"2022/04/12/敏捷教练第10课-技巧-持续改善和系统思考方法/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC10%E8%AF%BE-%E6%8A%80%E5%B7%A7-%E6%8C%81%E7%BB%AD%E6%94%B9%E5%96%84%E5%92%8C%E7%B3%BB%E7%BB%9F%E6%80%9D%E8%80%83%E6%96%B9%E6%B3%95/","excerpt":"","text":"是什么让一个团队与另一个团队有所不同？是什么使得一个团队在敏捷教练离开后仍能保持提升？答案是一个团队内建的持续改善与系统思考能力，特别是 Scrum Master。一个具备持续改善和系统思考能力的 Scrum Master，就已经不是只顶着这个角色的 Scrum Master，而已经是个敏捷教练了。这与外在的角色和职位名称无关。 本文按以下这个大循环介绍持续改善与系统思考方法： 思想准备：组织的常青，教练的使命，敏捷的逻辑，工作生活的四层逻辑，追求高绩效 技能准备：敏捷宣言，Scrum 框架，精益思想 现场现物：GROW 模型-真实的问题，真实的目标，真实的方法 识别问题：观察，交流 解决问题：因果逻辑，PDCA，回顾会议 固化成果：提炼和运用模式 打磨逻辑，不断实践 本文介绍的思想会落实到下一章典型敏捷教练周期六步法当中，是六步法背后的思想和逻辑依据。作为敏捷教练，需要不断做两件事：一是不断打磨逻辑，二是不断在实践中实证。这两点，也是本课程的灵魂，和敏捷教练技能未来发展的基调。 思想准备 组织的常青在《基业常青》一书中，科林斯和波拉斯确定“高瞻远瞩”公司的标准是：处于所在行业中第一流的水准、广受企业人士崇敬、对世界有着不可磨灭的影响、已经历很多代的 CEO、已经历很多次产品生命周期且在1950年前创立。根据这六条标准，他们选出的公司有：美国运通公司、波音公司、花旗银行、沃尔玛、迪斯尼公司等共18家。 这些常青公司有以下一些特质： “造钟，而不是报时” 科林斯指出，“伟大的公司的创办人，通常都是制造时钟的人，而不是报时的人。他们主要致力于建立一个时钟，而不只是找对时机，用一种适销对路的产品打入市 场；他们并非致力于领袖人物充满魅力的人格特质，而是致力于构建高瞻远瞩的公司组织特质，他们最大的创造物是公司本身及其代表的一切。”“造钟”就是建立一种机制，使得公司能够依靠组织的力量在市场中生存与发展，而不必依靠某位个人、某种产品或某个机会等偶然的东西。随着市场的进一步完善与规范，企业必须 越来越依靠一个好的机制，包括好的组织结构、好的评价考核体系、好的战略管理等。 “利润之上的追求”与“教派般的文化” 所有伟大的公司都是“务实的理想主义者”。“利润是生存的必要条件，而且是达成更重要目的的手段，但对很多高瞻远瞩的公司而言，利润不是目的，利润就像人体需要的氧气、食物、水和血液一样，这些东西不是生命的目的。但是，没有它们，就没有生命。”利润之上的更高追求在伟大的公司里，更是被作为像“教派般的文化”那样所灌输。“利润之上的追求”如果不明确、不具体，就会是空洞的大口号。企业要意识到企业文化的重要作用，“教派般的文化”指的是卓越公司必须具有很强的共同价值观。 “自家长成的经理人” “18家伟大的公司在总共长达1700年的历史中，只有四位 CEO 来自于外部”。“自家长成”的经理人熟悉了解本公司文化，更易带领本公司进行变革。 其实，任何一个公司无论长盛不衰还是昙花一现，都有意无意地由一种理念所指引。 这三个特质，跟我们在前文数次提到的作为精益和敏捷鼻祖的丰田 4P 完全吻合。 教练的使命:要成为一家高瞻远瞩的公司，需要有一个像丰田 4P 那样的体系。体系的落实，需要教练。在丰田，经理就是教练，教练是一种管理职责。在丰田之外，经理与教练分离。教练需要在一定范围内取得组织授权，以便履行教练的职责： 贯彻敏捷的工作方式。 打造被充分激励充分赋能的自组织团队。 以此创造价值实现目标。 敏捷的逻辑: 按照敏捷思想，采用一种方法，比如 Scrum。最起码在团队中取得形式上的共识和一致。 团队自组织。以敏捷思想为指导，通过共识机制，持续改善与解决问题。 以前两者为基础，取得好的业务成果。同时个人获得职业生涯和工作生活上长期全面的好处。 第1点相对直观，第2点的逻辑基础是：好公司。 在非理想状态下，重点可以放在第1点，对第2点做有限度的追求。不管环境如何，我们依然可以有所作为。好与坏不是非黑即白，而是同一频谱上的两个点。我们能做的，是施加一点好的影响。 好公司的标准 员工有相对体面的收入和足够的激励。 有相对公平平等透明的制度。努力与结果之间有清晰的影响关系。 经理与员工之间互相扶助的关系。 公司有前途，有奔头。 员工技能提升与发展。 有明确的努力方向。 晋升加薪的机会。 以员工为中心，员工有机会参与和影响公司管理。 工作环境和制度人性化。尊重人。 流程合理，不会有很多阻滞，办事不难。 工作的稳定性和保障性。 第1点和第2点的区别是，第1点是相对可以有形化和规定化，第2点需要大家的讨论和共识。没有共识，第1点也难以执行。而第2点也离不开敏捷思想的指导。自组织不是一个悬空的虚拟的概念的问题，而是一种持续改进的共识的共同的问题解决。自组织不是一种脱离内容的形式，而要以内容即工作组织和问题为出发点。 在非理想情况下，打造自组织，可以采用的方法是，受限的共识，和受限的自组织，受限的好团队。不管环境多么恶劣，共识的空间是有的。共识与流动即知行合一。 团队的发展阶段 无组织（各自为政，效率低下） 被动的自运转（Scrum Master 驱动） 自运转（自动按 Scrum 运作） 受限的自组织（大环境不理想） 自组织（基业长青） 找到一些问题（如跨职能团队，价值流优化，团队工作），推动团队达到哪怕是不完美的共识，和不完美的执行。 团队的三个角色之外，也要把经理纳入共识的范围。问题的解决不用那么急迫。因为没有共识，急迫也没有用。大环境之下，团队之间差异的原因，如团队的构成，团队的年资等，也是值得思考的点。以逻辑的甘露，在理念和团队事实上，有步骤地把一团意大利面拉直成拉面。 工作生活的四层逻辑 生产力，其逻辑是技能。工作方式是否能辅助大家的技能提升呢？ 生产关系，其逻辑是交易。如何影响分配的规则呢？ 五伦之外，其逻辑是合作。如何找出大家共同追求的东西呢？ 五伦之内，其逻辑是爱。如何激发人的善意呢？ 不管环境多么恶劣，合作还是可以存在的。合作既是敏捷的基础，也是敏捷的核心。 追求高绩效在一个组织中，大家有共同的目标，共同的规则，就可以追求高绩效了。 建立 One Team 的理念和实践。在一个团队中，可以就所有问题公开讨论，包括绩效管理，个人发展，技术，管理，业务，工作满意度，员工参与度等。 一个组织的形态，往往都是不够理想的。在这种情况下，我们心目中要有最终的理想，并根据情况，制定阶段性的理想。 技能准备技能准备的部分在本课程的前面部分已经涵盖，总结下来，包括： 敏捷宣言 Scrum 框架 精益思想 技术实践 教练方法 自我提升 对组织和团队发展阶段的认识 本章的系统思考和持续改善 有了思想和技能上的准备，就进入现场现物的了解问题。 现场现物 了解问题可采用 GROW 模型： Goal 目标：组织和团队的目标是什么？是速度提升吗？是质量提升吗？干系人对目标的共识程度如何？ Reality 现状：现状是什么？现在采用的工作方式是什么？实际运作是怎样的？实际运作中有些什么问题？ Opportunity 机会：是否存在提升的机会？各方干系人的支持程度如何？有什么不可克服的障碍？提升之后对团队意味着什么？ Way 方法：具体可采用哪种敏捷方法？是一步到位还是阶段性的？ 要注意三个真实真实的问题：亲自了解问题，而不是道听途说纸上谈兵。真实的目标：各方共识的目标。真实的方法：找到问题的根源再解决，而不只是一些形式上的措施，比如针对一个表象的问题搞一次形式上的培训。 识别问题识别问题的途径包括： 观察：亲自去看团队的运作，特别是各种项目会议，日常交流，工作物件。有条件的话，可以看下团队实际的日常工作。 交流：尽可能与所有团队成员和干系人一对一交流，了解他们观察到的现状，看法和建议。 解决问题：解决问题可以采用的方法包括： 因果逻辑：对问题要找到原因，从源头上解决。了解团队运作的系统动力。如果只是做一些表象上的规定，根本无法落实和执行。 PDCA：解决问题要有完整的循环，包括识别问题分析问题设定目标分析根源制定对策的计划阶段，贯彻对策的执行阶段，评估效果的检查阶段，和标准化的调整阶段。 回顾会议：利用群体智慧，解决问题。 固化成果从解决问题中学习，把解决问题的方法提炼成模式。 在模式一章已经介绍了一些模式。再补充一些如下： 在站会中，会观察到一些好模式，如细颗粒度的协作。当一位团队成员说他要启动一个任务，跟他的任务相关的其他团队成员会立即响应说他会同时启动那个相关的任务。把这种观察拿到回顾会议上讨论，让团队参与这种模式的提炼，并丰富细节，记录下来。因为模式是大家共同提炼的，可执行性更强。 团队共同参与的庆祝成功也是一种模式。团队有共同的目标，经过努力，取得了值得一提的成功，大家就要庆祝一下。这种成功是属于大家的，并且成功能促进更多的未来成功。 制度化客户反馈与产品想法的分享。产品负责人可以把向团队分享客户反馈和产品想法制度化，例如在产品列表精化会和迭代计划会分别固定一块时间拿来分享这类信息。目的是让团队更全面的了解客户，知道自己工作的意义。方式上可以打磨，产生一种花费时间最少又对所有人最有意义的形式。 打磨逻辑，不断实践 系统思考和持续改善并不是什么高深的东西，最后总结如下： 对于好的工作方式，心中要有标准，这个标准是我们追求的理想状态，也是问题的鉴别器。 心中带着标准，去观察现状，去和干系人交流，了解真实的问题。 制定改善目标和改善方案，并取得干系人的共识和支持。 实施方案，评估效果。 固化方法，固化成果。 以迭代的方法重复上述步骤，以敏捷的方式做敏捷。 逐步加深解决问题的深度。在解决问题的同时，深化共识和合作。 始终不忘事件之间的因果逻辑，并持续躬身实践。 唯晓成事之规律，方持不灭改善心。思考，就是想出事物当中的理所当然。改善，就是把理所当然的事做到极致。 本文作者： joni@efbiz.org 本文链接：[https://github.com/efbiz/2018/05/23/敏捷教练第10课-技巧-持续改善和系统思考方法/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第11课-实战-敏捷教练实战周期V形六步法","slug":"敏捷教练第11课-实战-敏捷教练实战周期V形六步法","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:23:00.968Z","comments":true,"path":"2022/04/12/敏捷教练第11课-实战-敏捷教练实战周期V形六步法/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC11%E8%AF%BE-%E5%AE%9E%E6%88%98-%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E5%AE%9E%E6%88%98%E5%91%A8%E6%9C%9FV%E5%BD%A2%E5%85%AD%E6%AD%A5%E6%B3%95/","excerpt":"","text":"本文按1-2-3-4-5-6的结构，对敏捷教练的基本功进行总结，并讲述敏捷教练的典型实战周期。 1-2-3-4-5-6指的是： 敏捷教练的1个目标。 Scrum 的双翼。 团队的3个阶段。 敏捷教练的4个发力的角度。 团队中的5种角色。 教练周期的6个阶段。 基本功1个目标敏捷教练的目标是帮助组织做得更好。正如敏捷宣言所说：我们一直在实践中探寻更好的软件开发方法，身体力行的同时也帮助他人。 Scrum 的双翼Scrum的双翼或两条轨道，一是关于人，即自组织团队，二是关于工作方式，即基于精益敏捷的Scrum框架的运用。掌握了这两个要点，即提纲挈领，纲举目张。 团队的三个阶段 第一阶段是无组织。团队绩效不稳定且相对较低，团队成员呈现出一种各自为政的状态，团队活动从目的到流程都缺乏聚焦。 第二阶段是自运转。团队绩效达到一个相对稳定的状态，各项团队活动目的明确，流程清晰，在 Scrum Master 不在的情况下也能自动运转起来。 第三阶段是自组织。团队绩效会阶段性地持续提升，团队成员的互动达到一种高效快乐的状态，团队能够持续地从根本上解决问题，和持续改善。流程中的浪费越来越少，越来越流畅。目标的完成越来越好。 敏捷运用就是把团队从无组织状态带到自运转状态，再带到自组织状态。 敏捷教练的4个发力的角度 流程导入：包括产品管理流程和团队迭代运作流程。 问题解决：持续解决问题，打造解决问题和改善的文化，形成学习型组织。 个人转变：让个人接收敏捷思维，探索更好的工作方式。 团队建设：基于 Scrum，打造高效快乐的自组织团队。 团队中的5种角色 产品负责人：产品负责人作为 Scrum 团队的掌舵人，对 Scrum 工作方式的成败至关重要。产品负责人可能有两种，一种是具备开放心态，一种并不理解但又排斥敏捷的价值。对于前者，可直接交流。对于后者，可以让他先观察，接受他先不行动。 Scrum Master：Scrum Master 作为流程的化身，从另一个角度对 Scrum 的成败有举足轻重的影响。Scrum Master 要了解敏捷和 Scrum 的基础框架，要相信和支持自组织团队，要有持续改善的系统思维，并通过大量琐碎辛苦的工作来使 Scrum 的运作尽善尽美。Scrum Master 同样可能有两种。一种是主动把这个角色当成一个职业，并且主动投入去精益求精。另一种只是偶然被安排了这个职位。 团队成员：通常来说 Scrum 不会直接影响团队成员的利益得失，大多数团队成员对于高效快乐的工作方式是不会反对的。有了产品负责人对 Backlog 的良好管理，和 Scrum Master 的有效引导，团队取得进步是自然而然的。 非专职人员：非专职人员需要理解 Scrum 工作方式，并与团队成员有效配合。非专职人员与团队的合作方式，需要打磨出显式化可执行的规则。 Team Leader：直接工作在 Scrum 团队内的 Team Leader 可能会受到 Scrum 工作方式的冲击。Team Leader 是否要受 Scrum 框架的制约，要根据组织的实际情况处理。 教练参与周期的6个阶段 调查与方案：了解团队的目标，所面临的问题，制定敏捷导入方案。 导入与反馈：导入敏捷，获得团队反馈。 痛点与问题解决：了解团队的问题和痛点，协助问题解决。 卓越驱动的系统改善，共识机制训练：制定改善架构，进行有系统的改善。 深入的问题解决：发现和解决那些影响敏捷工作方式发挥作用的障碍。 观察与拓展，发现好模式：观察团队中浮现出的好的模式，借鉴到其他团队。 本文的剩余部分将会详细介绍这个敏捷教练周期的6阶段。 阅读敏捷教练周期的6个阶段时，有几个注意事项： 谨记教练的目标是打造按敏捷方式完美运作，并内建了持续改善能力的自组织团队。 深入的敏捷实施涉及到工作方式与人两个维度。 6个阶段包括其顺序都不是绝对的，需要根据团队实际情况定制。 6个阶段，可能的话可以与迭代的节奏一致，即一个阶段对应一个迭代，但也不绝对。 阶段1：调查与方案，诊断与对策这个阶段的开始以收到教练需求邀请为标志，以制定出敏捷实施方案，并能够开始第一个迭代的敏捷导入为标志。这个阶段包括调查与方案两个环节。 在调查环节所要做的事情包括： 访谈关键干系人，包括部门经理、项目经理、产品负责人、Scrum Master、开发 Lead、测试 Lead、架构师和设计师等，了解团队的目标及面临的主要问题。 参与团队的会议，现场了解团队现有的工作方式。 方案包含两部分 在启动第一个迭代前所要做的事。 第一个迭代的启动计划。 在启动第一个迭代前所要做的事可能有 如果产品列表和用户故事的管理不能达到启动第一个迭代的条件，需要与产品负责人一起工作，打磨产品列表和用户故事，使之达到准备好的状态。 如果团队规模和技能配备妨碍了团队自组织和跨职能工作，需要先解决这部分问题。 如果 Scrum Master 对敏捷和 Scrum 的知识不够，需要先补足这部分知识。 在调查和方案阶段，所使用的标准是 Scrum、用户故事和精益敏捷的基础知识，以此来识别问题，补足知识，和解决问题。所使用的技巧主要是讲授和问题解决，也包括一定的指导。 在这个阶段最开始，还有一件最重要的事，就是教练启动会议。没有教练的关系，就没有教练的行为。 经过第1阶段，就具备了开始第一个迭代的敏捷导入的条件。第一个迭代的启动计划在下一节谈及。 阶段2：实施变化，导入与反馈这一阶段的目标是进行敏捷和 Scrum 工作方式的首次导入，并获得团队的反馈。对于已经使用 Scrum 的团队，是一种重新导入。已经使用 Scrum 的团队，有可能受制于团队已有模式的影响，只是在形式上采用了 Scrum，而丢掉了本质和核心的东西，重新导入需要以正确的敏捷修正受到侵蚀的敏捷。 这个阶段如果有条件的话，可以做一个全体团队成员参与的一到两天的启动仪式： 与 Scrum 建立连接：探讨已有工作方法中好的地方，不好的地方，建立改善的愿望，把 Scrum 当作好的工作方法的载体，建立良好使用 Scrum 的决心。 介绍精益敏捷、Scrum 和用户故事的核心和实践。 让团队成员深入了解彼此，制定团队的价值观和团队规范。 刷新产品愿景、路线图、发布计划和产品列表。 如果不能进行一到两天的启动仪式，则可以在每个 Scrum 仪式前分别用10分钟左右时间介绍该仪式及相关物件： 在产品列表精化会前介绍产品列表精化的目的和流程。 在迭代计划会前介绍计划的目的和流程，及每日站会。 在迭代评审前介绍评审会议及回顾会议的目的和流程。 在每个会议的介绍之后，协助该会议的进行。对于会议中偏离目的与流程的行为，进行指导。指导可以以在每个会议结束时发表评论的方式进行，可以在会后进行个别谈话，也可以拿到回顾会议讨论。 在第一迭代进行到中间的时候，即可以开始了解团队对工作方式变化的反馈，因为这时工作方式的效果已经能发生了。了解反馈的目的是对工作方式进行修正。在第一迭代中间了解反馈还有一个好处是，鼓励团队把反馈和问题带到回顾会议讨论。内建团队的问题发现和解决能力是敏捷实施的重要目标，为了达到这一目标，敏捷教练要有意压制自己对观察到的问题的表达，而是把团队推到前面，帮助团队成长。 在导入和反馈阶段，讲授、协助和指导的技巧都会用到。问题解决和持续改善也会触及。协作指挥和冲突领航则是择机采用。 经过第2阶段，团队初步体会了完整的正确的敏捷，并为工作方式的变化提供了反馈，以帮助后续工作方式的调节。 通常来说，经过一个迭代按正确 Scrum 的运转，团队对更加清晰透明的工作方式会有正面反馈，迭代的完成率等结果指标也会有明显提升。 第3阶段：痛点与问题解决在这一阶段和后续阶段持续要做的事是，持续观察团队的 Scrum 仪式，发现其中违背正确敏捷实践的行为，以及发现团队工作中涌现出的好的模式。对于观察到的结果，可以在当时即每个仪式结束时现场提出来，可以在个别谈话中进行，也可以留到回顾会议。依然是两个原则：只要不影响团队运作的行为，尽量延迟到回顾会议进行；培养团队的问题解决能力重于问题解决本身。 这种观察、思考、反馈和调整会延续到整个教练周期，并且以打造按正确敏捷运作、具有自我改善能力的自组织团队为目标。 在这个阶段，除了观察、思考、反馈和调整之外，可以设定另一主题，那就是痛点与问题解决。具体的方式采用一对一谈话。谈话的对象包含产品负责人、Scrum Master、团队 Lead 和其他对工作方式有热情的人。整个教练计划可以公开给团队，可以发起一对一交流，也鼓励团队成员来发起一对一交流。 这一轮交流的三个主题是： 进一步获得他们对工作方式变化的反馈。 探询他们的痛点和希望解决的问题。 同时提供对他们本身的反馈。 这种交流是一种一对一 Retrospective，其目的、边界和框架如下： 一对一 Retrospective 是对团队Retrospective 的鼓励和驯化。是为了帮助打磨团队Retrospective。 一对一 Retrospective 是对团队Retrospective 的补充。即使团队 Retrospective 已经搞得很好了，也还需要一对一 Retrospective。 一对一 Retrospective 可以由 Scrum Master 发起，也可以由任何人向任何人发起。 一对一 Retrospective 的目的，是加强人与人之间的连接，传递改善的信念，和计划和执行改善。 一对一 Retrospective 的边界，是围绕改善的基调，就与团队项目工作相关的事进行讨论。 一对一 Retrospective 的框架，可以包含探询交流对象对工作方式的反馈、探询痛点和关注的问题，和以 Scrum 实践和角色要求为基准、以观察到的行为为依据向交流对象提供的反馈。还可以包含不同团队之间的经验传递、桥梁和延展。 如果希望痛点和问题的探询更封闭一点，可以分解为几个角度：就团队项目工作的上下文而言，您的目标和期望的理想状态是什么？与现状的差距是什么？流程上有什么问题，或有什么妨碍理想状态的达到？团队合作方面呢？团队工作绩效和质量呢？任何其他方面？ 这个框架的运用要灵活。人的主动参与重于规则。如果人能主动参与改善事项的发掘、计划和行动，框架就可以放下。 Scrum Master 日常有力的观察是 Retrospective 的重要输入。 各个角色的普适标准：专业、尊重、坚持。 改变的第一原则：一切改变基于自愿。改善的用意是改善系统，不是改变个人。 这一阶段可能获得的问题有： 会议效率问题：在 Scrum 框架之下继续打磨提升，细致地进行会议每个环节的提升，包括会前准备、会中引导、会后跟踪。比如说在精化会前大家可以先熟悉一下故事，在评审会前需做好演示的准备。在会议中引导促进大家的互动。 三个角色的职责问题：产品负责人负责与客户和产品有关的问题，Scrum Master 负责与沟通协调有关的问题，团队负责与技术相关的问题。 提升团队的参与度和对完整故事的关注：通过提升透明性，在计划会和站会上更加清晰透明地呈现工作来提升团队的参与度，通过设置故事 owner 提升和训练对完整故事的关注。 ：需要团队共创解决方法，比如说，一个原则是，开发优先做需要测试的任务。还可以设置开发任务的检查点。开发与测试结对工作。 团队建设，心理安全与归属感问题：需要与管理者和团队讨论处理。建立产品团队的形态与心态。 这些问题大致分布在流程与效率、角色职责、团队感和业务学习方面。 对于收到的问题的解决思路有： 按敏捷框架中的原则和实践解决。 拿到回顾会议上，由团队讨论解决。 对于深层次的组织问题，第一步是清晰的呈现问题，第二步是与有影响力的人交谈。 鼓励学习其他团队的 Scrum 运作。 在这个阶段，对于团队的 Scrum 仪式，依然会讲授、协助和指导。更多的是以团队为中心的立体双向反馈，了解团队的痛点和问题，并协助解决。 经过这一阶段，团队的敏捷运作已经达到了有意识的状态，虽然还不能完全自运转，但已经会发生一些有意识的提升和改变。另一方面，对于团队的问题也有了更深入的了解，为下一阶段的系统化的改善打下基础。 第4阶段：卓越驱动的系统改善，训练共识机制如上一阶段所说，对团队的观察、思考、反馈、调整还在持续进行。 此外，可以引入卓越驱动的系统改善。卓越驱动的系统改善即是： 团队定义一组卓越指标，即团队认为对于提升团队工作方式最有价值的东西。 对于卓越指标，利用每一次回顾会议，进行评估和改善。 以这一套卓越指标驱动系统化的改善。 卓越指标示例： 卓越指标1. 跨职能团队定义：不要让技能不平衡成为障碍。 子条目： 定义人与技能矩阵，以及理想的技能配置。在理想情况下，每一所需的技能最好有至少两个人精通和一个人了解。找出现状矩阵与理想矩阵的差距。 在迭代工作中，在团队容量容许的情况下，有意识地安排结对工作，以传播技能。结对工作所造成的团队速率下降以不超过10%为宜。 跨职能与结对的安排需要考虑个人兴趣。 制定长期的跨职能团队建设计划。 卓越指标2. 价值流优化定义：移除障碍，让团队所有的努力都指向对客户价值的贡献。 子条目： 制定 DoR 准备好的定义，形成高质量的迭代入口。 制定 DoD 完成的定义，形成高质量的迭代出口。 -每迭代的故事分为承诺的故事和可选的故事，可选的故事可以在产品列表精化会或迭代计划会上产生。 识别和移除障碍，帮助工作更好地流动。 会议议程有清晰的结构，每一议程精确到十分钟的颗粒度。 卓越指标3. 团队工作定义：团队协作，以最大化团队产出。 子条目： 定期知识分享。 经常庆祝成功。 集体代码所有。 交叉演示工作，例如 Tom 可以演示 Jerry 的工作。 卓越指标的制定，要由团队一起完成。具体可采用团队会议与一对一交流相结合的形式。 经过阶段4，团队的协作和 Scrum 运作已经相对比较熟练了。但其中依然有大量可以改善的点。而这个卓越驱动的系统改善提供了一个结构，充当了一个面。日常点的观察，与卓越驱动的回顾会议的面的结合，让改善更深入地发生。 阶段5：深入的问题解决在这一阶段，观察与改善继续进行。 经过前面几个阶段，团队的 Scrum 运作得比较好了，也建立了改善系统和一定的改善能力，解决问题的意识也建立起来了。 下一步是发现和解决那些影响敏捷深化的问题。这一步所采用的方法主要有四个： 观察团队的 Scrum 运作。 在回顾会上讨论需要改善的问题。 一对一交流。 团队的敏捷成熟度评估。 在这一阶段所要解决的，可能是一些深层次的问题，例如：团队中存在层级结构，或者团队成员来自不同的部门，让团队无法真正自组织，产生高质量的互动。 在这一阶段，问题解决是采用的主要技巧。 阶段6：观察与扩展，发现好模式。这一阶段主要有三个任务： 持续观察和改善。 发现团队中涌现出的好的模式，并使之持久化，和扩展到其他团队。 培养 Scrum Master 的系统观察和思考能力。 改善是无止境的，改善之旅是没有终点的。唯晓成事之规律，方持不灭改善心。 最后用大野耐一对丰田生产方式两个支柱的解读帮助我们了解敏捷和 Scrum 最本质的东西：人，以及人的配合。 大野耐一在其所著的《丰田生产方式》一书中这样评价“准时化”和“自动化”之间的关系： “准时化”和“自动化”是丰田生产方式的两大支柱。如果用棒球比赛来打比方的话，那么准时化就相当于团队协作，也就是通过团队密切而巧妙地配合，将其实力发挥到极致。而自动化则是要求每一位选手的个人技术要越来越高超，并且进一步使得团队的整体实力在个人技术提高的基础上得到更充分的体现。准时化可以让问题明确化，自动化可以让解决问题的努力更有效。自动化——让每一个人提高自己的水平，整个团队也会因为每一位选手的高超技艺，相互之间的配合变得更加默契，战法也更加成熟和丰富起来。当然，由此带来的结果就是：比赛的成绩越来越好，也就是企业的经营业绩越来越优异。 本文作者： joni@efbiz.org 本文链接： https://github.com/efbiz/2018/05/23/敏捷教练第11课-实战-敏捷教练实战周期V形六步法/","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第06课-技巧-敏捷教练的四种心法","slug":"敏捷教练第06课-技巧-敏捷教练的四种心法","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:04:19.889Z","comments":true,"path":"2022/04/12/敏捷教练第06课-技巧-敏捷教练的四种心法/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC06%E8%AF%BE-%E6%8A%80%E5%B7%A7-%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%9A%84%E5%9B%9B%E7%A7%8D%E5%BF%83%E6%B3%95/","excerpt":"","text":"敏捷教练的职责是帮助组织做得更好。为了履行这个职责，首先是做好前面几章介绍的知识储备。其次是运用这些知识影响组织，也就是下一章开始讲的教练的六种方法。在进行教练之前，敏捷教练需要做一些自身的在敏捷知识之上的内在准备，也就是四种心法： 内建能力 更高标准 自我掌控 灵活变通 心法一：内建能力在敏捷开发指导过程中，对团队影响最大的其实是教练本人的内在品质和行为方式，而并非任何外在的具体的技术或者意见。教练的一言一行无不体现出其内在品质和对敏捷主要观念的理解。通过内在品质的体现，可以给个人、团队和组织带来深远而持久的影响，比照本宣科式刻板地执行敏捷思想中的具体的技术有更加深刻的意义。 敏捷教练的定位 一个能够准确掌握敏捷开发实践和理论中深层次内容，并且能够帮助团队理解这些内容的人。 一个面对过巨大挑战、内部阻力，并且能够在需要时为经理们或者其他团队的人员提供指导的人。 一个能够帮助组织内各级管理层去深刻理解有效的敏捷开发能够为日常工作带来哪些好处的人。 一个能够从专业辅导、冲突管理、矛盾调解、剧场表演等相关学科中引入新的观点和理论，从而让自己团队的表现不断提升的人。 既关注在复杂而又变幻无穷的世界中创造出有意义的产品，也关注为参与创造的人们的职业生涯带来更多益处。 敏捷教练应该克服与改变的方面对照下面“应该做到的”一起看 协调个人对团队的贡献。 做一个领域专家。 把精力花在追求特定产出上。 自己知道所有问题的答案。 自己全权管理整个项目。 推动。 注重截止日期和技术路线的选择。 注重行为的最优性。 亲自解决问题。 工作总能很好地计划，计划总能很好地被实施。 项目三要素可以相互调剂和妥协，以应对未知的突发情况。 随着需求、设计、开发、测试阶段的推进，可预测性越来越高。 准时和在预算内完成目标就是成功。 项目的范围可以事先锁定。 从头到尾控制整个项目的进程。 完成阶段性目标和任务是工作价值的度量。 敏捷教练应该做到的方面 指导整个团队进行协作。 充当团队的协助员。 把精力花在提升团队的整体表现上。 让团队自己寻找答案。 让团队自己寻找途径。 指导。 注重商业价值的达成。 注重于每时每刻都做有利于业务发展的事情。 将问题交予团队。 提前计划并不可少，但确定的计划并无用处。 时间和预算不变，范围可变。 随着时间推移，计划不断被修正，因而越来越准确。 客户获得的商业价值是成功的标准。 项目范围保持灵活，任何变化都可接受。 以团队贯彻敏捷思想来控制进程。 可工作的软件才是价值的度量。 敏捷教练的内在品质 能够读懂一个房间的空气中蕴含的情绪，并判断出是否一切正常。 关心人胜过关心产品，让团队成员感受到自己受到关心，自己的成长得到支持，进而团结一致，创造出卓越的产品。 不断培养自己的好奇心，清楚地意识到自己的疑惑在那里，不会主观揣测他人在想什么，及情况形成的原因，而是会如实发问。 相信人之初性本善，人的内心总有善良的一面，不过可能被客观情况耽误了，接受他们目前的状况，并尽可能帮助他们成长。 不是固执地执行事先制订的计划，而是时刻与团队一起解决新出现的问题。 有着学习的渴望，知道自己还需要不断成长和提高。 相信只要给予一个大胆的目标和一个成长的环境，任何一组人都能把事情做好。更高的目标值的追求。 不容忍人们为不求上进寻找的各种借口。例如：我们一直是这样做的。 相信预期之外的情况一定是会出现的，而混乱只是达到更好情况的必经阶段，做好应对混乱的准备。 愿意承担犯错的风险。承认错误，承担责任，但不会纠结于此。 形成个人特色 找到自己做敏捷教练的独特风格。 不断实践和总结。 把从本课程中学到的东西用自己的方式消化，然后指导团队。只有自己最熟悉自己周围的环境，也只有自己最了解自己。 对环境和自己要充满信心，不偏安一隅，勇于挑战自己，通过指导敏捷开发团队的工作来实现自己的成长和成功。 心法二：更高标准把高绩效作为自己的期望标准，并帮助团队去实现它，这些都能给你以重要而强大的动力。如果可以随时保持雄心勃勃，那么每个人都能获得最后的胜利。公司或组织不仅获得了更好的成果，还拥有了无所不能的团队。团队及每个成员则获得了更多自主权，掌握了高超的技能并实现了自己的目标。每个人都能从高绩效中受益。 设定高标准： 设定高绩效仅仅表明你相信高绩效是可以实现的，你相信团队能够实现这一目标。 需要用自己的信心激励团队朝着能共同达到的愿景而努力奋斗。 高绩效无关乎是否到达某一特定状态，而是一段通往更高目标的旅程。超越一切合理的期望，甚至对自己的进度感到惊讶，不断保持进步。 为帮助团队开启通往高绩效的旅途，需要为团队设定将要实现的目标期望。 接下来，指导他们迈出第一步，以及以后的每一步，并一步一步地朝着那鼓舞人心的高绩效的目标前进。 你首先需要先让自己产生一种对这段旅途期待向往和兴奋的感觉，然后再把这种感觉传导到整个团队中去。 高绩效是没有终点的旅途。 一旦开启高绩效之旅，就会有各种各样的障碍出现，团队要为他们能够彻底地快速地从挫折中恢复过来感到自豪。让高绩效成为他们对自己的期望和信心，支撑他们挺过一个又一个难关。 高绩效的隐喻之高绩效树 用高绩效树帮助团队描绘高绩效期望的愿景。 当出现问题或不足时，将它作为一种研究问题的方法。 高绩效树的树根：Scrum 的价值观 承诺：愿意对目标做出承诺，Scrum 会为人们提供兑现承诺所需的所有权限。 专注：做好本职工作，把所有精力和技能都专注在自己承诺的工作上，而不要因为任何无关事情分心。 公开：Scrum 中与项目有关的所有事情对大家都是公开透明的。 尊重：不同的背景和经历塑造出不同的个体，但是有一点很重要，那就是，我们需要对团队中不同的人保持尊重。 勇气：要有承诺的勇气、付诸行动的勇气、敞开心扉的勇气和期望得到别人尊重的勇气。 高绩效树的树根：还可以使用 XP 的价值观 沟通：只有通过很多实践才能保持正确的沟通，而这些实践又必须通过相互沟通才能完成。 简单：做简单并且只需要稍微改动就可以重用的事情。 反馈：对系统当前状态的真实地具体地反馈是非常宝贵的。 勇气：有勇气去开发高质量的软件，即使这意味着需要删除原有的代码，改变原有的设计方案甚至是延长开发周期。 高绩效树的枝叶：高协作和高绩效团队的特征 他们是自我组织起来的，而不是根据角色和头衔来组织的。 他们有权做出自己的决定。 他们坚信，作为一个团队他们可以解决任何问题。 他们致力于追求整个团队的成功，而不是为了个人利益不惜一切代价。 他们对他们自己的决定和承诺负责。 是信任而不是恐惧和愤怒在激励他们。 他们是多数人意见驱动的，并做到求同存异。 他们会不断提出富有建设性的反对意见。 高绩效树的果实 实现正确的商业价值。 更快地实现商业价值。 取得惊人的成果。 无所不能的团队。 团队和个人的成长空间。 高绩效树的用法 画在团队工作的地方，默默地提醒团队成员，高绩效是一件很自然的事。 当团队遇到麻烦或墨守陈规的时候，指着它说，我们的根系薄弱在哪里？ 当具备了高绩效团队的特征，产品却不尽人意时，可以说，你们现在想收获什么果实？ 当以这种方式来使用这棵树时，你的问题就变成了他们的挑战。一旦团队接受了挑战，就离该绩效目标又近了一步。慢慢地，他们就会走出一条属于自己的路。 如果一个团队对自己的工作质量感到不满，他们可能是没有做到多数人意见驱动，而过早地采用了第一个出现的可行方案。就在多数人意见驱动上画一个圈。 当迭代目标没有完成时，可能是因为分心的事是团队忘记了承诺，团队可以约定，从现在开始，互相帮助，排除干扰，真正全身心投入，完成承诺的工作。 最好是别为团队指出一条路，而是让他们去开创一条属于自己的路。 当团队把高绩效树当作自己选择的通往高绩效目标的道路时，这棵树就已经在团队中落地生根了。 高绩效的另一比喻：打好基础 经验论：从一系列短时间内发生的失败中汲取教训并最终取得成功。 自我组织：最了解问题本质的人最清楚该如何解决问题。 协作：培养一种“是的，然后呢？”的思维方式。 优先级：专注，做优先级最高的那件事。 节奏：深呼吸，然后顺其自然。 心法三：自我掌控个人的自我调节可以促使你成为团队所需的那种教练，但这往往不是一天两天能够做到的事情，而是一个漫长的反复的过程。这里面需要持续的剖析实践和不断地提高改进。 从自我剖析开始 明白自己在一些特殊情形的自然反应以及自己的底线，有助于认识自身的现状和将来可能会变成什么样子。 下一步是不断挑战自己的极限，不断找到和提升自己的短板。当你很紧张浑身不自在的时候，就是找到自己短板的时候了。 在这些情形之下，要有意识和自觉地面对自己的缺点，多花些时间做自我剖析和反思。 在这个过程中，还能认识到自己的本能行为，并有意识地在事情发生时选择自己本能或其他不一样的行为。 你本能的冲突应对模式是什么？ 竞争型：强硬且不配合。 合作型：强硬但配合。 妥协型：一般强硬和一般配合。 顺应型：配合且不强硬。 回避型：既不强硬也不配合。 你的沟通方式有多强硬？ 你是否每天会花些时间静下心来反思自己是如何和他人相处的？ 你是否记得所有人都有一样的需求？ 在你每次开口之前，你是否确认过把他人的需求和你的需求看得同等重要？ 当你让别人做事情时，你是否确认过自己是拜托的语气还是要求的语气？ 你是否倾向于告诉他人你希望他做的事情，而不是倾向于告诉他人你不希望他做的事情？ 你是否倾向于告诉他人你希望他采取什么样的行动来帮助他们完成目标，而不是仅仅告诉他人你希望他完成什么样的目标而已？ 你是否在提出同意或反对他人的意见前，尝试站在他人的立场换位思考一下他人的感受和需求？ 你是否在说不之前，想过是什么原因导致你不能说是？ 当你感到沮丧时，是否会问自己问题出在哪里，以及自己应该如何解决，而不是去怨天尤人？ 当别人做了件令你感到满意和高兴的事情时，你是否对别人表示感谢，且告之具体解决了你哪方面的需求，而不仅仅是一句简简单单的赞扬之辞？ 你能做团队的公仆吗？ 关于怎么培养自己的团队：定要确保优先满足团队成员的最高优先级的需求，你所提供的帮助和服务能够帮助团队成长和发展吗？ 关于倾听和给他人提意见的权利：自然地在回答任何问题之前先把别人的话听完。 关于认可其他人：尊重团队并认可他们取得的每一点进步。 服务型教练：帮助团队成员成长和进步，只有团队中的一个个个体变得强大了，整个团队才会强大起来，才会被激发出更多更好更有创意的想法。 你的应对方式聪明吗？ 你是如何应对矛盾冲突的 如何和团队沟通 是否做到了服务型教练 如何控制自己的情绪反应 摒弃命令加控制的方式 不要纠结于结果：给团队足够大的空间去提出最好的想法和开发出最好的产品。 把问题留给团队：无论是产品本身还是团队合作上出了问题，解决问题的最佳人选都是团队。 充当一面镜子：将自己所观察到的事情，以不夹杂个人意见的方式讲述给自己的团队。 留意自己的用词和表情：学习说话时不附加自己的判断，学习无暴力沟通。 习惯沉默：习惯那些不舒服的沉默和安静，让团队的其他人有说话的机会。 学着不讲情面：不把向来是这么做当作正常。 允许团队失败：一起经历失败，并一起从挫折中走出来的团队会比那些一直被保护着的团队更坚强和更高效。 做团队最大的粉丝但要谨慎：团队表现得好是因为他们是一个团队，但不要做出空谈式的赞赏。 自我掌控的日常实践 听一些可以舒缓心情的音乐。 读一些能带来灵感和启发的书、博客、名言警句。 慢跑，然后静静聆听大自然的声音。 写下三件你很感恩的事情。 做做瑜珈或者伸伸懒腰并深呼吸。 认可自己，享受当下生活的分分秒秒。 将你的电脑密码和你的当下工作联系起来。 只关注你所关心的 只关心那些你真正关心的事，放下不必要的焦虑。 如果你真正关心的是团队能否针对那些对他们自己有影响的事情发表看法，那么你要做的所有事情，就应该围绕着帮助团队去发表自己看法这个出发点。 一个有效的方法帮你弄清楚你真正关心的事情到底是什么，就是寻找这个问题的答案：我怎么才能为团队做出最大的贡献呢？ 保持关注一件你所关心的事，舍弃很多无关紧要的事。如同产品列表，你只能选择一件你最关心的事作为紧急事项。 时刻记住你真正关心的事情是什么，并保持对它的关注。 做好当下的事情 面对各种难题和不舒适的环境时，也要管理好自己的情绪，放开自己的心态，控制主观情感，选择最为恰当的应对方式。 有时，团队需要你表现得更加直率，这样他们才能看清你对刚刚发生的事情的真实反应。 你把团队看作日常工作中所不得不面对的障碍，还是和你一样有希望、梦想、恐惧和志向的人？ 只要一点点时间，我们就能分辨出我们是否受到敌视，被控制或者被愚弄。我们总能分辨出伪善。真实心意重于应用技巧。 把你遇到的人当作活生生的人来看待，这样才能真正扩大自己对团队的影响力。 多练习分辨自己对各种突发的自然反应，并熟练应对。通过学习特定的听、说和待人接物的方式，来锻炼自己这方面的技巧。 将周围的人都当成活生生的人看待，辅以从这些练习中得来的技巧，你就能很好地控制自己当下的心态了。 练习倾听 层次1：内心收听。非常认真地听着对方的话语，但是听到的每一句话其实都经过自己的重新解读。听到的每一句话其实都在回答自己心中的这个问题：这会如何影响到我自己。而这时回答问题时，往往会专注于展示自己的专业度，而错失了对真正问题的理解。 层次2：专心收听。听者和说者已经建立了切实的联系。听者设身处地地为说话人着想，专注于话语本身。摆脱了自身利益的束缚之后，就能听到问题本身，并据此作出客观无我的回应。或者保持沉默，让说话人能够自由地表达自己的想法。不会主观臆断或者带着自己的利益去片面理解说者的话。保持好奇心，探询问者的情绪。 层次3：全心收听。结合当时环境中的每个因素来真正收听每句话。在层次2中的切实联系仍然很强烈，再加上对每个细节的全面把握，就能对双方谈话的内容产生很多直觉层面的想法。双方都对所谈论的问题有了更深的理解。保持开放的态度，时刻提醒自己：你真的不知道他下一句是什么。 练习说话 当你有想说话的冲动时，审视一下自己，你的立场在哪里。为什么你要在此时有这种想法。不要只是为了显示自己是一个聪明人或者希望在团队面前表现自己的价值。你的价值不在此。 每次想说话时，确认自己的立场是基于为团队成员考虑。确保你每句话的目的,都是为了帮助他们成为一支更加优秀的团队。 不要马上开口。先从1数到10，在数数的时候，密切留意是不是有人说出了与你相同的想法。如果每有人说出你想的，那么再等一会儿，判断一下自己的想法是否仍然和谈话有关，并且是有帮助的。如果是，就简单明了阐述自己的想法。你要相信，谈话总是会沿着参与人的真实需求往下发展的。 缄口不言。当团队成员提问时，不要做第一个回答的人。用了这个技巧，你可能根本不用再回答了。当你是提问人时，如果无人回答，这个时候，你最恰当的方式是表现出你其实很安于这种令人不安的沉默。就大大方方坐在那里，不要求他们必须发言，但要维持眼神交流和邀请。总有人会说话。 练习融入团队 让自己融入当下，并调整自己的立场。 融入当下，意味着此时此刻，你全部的注意力和精力都集中在一处，对此时此刻完全专注，不畏过去，不畏将来。 对各种现象不满的杂念极易让人分散精力，使我们偏离于融入团队现在的关注点。 但真正融入当下时，就会发现团队当下真正的关注点在哪里，帮助他们用一种更加富有建设性和正面态度的方式成长。 通过全神贯注，磨练心智，你会更加关注当下，而且对自我的认识也会更加深入。 融入当下，你的立场会更加坚定鲜明。你的话语会清晰无误，你的每句话都会掷地有声。你的介入就能切实为团队作出贡献。 融入当下也是团队成员需要发展的一项技能。完完全全关注此时此地，关注其他人，关注目前手上的工作。 判断自己说的话是否对团队有益，适时调整立场，甚至撤回自己说的话。 不断自我提升，自我修炼，做团队的榜样。 心法四：灵活变通随着时间的推移，我们会面临不同的环境和不同的团队发展阶段。针对团队的不同阶段，要有不同的做法。灵活变通是敏捷教练的必备。 敏捷团队的发展阶段 守的阶段。原封不动地照搬老师所传授的那些招式，没有尝试去理解隐藏在里面的奥秘。一次又一次地反复对着规则模仿。 破的阶段。掌握了基本功之后，花时间琢磨所有事情的本质和真相，对功夫有很深层面的理解，而不止是停留在单纯的重复练习上。这时候可以借助检查和调整来打破规则。 离的阶段。招数已经融入学生的身体。可以抛开形式，但又不违背原则，甚至体现出更深层次的意义和作用。 为了不断超越，我们必须首先完完全全地掌握所有规则，然后才能安全地打破规则，最后创造出新规则。这些新规则不仅遵循隐藏在旧规则里面的原则，还能展现出更深层的意义和作用。 不同阶段的教练风格 守的阶段的教练风格：教学型。清楚了解团队的需要，保持坚定的立场和态度，为团队制定规则和纪律。 破的阶段的教练风格：指导型。随着团队不断从实践中总结，并将实践转化为自己思想的一部分，就不再是被动地遵从规则。教练可以指导团队对既定规则作出修订。 离的阶段的教练风格：顾问型。当团队能够将敏捷开发的实践、价值观和原则融会贯通时，就可以采用顾问型风格了。肯定团队的想法，鼓励他们善于听取别人的意见，善于交流彼此的想法，勇敢地面对各种困难，以及尽可能将事情简化。 在每个阶段，都需要深入了解团队，了解每个团队成员，以及整个团队的情况，帮助他们找到适合自己的对敏捷思想的解读。 对团队守破离状态的判断 团队对敏捷思想是否比较陌生？ 团队是否改变或干脆放弃了敏捷开发的实践行为模式，并忘记了这些模式背后所蕴含的思想？ 团队是否在刻板地照搬敏捷思想中的条目？他们是否能站在个人与集体、所开发产品、客户需求与不断应对变化的角度来思考问题？他们是否能顺畅地进行日常工作，并且从实践中不断获得自我提升？ 团队是否能够在保持敏捷思想核心价值观和原则的基础上，对自己的实践方式进行持续改进？他们是否能够冲破自己所在公司的某些既定障碍，来让自己的工作更加高效、更加快速地完成目标，获得更高的客户认可？他们是否具备了实现自我监控、自我修正所必需的技能和思想？ 本文作者： joni@efbiz.org 本文链接： https://github.com/efbiz/2018/05/23/敏捷教练第06课-技巧-敏捷教练的四种心法","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"敏捷教练第07课-技巧-敏捷教练的六脉神剑（上）","slug":"敏捷教练第07课-技巧-敏捷教练的六脉神剑（上）","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T02:05:19.796Z","comments":true,"path":"2022/04/12/敏捷教练第07课-技巧-敏捷教练的六脉神剑（上）/","link":"","permalink":"http://zhangyu.info/2022/04/12/%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%AC%AC07%E8%AF%BE-%E6%8A%80%E5%B7%A7-%E6%95%8F%E6%8D%B7%E6%95%99%E7%BB%83%E7%9A%84%E5%85%AD%E8%84%89%E7%A5%9E%E5%89%91%EF%BC%88%E4%B8%8A%EF%BC%89/","excerpt":"","text":"敏捷教练第07课-技巧-敏捷教练的六脉神剑（上） | EFbiz 敏捷教练的六脉神剑，指的是敏捷教练在教练团队和组织时可以使用的六种方法。本章介绍其中的三种方法：指导、协助和讲授。这几种方法，既有不同的角度，有时候也交织在一起使用。我们既要明辨概念上的细微区别，在使用时也无需纠结使用的到底是哪一种方法。三种方法分开讲是为了讲解的方便和概念的提炼，实际使用时可以把三种方法中同一场景下的具体方法糅合在一起使用。 在每一种方法之下，按一个一个场景，提供了一系列检查列表。每一个场景既相对独立，所有场景结合起来又构成了一个完整的全景。所以，在阅读时，要调动见木又见林，见全牛又见解牛的思维。既高屋建瓴，也在一事一物上磨，方能知行合一，唯精唯一，事竟功遂。为学之要，在博学之，审问之，慎思之，明辨之，笃行之。 对于检查列表，要辩证地看待。一方面，检查列表是前人和过去的经验总结，让我们在无所适从之际有个东西可以作为开始。另一方面，检查列表与实际情况之间一定有很深的代沟，使用者需根据情况，制定自己的检查列表。一个建议是，把本课程当成一个模板，直接在上面增删改查，形成自己的检查列表，并经常阅读、思考、聆听自己内心的声音，和不厌其烦不惧挫折地实践。完美来自实践和操练。按此方法坚持下去，三个月必见功效，小有所成。 第一剑：指导 Sprint 开始阶段的指导 可以在团队成立初期，在第一个 Sprint 的开始阶段，或后续 Sprint 的开始阶段有不同程度的使用。指导的内容要根据具体的场景（比如说计划会是一个场景，也可以细分为几个子场景，计划会之前的准备和之后的跟进也是场景）事先设计，也要根据指导过程中获得的反馈调整。 这个阶段教学型指导应占主导地位，比如帮助团队学习敏捷实践或教他们如何真正进入到各自的敏捷角色中。 时刻铭记指导的目标：帮助整个团队了解敏捷是如何完美工作的。 当一个可以教大家某个具体概念的绝佳机会出现时，要把握住并“大声地”指导他们。 当团队需要时，可以安排一个讲座来介绍或加深某块具体的敏捷知识。 一种做法是在每个会议之前，指导这个会议应该怎样运作。 指导与培训的区别是微妙的，指导更多指的是指导的内容马上用起来。 Sprint 中间阶段的指导 只要团队工作进行得比较顺利，要减少整个团队级别的指导。 只有当教练有意识地要发表对大家有非常大意义的见地时，才进行整个团队级别的指导，比如说当团队显著偏离了敏捷核心和实践时。 询问团队，如果进行中期检查是否会有效，比如说当燃尽图形状不太好时，检查并进行适当的调整。 如果中期检查引发 Sprint 中间的回顾，就要停下来。 进行一对一指导，逐个解决每个团队成员的问题。 不要影响团队的正常工作。 如果两个成员之间有些矛盾，鼓励提出问题的成员与另一成员一对一地把问题解决掉。 当有人做了特别有帮助或意义特别大的一件事时，要在整个团队面前讨论和鼓励这种行为。但也不要太过正式。 教练的工作以观察、思考、聆听、响应为主。站会是团队工作方式的缩影，也是很好的观察场所。 教练心中要有标准，标准是观察的取景框。 Sprint 收尾阶段的指导 在 Sprint 回顾时，要创造条件让大家积极讨论。回顾的形式多种多样，在网上可以找到海量资料。要点是让每个人都参与其中，有均等的发言机会，并且确保回顾产生的行动能够落实。 要指导团队成长到团队成员懂得相互学习是多么美好的那种境界。可以策划知识分享活动。可以在回顾会中设置互相感谢环节。 可以把 Sprint 收尾阶段和下个 Sprint 开始阶段的指导合二为一：向后的回顾和为将来的任务进行培训。 产品发布层面的指导 与 Sprint 层面的介入周期类似。 在发布开始阶段进行教育。 在发布收尾阶段进行回顾。 在发布中间阶段进行整个团队的检查和对团队成员的一对一指导。 在指导个人和指导团队之间找到一个平衡点，选择最有影响力而又干预最小的指导方式。 指导的基调 爱心：爱，认可和支持他们成为更优秀的团队和个人。 同情心：尊重他们的现在，帮助大家成为想要成为的那个人。 永不妥协。不能偏离敏捷核心和实践。 但不需要在工作场所表现的过于感情丰富。爱心和同情心必须是真实的，大家能从你的眼睛，听和说中感受到你的尊重。 充满爱心和同情心会让你坚定信心不妥协。 将这种对信念的坚持传递给他们，完全相信他们能成为自己想成为的人。强调每个人都具备不断提高自身敏捷能力的可能性和需求。 不要期望团队成员的表现能够立即达到你的高要求，但不要容忍他们用折中去改变敏捷标准的定义。始终强调真正高效的敏捷应当如何运行。 要了解，变得更好是一个历程。耐心和设身处地去了解每个人的实际处境，而不是去追求从概念世界推导出来的理想的完美。 不要自卑和自怜于自己的人微言轻。教练是一种职业。跟其他职业一样，不是以权威，而是以你的专业度令人信服。记住帮助团队做得更好这个使命和目标。 一对一指导的前提 在超前半步的层次上进行指导：用心倾听每个人的内心，了解他们面临的矛盾和麻烦，了解他们在敏捷之路上处在哪个阶段，注意观察你的指导给他们带来的改变。 置身于充满安全的环境：允许他们犯错，跌跌撞撞，抱怨。所有这些行为都不会受到绩效考核的影响。在团队内部发生的事就让它留在团队里。确保有足够的空间让彼此表现人性固有的一些弱点，并对团队内部发生的事情保密。但如果出现了极端情况，例如骚扰、歧视或暴力，就要打破保密的原则。 与管理者们合作：直属经理会在显性（绩效考核）和隐性（日常谈话）的层面上影响团队。需要与管理者同步工作思路。 创造一种积极的氛围：不要把人当作一个亟待解决的问题，而要看做一个有希望、有梦想、有需求的活生生的人。每个人都在通过自己的能力和拥有的资源尽全力来达到最好的结果。 三个支点：专业，尊重，坚持不懈。 一对一指导谈话 谈话首先要诚恳。 然后要真实切实。不真实的谈话，对个人和公司来说都代价昂贵。 一旦开始谈话，就要顺其自然。思考指导对象处在哪个阶段，应设定什么目标和路径。 可以主动发起谈话，开场白可以是观察式（观察到了什么）或邀请式（邀请团队成员评估当前状态）。 时刻记住你的目标：帮助每个队员在他们的敏捷之路上不断提高。 在谈话的开始阶段，需要认认真真做一名倾听者，才能听到真实的问题。被指导对象开口，就是成功的一半。 在谈话的中段，通过一些有影响力的问题来引起被指导对象的反思。要注意教练在谈话过程中要回避的事情：解决问题。只有被指导对象自由选择的结果才是最有意义的结果。 在谈话结尾时，讨论下一步应该采取什么具体行动，帮助被指导对象更加可靠地完成今后的工作。这种可靠应当是自愿的，确保他切实承担自己应该承担的责任。 不要显得教练的地位是高于被指导者的。在整个谈话的过程中，教练与被指导对象应该始终处在平等的地位。 你不必非要是某个领域的专家才能指导在这个领域工作的队员，因为你的角色是一名教练，而不是代替他们直接去解决问题的人。 教练要明白自己的界限，例如是否可以讨论工作之外的事，是否要对被指导对象的工作领域提供意见。 对话的线索可以是优化流程和解决问题，并以此调动团队成员的积极参与。 指导产品负责人 教练与产品负责人之间谈话的唯一目的是为了确保团队的健康成长，而与个人恩怨和办公室政治毫无关系，这种专业精神和对自己角色内容的澄清能够赋予你更多的正式权力。 指导产品负责人做好自己的本质工作。摆脱命令与控制的工作方式，专注于商业价值的达成，而不是去具体管理每个团队成员的下一步行动。 帮助产品负责人建立以商业价值为导向的思想体系，请产品负责人以商业价值为导向，重新审视整个产品开发流程，并以商业价值作为他们制定每一个决策的基础依据，对优先级按商业价值排序。 指导产品负责人成为为团队着想的优秀产品负责人。帮助团队和产品负责人从失败中学习，一起改正错误，然后变得更加强大。 回归根本，参照 Scrum 指南和前面章节，理清产品负责人在 Scrum 中应有的职责和行为。 指导产品负责人的时机，可以以 Scrum 流程为线索，在其中寻找产品负责人的发力点或乏力点。 指导敏捷教练和 Scrum Master 让他们观察和探索，你作为敏捷教练如何工作。让他们自己客观冷静地作出自己的决定。 如果他们决心成为一名敏捷教练，开始教学。让他们了解敏捷教练这一角色的全部含义：指导整个团队、指导特定个人、教授敏捷思想、协助敏捷会议，以及通过谨言慎行来把握自己。 然后逐步向这位新教练转移指导工作。第一个月让他看你怎么做，第二个月你看他怎么做，第三个月，你只指导这位新教练而不再干预团队。 回归根本，参照 Scrum 指南和前面章节，理清 Scrum Master 在 Scrum 中应有的职责和行为。 指导 Scrum Master 的时机，可以以 Scrum 流程为线索，在其中寻找 Scrum Master 的发力点或乏力点。 指导敏捷经理 在团队管理方面的指导：将团队的自组织能力和经理们的有效领导相结合。 在投资管理方面的指导：让团队从以计划为导向的思维方式转化到以价值为导向的思维方式。 在环境管理方面的指导：在由各种流程和外部资源组成的组织环境中，高效地审视组织内各流程的设计以消除各种对组织资源的浪费。 依然以 Scrum 流程为线索，寻找与敏捷经理的交互点，进行指导。有问题就是契机，没问题反而无处下手。 第二剑：协助本部分关于协助会议的检查列表，不是对会议基础知识的重复，而是假定您已经掌握了会议的基础知识之后，如何以运用基础知识为基础，把协助会议这件事做好。 您在协助每个会议之前，需要重温前面章节中会议的基础知识，加上本节的一些技巧，制定出自己的详尽完备的会议议程和脚本。这一点是关于功夫在会前。另一点是功夫在会外，会议日程的素材来自日常的观察和收集。运筹帷幄，才能把会开好。 协助每日站会 强调站会的规则：15分钟，三个问题，杜绝超长时间的讨论。 强调规则之后，停止干预，重点放在观察上。 在站会之后，征得团队同意，提供观察和见解。 把一些轻微的违规行为留到回顾时再进行解决。 训练让团队自己启动站会。比如说到了时间准时开始，不等人，让迟到的人感受到团队之车运转的压力而不再迟到。 一旦15分钟到了，宣布站会结束。 站会期望获得的效果：产生健康的同侪压力、细粒度的协作、同时聚焦少数任务、每位团队成员每天都需要对团队做出承诺、提出障碍。 对于什么时候修正站会中的问题，什么时候不做任何处理，做出审慎的决定。当前做法是否影响到团队的自组织能力，是判断的基线。 解决站会问题的方法之一是强调站会希望获得的效果。 另一个方法是要求眼神支持，即当一个人在站会中发言时，其他团队成员都要直视发言的成员，进行眼神交流。一次一个焦点（任务、对话）。 为团队创造空间并在回顾的时候把问题提出来，而不是马上寻找其他方法去解决问题。 还可以采用一对一指导。 找到低效的站会与付出的代价之间的因果关系，并让团队知道。 要有耐心，并且坚持从多角度尝试。 协助迭代计划会议 当我们可以回答这些问题时，迭代计划就完成了：迭代目标是什么？团队构成是什么？总人力投入是多少？具有最高商业价值的待处理事项是什么？对于这些待处理事项的顾虑（技术的、政治的、文化的）是什么？团队还有什么其他顾虑？团队对本迭代的最终承诺是什么？ 当我们达到这些目标时，迭代计划就完成了：了解工作—理解它、选择它、把它任务化、志愿完成它，获得一个全新的开始，为共同目标做出承诺，创建重点和充裕感。 为迭代计划做准备：确保 PO 已准备好待处理事项，Scrum Master 准备好会议结构。 在迭代计划期间的协助：介绍会议的结构，包括会议的时间盒。 在迭代计划会上可教授的时机和发力点：专注于交付的商业价值、强化 PO 作为产品愿景和决策的唯一声音、维护健康的角色边界、利用思维导图和静默任务分解改善会议的进程和共同的理解。 协助迭代评审 在即将进行迭代评审前，提醒团队把做过的所有任务整合在一起，并演示当前迭代开发的真正产品。 不需要完美展示，只需要真实展示。更多的时间花在真正的工作上，而不是让事情看起来更好看。 评审的目标：展示承诺中什么完成了和什么没有完成，获得干系人和客户的直接反馈，介绍团队是如何一起工作、处理挑战和解决问题的，针对一些大的障碍向干系人寻求帮助。 按价值第一的原则发言，一是先讲重要的事，二是要考虑为什么这个功能对用户有价值。 教练以观察为主：团队是如何进行互动和互助的？团队与 PO、干系人、客户的互动是怎样的？PO 是否以产品待办列表作为管理需求的方式？有没有人被欺压或者被强制沉默？会议是否在同一时间有一个焦点并保持流畅？对话中有哪些对敏捷的误解和误用？ 跟团队分享你的观察。 观察有两种：加强的观察即哪些行为加强了敏捷的理念和实践，深化的观察即揭示团队的内部工作方法的特质。 协助回顾会议 回顾的目的：检查并调整，回头看团队是如何一起把工作做完而不是产品怎么样，以及怎样才能下次做得更好。 教练在日常观察到的问题，重要的问题可以即时提出，其他问题可以留到回顾会议时提出。 教练在日常观察和思考的问题：团队是否使用敏捷框架来促进协同？团队正在忍耐什么？工作流是否顺畅？彼此的沟通协调关照关注和协作有什么不足的地方？出彩的瞬间是什么？哪里进展慢？整个迭代期间，团队的焦虑程度如何变化？大家是否全身心地参与进来？兴奋程度何时和如何变化？ 可以从对观察的提炼中找出回顾的主题，也可以事先通过与团队和 PO 沟通获得回顾的输入。 议程的基调是关注重要的事情。 回顾会议要遵守时间盒。 一旦达成付诸实践的协议，就写下来，并张贴在显眼的位置。 回顾会议之后，观察协议是否被执行，并为下一次回顾会议收集意见。 分享因回顾而带来的收益。 协助团队对话 教练的关注在对话的质量而不在对话的内容。 在高质量的对话中，每人都发言，认真听其他人发言，从对话中涌现出许多想法，这些想法又互相催化产生一些新的想法，把每一个想法当作一个礼物并一直向前推进。 教练在对话中强有力的观察：是不是每个想发言的人都得到了发言的机会？这些想法是高质量的吗？团队是不是尽可能采用简单的想法？团队是不是疲劳了？团队是不是很紧张？团队是否足够大胆而不墨守成规？他们是不是尽可能多地完成工作？是否以客户价值为中心？被卡住了吗？是否有新的视角？ 择机分享观察和思考。 教练提出强有力的问题：还有什么地方不清楚？可能性是什么？想要探索的是什么？还有哪些角度可以考虑？如果可以自由选择，你会做什么？这件事的实质是什么？这会让你得到什么？你预想的是什么？对于类似情况，你最好的经验是什么？主要的障碍是什么？最大的顾虑是什么？机会和挑战是什么？ 教练提出强有力的挑战：放大他们的想法，带到一个全新的方向，高标准，打破局限。 第三剑：讲授讲授、指导和协助三者的关系是：在团队启动或迭代启动时进行讲授，在会议和对话中进行协助，为了保证讲授的理论能够落实以及能够以贴合团队实际情况的方式落实，需要对团队和个人进行指导。通常来说，讲授会发生在迭代启动时，协助发生在每一次会议和对话，而指导会根据实际情况发生在任何时刻。 这一节还包括了对不同角色的教授，其内容是对不同角色基本职责的补充。教授的时机依然可以是以流程和解决问题驱动，在流程和解决问题的过程中寻找教授的时机。 在团队起步时的讲授 这种时机只会出现一次，并且一去不复返。 强有力的团队启动在一两天内就可以完成。 启动期间要解决的问题：学习将要使用的流程，了解团队，了解将要做的工作，前进！ 在启动期间，重点关注面向任务而不是面向人的事情，更容易成功。也就是，更多时间放在学习流程和了解将要做的工作，较少时间放在了解团队。 学习流程：对于从未接触过敏捷的团队，是真正意义上的新开始。对于已经用过敏捷的人，他们自认为了解敏捷，但可能受制于之前所在团队的模式和局限，实际上已远离敏捷，教练需要向他们刷新可信的敏捷，重回敏捷的核心。 了解团队：从了解团队中的每一个人开始，然后创建一个共有的团队特征。 作为个体相互了解：可以让每个人描绘自己的职业历程，画出来并分享；可以让每个人展示自己的技能，其他人提供我可以如何帮助你和你可以如何帮助我的反馈；可以以星座为载体让每个人陈述自己的偏好；可以让每个人从一组价值观中选取重要的，并进行交流。这些活动是为了在团队成员之间建立深刻的理解。 创建共有的团队特征：创建共同的团队愿景；创建团队规范。 了解将要做的工作：展望产品愿景，评审产品待办事项，创建第一个迭代的目标和计划。 团队启动的三个层次：一是按上述框架设计启动议程，二是了解并满足主要合作者的目标，三是深入了解每个人的情况并设计有针对性的启动。 教授团队的新成员 有可能的话，保持团队稳定。 当一位团队成员离开时，确保团队对他的贡献给予答谢。 当一位成员加入时，向他介绍团队，团队规范，团队如何使用任务板合作完成迭代目标，团队的愿景。 向新成员教授敏捷。 让已有成员向新成员介绍产品。 定期了解和跟进新成员在敏捷实践中的进展。 教授产品负责人 产品负责人是价值推动者。产品负责人的任何决定，都要考虑是否给公司带来最大价值。 产品负责人要经常与团队在一起，以便在需要时作出日常决策。 产品负责人是愿景管理者，要帮助团队了解愿景及确保每一个迭代都是朝愿景推进。 产品负责人要保护团队免受外界的噪音和压力。 产品负责人是最终责任人，为产品的业务成果负责。 产品负责人要对工作作出承诺，并充分参与。 产品负责人要得到项目发起人的授权。 产品负责人要与各方协作。 产品负责人要对所从事的领域有渊博的知识。 了解并沟通团队对产品负责人的期望。 定期与产品负责人交流，哪些方面做得好，哪些方面还可以提高。 教授敏捷经理 敏捷经理包括团队成员的职能经理、利益相关者和其他团队的经理。 敏捷经理身受双重的挤压，一方面是团队自组织管理的挤压，另一方面是高层想要看到进度表和状态报告的挤压。 当他们看到团队交付成果时，他们所受的挤压会被减轻。 敏捷经理可以是组织变革家，引导组织对敏捷的采用。 敏捷经理可以是边界管理家：强化健康的角色边界，包括团队内部和团队之间。 敏捷经理是价值最大化的倡导者：管理项目组合。 敏捷经理是精益管理者：使用精益思想来管理组织流程，加速流动，减少浪费。 敏捷经理是组织障碍消除者：以坚忍不拔的勇气来消除根深蒂固的障碍。 敏捷经理是团队拥护者：信任和支持团队，让他们发挥潜能。 敏捷经理可以通过产品负责人把工作项加到待办列表。 敏捷经理可以把观察到的问题交给敏捷教练。 敏捷经理可以参加站会，但要保持安静。 敏捷经理可以参加迭代评审并给出反馈。 敏捷经理在得到请求时帮助移除障碍。 教授敏捷教练 敏捷教练是清道夫，帮助移除障碍。 敏捷教练是领头羊，引导团队回归敏捷的实践和本质。 敏捷教练是服务型领导，帮助团队更好地工作。 敏捷教练是质量和成果的监护者，检查并调整团队生产什么和如何生产。 本文作者： joni@efbiz.org 本文链接： https://github.com/efbiz/2018/05/23/敏捷教练第07课-技巧-敏捷教练的六脉神剑（上）","categories":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"}],"tags":[{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"}]},{"title":"云服务器ECS选购指南及省钱法宝","slug":"云服务器ECS选购指南及省钱法宝","date":"2022-03-07T16:00:00.000Z","updated":"2022-04-30T04:34:12.753Z","comments":true,"path":"2022/03/08/云服务器ECS选购指南及省钱法宝/","link":"","permalink":"http://zhangyu.info/2022/03/08/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8ECS%E9%80%89%E8%B4%AD%E6%8C%87%E5%8D%97%E5%8F%8A%E7%9C%81%E9%92%B1%E6%B3%95%E5%AE%9D/","excerpt":"","text":"云服务器ECS选购指南及省钱法宝（强烈建议收藏） 作者 | 阿**里云弹性计算产品专家 马小婷** https://developer.aliyun.com/article/872102 今天给大家带来的分享是如何购买云服务器ECS以及怎么买更省钱，分为四个部分： 第一部分介绍云服务器ECS的基本概念，告诉大家购买ECS实例时看哪些参数。就像小书生要买一部手机，会关注内存大小、CPU频率、屏幕分辨率、相机参数等，选购ECS实例同样可以通过参数选择来满足自己的上云需求。 第二部分介绍接云服务器ECS实例规格族，详细介绍阿里云主要的ECS产品系列。（时间紧张的情况下可以跳到第三部分） 第三部分详细讲解ECS的选型技巧，具体讲解不同场景（如大数据/数据库等）下如何选择ECS实例，或者某个ECS实例适用于怎样的生产、工作场景，重点干货部分，不容错过。 第四部分介绍如何省钱省力的来使用ECS，在满足自己需要的前提下，让你上云省钱更经济。 01 云服务器ECS基础概念 云服务器的基础概念第一部分会给大家介绍云服务器的一些基本概念。 在开始前，大家可以回想一下，我们自己购买笔记本电脑的时候会考虑哪些因素？我自己会先选择品牌，一般情况下在确定了品牌之后，接下来就会考虑硬件配置，主要是物理硬件的配置和软件的配置。 硬件配置上，我首先会考虑计算性能，像CPU和内存的大小、CPU的型号等；第二就是存储，笔记本电脑的磁盘有多大；第三部分就是网络能力，比如网卡有几个，对于玩游戏的同学来说，显卡配置也很重要。除了硬件配置外，我也会考虑电脑的操作系统是什么样的，比如Mac OS， windows或ubuntu等。而拿到电脑之后，我们首先会做一些基础应用软件的安装和配置，包括防火墙等保证我们整个应用环境的安全性。 这是我在现实生活中去购买一台物理电脑的流程，其实这些概念在云上也是适用的，比如说我们在选择一些物理硬件的参数的时候，选CPU和内存，对应在云上的话，就是选择ECS实例的 CPU 和内存大小以及 CPU 的型号。 存储这一块，磁盘在云上对应的概念就是块存储，在云上块存储其实是包含两个概念，一个概念是云盘，一个概念是本地盘。有一个跟我们现实生活中不太一样的点，是云上的块存储，我们在购买的过程中需要指定用作系统盘还是用作数据盘的。而现实生活中买了一个电脑里面是有一块磁盘，然后我们自己会把磁盘分成系统盘还是数据盘，但在云上的系统盘和数据盘是需要分开购买的，这是一点点区别。 在网络这一块其实也是类似的，云上提供弹性网卡，让用户通过访问云服务器就能够联通到网上。 除了这些物理硬件以外，要让一个云服务器真正的跑起来，跟现实生活一样，我们也需要去安装一个操作系统，这个操作系统在云上的概念就是镜像，阿里云提供多种不同的镜像版本供大家选择。 除此以外，云服务器还会有一些特殊的概念，比如安全组，本质上是通过一些规则来限定访问的流量，即被哪些应用可以访问。 我们在买一个电脑之后，这个物理机在手上，你想要什么时候使用就可以什么时候使用。在云上买完一个云服务器之后，因为这个服务器是在云端或者说在远端，我们访问云服务器的方式就跟我们平时打开一个电脑不太一样，我们需要通过阿里云的控制台或者通过远程连接的工具来登录到我们的云服务器上去。 还有一个小概念是云上的容灾备份能力，就是快照。现实生活中，如果我们的电脑磁盘出现了故障，数据出现了损坏就无能为力了，或者只能够找专业的人把数据能够找回来，但是不能够保证说所有的数据都能找回来。云上有快照这样一个概念，它的意思是说对云盘的某一个时间点的数据拍一张照，本质上就是会把磁盘上所有的数据记录下来，如果出现了问题，我们就可以通过快照，快速的回滚到某一个时间点的数据，这样能够保证在业务出现了问题的情况下，快速做灾备的恢复。 整体介绍完云服务器的基本概念之后，接下详细介绍一下云服务器的存储和网络的概念。 云上的三种存储方式第一种是前面已经介绍的块存储的模式，用户创建了一个块存储之后，可以把块存储挂载到实例上，就跟自己使用笔记本电脑过程中，电脑自带的磁盘不够用了，去买移动硬盘来插上来类似。块存储有三种类型，包括普通的高效云盘，还有SSD云盘，以及超高性能超低延迟的ESSD云盘。 第二种存储方式是文件存储，每一个块存储只能够挂载到一个云服务器上，而每个文件存储可以被多台ECS使用。 第三种存储形态是对象存储形态OSS，这个就类似于百度云盘，使用这种存储的方式，更多的通过一个链接来做文件的读取。 云上的网络网络部分主要是两个概念，专有网络VPC和交换机。 第一个是专有网络VPC，专有网络是在云上为用户划分一个私有网络，用户通过创建VPC可以创建逻辑上彻底隔离的一个网络环境，每一个VPC都是由一个路由器以及一个以上的交换机组成的。用户一旦创建了一个VPC专有网络，阿里云会自动为用户创建一个对应的路由器，来完成VPC下所有网络的转发。同一个VPC下的实例之间的内网是互通的，即在同一个VPC下实例之间可以通过内网IP地址来互相访问。 第二个概念是交换机，前面已经介绍了，一个VPC至少有一个路由器。交换机是专有网络的基础网络设备，用来连接不同的实例资源，我们可以通过交换机，在每一个可用区创建多个交换机来划分子网，然后多个交换机之间是可以通过路由器来实现连接和转发。以上是存储和网络的一些基础的概念。 云服务器ECS的使用流程下面我们介绍一下使用ECS的流程。 一个ECS的实例，我们可以把它理解成一台虚拟机，它包含内存、磁盘、网络和操作系统等软硬件。而一个ECS服务器实例是多大的规格，底层的物理硬件是什么样子的，是由对应的实例规格和实例规格族来决定的。实例规格族代表了实例适用的业务场景，它决定了CPU和内存配比，以及底层的物理硬件是什么样子的。实例规格代表的是实例的大小，比如说 CPU的数量是多少。 在确定了实例规格之后，我们还需要去选择对应的存储，因为只有CPU和内存的话，数据是没有办法存放的，所以就会有一个块存储。块存储有两种，一种是云盘，一种是本地盘。云盘其实是云上的一种三副本的存储形态，能够给用户提供高可用的能力。云盘主要用来做系统盘和数据盘，只需要像物理盘一样把它格式化就可以使用了，而本地盘可能更多的主要是用来做数据盘。 选择完了计算存储，我们接下来就要看对应的操作系统，云上的操作系统指的是镜像，目前阿里云提供多种镜像的来源，包括官方提供的这种公共镜像、第三方市场提供的镜像、用户自定义镜像，还允许不同的用户之间共享镜像。 网络方面阿里云会有一个网络带宽，用户可以直接指定。 我们把实例的计算、存储、网络以及操作系统等参数制定好之后，就可以创建一个跟我们物理的笔记本电脑一样的云服务器。 创建完之后，我们通过阿里云的控制台，或者是通过阿里云的APP，可以直接连接和访问已购买的云服务器。 02 ECS实例规格族介绍 第二部分我会给大家介绍一下ECS实例的规格族是怎么命名的，大家可能在这一块会有比较多的疑问。目前阿里云提供几百种实例规格，所以在选择的过程中会眼花缭乱，其实只要理解了ECS的实例规格族的命名方式，和它的信息布局，我们就能够很好的选型了。 实例的架构类型、规格分类与详细信息 在阿里云控制台的购买页面上可以看到，实例规格族的选择上分成三大模块：架构、分类、具体信息。最上面就是我们的实例规格架构的类型，有三种架构类型，分别是通用的X86的架构、异构计算（像GPU或者是FPGA、NPU等）、阿里云自研的神龙裸金属架构。 在每种架构下面会有实例规格的分类，从上图可以看到在X86的这种计算型态下，分成了7大类实例规格，不同实例规格代表了不同的硬件配置，选择任何一个实例规格的分类之后，我们可以看到对应实例规格的详细信息，这些信息主要分为四部分： 第一个就是实例规格族的详细信息，包括对应的规格族和实例规格的代称，这里可以通过点击小问号，能够看到实例规格族的一些详细的描述。第二部分是 CPU和内存大小的信息，这里是大家在选型的过程中会比较关注的。第三部分是实例的网络能力信息，包括实例内网的带宽和收发包的能力。第四部分是CPU的处理型号的信息，包括处理器的主频和睿频这两部分信息。 企业级实例 VS 入门级实例 在控制台的购买页面上可以看到，ECS的实例规格族特别多，单纯从CPU和内存是无法判断它们的区别，所以我们需要从宏观上来看。阿里云ECS的实例规格整体是分成两大类，一类是企业级实例，一类是入门级实例。 企业级实例是阿里云在2016年9月份才推出的，其特点是vCPU是独享的，也就意味着我们创建一个企业级实例的时候，实例vCPU与我们底层物理的 CPU是绑定了的，底层的物理CPU就不可能再分配给其他的实例了，所以企业级的实例不会出现资源的争抢，因此能保证性能稳定，并且企业级实例提供了非常严格的SLA性能保证。 而入门级实例就是vCPU跟底层的物理的CPU是不绑定的，意味着可能每个vCPU是随机分配到底层的空闲的一个物理CPU上，如果同一个物理的物理服务器上有多个共享入门级实例的话，不同的实例就会出现资源的争抢，导致CPU的性能不稳定。 因为入门级实例存在性能不稳定的特性，所以阿里云现在仅仅提供一种入门级实例，就是在X86架构中的共享型实例， 而X86架构中的其他实例规格，以及异构架构和神龙架构中的所有实例，都是属于企业级实例。 由于企业级实例性能稳定，并且有严格的SLA的保证，所以它比较适合于对业务稳定性有比较高的要求的场景。入门级实例由于不能够保证性能稳定性，所以价格相对便宜，比较适合于一些对性能没有严格要求，或者在某些时段下才会有性能突发要求的场景，比如有些轻负载的应用或者是微服务。 共享型实例在介绍完ECS实例大的分类之后，我们来看一下共享型实例的具体信息。 我们前面讲到了只有X86架构下的共享型实例才是入门级实例。这类实例比前面实例在四要素以外多出一个参数，即“平均基准的CPU计算性能”，基准性能即实例能够持续提供的CPU性能。 共享型实例也就是入门级实例，分成两大类，第一类是属于标准的共享型实例， CPU是不绑定的，只提供基准CPU性能，所以当出现资源的争抢，是否能超出基准性能是没有保障。 另外一种特殊的共享型实例，名为突发性能型的共享实例，它主要就是照顾到某些应用在绝大多数的时候CPU的使用率可能都不高，负载都不高，但是在某些时候可能会有临时的突发的高性能要求，所以阿里云会提供突发性能的参数，所以您在购买共享型实例的时候，能够通过突发性性能来获得高于平均基准CPU性能的能力。 突发性能型的共享实例，如果应用实际用量低于了平均的基准性能，会获得对应的CPU的积分，如果在某些场景下性能要求突然提升之后，比如实例对应的 CPU的使用率超过了20%，会消耗之前累积的CPU的积分，去提升计算性能，让计算性能不会受到影响，这个是突发性能的共享型实例独有的特性。 两个特殊的实例规格除了共享型的入门级实例以外，阿里云还有两个实例规格比较特殊，就是大数据型和本地SSD。 这两种实例规格会附带一个本地存储，大数据型实例的本地存储是HDD盘，本地SSD新增的本地存储是具有非常高I/O吞吐，并且有低延迟的本地SSD盘，具体的信息大家可以在阿里云控制台查看。 企业级实例规格家谱 下面介绍企业级实例规格的家谱，方便我们快速了解各个实例家族的“亲属”关系。企业级实例规格族分成三大块，第一大块是X86计算，除了共享型以外，包括通用、计算、内存、高主频、本地SSD和大数据型都属于我们的企业级实例，企业级实例每年都在不停地迭代，所以会分成不同的代系，我们在后面会详细介绍不同的代际之间的区别。异构计算里面所有的GPU和FPGA都是属于企业级的实例，裸金属和高性能计算也是一样的。 首先，我们来介绍X86的实例规格的命名方式，分成了5种： 第一种实例规格是通用型，顾名思义就是什么场景都能够用，所以这种型号的代称是g系列，它的vCPU和内存的一个配比是1:4。 第二种实例规格是计算型，顾名思义就是在某些场景下对CPU算力的要求会更高一点，所以它的vCPU和内存的配比是1:2，然后简称为c系列。 第三种类型是内存型，提供更多的内存能力，所以它的CPU和内存的配比是1:8，也简称为r系列，r是RAM的简称 第四种和第五种分别是大数据型和本地SSD型，这两种的CPU和内存的配比都是1:4，只是它们配的本地盘的类型是不一样的，导致它们的技能和适合的场景也是不一样的。所以大数据型的简称是d，本地SSD型简称是i。 在这5个基础的实例规格上面，我们会去做一些额外的能力提升，比如说在通用型、计算型和内存型这三种类型下，增加了一些高主频的能力，正常的 CPU的主频应该是2.5G赫兹，但是我们有一些可以是做到3.2G赫兹，这种加上高主频的能力就变成了高主频型，会在前面去加上一个hf这样的一个标识。 随着技术的演进，神龙架构的神龙卡也是在不断地迭代和改善，搭载了第三代的神龙卡可以整体提升通用型、计算型和内存型这三种实例规格的性能，所以就会出现一个平衡增强型。对于大数据型的话，做了计算和存储的分离，形成了大数据存储型，简称为d2，而 d2s是在大数据的基础上，做了一些网络能力的增强，就变成了一个网络增强型。 实例规格的命名方式和规律大家通过下图能够看到阿里云实例规格的命名方式和规律。 普通的X86实例规格名称是分成了三段，第一部分表示的是产品名称，ECS是阿里云的产品；第二部分表示了实例的规格和代系，前面已经讲过hfg表示是在通用型的基础上增加了高主频的能力，然后6代表的是什么？其实它代表的是我们产品的代系，可以根据产品的代系推算对应的产品的一个新旧，比如说6代表第6代，5代表的是第5代，这个数字越大代表它是更新的一个代系，它底层的物理硬件也会越新，它的性价比相对而言也会越高。 最后一部分是实例的规格，表示的是实例的vCPU的核数，large代表2个vCPU， xlarge代表4个 vCPU，2xlarge代表的是8个vCPU，以此类推。 了解了以上命名规律，就能通过实例规格族的名称推断出来当前这个实例的CPU是什么型号、它的是什么样的代系，以及它的 CPU的数量是多少。 GPU命名规则也是类似的，只有一个不一样的点，GPU名称的的中间这一部分会提供CPU和GPU的的配比关系，因为 GPU是除了CPU以外还会提供一个额外的GPU的卡。所以我们也是直接可以通过它的规格族的格式，能够去推断出来它底层的物理的配置。 03 ECS实例选型实战 第三部分给大家实战讲一下如何做云服务器ECS的选型。 简述各种规格实例的适用场景 X86计算: • X86的通用型、计算型和存储型三种实例，CPU和内存的配比比较一致，所以比较适合做一些中小型的数据库，或者是一些数据处理的任务。• c系列的话，主要是计算型，所以比较适合于做一些计算要求比较多的，比如说做一些外部应用，或者做一些批量计算，或者是一些高性能的科学计算类的。• r系列的话，因为它的内存比较多，所以比较适合于做一些数据库或者数据分析的应用。• 高主频实例规格也是比较适合于对CPU的主频有比较高要求的高性能科学计算。• 本地SSD类型，更多的适合于做一些关系型数据库或者是NoSQL数据库的• 而D系列的大数据型，可能更适合于做一些大数据集群的一个场景，比如说像这种Map Reduce这种。 在异构这一块，分成了两大类: • GPU比较适合于做深度学习或者是图像视频的可视化的处理;• FPGA就比较适合于做图像的转码，或者音视频的解码。 裸金属和高性能计算: • 更垂直和性能要求更高的一些场景，像一些高性能的数据库或者高性能科学计算场景。 下面我们举几个例子详细介绍一下选型方法。 X86实例选型推荐 我们可以把一个web应用分成以下几个层次，每个层次做对应的推荐： • 对于Apache和Nginx的web服务器，，因为它主要做一些计算处理，所以推荐是使用一些计算型的，比如说c5、c6这样的；• 对于像 spring cloud或者说MQ这样的中间件的话，它是属于对于计算和存储的诉求都比较正常的，所以我们是推荐一些通用性的，比如说g6这样的实例规格；• 而应用型因为是属于比较通用的场景，所以G6系列就能够满足；• Redis和Memcache这种缓存应用，对内存的要求是比较高的，所以我们推荐使用内存型的，像r系列；• 对于关系型数据库，我们是可以直接使用内存型，比如说r系列配上我们的SSD云盘；• 对于NoSQL，我们推荐本地SSD型的，比如I系列；• 对于大数据的话，类似于HDFS或spark的这种，我们也有专门的大数据型的，像d系列这种的来处理;• 对于最底层的机器学习的，比如MXNet这种训练框架，会有对应的专门的GPU计算型。 GPU实例选型推荐GPU云服务器的场景主要分成两大类，第一大类是人工智能，或者叫机器学习，第二块是图形图像的处理。在机器学习里面也会分成两个场景，一个是训练，一个是推理。所以对于不同细分的垂直领域，我们给了一些规格的推荐，具体可见下图。 下面我们介绍两个相对而言比较复杂的选型场景。 大数据场景实例选型实战 第一个复杂场景是大数据的场景，类似于Hadoop、Spark这种大数据集群搭建的时候，如果我们自己手动做搭建，会把过程分成三大块：第一大块就是集群的管理节点的实例规格选型，第二块是集群的计算节点的选型，第三块是集群的数据节点的选型。 • 管理节点是比较通用的场景，所以直接选择g系列就能够很好地处理管理的任务；• 计算节点更多的是属于比较偏正常的业务负载，所以可以把g系列作为主要的选择，搭配SSD云盘；• 数据节点对存储的吞吐和网络的吞吐有比较高的要求，所以推荐使用 d系列，搭配对应的本地盘，能够完成这种数据的读取量； 所以整体来说，在同样一个大数据的集群里面，不同的任务有不同的特征，所以会选择不同的实例规格。 数据库场景实例选型实战 第二个复杂场景是关于数据库选型的： • 对于普通的业务，负载比较轻的数据库，有专门的通用型g系列，或者内存型r系列搭配高效云盘和SSD云盘就能处理，性价比会比较高。因为g系列和本地盘或者本地SSD比起来，价格还是很有优势的。高效云盘和SSD云盘的整体性能，其实也是能够满足日常数据库的场景的。• 对于业务负载要求非常高的集群，推荐本地SSD的 i 系列搭载NVMe SSD的云盘，能够实现存储的高IOPS和低延时，能够满足重载数据库的性能要求。 X86 第6代vs第5代 实例价格对比除了性能以外，大家也会关注价格，这里有一个X86里面第6代和第5代的一个价格的对比。 可以看到除了计算型的实例在某些区域下，第6代实例会比第5代10实例的价格会略高4%以外，通用通用型和内存型的包月价格，第6代普遍比第5代要便宜2%-12%，所以整体来说的话，第6代不仅仅是性能有20%的提升，而且绝大多数的产品会更便宜。 而按量付费的话，第6代的价格比第5代的价格会低37%-47%，这其实是一个非常大的让利的空间。所以在选择按量去购买ECS的时候，选择第6代会比第5代要便宜的要便宜的更多。 选型实战总结总结选型方法，有三个法则，大家可以记在心里面，在选型的过程中运用。 第一个法则是相同大小的企业级的实例比入门级的实例性能更稳定，但是入门级的实例性价比更高，因为企业级的实例它是独占了vCPU，不存在一个资源的争抢，有性能的保障，但是对于一些个人或者中小网站的应用，如果对性能的诉求并不是那么强的话，选择入门级的实例其实是一个更好的选择。 第二个法则是在相同的实例规格下，新一代的实例规格比老一代的实例规格性价比更高，这是因为新一代的实例规格，做了很多技术的演进和更新换代，能够给公有云用户释放更多的技术红利。 第三个法则是选型时不仅仅要选择合适的实例规格，而且还需要搭载合适的块存储，才能够让云上的应用达到预期的性能。云上会提供4种不同的块存储，包括高效云盘、SSD云盘、ESSD云盘和本地盘，不同的类型盘的IOPS和吞吐是不一样的，所以不仅仅要选合适的实例规格，还要选择合适的块存储，才能够形成合力，达到最佳的性能。 04 ECS省钱省力之道 在购买云服务器的时候，除了要做实例规格的选型，让选择的实例规格和业务的匹配度更高以外，我们还需要去考虑能不能更便宜，能不能够快速完成资源的交付，所以最后一部分给大家介绍一下ECS省钱省力的技巧。 省钱大法第一个是省钱大法，省钱大法意思是选好了实例规格，还需要选择最适合的付费方式，才能够得到更好优化成本。阿里云目前提供7种付费方式，例如节省计划（Saving Plan）、包年包月、预留实例券、按量付费、抢占式实例等。 如何选择合适的付费方式呢？有一个攻略，就是我们需要根据业务的稳定性和峰谷的波动情况，来选择最适合的付费方式。像节省计划、包年包月、预留实例券就比较适合于稳定的业务负载；有状态并且是动态变化的业务负载的话，可以使用按量付费；而对于完全没有状态，并且具有很高的容灾能力的，可以使用抢占式的实例来交付，因为抢占式实例的价格是可以做到按量付费实例的10% 的。 省力之道第二个是省力之道。在云上购买资源的时候，有时候会批量购买，阿里云会提供多种自动化的资源交付模式和工具，能够实现一次配置重复使用，从而提升整个云上部署的速度和效率。 比如通过控制台做批量的交付；通过部署集可以完成底层具有容灾能力的算力集群的交付；通过弹性伸缩和弹性供应，能自动化地完成资源的交付；而通过资源编排，可以把多种不同的资源组合交付。 上云选型四步走 总结一下，上云的过程中，我们需要走好四步： 第一步：对自己的业务特征做一些分析，包括对性能的要求，对网络的要求，形成一个基本的判断；第二步：针对业务特征来选择对应的ECS实例规格；第三步：选择对应的一个付费方式，只有选择最合适的付费方式，才能够实现云上的成本最优；第四步：选择合适的交付方式，帮我们省时省力地完成资源的交付。 省钱法宝的更多分享，请参考：阿里云万郁香：多样付费选择构筑成本最优的弹性体验","categories":[{"name":"公有云","slug":"公有云","permalink":"http://zhangyu.info/categories/%E5%85%AC%E6%9C%89%E4%BA%91/"}],"tags":[{"name":"公有云","slug":"公有云","permalink":"http://zhangyu.info/tags/%E5%85%AC%E6%9C%89%E4%BA%91/"}]},{"title":"阿里云弹性计算研发团队如何从0到1自建SRE体系","slug":"阿里云弹性计算研发团队如何从0到1自建SRE体系","date":"2022-03-07T16:00:00.000Z","updated":"2022-03-08T17:16:40.322Z","comments":true,"path":"2022/03/08/阿里云弹性计算研发团队如何从0到1自建SRE体系/","link":"","permalink":"http://zhangyu.info/2022/03/08/%E9%98%BF%E9%87%8C%E4%BA%91%E5%BC%B9%E6%80%A7%E8%AE%A1%E7%AE%97%E7%A0%94%E5%8F%91%E5%9B%A2%E9%98%9F%E5%A6%82%E4%BD%95%E4%BB%8E0%E5%88%B01%E8%87%AA%E5%BB%BASRE%E4%BD%93%E7%B3%BB/","excerpt":"","text":"https://blog.csdn.net/weixin_46593167/article/details/117708269 SRE 最早在十多年前 Google 提出并应用，近几年随着DevOps的发展，SRE 开始被大家熟知。而在国内，非常多的 SRE 部门与传统运维部门职责类似，本质来说负责的是互联网服务背后的技术运维工作。构建区别于传统运维的 SRE、如何在业务研发团队落地SRE，是许多企业都在攻克的难题。 本届全球运维大会 GOPS 上，阿里云弹性计算团队技术专家杨泽强以《大型研发团队SRE 探索与实践》为题，分享了在 SRE 体系建设上的思考和落地实践。 本文为演讲内容整理，将从以下三部分进行介绍： 阿里云弹性计算（ECS）自建 SRE 体系的原因； ECS 建设 SRE 体系的探索与实践； 以弹性计算 SRE 体系建设的四年经验为例分享对 SRE 未来的看法。 为什么ECS要自建SRE体系？ECS 团队之所以会单独建 SRE，与产品业务特性以及组织层面上的背景有关。 下图展示了 ECS 的业务特点： 首先，从产品业务来看，ECS 是阿里云最大最核心的云产品。ECS 作为阿里巴巴经济体的以及其它部署在 ECS 上的云产品的底座，也支撑了国内外非常多的业务。阿里云全球份额排名第三，其中 ECS 的贡献毫无悬念是排名第一的，ECS 作为基础设施底座，稳定性要求特别高。 其次，由于阿里内部的经济体上云和整个云计算普及，ECS 对外的 OpenAPI 调用量每年都出现数倍增长，这意味着系统的容量每年都会面临新的挑战。 而与此同时，阿里云弹性计算启动了去 PE 的组织调整，即业务团队没有专职的运维工程师以及系统工程师，这将意味着运维类的事情、系统架构规划与横向产品需要有团队来承接。 ECS 建设 SRE 体系的探索与实践从 2018 年刚开始建设至今，在一路的摸索中，ECS 的 SRE 体系建设借鉴了 Google 和Netflix 的做法，并结合团队和业务的特性，最终 SRE 体系可以简单概况为下图的五个层次： 打基础。阿里云的文化主张里有一句话是“基础不牢，地动山摇”。在团队里具体的事情就是全链路稳定性治理体系以及性能容量工程，也是重要的基础。 定标准。这在一个大型研发团队里非常重要， ECS团队主要从软件的生命完整生命周期来看，从设计-&gt;编码-&gt;CR-&gt;测试-&gt;部署-&gt;运维-&gt;下线，各个环节定义了标准。通过定期的技术培训和定期运营，先在意识上给大家普及，同时会通过小团队的试运行来看效果，如果符合预期就想办法自动化掉。 建平台。通过建设自动化平台来不断释放SRE的人肉工作。 做赋能。业务团队的SRE除了做好横向的基础组件和自动化平台外，还要做很多推广和协助业务研发的事情；同时SRE每天都要处理非常多的预警响应，线上问题排障以及故障响应，如何把SRE的价值最大化，我觉得最核心的是赋能。 建团队。最后我将以弹性计算为例介绍一下SRE团队的职责，文化理念以及如何成为一名合格的SRE。 打基础基础框架建设与性能调优 弹性计算的核心业务都是 Java 技术栈，还有少部分 golang 和 python，本质上是一个Java 研发大型分布式系统。在内部为了支撑业务规模和尽可能的抽象整合，我们自研了一系列基础框架给业务同学使用，包括轻量bpm框架、幂等框架、缓存框架、数据清理框架等，其中每个框架的抽象和设计我们都考虑了规模化容量的支撑以及小型化的输出，以工作流框架为例，我们支持了每天数亿工作流的创建运行，框架调度开销做到了5ms级别。 除了基础框架，在性能优化上针对 JVM 进行了一系列调优，比如针对IO密集型的应用开启了wisp协程，以及针对每个核心应用JVM进行调优，减少STW的时间。 另外，从服务性能角度，数据层，我们对全网超过100ms的慢SQL进行了调优；应用层，我们针对核心链路提供了多级缓存方案，可以保障最热的数据可以从内存里最快的返回；业务层，我们通过提供批量API以及异步化改造。 全链路稳定性治理 上图罗列了几个比较典型的点，比如预警治理，普遍问题是预警量太大了，信噪比又不高，预警能提供的信息非常有限，对于排查排障帮助比较局限。 早年间，我们也面临同样的问题，分享预警治理的两个真实故事： 一个核心应用的数据库在晚上down了，但预警配置的通知渠道是email和旺旺，并且接收人不是当前应用owner，导致owner在发现故障问题的时候花了非常长时间定位到是数据库问题。 之前发生了一起全链路连锁反应的故障，故障发生的起因是其中某一个业务导致的，当时我们花了两个小时来定位和恢复问题，在事后复盘才发现故障开始前5分钟，已经有相关报警，但该报警接收同学的预警量太大，漏掉了重要预警。 所以，稳定性治理很重要的一部分就是预警治理，主要治理的方法就是监控分层、统一预警配置平台、统一预警优化配置策略，比如预警的接收人、通知方式等。 数据库稳定性治理 数据库是应用的命脉，发生在数据库上的故障往往非常致命。不论是数据的准确性或者数据的可用性受损，给业务带来的灾难通常是毁灭性的。 两个难题：慢SQL和大表 当在使用MySQL做数据存储的时候，最高频遇到的场景就是慢SQL和大表这两个难题。慢SQL会导致业务变慢甚至产生全链路的连锁反应导致雪崩，而大表问题和慢SQL通常也分不开，当表的数据量大到一定程度，MySQL的优化器在做索引选择的时候也会遇到一些奇怪的问题，所以在数据库的治理上基本围绕慢SQL和大表。 针对慢SQL的治理方案 大致的解法是把采集的慢SQL同步到SLS上，通过SLS做近实时的慢SQL分析，然后通过库表信息把慢SQL分给指定团队来修复，这个过程SRE同学会给出优化方案以及通用的基础组件，比如针对大页查询的提供bigcache以及nexttoken方案，针对热点数据的common cache以及读写分离降级能力。 针对大表的治理方案 针对大表问题，业界通常的解决方案是分库分表，但是其带来的研发和运维成本很高，我们内部一般业务更常用的方式是通过历史数据归档来做，在这里SRE也有统一的基础框架提供，业务方只需要给出数据归档条件以及触发频率，框架会自动将历史数据归档到离线库并把业务库数据做物理删除，这样通过腾挪数据空洞来达到空间的一定复用，保障有限的数据空间不扩容的前提来支撑业务的发展。 高可用体系 分布式系统的高可用可以分四个层次来看。从最底层的部署层，由下至上分别是运行时、数据层、业务层，我们的高可用体系也是按照四个层次来划分的。 部署层，作为ECS的研发我们对外推荐的最佳实践都是多可用区部署，理由很简单，因为容灾性更好、更柔性。 数据层，如前面提到的数据库稳定性治理，我们数据层一方面的工作就围绕像慢SQL、热点SQL和大表的持续治理，另外一方面就是从技术架构上我们做了多读和读写自动降级框架，可以将一些大表查询自动降级到只读库，同时可以保障读写库异常情况，核心API仍然可以通过只读库提供服务，进而来保障数据库整体的高可用。 业务层的高可用体系，一个复杂的分布式系统，难题之一就是解决依赖的复杂性，如何在依赖方不稳定的情况下仍然保障或者有损保障自身的核心业务可以玩转，这是分布式业务系统非常有挑战与有意义的一件事情。 故障案例 我们曾经出过一个故障，一个核心系统被一个非常不起眼的边缘业务场景搞到雪崩。核心系统里引入了一个三方系统依赖，当依赖方服务RT变慢的时候，我们所有的HTTP请求由于设置的超时时间不合理全部阻塞，进而导致所有线程都block在等待该服务返回，结果就是所有服务RT变长，直至不再响应。 要知道在庞大的分布式系统里，没有绝对可靠的授信链，我们的设计理念就是Design For Failure，以及Failure as a service。 可参考以下思路： 在设计阶段时定义该依赖的性质，是强依赖还是弱依赖 对方提供的SLO/SLA是什么，依赖方可能会出现什么问题以及对我们服务的影响是什么？ 如果依赖方出现了预期/非预期的异常，我们的策略是什么？ 如何保障我们服务的最大可用性。 最大可用性，意味着做出的响应可能有损，比如对端是弱依赖，我们可能会直接降级，返回一个mock结果，如果对端是强依赖，我们可能采取的是隔离或者熔断策略，快速失败部分请求，并尽可能记录更多信息，方便后续通过离线方式进行补偿。 定标准弹性计算的研发人员大概是百人以上规模，同时还会有一些兄弟团队以及外包人员一起参与研发，自SRE建设的第一天我们就开始逐步建立各种研发标准和流程。 以UT标准案例为例，UT缺失导致的故障占比高；难度是量大，研发不重视，实际没办法收敛。解法是通过建立UT标准，和CI自动化体系，量化指标来持续改进。效果是UT缺失导致的故障大幅降低，从占比30%降低至不到0.3%。 研发流程体系 我们从设计到发布几乎各个环境都定义了一套标准。 1.设计阶段。我们统一定义了一套设计文档模版来规范和约束研发人员。从软件工程角度来看，越早引入问题带来的成本越低，所以我们的研发原则之一也是重设计。一个好的设计不仅要从业务上定义清楚问题，定义清楚UserStory和UserCase以及约束，从技术角度也要清楚的讲清楚技术方案的tradeoff以及Design for failre如何实现、如何灰度、监控回滚等等。我们希望研发多在设计阶段下功夫，少在中途返工或者发布后打补丁。同时我们的评审机制也从线下大团队评审转变为小团队线下+大团队直播方式进行，尽可能少开会，节约大家时间。 2.研发阶段。我们的研发流程类似git-flow。是多feature并行开发，开发测试后合并进入develop分支，每天会有统一的流程基于develop进行daily deploy，我们基于阿里集团的Java规约做了扩展，加入了自定义的静态扫描规则，同时统一的UT框架，实现了CR后自动运行规约扫描执行静态检查，同时运行CI产出UT运行报告，只有静态扫描，CI结果主要是UT成功率和行增量覆盖率以及代码点赞数同时满足条件MR才会被合并，进入下次发布list。 3.测试阶段。主要分日常测试，预发测试以及上线前的功能测试以及灰度期间的回归测试。大规模研发团队大家面临的难题就是环境怎么搞？这么快速复制以及隔离？以往模式下我们只有一套日常和预发，经常出现某个人代码问题或者多人代码冲突导致测试特别耗时。后来我们做了项目环境，简单功能可以通过容器方式快速复制全链路项目环境。针对有全链路联调需求的case，我们扩展了多套预发环境，可以做到每个业务研发团队一套预发，大家互相不争抢，这样预发的问题就解决了。 上线前的功能测试主要是针对daily deploy的，我们会在每天晚上11点自动从develop拉取分支，并部署到预发环境，同时这个时候会自动运行全量的功能测试用例来保障发布的可靠性，在日常发布如果FVT非预期失败，会导致发布取消。 最后一个测试流程是灰度期间自动回归core fvt，我们的发布是滚动发布模式，通常会灰度一个地域来做灰度验证，core fvt就是做这个的，当core fvt运行通过后可以进行后续批次发布，反之判断是否回滚。 变更流程体系 在SRE建设的时候，我们特意规划了变更管控流程。针对当前的变更类型做了不同的标准要求，比如数据库变更checklist+review机制，日常发布/hotfix/回滚的批次以及暂停时长，还有就是中间件的配置规范以及黑屏变更等。 有了变更流程和规范只是第一步，接着我们针对高频率的运维操作做了工具化建设，其中有部分和现有的DevOps平台合作，游离在现DevOps之外的部分我们都自己做了研发支持，比如日志清理以及进程自动重启，并开发了自动化工具可以自动化清理大文件以及重启故障进程。 举一个例子就是数据订正，数据都是通过异步编排来实现最终一致性，所以数据订正会是一个特别高频的变更，简单的一条订正SQL蕴藏的威力有时候超过我们的想象，我们之前有一个故障就是因为一条数据订正错误导致，影响非常严重，排障过程也非常困难。 建平台SRE 自动化平台 我们做SRE自动化平台就是为了将标准通过自动化方式来实现，比如研发阶段的高可用体系里的读写降级，限流等。我们在提供框架能力的同时，提供了运维接口和白屏化工具，帮助研发实现自动化/半自动化的高可用能力。 弹性计算团队的1-5-10指标 后面的变更、监控、预警、诊断、恢复，对应的就是MTTR的各个细分阶段，在阿里集团有个1-5-10的指标，意思就是分钟发现问题，5分钟定位问题，和10分钟恢复问题，这是一个非常难达到的高标准。 弹性计算团队为了满足提升1-5-10指标，建立了自己的监控平台和预警平台。我们做的是预警能力的二次消费，将所有的基础监控包括系统指标cpu和mem、JVM监控以及各种中间件监控，还有非常多的业务监控做了分层，而每一个预警都会囊括各种meta信息，比如归属团队、重要性、关联的诊断场景、快恢策略，以及推荐的变更等。 这就闭环了变更、预警、诊断、快恢整个过程。当一条预警出现的时候可以自动根据TraceID分析全链路寻找局点，同时推荐相关变更，并生成影响面比如多少API、用户是哪些，以及该预警推荐的解决方案是什么，同时提供一个hook可以执行快恢动作。 举个例子，有位同学订正了一条业务，sql写的问题有问题，导致线上几个大客户的服务异常，在sql执行完的几秒内我们的监控系统就识别出了业务异常，并产生了预警信息以及预警的stack和影响面分析，同时关联的数据库变更信息也被推荐了出来，这些信息组合在一起的1分钟内我们就定位到了问题，并立马执行了回滚，业务很快就恢复了。当然该平台目前仍然有局限性，我们今年规划会做更多智能预警和诊断的事情。 最后，必须要提一下演练，即混沌工程 Chaos engineering，最早由Netflix提出。在过去的两年里，我们通过故障演练，故障注入的方式多次回放了历史故障，同时对发现线上问题也非常有帮助，故障演练现在已经作为一个常态化的事情融入到了日常。 分享完我们的SRE自动化平台体系，有了平台之后，非常重要的一个事情就是赋能。 我认为业务团队SRE最大的价值就是赋能，通过赋能来使众人行。 做赋能全链路SLO量化体系 ECS的上下游依赖方众多，任何一个环境出现不稳定都会影响ECS出口服务的稳定性。 举个例子ECS向下对接的是虚拟化和块存储，只要虚拟化和快存储慢了体现用户层面就是ECS实例启动慢了，而这个快慢究竟如何评定呢？可能对于我们做分布式服务来说可能觉得5ms已经很慢了，但是对于虚拟化来说他认为我这个接口1s都是正常的，这个时候就需要SLA了。 做SRE的第二年，我们梳理了全链路数十个依赖以及数百个核心API与各个业务方选择最关心的指标也就是SLI，针对不同的置信空间设置了SLO值，并且建设了统一的量化平台，通过实时与离线方式持续跟进起来。通过SLO体系建立到持续运营的一年多时间，我们的依赖方可用性和时延的SLO达标率从最初的40%多治理到98%以上。这个直接产生的业务就是我们对用户API成功率的提升，用户的体感更加好了。 落地SLO的四个阶段 选取SLI，即和你的依赖方确定哪些指标是需要关注的，比如通常都关心的可用性和时延； 和依赖方约定SLO，即明确某个API某个SLI的目标值 P99、P999分别是什么； 计算SLO，通常的方式都是通过记录日志，将日志采集到SLS，通过数据清洗再加工计算出指标值。 可视化，通过将SLO做成实时/离线的实时报表，来做持续跟进。 知识库 SRE 很大的一部分职责在于故障排查和疑难问题处理，同时我们做了一系列框架和工具，还要非常多的运维手册以及故障复盘的资料，这些我们都按照统一模版沉淀下来，可以用来指导研发同学日常问题排查和变更使用，其中部分文档我们还共享给了阿里云其它产品。 通过知识库我们也赋能了非常多的兄弟团队，另外我们研发过程中的很多业务基础组件像工作流、幂等、缓存、降级、Dataclean 等框架也都有阿里云其它团队在使用。 稳定性文化建设 SRE 是稳定性的捍卫者，也是布道师。只有人人都意识到稳定性的重要性，我们的系统才可能真正的稳定。我们从建设 SRE 团队的第一天就开始建流程，团队内通过日报，周报，月报以及不定期的线上直播以及线下培训来宣传稳定性文化。逐步在团队形成我们特有的稳定性文化，比如 Code Review 文化，安全生产文化和事后故障复盘文化。 故障复盘实践 在 SRE 初期，我们开始推行故障复盘。我们对于故障的定义是所有有损业务或者人效的异常 case 都是故障。一开始，故障复盘由 SRE 主导，由业务团队配合，但整个过程非常不愉快。 随着后面 SRE 的一些自动化工具以及一些流程真正帮助了研发避免故障，以及在故障复盘过程中 SRE 的一些见解给到了研发正向反馈，慢慢故障复盘的文化在团队开始慢慢被接受，各个业务自己会写故障复盘报告，并开直播分享，其它团队的同学也会非常积极地给反馈。文化真正的深入人心后，产生的会是非常好的良性循环。 建团队最后分享一下SRE团队组建主要注意的几个方面。 人 在弹性计算团队，我们对 SRE 的要求是T型人才，要一专多能。 精通一门编程语言 两个基本能力：抽象能力、标准化能力 拥有全局视野，具备赋能意识 事 事，基本上即是前面提到的建标准、建自动化平台、做基础服务和赋能业务团队，还有on-call支持等日常工作。 同时，我们需要在团队中建立几个共同的核心理念，我个人认为SRE几条最核心的理念： 稳定性就是产品，稳定性不是一锤子买卖。SRE 要做的是赋予稳定性产品的灵魂，要按照产品一样去养育，去不断迭代，去持续演进。 软件工程的方法论解决生产系统稳定性问题。SRE 区别于业务研发和运维的很大一点是，SRE 解决的是生产环境的稳定性问题，是通过软件工程的方法论来重新定义运维模型。 自动化一切耗费团队的事情。SRE 的精髓在于软件工程定义运维，通过自动化以及赋能业务来最大化价值。 SRE 建设的回顾与个人展望简单概括下弹性计算团队四年的 SRE 发展历程就是，建体系-量化-自动化-智能化。 第一年：体系化探索 这一年主要的工作就是从0-1结合当前业务和团队的现状把SRE体系建设起来，比如基础框架的统一建设，稳定性治理体系。 第二年：SLO体系 第二年的重点主要是针对全链路数十个系统依赖，以及内部系统的核心功能定义了SLO（service level object，SLO）量化体系，并跨BU完成了整个链路的SLO量化体系建设。同时开始重点建设稳定性运营体系，以及自己的数据运营平台，将内外部核心依赖的核心API的可用性，时延的数据全部量化并持续跟进治理起来。 第三年：自动化 我们把研发过程中从设计、编码、测试、部署到上线后的预警响应等所有需要人工参与的事情都做了尽可能的自动化。比如在CR阶段加入UT覆盖率卡点，不符合标准的CR会自动拦截。在发布阶段接入了无人值守，根据发布期错误日志的情况来进行发布拦截，当然这里更好的方式可能是通过灰度机器的服务SLA等综合指标来判断。另外，在灰度地域发布暂停期间，我们会自动化运行corefvt来回归核心测试用例验证发布的可靠性，异常情况会自动拦截灰度，并推荐一键回滚操作。 第四年：智能心智 今年我们正在做的一些事情是高度自动化，比如无人发布值守，还有自动化预警根因分析等。当我布式系统规模足够大，应用复杂度足够高的时候，靠人的判断是非常困难的。所以，SRE要建设的自动化平台要有智能心智，通过系统化的能力来代替甚至超越人。 对SRE未来发展的个人看法： 稳定性即产品，我们真正是需要把稳定性当作产品来看待的，做产品意味着我们要清楚的定义问题，并产生解决方案，并且持续的迭代演讲，这不是一锤子买卖，是需要养育的。 我觉得随着云计算的普及，SRE的技能会倾向于研发技能，当然系统工程的能力也是必须的，因为云计算作为基础设施会帮助我们屏蔽掉非常多的机房、网络、OS层面的问题，这样SRE的重点就在于用软件工程的方法论来重新定义运维，使用自动化来提高效能。 Netflix提出了一个CORE SRE的概念，NetFlix是这样解读CORE的，C就是Cloud，我们都知道Netflix是run on AWS，Cloud是基础。 Operation就是运维了，Reliability和Enginering就不多说了。 而我对CORE SRE的另一个解读是少量核心的SRE人员支撑并保障大规模服务的稳定性。 少量的SRE支撑大规模服务背后的最核心理念我觉得是自动化，尽可能最大化一切可以自动化的事情，并且要智能的自动化。 总结下稳定性就是，产品 + Dev的比重会增大 + CORE SRE + 自动化一切。 以上就是我今天想要和大家分享的内容，谢谢大家的聆听。","categories":[{"name":"SRE","slug":"SRE","permalink":"http://zhangyu.info/categories/SRE/"}],"tags":[{"name":"SRE","slug":"SRE","permalink":"http://zhangyu.info/tags/SRE/"}]},{"title":"云上资源自动化部署新模式","slug":"云上资源自动化部署新模式","date":"2022-03-07T16:00:00.000Z","updated":"2022-03-08T17:15:38.066Z","comments":true,"path":"2022/03/08/云上资源自动化部署新模式/","link":"","permalink":"http://zhangyu.info/2022/03/08/%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E6%96%B0%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"从原子操作走向模板部署，详解云上资源自动化部署新模式https://blog.csdn.net/bjchenxu/article/details/117794680 5 月 29 日，阿里[云开发]者大会的《应用开发的基础设施云上优化》分论坛上，阿里云技术专家王斌鑫发表了主题为《云上资源自动化部署新模式》的分享，详细阐述了云上资源自动化部署新模式——基于资源编排、Terraform托管、ROS CDK的自动化部署最佳实践。 本文为根据王斌鑫的演讲整理成文。 当前云上资源部署模式 云上资源传统部署模式的挑战 大部分用户一般是通过控制台/API/SDK等传统模式进行云上资源的部署，这种部署模式会面临规模、效率、规范和成本四个方面的挑战： 1. 规模上，随着业务的发展需要管理的资源规模不断上升，部署和管理种类繁多的资源带来挑战。 2. 效率上，随着规模上升，手动批量部署变得难以为继。 3. 规范上，如何确保对基础设施的变更均符合组织管理规范。 4. 成本上，手动部署的方式无法极致地利用云上弹性能力，其成本仍有优化空间。 阿里云自动化部署模式 资源编排服务（ROS）的核心价值 面对传统部署模式的挑战，我们在想是否能够帮助客户对云上资源进行自动化部署？因此有了阿里云资源编排服务（ROS），它基于基础设施即代码（IaC）的理念，让开发者和管理员使用模版的方式，编排云上的多种资源，进行自动化部署。 对比手动部署，使用资源编排服务ROS进行自动化部署会带来如下好处： • 效率提升，针对诸如SAP这样复杂的解决方案能有效提升部署效率，也能够帮助MSP、ISV、onECS服务提升部署效率； • 架构优化，ROS提供了种类丰富的阿里云最佳实践模板，用户无需丰富的架构经验即可部署解决方案级别的架构； • 流程管控，由IT管理员统一管理基础设施以避免各类风险，且可基于模板进行审核再进一步结合CI/CD以规范化IT管理流程； • 节省成本，自动化部署方式可以按需部署和释放资源，从而极致地使用云上弹性能力来降低成本。 ROS 的使用流程和核心功能 使用 ROS 进行自动化部署的过程非常简单： 1. 按照ROS 模板语法编写模板，定义想要创建的各类云上资源。 2. 在 ROS 控制台 使用模板创建资源栈，以执行部署。其中，资源栈是一组资源的集合，这些资源均是模板中定义的资源。 3. 在 ROS 控制台 查看资源栈，可以查看栈中各种资源的创建情况，并可以跳转到对应资源的控制台。 值得一提的是，ROS 服务本身完全免费，集成了身份认证和安全审计的功能，资源创建结果是可视化的，且能够进行多账号跨地域地部署，支持资源栈和实际资源的差异检测并进行修正。 除了直接使用 ROS 模板来做自动化部署，是否还有别的方式呢？ 新模式一：Terraform 托管 Terraform 是什么 Terraform 同 ROS 一样，也是基于基础设施即代码（IaC）的理念的自动化编排工具。它使用一种特定的配置语言（HCL, Hashicorp Configuration Language）来描述基础设施资源，语法样例如上图所示。 Terraform &amp; ROS 既然 Terraform 和 ROS 都是基于相同的理念的自动化编排工具，那它们的目标也是一致的，都是为用户打造良好的云上部署体验。 两者有很多相同之处，比方说 Terraform 的配置文件相当于ROS的模板，Terraform 的状态相当于 ROS 的资源栈，Terraform 的 CLI 程序则相当于 ROS 的编排引擎。 两者也各有优势，Terraform 的语法更简洁，对多云支持地很完善；而 ROS 则提供免费的服务托管，且有云原生的鉴权和审计能力。 那么是否能够将两者的优势结合呢？因此就有了 Terraform 托管能力。 Terraform 托管 用户直接在本地使用 Terraform 时，需要根据当前的操作系统下载对应的 Terraform CLI，编写模板，管理所使用的各类 Provider 的版本，且要管理状态等文件。 而使用 Terraform 托管功能时，只需在 ROS 的控制台编写 Terraform 模板便可直接部署，后续则通过资源栈来管理模板中定义的资源。底层的各类管理都交给 ROS。 在使用原理上，ROS 控制台会将 Terraform 模板组合成符合 ROS 语法规范的模板，ROS 服务端会其进行语法校验，生成租户信息，调度到 ROS 的 Terraform 服务进行资源的部署。 定时与多云场景实践 我们可以在很多场景中使用 Terraform 托管的功能。 场景一：定时部署资源 假设我们需要通过 Terraform 定时部署资源，传统方式下需要本地创建定时任务，执行 Terraform CLI 来做。而在云上，我们可以： • 事先编写一个 Terraform 模板，声明想要部署的云资源； • 事先编写一个 OOS 运维模板来声明由它调用 ROS 进行资源部署； • 在 OOS 中设置为定时执行，OOS 会定时触发 ROS，ROS 则会使用 Terraform 托管功能进行资源部署。 场景二：多云管理 如果我们既想要对多种云平台（如阿里云、AWS等）的资源进行部署，又想有可视化的结果反馈，可以直接编写 Terraform 的模板来声明各个云上资源，并使用 ROS 的 Terraform 托管功能来进行部署。 Terraofrm 托管总结 Terraform 托管功能能让用户在云上直接使用 Terraform，和直接使用 ROS 模板部署有一致的控制台体验，且兼容了 ROS 原生的API，同时兼备了统一的身份认证和权限控制。相比于本地使用 Terraform，不再需要管理多种 Provider 和多个 Terraform CLI版本。 新模式二：ROS CDK 现有资源定义方式的不足 事实上，通过直接编写 ROS 模板，或者通过可视化编辑器生成模板，然后进行资源部署的方式是能够大大提升资源的部署效率的，但是也有一些不足之处： • 缺少对过程式的支持 • 复杂场景的编写效率较低 • 对程序的友好性较低 • 动态性支持较差 针对这些问题，是否可以更进一步，在模板之上解决这些不足呢？ ROS CDK 是什么 ROS CDK 是资源编排（ROS）提供的一种命令行工具和多语言SDK，利用面向对象的高级抽象模式对云资源进行标准定义，从而快速构建云资源。 ROS CDK 以应用作为资源管理的入口，一个应用管理多个资源栈，而每个资源栈中则可以有多个构件。构件可以理解为云上资源的组件，能包含一个或多个资源。 我们可以选择自己熟悉的编程语言（TypeScript/JavaScript/Java/Python/C#）编写应用代码声明想要部署的资源，ROS CDK 会将项目代码转换成 ROS 模板，然后使用该模板进行自动化部署。 使用步骤和项目生命周期 ROS CDK 的使用步骤也很简单： 1. 首先，就是初始化项目，配置阿里云的访问凭证（AccessKey） 2. 其次，就是编写资源代码和测试用例进行本地测试 3. 最后，通过CDK CLI或者直接程序进行资源部署，并管理资源栈 在进行部署的阶段，CDK会根据用户编写的资源代码进行构造，实例化出各种资源对象；然后在准备阶段做终态前的调整（通常由框架自动完成）；进而验证各种资源属性，确保能够正确部署；最终合成出一个 ROS 模板，并使用该模板部署为资源栈。 代码、模板示例 上图中，左边是 ROS CDK的资源代码，其中声明了一个 VPC，并使用循环动态生成3个 VSwitch。而右边则是由 ROS CDK 生成的 ROS 模板。由此可以看出针对动态生成的场景，ROS CDK 可以大大简化模板编写的复杂度。 应用程序集成CDK实现持续部署场景 假设我们需要实现一个CI/CD系统，能够部署这样的资源架构：使用API网关中提供API，使用函数计算的函数提供业务逻辑，要分别部署测试、预发、线上环境的资源，并且支持从测试发布到预发，从预发部署到线上。 针对这样的资源架构，在直接使用 ROS 模板的方式中，需要分别为三个环境准备三个模板，而环境间的部署则还需要动态拼接模板，对应用程序来说并不友好。这里就建议使用ROS CDK，这样应用程序可以根据环境的不同指定对应的变量，生产对应的资源，从而满足环境的动态性部署。 ROS CDK 总结 相较于直接使用 ROS 模板部署，使用 ROS CDK 允许开发者选择自己熟悉的编程语言，并能借助其动态特性来实现复杂的编排效果。ROS CDK 能够非常容易地集成到应用程序中，从而能够方便地在程序中进行资源部署。 总结 企业上云规模逐渐增大，企业云上资源的部署方式从人工开始走向自动，从单云走向多云，从原子操作走向模板部署。随着基础设施即代码的理念而兴起，资源的部署模式也因场景的不同而不同，总体来说有以下四个部署模式的建议： 1. 作为入门级用户，只需管理有限几个资源，直接使用控制台的方式是最为简单直观的； 2. 作为企业IT管理员，需管理规模较大的云上资源，使用 ROS 模板管理基础设施会是最有效率的选择； 3. 作为运维研发人员，需要在业务系统中实现资源部署逻辑，那么 ROS CDK 会是最佳选择； 4. 作为多云管理员，需可视化管理阿里云、AWS、Azure等多种云的资源，使用 ROS Terraform 托管功能是不二之选。","categories":[{"name":"SRE","slug":"SRE","permalink":"http://zhangyu.info/categories/SRE/"}],"tags":[{"name":"SRE","slug":"SRE","permalink":"http://zhangyu.info/tags/SRE/"}]},{"title":"开启shareProcessNamespace后容器异常","slug":"cotainer-init","date":"2021-05-30T16:00:00.000Z","updated":"2021-05-31T12:41:15.290Z","comments":true,"path":"2021/05/31/cotainer-init/","link":"","permalink":"http://zhangyu.info/2021/05/31/cotainer-init/","excerpt":"","text":"https://qingwave.github.io/cotainer-init/ 背景目前k8s不支持容器启动顺序，部分业务通过开启shareProcessNamespace监控某些进程状态。当开启共享pid后，有用户反馈某个容器主进程退出，但是容器并没有重启，执行exec会卡住，现象参考issue 复现 创建deployment apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx name: nginx spec: shareProcessNamespace: true containers: image: nginx:alpinename: nginx 查看进程信息 由于开启了shareProcessNamespace, pause变为pid 1, nginx daemonpid为6, ppid为containerd-shim # 查看容器内进程 / # ps -efo “pid,ppid,comm,args” PID PPID COMMAND COMMAND 1 0 pause /pause 6 0 nginx nginx: master process nginx -g daemon off; 11 6 nginx nginx: worker process 12 6 nginx nginx: worker process 13 6 nginx nginx: worker process 14 6 nginx nginx: worker process 15 0 sh sh 47 15 ps ps -efo pid,ppid,comm,args 删除主进程 子进程被pid 1回收, 有时也会被containerd-shim回收 / # kill -9 6 / # / # ps -efo “pid,ppid,comm,args” PID PPID COMMAND COMMAND 1 0 pause /pause 11 1 nginx nginx: worker process 12 1 nginx nginx: worker process 13 1 nginx nginx: worker process 14 1 nginx nginx: worker process 15 0 sh sh 48 15 ps ps -efo pid,ppid,comm,args docker hang 此时对此容器执行docker命令(inspect, logs, exec)将卡住， 同样通过kubectl执行会超时。 分析在未开启shareProcessNamespace的容器中，主进程退出pid 1, 此pid namespace销毁，系统会kill其下的所有进程。开启后，pid 1为pause进程，容器主进程退出，由于共享pid namespace，其他进程没有退出变成孤儿进程。此时调用docker相关接口去操作容器，docker首先去找主进程，但主进程已经不存在了，导致异常(待确认)。 清理掉这些孤儿进程容器便会正常退出，可以kill掉这些进程或者killpause进程，即可恢复。 方案有没有优雅的方式解决此种问题，如果主进程退出子进程也一起退出便符合预期，这就需要进程管理工具来实现，在宿主机中有systemd、god，容器中也有类似的工具即init进程(传递信息，回收子进程)，常见的有 docker init, docker自带的init进程(即tini) tini, 可回收孤儿进程/僵尸进程，kill进程组等 dumb-init, 可管理进程，重写信号等 经过测试，tini进程只能回收前台程序，对于后台程序则无能为力(例如nohup, &amp;启动的程序)，dumb-init在主进程退出时，会传递信号给子进程，符合预期。 开启dumb-init进程的dockerfile如下，tini也类似 FROM nginx:alpine # tini# RUN apk add –no-cache tini# ENTRYPOINT [“/sbin/tini”, “-s”, “-g”, “–”] # dumb-initRUN wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init\\_1.2.2\\_amd64RUN chmod +x /usr/bin/dumb-initENTRYPOINT [“/usr/bin/dumb-init”, “-v”, “–”] CMD [“nginx”, “-g”, “daemon off;”] init方式对于此问题是一种临时的解决方案，需要docker从根本上解决此种情况。容器推荐单进程运行，但某些情况必须要运行多进程，如果不想处理处理传递回收进程等，可以通过init进程，无需更改代码即可实现。 参考[1] https://github.com/Yelp/dumb-init[2] https://github.com/krallin/tini[3] https://github.com/kubernetes/kubernetes/issues/92214","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"k8s中shell脚本启动如何传递信号","slug":"docker-shell-signal","date":"2021-05-30T16:00:00.000Z","updated":"2021-05-31T12:43:18.387Z","comments":true,"path":"2021/05/31/docker-shell-signal/","link":"","permalink":"http://zhangyu.info/2021/05/31/docker-shell-signal/","excerpt":"","text":"https://qingwave.github.io/docker-shell-signal/ 背景在k8s或docker中，有时候我们需要通过shell来启动程序，但是默认shell不会传递信号（sigterm）给子进程，当在pod终止时应用无法优雅退出，直到最大时间时间后强制退出（kill -9）。 分析普通情况下，大多业务的启动命令如下 command: [“binary”, “-flags”, …] 主进程做为1号进程会收到sigterm信号，优雅退出(需要程序捕获信号); 而通过脚本启动时，shell作为1号进程，不会显示传递信号给子进程，造成子进程无法优雅退出，直到最大退出时间后强制终止。 解决方案exec如何只需一个进程收到信号，可通过exec，exec会替换当前shell进程，即pid不变 # do somethingexec binay -flags … 正常情况测试命令如下，使用sleep来模拟应用sh -c &#39;echo &quot;start&quot;; sleep 100&#39;：pstree展示如下，sleep进程会生成一个子进程 bash(28701)───sh(24588)───sleep(24589) 通过exec运行后，命令sh -c &#39;echo &quot;start&quot;; exec sleep 100&#39; bash(28701)───sleep(24664) 加入exec后，sleep进程替换了shell进程，没有生成子进程 此种方式可以收到信号，但只适用于一个子进程的情况 trap在shell中可以显示通过trap捕捉信号传递给子进程 echo “start”binary -flags… &amp;pid=”$!” _kill() { echo “receive sigterm” kill $pid #传递给子进程 wait $pid exit 0} trap _kill SIGTERM #捕获信号wait #等待子进程退出 此种方式需要改动启动脚本，显示传递信号给子进程 docker-initdocker-init即在docker启动时加入--init参数，docker-int会作为一号进程，会向子进程传递信号并且会回收僵尸进程。 遗憾的是k8s并不支持--init参数，用户可在镜像中声明init进程，更多可参考container-init RUN wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init\\_1.2.2\\_amd64RUN chmod +x /usr/bin/dumb-initENTRYPOINT [“/usr/bin/dumb-init”, “-v”, “–”] CMD [“nginx”, “-g”, “daemon off;”]","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"在Kubernetes中实施审计策略","slug":"enforce-audit-policy-in-k8s","date":"2021-05-30T16:00:00.000Z","updated":"2021-05-31T12:49:59.507Z","comments":true,"path":"2021/05/31/enforce-audit-policy-in-k8s/","link":"","permalink":"http://zhangyu.info/2021/05/31/enforce-audit-policy-in-k8s/","excerpt":"","text":"在Kubernetes 中实施审计策略 作者：Vinod Kumar Nair 翻译：Bach (K8sMeetup) 校对：星空下的文仔 如果我们想检查 Kubernetes 生产环境中的以下活动： 谁登录了 Kubernetes 集群？ 哪个服务帐户或用户访问了集群中的哪些资源？ 谁创建了 secret 或 configmap？ 谁看了 ETCD 的 secrets ，或者其他更多？ 那么在 Kubernetes 中执行审计策略（Audit Policy）是非常正确的选择。 典型的 Kubernetes 环境 没有审计策略的 Kubernetes 启用后，审计记录将在 kube-apiserver 组件内开始其生命周期。每个请求在其执行的每个阶段都会生成一个审计事件，然后根据特定策略对其进行预处理，并写入后端。该策略确定记录的内容，后端将保留记录。当前的后端实现包括日志文件和 webhooks。 每个请求都可以记录一个关联的阶段（stage）。定义的阶段有： RequestReceived - 此阶段对应审计处理器接收到请求后，并且在委托给其余处理器之前生成的事件。 ResponseStarted - 在响应消息的头部发送后，响应消息体发送前生成的事件。只有长时间运行的请求（例如 watch）才会生成这个阶段。 ResponseComplete - 当响应消息体完成并且没有更多数据需要传输的时候。 Panic - 当 panic 发生时生成。 审计策略规则和级别 审计策略定义了有关应该记录哪些事件以及应包含哪些数据的规则。审核策略对象结构在 audit.k8s.ioAPI 组中定义。处理事件时，会按顺序将其与规则列表进行比较。第一个匹配规则设置事件的级别（audit levels）。定义的审核级别有： None - 符合这条规则的日志将不会记录。 Metadata - 记录请求的元数据（请求的用户、时间戳、资源、动词等等），但是不记录请求或者响应的消息体。 Request - 记录事件的元数据和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse - 记录事件的元数据，请求和响应的消息体。这不适用于非资源类型的请求。 下面是一个典型的审计策略文件： Log all requests at the Metadata level.apiVersion: audit.k8s.io/v1kind: Policyrules:- level: Metadata 复杂一点就是： apiVersion: audit.k8s.io/v1kind: PolicyomitStages: “RequestReceived”rules: level: RequestResponseresources: group: “”resources: [“pods”] level: Metadataresources: group: “”resources: [“pods/log”, “pods/status”] level: Noneresources: group: “”resources: [“configmaps”]resourceNames: [“controller-leader”] level: Noneusers: [“system:kube-proxy”]verbs: [“watch”]resources: group: “” # core API groupresources: [“endpoints”, “services”] level: NoneuserGroups: [“system:authenticated”]nonResourceURLs: “/api*“ # Wildcard matching. “/version” level: Requestresources: group: “” # core API groupresources: [“configmaps”]namespaces: [“kube-system”] level: Metadataresources: group: “” # core API groupresources: [“secrets”, “configmaps”] level: Requestresources: group: “” # core API group group: “extensions” # Version of group should NOT be included. level: MetadataomitStages: “RequestReceived” 架构-Kubernetes 中的审计策略 Kubernetes 启用了审计策略 我们可以使用 Webhooks 将审核日志发送到文件或远程 Web API。 在本文中，我们将强制 kube api-server 将审核日志发送到文件。 在 Kubernetes 中启用审计策略（对于审计日志文件） 创建审计策略 YAML 文件：前往 Kubernetes 集群，并使用以下规则创建 audit-policy.yaml： apiVersion: audit.k8s.io/v1kind: Policyrules: Log the request body of configmap changes in kube-system. level: Requestresources: group: “” # core API groupresources: [“configmaps”]namespaces: [“kube-system”] Log configmap and secret changes in all other namespaces at the Metadata level. level: Metadataresources: group: “” # core API groupresources: [“secrets”, “configmaps”] A catch-all rule to log all other requests at the Metadata level. level: MetadataomitStages: “RequestReceived” 更新 kube api-server 清单文件： - kube-apiserver - –advertise-address=10.156.0.6 - –audit-policy-file=/etc/kubernetes/audit-policy.yaml - –audit-log-path=/var/log/audit.log mountPath: /etc/kubernetes/audit-policy.yamlname: auditreadOnly: true— mountPath: /var/log/audit.logname: audit-logreadOnly: false-–volumes: name: audithostPath: path: /etc/kubernetes/audit-policy.yaml type: File name: audit-loghostPath: path: /var/log/audit.log type: FileOrCreate k8s-api-server —清单文件 k8s-api-server —清单文件 就这样，转到生成的审计日志文件。 在这个案例中，这是 audit.log。我们可以看到在阶段级别捕获的有关 Kubernetes 集群的审计日志信息，如以下示例中所示： { “kind”:”Event”, “apiVersion”:”audit.k8s.io/v1”, “level”:”Metadata”, “auditID”:”a42fa658-f143–43d8-b5e6–4e101d3e15ea”, “stage”:”ResponseComplete”, “requestURI”:”/api/v1/namespaces/default/secrets?fieldManager=kubectl-create”, “verb”:”create”, “user”:{ “username”:”kubernetes-admin”, “groups”:[ “system:masters”, “system:authenticated” ] }, “sourceIPs”:[ “10.156.0.2” ], “userAgent”:”kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19”, “objectRef”:{ “resource”:”secrets”, “namespace”:”default”, “name”:”test-secret”, “apiVersion”:”v1” }, “responseStatus”:{ “metadata”:{ }, “code”:201 }, “requestReceivedTimestamp”:”2021–04–03T13:50:37.009656Z”, “stageTimestamp”:”2021–04–03T13:50:38.040874Z”, “annotations”:{ “authorization.k8s.io/decision”:”allow”, “authorization.k8s.io/reason”:”” }}{ “kind”:”Event”, “apiVersion”:”audit.k8s.io/v1”, “level”:”Metadata”, “auditID”:”f1466b01–9b68–45ec-b3bb-b440397f6481”, “stage”:”ResponseComplete”, “requestURI”:”/api/v1/namespaces/default/secrets/test-secret”, “verb”:”get”, “user”:{ “username”:”kubernetes-admin”, “groups”:[ “system:masters”, “system:authenticated” ] }, “sourceIPs”:[ “10.156.0.2” ], “userAgent”:”kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19”, “objectRef”:{ “resource”:”secrets”, “namespace”:”default”, “name”:”test-secret”, “apiVersion”:”v1” }, “responseStatus”:{ “metadata”:{ }, “code”:200 }, “requestReceivedTimestamp”:”2021–04–03T13:51:08.603724Z”, “stageTimestamp”:”2021–04–03T13:51:08.607716Z”, “annotations”:{ “authorization.k8s.io/decision”:”allow”, “authorization.k8s.io/reason”:”” }}{ “kind”:”Event”, “apiVersion”:”audit.k8s.io/v1”, “level”:”Metadata”, “auditID”:”30be8c70-fda6–44de-8a83–3fe56161d44e”, “stage”:”ResponseComplete”, “requestURI”:”/api/v1/namespaces/default/secrets/test-secret”, “verb”:”get”, “user”:{ “username”:”kubernetes-admin”, “groups”:[ “system:masters”, “system:authenticated” ] }, “sourceIPs”:[ “10.156.0.2” ], “userAgent”:”kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19”, “objectRef”:{ “resource”:”secrets”, “namespace”:”default”, “name”:”test-secret”, “apiVersion”:”v1” }, “responseStatus”:{ “metadata”:{ }, “code”:200 }, “requestReceivedTimestamp”:”2021–04–03T13:54:57.867317Z”, “stageTimestamp”:”2021–04–03T13:54:57.871369Z”, “annotations”:{ “authorization.k8s.io/decision”:”allow”, “authorization.k8s.io/reason”:”” }} 此外， 我们可以使用以下 kube-apiserver 标志配置 Log 审计后端，来更改审计日志文件的状态： --audit-log-maxage 定义保留旧审计日志文件的最大天数。 --audit-log-maxbackup 定义要保留的审计日志文件的最大数量。 --audit-log-maxsize 定义审计日志文件的最大大小（兆字节）。 总结 审计策略会检查 Kubernetes 集群中发生的所有请求、响应。这是一个最佳实践，应在早期阶段就启用。在本文示例中，和大家展示了如何将审计数据发送到文件。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"Cilium网络概述","slug":"Cilium-Network-Overview","date":"2021-05-25T16:00:00.000Z","updated":"2021-05-26T02:49:26.327Z","comments":true,"path":"2021/05/26/Cilium-Network-Overview/","link":"","permalink":"http://zhangyu.info/2021/05/26/Cilium-Network-Overview/","excerpt":"","text":"Cilium 对系统的要求比较高，例如内核的版本要求Linux kernel &gt;= 4.9.17 受限于eBPF比较新，且需要的内核版本较高，因此目前还没有被kubernetes大规模推广，但该网络方案是一个大趋势。 目前calico已经支持eBPF模式(不建议生产使用)，且阿里云的Terway插件也是基于eBPF。 Google 声明GKE将选择 Cilium作为 GKE 网络的数据面V2以便增加其容器安全性和可观测性。 GKE 使用 Cilium 的声明: https://cloud.google.com/blog/products/containers-kubernetes/bringing-ebpf-and-cilium-to-google-kubernetes-engine 原文链接：https://mp.weixin.qq.com/s/NrlxI5uMqQQ3sDrrPSKhZA Cilium是一种开源网络实现方案，与其他网络方案不同的是，Cilium着重强调了其在网络安全上的优势，可以透明的对Kubernetes等容器管理平台上的应用程序服务之间的网络连接进行安全防护。 Cilium在设计和实现上，基于Linux的一种新的内核技术eBPF，可以在Linux内部动态插入强大的安全性、可见性和网络控制逻辑，相应的安全策略可以在不修改应用程序代码或容器配置的情况下进行应用和更新。 Cilium在其官网上对产品的定位称为“API-aware Networking and Security”，因此可以看出，其特性主要包括这三方面： 提供Kubernetes中基本的网络互连互通的能力，实现容器集群中包括Pod、Service等在内的基础网络连通功能； 依托eBPF，实现Kubernetes中网络的可观察性以及基本的网络隔离、故障排查等安全策略； 依托eBPF，突破传统主机防火墙仅支持L3、L4微隔离的限制，支持基于API的网络安全过滤能力。Cilium提供了一种简单而有效的方法来定义和执行基于容器/Pod身份（Identity Based）的网络层和应用层（比如HTTP/gRPC/Kafka等）安全策略。 架构Cilium官方给出了如下的参考架构，Cilium位于容器编排系统和Linux Kernel之间，向上可以通过编排平台为容器进行网络以及相应的安全配置，向下可以通过在Linux内核挂载eBPF程序，来控制容器网络的转发行为以及安全策略执行。 图1 Cilium架构 在Cilium的架构中，除了Key-Value数据存储之外，主要组件包括Cilium Agent和Cilium Operator，还有一个客户端的命令行工具Cilium CLI。 Cilium Agent作为整个架构中最核心的组件，通过DaemonSet的方式，以特权容器的模式，运行在集群的每个主机上。Cilium Agent作为用户空间守护程序，通过插件与容器运行时和容器编排系统进行交互，进而为本机上的容器进行网络以及安全的相关配置。同时提供了开放的API，供其他组件进行调用。 Cilium Agent在进行网络和安全的相关配置时，采用eBPF程序进行实现。Cilium Agent结合容器标识和相关的策略，生成eBPF程序，并将eBPF程序编译为字节码，将它们传递到Linux内核。 图2 Cilium部署架构 Cilium Operator 主要负责管理集群中的任务，尽可能的保证以集群为单位，而不是单独的以节点为单位进行任务处理。主要包括，通过etcd为节点之间同步资源信息、确保Pod的DNS可以被Cilium管理、集群NetworkPolicy的管理和更新等。 组网模式Cilium提供多种组网模式，默认采用基于VXLAN的Overlay组网。除此之外，还包括： 通过BGP路由的方式，实现集群间Pod的组网和互联； 在AWS的ENI（Elastic Network Interfaces）模式下部署使用Cilium； Flannel和Cilium的集成部署； 采用基于ipvlan的组网，而不是默认的基于veth； Cluster Mesh组网，实现跨多个Kubernetes集群的网络连通和安全性等多种组网模式。 本文将针对默认的基于vxlan的overlay组网，进行深度的原理和数据包路径分析。 Overlay组网使用官方给出的yaml文件，通过下述命令，实现Cilium的快速部署。 root@u18-161:~# kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.6.5/install/kubernetes/quick-install.yaml 部署成功后，我们可以发现，在集群的每个主机上，启动了一个Cilium Agent（cilium-k54qt，cilium-v7fx4），整个集群启动了一个Cilium Operator（cilium-operator-cdb4d8bb6-8mj5w）。 root@u18-161:~# kubectl get pods –all-namespaces -o wide | grep ciliumNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEkube-system cilium-k54qt 1/1 Running 0 80d 192.168.19.161 u18-161kube-system cilium-v7fx4 1/1 Running 0 80d 192.168.19.162 u18-162kube-system cilium-operator-cdb4d8bb6-8mj5w 1/1 Running 1 80d 192.168.19.162 u18-162 在这种默认的组网情况下，主机上的网络发生了以下变化：在主机的root命名空间，新增了如下图所示的四个虚拟网络接口，其中cilium_vxlan主要是处理对数据包的vxlan隧道操作，采用metadata模式，并不会为这个接口分配ip地址；cilium_host作为主机上该子网的一个网关，并且在node-161为其自动分配了IP地址10.244.0.26/32，cilium_net和cilium_host作为一对veth而创建，还有一个lxc_health。 在每个主机上，可以进入Cilium Agent，查看其隧道配置。比如进入主机node-161上的Cilium Agent cilium-k54qt，运行cilium bpf tunnel list，可以看到，其为集群中的另一台主机node-162（192.168.19.162）上的虚拟网络10.244.1.0创建了一个隧道。同样在node-162上也有一条这样的隧道配置。 图3 Cilium默认Overlay组网 接下来创建Pod1和Pod2运行于node-161，Pod3和Pod4运行于node-162。其与主机的root命名空间，通过veth-pair连接，如下图所示。 图4 测试环境组网示例 进入Pod1，可以发现，Cilium已经为其分配了IP地址，并且设置了默认的路由，默认路由指向了本机的cilium_host。初始状态Pod内的arp表为空。 root@u18-161:~# kubectl exec -it test-1-7cd5798f46-vzf9s -n test-1 bashroot@test-1-7cd5798f46-vzf9s:/# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.244.0.26 0.0.0.0 UG 0 0 0 eth010.244.0.26 0.0.0.0 255.255.255.255 UH 0 0 0 eth0root@test-1-7cd5798f46-vzf9s:/# arproot@test-1-7cd5798f46-vzf9s:/# 在Pod1中ping Pod2，通过抓包可以发现，Pod发出的ARP请求，其对应的ARP响应直接通过其对端的veth-pair 接口返回（52:c6:5e:ef:6e:97和5e:2d:20:9d:b1:a8是Pod1对应的veth-pair）。这个ARP响应是通过Cilium Agent通过挂载的eBPF程序实现的自动应答，并且将veth-pair对端的MAC地址返回，避免了虚拟网络中的ARP广播问题。 No. Time Source Destination Protocol Length Info133 39.536478 52:c6:5e:ef:6e:97 5e:2d:20:9d:b1:a8 ARP 42 Who has 10.244.0.26 Tell 10.244.0.71134 39.536617 5e:2d:20:9d:b1:a8 52:c6:5e:ef:6e:97 ARP 42 10.244.0.26 is at 5e:2d:20:9d:b1:a8 主机内Pod通信分析完组网状态之后，那么同一个主机内，两个Pod间通信的情况，就很容易理解了。例如，Pod1向Pod2发包，其数据通路如下图所示Pod1 –&gt; eth0 –&gt; lxc909734ef58f7 –&gt; lxc7c0fcdd49dd0 –&gt; eth0 –&gt; Pod2。 图5 主机内Pod通信路径 跨主机Pod通信在这种Overlay组网模式下，Pod跨节点之间的通信，通过vxlan实现隧道的封装，其数据路径如下图所示pod1 –&gt; eth0 –&gt; lxc909734ef58f7 –&gt; cilium_vxlan –&gt; eth0(node-161) –&gt; eth0(node-162) –&gt; cilium_vxlan –&gt; lxc2df34a40a888 –&gt; eth0 –&gt; pod3。 图6 跨主机节点Pod通信路径 我们在cilium_vxlan虚拟网络接口上抓包，如下所示。从抓包分析可以看出，Linux内核将Pod1发出的原始数据包发送到cilium_vxlan进行隧道相关的封包、解包处理，然后再将其送往主机的物理网卡eth0。 图7 cilium_vxlan抓包 在物理网卡eth0抓包可以发现，Pod1出发的数据包经过cilium_vxlan的封装处理之后，其源目的地址已经变成物理主机node-161和node-162，这是经典的overlay封装。同时，还可以发现，cilium_vxlan除了对数据包进行了隧道封装之外，还将原始数据包进行了TLS加密处理，保障了数据包在主机外的物理网络中的安全性。 图8 node-161 eth0抓包 API感知的安全性安全可视化与分析Cilium在1.17版本之后，推出并开源了其网络可视化组件Hubble，Hubble是建立在Cilium和eBPF之上，以一种完全透明的方式，提供网络基础设施通信以及应用行为的深度可视化，是一个应用于云原生工作负载，完全分布式的网络和安全可观察性平台。 Hubble能够利用Cilium提供的eBPF数据路径，获得对Kubernetes应用和服务网络流量的深度可见性。这些网络流量信息可以对接Hubble CLI、UI工具，可以通过交互式的方式快速发现诊断相关的网络问题与安全问题。Hubble除了自身的监控工具，还可以对接像Prometheus、Grafana等主流的云原生监控体系，实现可扩展的监控策略。 图9 Hubble架构图 从上图的架构以及Hubble部署可以看出，Hubble在Cilium Agent之上，以DaemonSet的方式运行自己的Agent，笔者这里的部署示例采用Hubble UI来操作和展示相关的网络以及安全数据。 root@u18-163:~# kubectl get pods –all-namespaces -o wide | grep hubblekube-system hubble-5tvzc 1/1 Running 16 66d 10.244.1.209 u18-164 kube-system hubble-k9ft8 1/1 Running 0 34m 10.244.0.198 u18-163 kube-system hubble-ui-5f9fc85849-x7lnl 1/1 Running 4 67d 10.244.0.109 u18-163 依托于Hubble深入的对网络数据和行为的可观察性，其可以为网络和安全运维人员提供以下相关能力： 服务依赖关系和通信映射拓扑：比如，可以知道哪些服务之间在相互通信？这些服务通信的频率是多少？服务依赖关系图是什么样的？正在进行什么HTTP调用？服务正在消费或生产哪些Kafka的Topic等。 运行时的网络监控和告警：比如，可以知道是否有网络通信失败了？为什么通信会失败？是DNS的问题？还是应用程序得问题？还是网络问题？是在第4层(TCP)或第7层(HTTP)的发生的通信中断等；哪些服务在过去5分钟内遇到了DNS解析的问题？哪些服务最近经历了TCP连接中断或看到连接超时 TCP SYN请求的未回答率是多少 等等。 应用程序的监控：比如，可以知道针对特定的服务或跨集群服务，HTTP 4xx或者5xx响应码速率是多少？在我的集群中HTTP请求和响应之间的第95和第99百分位延迟是多少 哪些服务的性能最差 两个服务之间的延迟是什么 等等这些问题。 安全可观察性：比如，可以知道哪些服务的连接因为网络策略而被阻塞？从集群外部访问了哪些服务？哪些服务解析了特定的DNS名称？等等。 图10 Hubble界面功能 从上图Hubble的界面，我们可以简单的看出其部分功能和数据，比如，可以直观的显示出网路和服务之间的通信关系，可以查看Flows的多种详细数据指标，可以查看对应的安全策略情况，可以通过namespace对观测结果进行过滤等等。 微隔离默认情况下，Cilium与其他网络插件一样，提供了整个集群网络的完全互联互通，用户需要根据自己的应用服务情况设定相应的安全隔离策略。如下图所示，每当用户新创建一个Pod，或者新增加一条安全策略，Cilium Agent会在主机对应的虚拟网卡驱动加载相应的eBPF程序，实现网络连通以及根据安全策略对数据包进行过滤。比如，可以通过采用下面的NetworkPolicy实现一个基本的L3/L4层网络安全策略。 apiVersion: “cilium.io/v2”kind: CiliumNetworkPolicydescription: “L3-L4 policy to restrict deathstar access to empire ships only”metadata:name: “rule1”spec:endpointSelector: matchLabels: org: empire class: deathstaringress: fromEndpoints: matchLabels: org: empire toPorts: ports: port: “80” protocol: TCP 图11 Cilium网络隔离方案示意图 然而，在微服务架构中，一个基于微服务的应用程序通常被分割成一些独立的服务，这些服务通过API（使用HTTP、gRPC、Kafka等轻量级协议）实现彼此的通信。因此，仅实现在L3/L4层的网络安全策略，缺乏对于微服务层的可见性以及对API的细粒度隔离访问控制，在微服务架构中是不够的。 我们可以看如下这个例子，Job Postings这个服务暴露了其服务的健康检查、以及一些增、删、改、查的API。Gordon作为一个求职者，需要访问Job Postings提供的Jobs相关信息。按照传统的L3/L4层的隔离方法，可以通过iptables -s 10.1.1.1 -p tcp –dport 80 -j ACCEPT，允许Gordon来访问Job Postings在80端口提供的HTTP服务。但是这样的网络规则，导致Gordon同样可以访问包括发布信息、修改信息、甚至是删除信息等其他接口。这样的情况肯定是我们的服务设计者所不希望发生的，同时也存在着严重的安全隐患。 图12 L7微隔离示例 因此，实现微服务间的L7层隔离，实现其对应的API级别的访问控制，是微服务网络微隔离的一个重要部分。Cilium在为Docker和Kubernetes等基于Linux的容器框架提供了支持API层面的网络安全过滤能力。通过使用eBPF，Cilium提供了一种简单而有效的方法来定义和执行基于容器/pod身份的网络层和应用层安全策略。我们可以通过采用下面的NetworkPolicy实现一个L7层网络安全策略。 图13 Cilium实现微服务安全 apiVersion: “cilium.io/v2”kind: CiliumNetworkPolicydescription: “L7 policy to restrict access to specific HTTP call”metadata:name: “rule1”spec: endpointSelector: matchLabels: org: empire class: deathstaringress: fromEndpoints: matchLabels: org: empiretoPorts: ports: port: “80”protocol: TCPrules: http: method: “POST” path: “/v1/request-landing” Cilium还提供了一种基于Proxy的实现方式，可以更方便的对L7协议进行扩展。如下图所示，Cilium Agent采用eBPF实现对数据包的重定向，将需要进行过滤的数据包首先转发至Proxy代理，Proxy代理根据其相应的过滤规则，对收到的数据包进行过滤，然后再将其发回至数据包的原始路径，而Proxy代理进行过滤的规则，则通过Cilium Agent进行下发和管理。 当需要扩展协议时，只需要在Proxy代理中，增加对新协议的处理解析逻辑以及规则处置逻辑，即可实现相应的过滤能力。 图14 L7层访问控制协议扩展原理图 总结Cilium是一个基于eBPF和XDP的高性能网络方案，本文着重介绍了其原理以及默认的overlay组网通信。除了基本的网络通信能力外，Cilium还包含了基于eBPF的负载均衡能力，L3/L4/L7的安全策略能力等相关的内容，后续会进行更详细的实践分析。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Cilium","slug":"Cilium","permalink":"http://zhangyu.info/tags/Cilium/"}]},{"title":"快速生成k8s的yaml配置的4种方法","slug":"generating-yaml-for-k8s","date":"2021-05-18T16:00:00.000Z","updated":"2021-05-19T07:34:35.208Z","comments":true,"path":"2021/05/19/generating-yaml-for-k8s/","link":"","permalink":"http://zhangyu.info/2021/05/19/generating-yaml-for-k8s/","excerpt":"","text":"[快速生成kubernetes(k8s)的yaml配置的4种方法](https://www.toutiao.com/i6952422377816965639/ wid=1621396706552) 快速生成k8s的yaml配置的4种方法1、通过kubectl命令行快速生成一个deployment及service的yaml标准配置 #我们在后面加上`--dry-run -o yaml` --dry-run代表这条命令不会实际在K8s执行，-o yaml是会将试运行结果以yaml的格式打印出来，这样我们就能轻松获得yaml配置了 # kubectl create deployment nginx --image=nginx --dry-run -o yaml apiVersion: apps/v1 # &lt;--- apiVersion 是当前配置格式的版本 kind: Deployment #&lt;--- kind 是要创建的资源类型，这里是 Deployment metadata: #&lt;--- metadata 是该资源的元数据，name 是必需的元数据项 creationTimestamp: null labels: app: nginx name: nginx spec: #&lt;--- spec 部分是该 Deployment 的规格说明 replicas: 1 #&lt;--- replicas 指明副本数量，默认为 1 selector: matchLabels: app: nginx strategy: &#123;&#125; template: #&lt;--- template 定义 Pod 的模板，这是配置文件的重要部分 metadata: #&lt;--- metadata 定义 Pod 的元数据，至少要定义一个 label。label 的 key 和 value 可以任意指定 creationTimestamp: null labels: app: nginx spec: #&lt;--- spec 描述 Pod 的规格，此部分定义 Pod 中每一个容器的属性，name 和 image 是必需的 containers: - image: nginx name: nginx resources: &#123;&#125; status: &#123;&#125; # 基于上面的deployment服务生成service的yaml配置 # kubectl expose deployment nginx --port=80 --target-port=80 --dry-run -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: nginx name: nginx spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx status: loadBalancer: &#123;&#125; 2、利用helm查看各种官方标准复杂的yaml配置以供参考 # 以查看rabbitmq集群安装的配置举例 # 首先添加chart仓库 helm repo add aliyun-apphub https://apphub.aliyuncs.com helm repo update # 这里我们在后面加上 --dry-run --debug 就是模拟安装并且打印输出所有的yaml配置 helm install -n rq rabbitmq-ha aliyun-apphub/rabbitmq-ha --dry-run --debug 3、将docker-compose转成k8s的yaml格式配置 # 下载二进制包 # https://github.com/kubernetes/kompose/releases # 开始转发yaml配置 ./kompose-linux-amd64 -f docker-compose.yml convert 4、docker命令输出转换成对应的yaml文件示例 这里以 Prometheus Node Exporter 为例演示如何运行自己的 DaemonSet。 Prometheus 是流行的系统监控方案，Node Exporter 是 Prometheus 的 agent，以 Daemon 的形式运行在每个被监控节点上。 如果是直接在 Docker 中运行 Node Exporter 容器，命令为： docker run -d \\ -v &quot;/proc:/host/proc&quot; \\ -v &quot;/sys:/host/sys&quot; \\ -v &quot;/:/rootfs&quot; \\ --net=host \\ prom/node-exporter \\ --path.procfs /host/proc \\ --path.sysfs /host/sys \\ --collector.filesystem.ignored-mount-points &quot;^/(sys|proc|dev|host|etc)($|/)&quot; 将其转换为 DaemonSet 的 YAML 配置文件 node_exporter.yml： apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: node-exporter-daemonset spec: template: metadata: labels: app: prometheus spec: hostNetwork: true # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 1 直接使用 Host 的网络 containers: - name: node-exporter image: prom/node-exporter imagePullPolicy: IfNotPresent command: # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 2 设置容器启动命令 - /bin/node_exporter - --path.procfs - /host/proc - --path.sysfs - /host/sys - --collector.filesystem.ignored-mount-points - ^/(sys|proc|dev|host|etc)($|/) volumeMounts: # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 3 通过Volume将Host路径/proc、/sys 和 / 映射到容器中 - name: proc mountPath: /host/proc - name: sys mountPath: /host/sys - name: root mountPath: /rootfs volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys - name: root hostPath: path: /","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"使用fklek搭建Kubernetes日志收集工具栈","slug":"fklek-to-k8s-log","date":"2021-04-27T16:00:00.000Z","updated":"2021-04-28T05:58:46.365Z","comments":true,"path":"2021/04/28/fklek-to-k8s-log/","link":"","permalink":"http://zhangyu.info/2021/04/28/fklek-to-k8s-log/","excerpt":"","text":"使用 Fluentd+Kafka+Logstash+Elasticsearch+Kibana搭建 Kubernetes 日志收集工具栈 https://mp.weixin.qq.com/s/lPeYavvFJ6GdivkT0iwTGw","categories":[{"name":"日志","slug":"日志","permalink":"http://zhangyu.info/categories/%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"日志","slug":"日志","permalink":"http://zhangyu.info/tags/%E6%97%A5%E5%BF%97/"}]},{"title":"只有黑话才能拯救互联网人","slug":"只有黑话才能拯救互联网人","date":"2021-04-26T16:00:00.000Z","updated":"2022-06-09T04:58:52.506Z","comments":true,"path":"2021/04/27/只有黑话才能拯救互联网人/","link":"","permalink":"http://zhangyu.info/2021/04/27/%E5%8F%AA%E6%9C%89%E9%BB%91%E8%AF%9D%E6%89%8D%E8%83%BD%E6%8B%AF%E6%95%91%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA/","excerpt":"","text":"只有黑话，才能拯救互联网人 原创 东半球第二正经の 牛顿顿顿 4月8日 最近，一鸣同学在字节年会上怒斥了公司报告中的 “ 互联网黑话 “ 现象，我看了一眼他举的例子，整个人当场就被生态化反了： 给大家念一段我用咱们双月会材料里摘出来的词，拼凑出来的一段话： 过去我们主要依靠推荐技术赋予的信息分发能力、跨端联动抖头西、分多个产品自研，实现深度共建，形成组合拳，打造内容生态闭环，以此赋能客户用户创造价值。未来我们要增加横向不同场景价值，延长服 务链路。同时纵深满足用户需求，借助人类年龄的自然势能，在小中青多个年龄用户深度渗透。另外通过加强基建投入，多种阵地相关产品完善经营价值链路，建立对外用户持久影响力。 黑话不是没见过，但这种级别的，堪称 “ 互联网黑神话 “，玉皇大帝来了也只能喊一声： 你这个颗粒度的输出，很难击穿我的心智啊。 为了发力，拆解，抨击这个现象，我和阿半紧张的孵化了一整宿，通过增强耦合性，我们一起进行了串联，拉通，输出，为这篇稿子赋能。 我对阿半个说，写完稿子咱们就联动，我喜欢在后面发力，你最好给我一个抓手，我要牢抓你的两个突出点，两手抓两手都要硬，我们对齐水位，聚焦痛点，打通要害，完成闭环，你要提高感知度，我们一起达到引爆点。 阿半当时脸就红了，发出了灵魂的拷问： 那个务实、奋进、用结果说话，承载着年轻人光荣与梦想的互联网死哪儿去了？ 1 为什么这些互联网人在职场不讲人话，整天搞这些 “ 务虚 “ 的八股文 “ 黑话 “？ 因为，黑话是最好的 “ 职场遮羞布 “。 大厂今天 OKR，明天 361，大搞周报文化，甚至早有早报，晚有晚报，月有月报，仔细一看，全是福报。 原来是为了汇报工作写 PPT，现在为了写 PPT 写 PPT。 入行互联网，别的没学会，先成了 PPT 精装师，模板搬运工。 但问题是，大部分人的工作价值感是很低的，一周搞不出什么东西来，很多人既没有 Power 也没有 Point。 那么如何让自己平庸的，低效的无价值工作，听上去，稍微有那么点价值和深度呢？ 这时候就需要点专业的 “ 黑话 “ 来装点门面了。 我今天虽然别的没干，找人修了个打印机，但只要用黑话包装一下，就成了： “ 联动协同多部门，多维度发力支撑办公场景，打造硬件故障紧急处理 SOP，提前布局对接团队，以易用性组合拳为抓手，提高团队运作效率。”装神弄鬼，故弄玄虚，把简单的东西讲复杂还不容易吗？ 2 大厂里那些大大小小的领导，嘴里的黑话也越来越高了。 为什么？ 别觉得当上了领导，就有真本事，输出个 “ 洞察 “： 这几年的互联网人，尤其是大厂的中层，草包含量越来越高。 越是废物，越热衷拼凑这些，晦涩难懂，佶屈聱牙，专业色彩浓厚的 “ 术语 “ 为自己 “ 赋魅 “。 他们自己都没意识到，能当上领导，完全不是因为能力强，只是运气好，沾上了时代尿频一样洒下的红利。 很多中层高 P，无非就是出生早一点，毕业早一点，在早期混进了大厂。 大厂也有年功序列制，公司高速成长，新的管理岗缺口自然就会出现，哪怕做不出什么成绩来，只要你苟的聪明，同届有本事的人跳出去闯了，那就是剩者为王。 老人们的职场，就像坐电梯往上升，就算你是条狗，跟着走，也能干成保卫科科长。 这就是时代红利滋中了你。 不管是时代的红利也好，空降兵也好，外行领导内行也好，越来越多的草包领导们面临同一个困境： 不够屌，怎么才能装逼呢？ 如果完全不专业，那至少也要想办法让自己 “ 看起来 “ 专业。 赋魅的本质，就是包装。 搓澡不叫搓澡，叫人体表皮组织研究； 搬砖不叫搬砖，叫研究物质空间位置转移的科研项目； 贴膜不叫贴膜，叫智能数字通讯设备表面高分子化合物平面处理； 互联网黑话是对酒囊饭袋最好的包装。 3 更重要的是：务虚是最安全的。 这几年一个趋势，大佬们也不爱讲真话了。 其实，早些年，很多互联网大佬其实还是讲人话的，不仅讲，还十分爱讲。 一鸣同学当年喜欢出来谈延迟满足，王兴在写了一万多条饭否传经布道，黄峥甚至开公众号写连载，当年，讲的都是大实话。 然后呢？ “ 悔创阿里 “” 不知妻美 “” 一无所有 “” 普通家庭 “” 名下无房 “” 顺便挣钱 “ 实话很快遭到了反噬，各路媒体围观解读，挖坟打脸。 难看吗？很难看。 这个时候，老一辈的生存哲学才发出了闪闪的光芒。 《是，大臣》里面有句名台词 “ 如果人们不知道你在干什么，他们就不知道你做错了什么。” 多说多错，少说少错，不说不错。 非要你讲呢？那就尽量让说了和没说一样。 黑话，闪闪发光。 互联网黑话只是一种职场文化，或者说是生存哲学吗？ 不，黑话的背后，隐藏着一个冰冷的现实： 互联网来到了 “ 守成时代 “。 群雄逐鹿，尘埃将落。 各个赛道上的大厂已经划完了地盘，站稳了脚跟，那些最暴利的业务早就被大厂吃干抹净。 甚至连不怎么挣钱，曾经让他们不屑一顾的边缘业务都成了大厂的爪牙们争夺的焦点。 扩张逼近边界，格局固化，增量见顶。 那个曾经改变无数普通年轻人命运窗口要关闭了，一个充满光荣与梦想的时代要落幕了。 我国的互联网普及率，用了不到 15 年的时间，就从 2006 年的 10% 增长到了 70%。 在这 15 年里，无数怀着理想的年轻人涌进来。 互联网一度是年轻人的理想国。 你讨厌传统行业里的蝇营狗苟你可以来这里； 现实里一无所有，迷惘无依，你可以来这里； 不需要你有一个区长父亲，不需要你有一个企业家母亲，有一腔热血，这里就有梦可以给你做。 然后一代年轻人用青春和热血敲下一行行代码，开发出一个又一个改变我们生活的应用，用 996 和青春饲喂出一个又一个世界级的互联网巨鳄和独角兽。 然后，没有了然后。 存量和守成的时代，是一个不说人话和逆向淘汰的时代。 “Talk is cheap， Show me the code” 的时代结束了。 那个普通人可以闪闪发光的时代也结束了。","categories":[{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/categories/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"}],"tags":[{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/tags/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"}]},{"title":"漫画一个NB互联网项目的上线过程","slug":"漫画一个NB互联网项目的上线过程","date":"2021-04-26T16:00:00.000Z","updated":"2022-06-09T04:56:38.857Z","comments":true,"path":"2021/04/27/漫画一个NB互联网项目的上线过程/","link":"","permalink":"http://zhangyu.info/2021/04/27/%E6%BC%AB%E7%94%BB%E4%B8%80%E4%B8%AANB%E4%BA%92%E8%81%94%E7%BD%91%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%B8%8A%E7%BA%BF%E8%BF%87%E7%A8%8B/","excerpt":"","text":"##漫画 一个NB互联网项目的上线过程 作者 | 苏南 来源 | 前端布道师（ID：honeyBadger8） 今天这篇漫画讲述的是一个大型项目，从需求诞生到进入实际开发、对外发布的过程… 本期漫画情节纯属虚构 如有雷同，纯属巧合. 我们经常会看到某某知名互联网公司开产品发布会，又或者某些分享会上，演讲者常常会提到我们公司的项目致力于解决用户的某一痛点，并且采用了当下最顶尖的云服务部署、微服务架构、模块化开发、大数据精准分析等互联网黑话，但这些华丽的外表背后真的如此吗？ 可能也只有身在其中的程序员才最清楚了吧，当然也不排除部分公司确实做的非常好。但是绝大部分的公司都很难做到流程标准化，项目长远规划，都只是一味的追求快，快、意味着时间的减少，而程序员前期的规划、架构也随之被抛在脑后，为了完成需求编码而编码，压根没有时间去思考项目长远的开发问题… 话说，你工作中的项目状态是如画中的王大拿这样的吗？","categories":[{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/categories/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"}],"tags":[{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/tags/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"}]},{"title":"Pulsar vs Kafka，CTO 如何抉择","slug":"comparing-pulsar-and-kafka-from-a-ctos-point-of-view","date":"2021-04-23T16:00:00.000Z","updated":"2021-04-25T08:20:42.115Z","comments":true,"path":"2021/04/24/comparing-pulsar-and-kafka-from-a-ctos-point-of-view/","link":"","permalink":"http://zhangyu.info/2021/04/24/comparing-pulsar-and-kafka-from-a-ctos-point-of-view/","excerpt":"","text":"https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi 作者 | Jesse Anderson译者 | Sijiahttps://mp.weixin.qq.com/s?src=11&amp;timestamp=1619338804&amp;ver=3029&amp;signature=LstPwE5cJ7yKmBlTCfazpUH3EqJ3tPgHscCjhRBoc1lJ1IHTlI57lOKONxCKmfLeWlqDAKdaKutZXVdxm3XEVFDvKt2z3fNX6eoKV0qMt在评估新技术时，高层管理人员的视角通常与中层管理人员、架构师、数据工程师等有所不同。高层管理人员不仅要关注基准测试结果、产品支持的特性，还要从长远角度考虑新技术的可靠性，新技术能够为企业带来哪些竞争优势，以及是否可以缩短上市时间、节约开销。 我是 Big Data Institute 的常务董事，技术评估是我的一项主要工作。我们帮助企业根据业务需求选择并落地最合适的技术。我们不与供应商合作，因此客户尤为看中我们能够客观地评估不同的技术。 在本文中，我将从 CTO 的视角出发，对比 Apache Pulsar 和 Apache Kafka。只进行理论上的对比空洞无效，也不能帮助我们作出决策，实际用例才真正值得参考。所以，在本文中，我会通过一些常见的实际使用场景来对比 Pulsar 和 Kafka，即简单消息使用场景、复杂消息使用场景和高级消息使用场景。在这些实际使用场景下，Pulsar 和 Kafka 的表现能够帮助我们更好地理解二者的性能和优势，进而作出决策。 简单消息使用场景假设有一个企业，之前从未使用过消息系统，现在需要通过一个简单的消息系统，将消息从位置 A 发送到位置 B，但不需要复制消息。 数据架构师团队在深入研究 Pulsar 和 Kafka 的业务案例后，得出如下结论：在这一使用场景中，Pulsar 和 Kafka 都没有绝对优势。并且，他们认为在短时间内，该使用场景基本不会发生改变。 对于类似这样的简单消息使用场景而言，我也赞同 Pulsar 和 Kafka 都没有绝对优势。仅从技术角度出发，Pulsar 和 Kafka 这一回合打成平局，那么我们只能考虑成本。二者的运营成本、员工培训成本分别是多少？我打算根据 Kafka 或 Pulsar 的服务提供商的收费标准进行对比。对比开销时，选好服务提供商也可以在一定程度上减少运营成本和员工培训成本。Kafka 的云服务提供商，我参考了使用 Kafka API（Azure）的 Confluent Cloud、MSK（AWS）和 Event Hubs。Pulsar 的云服务提供商，我选择 StreamNative Cloud。 对比结果出于稳妥考虑，我们决定选择 Kafka API。目前，已有多种技术支持非 Kafka broker 使用 Kafka API 或传输协议。使用 Kafka API，非 Kafka broker 可通过添加新库支持 Kafka 的传输协议，保证对 Kafka API 的兼容性，从而最大化技术选择的多样性。例如，可以通过修改 Kafka API 的实现重新编译或通过 Pulsar broker 解析 Kafka 的协议（KOP），将 Pulsar 用作 Kafka 的后端。 我们在对比单位成本后，选择了成本效益高的一方。Kafka API 可以保证后端质量，用户在后端之间的数据移动不会受到影响，有效规避风险。即使社区不活跃，技术热度不高，我们的使用也不会受到影响。 复杂消息使用场景假设一个公司需要复杂消息系统。由于需要处理世界各地的数据，必须支持跨地域复制。该企业一直在使用消息系统，因此对实时系统的复杂性有一定的了解，也发现了当前消息系统的不足之处。因此该企业对消息系统的要求是能够处理高级的消息传递和复杂的消息特性。 数据架构师团队和股东以及业务部门详细讨论了当前和未来需求。最后得出的结论是，Pulsar 和 Kafka 各有优势。同时，他们认为随着时间的推移，该使用场景和数据量都会有所增长。 在这种情况下，Pulsar 和 Kafka 难分胜负。要想作出正确决策，必须深入研究二者的使用场景。 跨地域复制Kafka 既提供私有的（价格高）跨地域复制，也提供开源的（附加服务）跨地域复制解决方案。私有的跨地域复制解决方案为其内置特性，但价格高昂。开源的解决方案（MirrorMaker）实际上就是数据复制，但由于不是其内置特性，会增加运营开销。 Pulsar 提供开源内置的跨地域复制特性，支持复杂的复制策略。对于使用场景和数据量都在增加的企业而言，显然，支持内置跨地域复制策略的 Pulsar 完胜。 就跨地域复制而言，我们选择 Pulsar。 复杂消息由于企业正在向新消息平台迁移，消息系统最好可以处理新使用场景。数据架构师团队一直在了解各个平台，尝试寻找最佳解决方案。在当前使用的消息系统中，一旦出现处理错误，必须重新生成消息，再手动重试，因此最好还可以引入消息延迟发送。另外，当前消息系统的 schema 实施功能也有待加强，各个团队选择不同的 schema 实现时，团队合作的难度显著增加。 Kafka 没有内置死信队列特性，一旦消息处理失败，必须手动处理，或修改代码重试。Kafka 也没有延迟发送消息的内置机制，延迟发送消息流程复杂、工作量大。另外，Kafka 没有内置 schema 实施机制，导致云服务提供商分别提供了不同的 schema 解决方案。 Pulsar 内置死信队列特性，当消息处理失败，收到否认 ack 时，Pulsar 可以自动重试，但次数有限。Pulsar 也支持延迟发送消息，可以设定延迟时间。对于 Pulsar 而言，schema 级别高，因此 Pulsar 有内置 schema 注册，Pulsar API 也原生支持 schema。 就复杂消息而言，我们选择 Pulsar。 高级消息传递随着对架构的深入了解，我们发现为了确保均匀分配资源，需要循环发送同一 topic 上的数据，并且需要通过排序确保消息有序排列。 Kafka 不能分发消息给指定的 consumer。当 consumer 接收到不属于它消费的消息时，要保证这些消息被正确消费，我们只能重新发送这些消息到额外的 topic 中，但这样会造成数据冗余，增加使用成本。因此，我们需要可以制定路由规则发送给指定 consumer 的产品。 Pulsar broker 可以通过制定的路由规则，把一个 topic 的不同消息根据路由规则发送到指定的 consumer 中。Pulsar broker 轻松实现了我们的目标，无需任何额外工作。 就高级消息传递而言，我们选择 Pulsar。 部署和社区为了全面比较 Pulsar 和 Kafka，我们还需要看一下二者的部署数量和社区概况。 从服务市场来看，Kafka 的提供商更多，销售和支持 Kafka 产品的团队也更多。Kafka 和 Pulsar 的开源社区都积极活跃，但 Kafka 的社区规模更大。 从使用市场来看，Kafka 和 Pulsar 都已部署在大公司的大型生产环境中。在生产环境中部署 Kafka 的公司在数量上更胜一筹。 从用户数量来看，Kafka 的用户更多。但是，数据工程师团队认为， Kafka 的使用者可以轻松学习 Pulsar。 就服务支持和社区而言，我们选择 Kafka。但值得一提的是，Pulsar 社区正在迅速发展。 对比结果由于 Pulsar 和 Kafka 在这一使用场景中都有明显的优劣势，决策难度大大增加。 Pulsar 可以在社区和部署上奋起直追，Kafka 则可以努力丰富产品特性。 在作出决策前，我们先来总结一下，该企业在技术上最看重哪方面；在技术方面，我们是否需要做最保守的选择。根据以往的经验，新的开源技术会带来更多惊喜，因此我们更倾向于选择 Pulsar。 如果选择 Kafka，我们需要承担向业务赞助商坦诚“我们无法处理这一使用场景”的风险。甚至，即使支付大笔资金购买跨地域复制许可，也无法保证顺利实现客户的需求。业务团队最终可能需要花大量时间（甚至几个月）来编写、完善、测试他们的工作方案。 如果选择 Pulsar，我们可以告诉业务赞助商“一切尽在掌握中”。由于 Pulsar 的各项内置特性都已经过测试，使用团队可以在短时间内完成部署。 在这种情况下，因为我们不需要 Kafka API 的独有特性，所以我们没有使用支持 Kafka 协议（KOP）的 Pulsar Broker，而是选择 Pulsar API，因为 Pulsar API 支持所有我们需要 Kafka API 提供的功能。 决策如下：选择 Pulsar，可以优先处理业务请求，开发团队只专注编写代码，而不是解决其他问题。选择 Pulsar 的同时，也关注 Pulsar 社区和提供商的动态。 如果采取保守决策选择 Kafka，需要接受可能无法实现某些使用场景的事实。对于相似的使用场景，我们采取相应解决方案。调整项目时间规划，增加实行预期解决方案的时间。联系运营团队，确保可以承受执行预期解决方案的开销。 高级消息使用场景假设一个公司已经在使用多种消息和队列系统。从运营、架构和开销的角度来看，我们认为有必要迁移到单个系统。同时，我们也希望降低运营成本。 数据架构师团队在和股东以及业务部门详细讨论了当前和未来需求后，给出的结论是，Pulsar 和 Kafka 各有优势。 队列和消息最大的难题是 RabbitMQ 系统。我们使用 RabbitMQ 发送太多消息，RabbitMQ 已经无法满足需求。我们调整了 RabbitMQ 的代码，将消息缓冲在内存中，并继续创建新集群来处理负载。但是我们需要的不是变通方法，而是一个能够处理大规模消息的系统。 数据架构师在研究这一使用场景时，得出结论：新系统必须可以同时处理消息流模型和队列模型。我们不仅需要继续使用 RabbitMQ 处理消息，也需要更高级的消息技术。 Kafka 擅长消息传递，也可以处理大规模消息流，但是无法处理队列。开发团队可以尝试一些解决方案，但这样就不能实现使用单个系统的预期目标。要处理队列使用场景，就同时需要 Kafka 集群和 RabbitMQ 集群。Kafka 集群更像一个缓冲区，可以有效防止 RabbitMQ 集群过载。但是 Kafka 不支持原生 RabbitMQ，我们需要与提供商合作或自己编写代码，才可以实现在 Kafka 和 RabbitMQ 之间移动数据。 Pulsar 可以在同一集群中处理队列和消息，还支持扩展集群。Pulsar 可以将所有消息流模型和队列模型的使用场景整合到一个集群中。用户可以继续使用 RabbitMQ 代码，Pulsar 支持 RabbitMQ 连接器，或者在 broker 中使用 StreamNative 开发的 AoP（AMQP 协议处理插件），该插件已获得 Apache 许可。 如果不想继续使用 RabbitMQ 代码，则可以使用 Pulsar API。Pulsar API 具有和 RabbitMQ 相同的队列功能。用户需要对代码进行相应修改，工作量取决于原代码的结构和细节，修改代码后，还需要对代码进行评估测试。 就队列模型和消息流模型而言，我们选择 Pulsar。 高级保留数据架构师分析了数据使用情况，发现 99.99% 的数据在首次使用后就未被读取。但是，他们决定采取保守策略，保留消息一周。虽然决定存储数据一周，但我们不希望增加太多运营成本。分层存储可以保存数据到本地，然后卸载其他数据到 S3，降低长期保存数据的成本。 Kafka 团队正在开发分层存储，但 Kafka 目前还不支持这一特性。一些服务商提供私有分层存储，但我们不确定是否可以直接用于生产环境中。 分层存储是 Pulsar 的原生特性，可以直接用于生产环境。目前已有多个企业在生产环境中部署该特性。 就分层存储而言，我们选择 Pulsar。Kafka 正在全力开发分层存储，这一特性的重要性不言而喻。 路由 Topic由于我们使用多个 topic 来分解数据，我们期待新系统可以创建大量 topic。数据架构师认为，我们起初需要 10 万个 topic，随着时间的推移，这个数字将会涨到 50 万。 Kafka 集群支持创建的分区数量有限且每个 topic 至少需要一个分区。Kafka 正在增加可支持 topic 的数量，但新特性尚未发布。另外，Kafka 没有命名空间和多租户，因此无法基于 topic 对资源进行分片，十万个 topic 需要存储在同一个命名空间中。 一些企业的确在使用 Kafka 集群存储甚至更多的 topic，同时进行了资源分片。但他们放弃使用单一集群，同时还需要为此支付费用。 Pulsar 支持存储数百万个 topic，这一功能早已发布并投入生产环境。Pulsar 支持命名空间和多租户，用户可以为每个 topic 设置资源配额，进而节约开销。 就 topic 而言，我们选择 Pulsar。 路由由于我们假设该企业曾经使用 RabbitMQ，在设计上，一般通过 broker 路由机制把 topic 上的数据转发到不同的 topic 中。例如，有一个用于存储世界范围数据的 topic，而 RabbitMQ broker 把它处理成以国家为单位的 topic。 数据架构师团队深入研究了如何在消息系统中使用单一 topic 存储世界范围的数据。他们发现当接收数据量增大时，下游 consumer 无法继续处理数据。对每个下游系统进行反序列化、查看数据，再丢弃数据的流程繁杂，且费时费力。 Kafka 将所有数据存储在单一 topic 中，但是，当 consumer 需要过滤的数据量增加或集群过载时，这个方法不可行。我们通常需要进行水平缩放，增加 consumer 数量，才可以读取全局 topic 并做进一步处理。用户只能选择：编写自定义 consumer / producer，编写 Kafka Streams 程序，或使用专有 KSQL。 Pulsar 支持使用 Pulsar Functions 或自定义 consumer / producer 进行路由，因此可以先读取全局 topic，再将数据保存到以国家为单位的特定 topic 上。使用独立 topic，consumer 可以按需订阅 topic，只接收相关消息。 就路由而言，我们选择 Pulsar。 最终决策时间是影响最终决策的主要原因。我们是否有时间让 Kafka 赶上 Pulsar？我们是否有时间让数据工程师来实现 Kafka 的解决方案？等待会让公司错失良机，延缓增加新的使用场景，影响业务发展。 最终决策：我们选择 Pulsar。 时间充足情况下的决策：延迟使用新架构。给 Kafka 半年时间，看 Kafka 是否可以在性能上赶超 Pulsar。如果可以，我们将在生产环境中测试这些新特性，评估稳定性。如果 Kafka 不能让人眼前一亮，我们仍然会选择 Pulsar。 结语本文涉及的三个使用场景都是我在实际工作中遇到的，希望本文给出的解决方案可以为您提供参考，帮助您根据具体使用场景进行技术评估。 原文链接： https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi","categories":[{"name":"消息系统","slug":"消息系统","permalink":"http://zhangyu.info/categories/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Pulsar","slug":"Pulsar","permalink":"http://zhangyu.info/tags/Pulsar/"}]},{"title":"生产环境中的Kubernetes最佳实践","slug":"kubernetes-best-practices-in-production","date":"2021-04-20T16:00:00.000Z","updated":"2021-04-21T08:50:20.907Z","comments":true,"path":"2021/04/21/kubernetes-best-practices-in-production/","link":"","permalink":"http://zhangyu.info/2021/04/21/kubernetes-best-practices-in-production/","excerpt":"","text":"生产环境中的Kubernetes最佳实践https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&amp;mid=2649721992&amp;idx=1&amp;sn=31b9a4352a147a1fa585b8d8ef4c68b0&amp;scene=21#wechat_redirect（翻译：易理林）https://my.oschina.net/u/1787735/blog/4870582 DevOps从提出到现在，已经走过了一段很长的路。包括Docker和Kubernetes在内的多种平台也已经帮助企业用前所未有的速度实现了软件应用的交付。同时，随着应用的容器化构建和发布比率不断上升，作为事实上的容器编排工具，Kubernetes在企业用户中备受欢迎和广泛认可。 Kubernetes具有支持伸缩、零中断部署、服务发现、自动更迭和自动回滚等卓越功能特性。在管理大规模容器部署方面，Kubernetes因支持资源和工作负载的灵活分配能力，而成为了企业的必选工具，在生产环境中广泛应用。但与此同时，Kubernetes的应用需要操作人员花许多时间来熟悉和掌握它，存在一定技术门槛。鉴于目前许多公司都希望在生产中使用Kubernetes，因此有必要率先梳理这方面的最佳实践。在本文中，我们将介绍Kubernetes在生产环境中的一些最佳实践。 生产环境中Kubernetes表现 根据Garner的预测，到2022年时，全球超过75%的组织将在生产环境中运行容器化应用。这个比率在当前还不足30%，而预计到2025年时，这个比率将在2022年的基础上，继续增长到85%。快速增长的一个主要原因是云原生的软件应用在基础设施自动化、DevOps、专业操作技能方面的需求越来越强烈，而且这些工具和技术在企业的IT组织中往往很难找到。 其次，业界普遍认为在生产环境中运行容器并不容易，需要大量的计算资源和相关工作投入。目前市场上有多款容器编排平台产品可供选择，但已经获得了主要云提供商的支持和认可的平台只有Kubernetes。 再次，Kubernetes、容器化和微服务给企业用户带来的技术受益的同时，也带来了新的安全挑战。Kubernetes的Pod具备在所有基础设施类之间快速切换的能力，从而导致更多的内部流量和与之相关的安全风险，加上Kubernetes被攻击面往往比我们预期的更大，以及Kubernetes的高度动态和临时的环境与原有安全工具的融合差距等因素，可以预测使用Kubernetes并非是一件容易的事情。 最后，Kubernetes丰富的功能导致它的学习曲线复杂而陡峭，在生产环境中的操作需应尽可能小心和谨慎。企业如果没有熟悉这方面的专业人员，可以考虑外购Kubernetes-as-a-service（KaaS）提供商的服务，获取Kubernetes最佳实践。但假设用户是完全依靠自己的能力，管理生产环境中的Kubernetes集群，在这种情况下，理解和实现Kubernetes最佳实践尤其重要，特别是在可观察性、日志记录、集群监控和安全配置等方面。 综上所述，非常有必要开发一套Kubernetes管理策略，以实现在安全性、监视、网络、容器生命周期管理和平台选择等方面应用最佳实践。如下是Kubernetes应用管理需要重点考虑的措施。 使用服务状态探针进行健康检查 管理大型分布式系统是一件复杂的工作，尤其是出现问题的时候。因此为了确保应用的实例工作正常，配置Kubernetes健康检查至关重要。通过创建自定义运行状况检查，可以更好地满足用户的环境和应用的检测需要。服务状态探针包括服务就绪探针和服务活性探针。 Readiness-就绪探针：目的是让Kubernetes知道应用程序是否准备好提供服务。Kubernetes始终会在确认准备就绪探针通过检测后，然后才允许向POD发送服务请求流量。 Liveness-存活探针：目的是帮助用户确认应用程序是否正常存活，如果应用出现了异常，Kubernetes将启动新的Pod，替换异常的Pod。 资源管理 为单个容器指定资源需求和资源限制是一个很好的实践。另一个好的实践是为不同团队、部门、应用程序和客户端，划分独立的Kubernetes命名空间环境。提供相对独立的运行资源环境，减少资源使用冲突。 资源使用 Kubernetes资源使用情况掌握了生产环境中容器/Pod的资源数量使用情况。因此，密切关注Pod和容器的资源使用情况非常重要，资源使用越多，运行成本就越高。 资源利用 运维团队通常致力于优化和最大化Pod分配资源的利用百分比。资源使用情况往往也是Kubernetes优化程度的重要指标之一。可以说，优化最好的Kubernetes环境，内部运行容器的平均CPU利用率也是最优的。 开启RBAC策略 基于角色的访问控制（RBAC）是系统或网络中限制用户和应用程序的接入或访问的一种控制方法。 Kubernetes 从1.8版本开始，引入了RBAC访问控制技术，使用rbac.authorization.k8s.io程序API创建授权策略。RBAC的授权使用包括开启访问用户或帐户、添加/删除权限、设置规则等。它为Kubernetes集群添加了一个额外的安全层，限制哪些访问可以到达Kubernetes集群的生产环境。 集群配置和负载均衡 生产级Kubernetes基础设施通常需要具备高可用性，具备多控制节点、多etcd集群等关键特性。此类集群特性的配置实现通常需要借助如Terraform或Ansible等工具实现。 通常情况下，当集群的所有配置都完成，并创建了Pod时，此时的Pod基本都会配置有负载均衡器，用于将流量路由到适当的应用服务。但这其中的负载均衡器并不是Kubernetes项目的默认配置，而是由Kubernetes Ingress控制器的扩展集成工具提供的。 标注Kubernetes对象 为Kubernetes的Pod等对象打上键/值对类型的标签，通常可以用来标记重要的对象属性，特别是对用户意义重大的属性。因此，在生产环境中使用Kubernetes时，不能忽视的重要实践就是利用标签功能，它们可以帮助实现Kubernetes对象的批量查询和批量操作。同时，标签还具有将Kubernetes对象组织成集群的独特作用，这样做的一个最佳实践应用就是能够根据应用对Pod进行分组管理。除此之外，标签没有数量和内容的限制，运维团队可以任意创建和使用。 设置网络策略 网络策略设置对于生产环境中的Kubernetes平台非常重要。 网络策略本质上也是一种对象，让用户能够声明和决定哪些流量是允许或禁止传输的。Kubernetes能够阻止所有不需要的和不合规的流量。因此，强烈建议Kubernetes将网络策略配置作为基本和必要的安全措施之一，执行定义和限制集群中的网络流量。 Kubernetes中的每条网络策略都被定义成一个授权连接列表。无论何时创建的网络策略，平台全部的Pod都有权利建立或接受该连接列表。简单来说，网络策略其实就是授权和允许连接的请求白名单，无论是“输入”还是“输出”到Pod，在至少有一条网络策略允许的情况下，到该Pod流量才被允许通行。 集群监控与日志 监控对于运行状态的Kubernetes至关重要，它直接影响到平台配置、性能和流量的安全。能够帮助用户及时掌握平台状态，执行问题诊断、确保运行合规，是平台运行的必要功能部署。在开启集群监视时，必须在平台的每一层都开启日志记录，让产生的日志能够执行安全、审计和性能分析。 采用无状态应用 虽然这种观念正随着Kubernetes应用组织的增加在不断改变，但管理和运行无状态应用要比有状态应用要容易很多。事实上，对于刚接触Kubernetes的团队，建议一开始就采用无状态应用的设计。同时，还建议采用无状态的后端程序，从而让开发人员更有效地部署应用程序，实现服务的零停机时间。但前提是需要开发团队确保后端没有长时间运行的连接，不会影响到运行环境的弹性扩展。无状态应用还被认为具备根据业务需要进行简便迁移和快速扩展的能力。 启用自动扩展 Kubernetes的服务部署拥有3个自动扩展能力：Pod水平自动扩展（HPA），Pod垂直自动扩展（VPA）和集群自动扩展。 Pod水平自动扩展能够基于CPU的利用率，自动扩展运行应用的Pod数量，调整副本控制器、副本集或状态配置。 Pod垂直自动扩展建议为应用设定适当的CPU，内存的需求值和上限值。VPA能够根据情况，自动伸缩配置适当的资源数量。 集群自动扩展能够伸缩工作节点的资源池规模，从而根据当前的资源使用情况，自动调整Kubernetes集群的大小。 控制镜像拉取来源 如果允许Pod从公共库中拉取镜像，而不知道其真正运行内容的时候，用户应该控制所运行容器集群的资源，以避免资源使用的失控。而如果是从受信任的注册节点提取镜像，则可以在注册节点上采用控制策略，限制只允许提取安全且经过认证的镜像。 保持持续学习 对应用程序的状态不断评估、学习和改进。例如，通过查看容器的历史内存使用情况，确定可以分配更少的内存来节省成本。 重点保护核心服务 使用Pod优先级功能，可以为不同的服务设置重要度。例如，可以配置RabbitMQ Pod的优先级高于应用程序Pod，以获得更好的稳定性。或为输入控制器Pod配置比数据处理Pod更高的重要度，以保持服务的可用性。 保证服务零停机 服务的零停机能力可以通过全方位HA架构，支持集群和服务的零停机升级。从而为客户获得更高的服务可用性提供了保证。使用Pod反亲和性配置，确保多个副本Pod被调度到不同的节点上，从而保证计划和非计划的集群节点停机不会影响服务的可用性，或使用Pod中断预备能力，确保在可用成本内，保留最少的副本数量。 为失败指定计划 借用一句名言来理解如果应对硬件故障。硬件最终会失败，软件最终会运行。–（迈克尔·哈顿） “Hardware eventually fails. Software eventually works.”（Michael Hartung）。 结论 业界共知的Kubernetes，实际上已经是DevOps的标配编配平台。生产环境中运行的Kubernetes环境必须具备可用性、可伸缩性、安全性、弹性、资源管理和监控等功能和性能特征。由于许多公司都在生产中使用Kubernetes，因此建议遵循上面提到的Kubernetes最佳实践，以便顺利、可靠地运维和管理应用程序。 原文链接：https://containerjournal.com/topics/container-management/kubernetes-best-practices-in-production/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"你的技术成长战略是什么","slug":"jishuchengzhang","date":"2021-04-18T16:00:00.000Z","updated":"2021-04-19T16:03:49.781Z","comments":true,"path":"2021/04/19/jishuchengzhang/","link":"","permalink":"http://zhangyu.info/2021/04/19/jishuchengzhang/","excerpt":"","text":"你的技术成长战略是什么?_架构师波波的专栏-CSDN博客 https://blog.csdn.net/yang75108/article/details/112511324?spm=1001.2014.3001.5502) 一、前言在波波的微信技术交流群里头，经常有学员问关于技术人该如何学习成长的问题，虽然是微信交流，但我依然可以感受到小伙伴们焦虑的心情。 技术人为啥焦虑？恕我直言，说白了是胆识不足格局太小。胆就是胆量，焦虑的人一般对未来的不确定性怀有恐惧。识就是见识，焦虑的人一般看不清楚周围世界，也看不清自己和适合自己的道路。格局也称志向，容易焦虑的人通常视野窄志向小。如果从战略和管理的视角来看，就是对自己和周围世界的认知不足，没有一个清晰和长期的学习成长战略，也没有可执行的阶段性目标计划+严格的执行。 因为问此类问题的学员很多，让我感觉有点烦了，为了避免重复回答，所以我专门总结梳理了这篇长文，试图统一来回答这类问题。如果后面还有学员问类似问题，我会引导他们来读这篇文章，然后让他们用三个月、一年甚至更长的时间，去思考和回答这样一个问题：你的技术成长战略究竟是什么？如果你想清楚了这个问题，有清晰和可落地的答案，那么恭喜你，你只需按部就班执行就好，根本无需焦虑，你实现自己的战略目标并做出成就只是一个时间问题；否则，你仍然需要通过不断磨炼+思考，务必去搞清楚这个人生的大问题！！！ 下面我们来看一些行业技术大牛是怎么做的。 二、跟技术大牛学成长战略我们知道软件设计是有设计模式(Design Pattern)的，其实技术人的成长也是有成长模式(Growth Pattern)的。波波经常在Linkedin上看一些技术大牛的成长履历，探究其中的成长模式，从而启发制定自己的技术成长战略。 当然，很少有技术大牛会清晰地告诉你他们的技术成长战略，以及每一年的细分落地计划。但是，这并不妨碍我们通过他们的过往履历和产出成果，去溯源他们的技术成长战略。实际上，越是牛逼的技术人，他们的技术成长战略和路径越是清晰，我们越容易从中探究出一些成功的模式。 2.1 系统性能专家案例国内的开发者大都热衷于系统性能优化，有些人甚至三句话离不开高性能/高并发，但真正能深入这个领域，做到专家级水平的却寥寥无几。 我这边要特别介绍的这个技术大牛叫Brendan Gregg(布兰登·格雷格)，他是系统性能领域经典书《System Performance: Enterprise and the Cloud》(中文版《性能之巅：洞悉系统、企业和云计算》)的作者，也是著名的性能分析利器火焰图(Flame Graph)的作者。Brendan Gregg目前是Netflix公司的高级性能架构师，已经在Netflix工作近7年，之前他是Joynet公司的Lead Performance Engineer。总体上，他已经在系统性能领域深耕超过10年，Brendan Gregg的过往履历可以在linkedin上看到。在这10年间，除了书籍以外，Brendan Gregg还产出了超过上百份和系统性能相关的技术文档，演讲视频/ppt，还有各种工具软件，相关内容都整整齐齐地分享在他的技术博客上，可以说他是一个非常高产的技术大牛。 上图来自Brendan Gregg的新书《BPF Performance Tools: Linux System and Application Observability》，其中红色标注的是他开发的各种性能工具。从这个图可以看出，Brendan Gregg对系统性能领域的掌握程度，已经深挖到了硬件、操作系统和应用的每一个角落，可以说是360度无死角，整个计算机系统对他来说几乎都是透明的。波波认为，Brendan Gregg是名副其实的，世界级的，系统性能领域的大神级人物。 2.2 从开源到企业案例我要分享的第二个技术大牛是Jay Kreps(杰·克雷普斯)，他是知名的开源消息中间件Kafka的创始人/架构师，也是Confluent公司的联合创始人和CEO，Confluent公司是围绕Kafka开发企业级产品和服务的技术公司。 从Linkedin的履历上我们可以看出，Jay Kreps之前在Linkedin工作了7年多(2007.6 ~ 2014. 9)，从高级工程师、工程主管，一直做到首席资深工程师。Kafka大致是在2010年，Jay Kreps在Linkedin发起的一个项目，解决Linkedin内部的大数据采集、存储和消费问题。之后，他和他的团队一直专注Kafka的打磨，开源(2011年初)和社区生态的建设。到2014年底，Kafka在社区已经非常成功，有了一个比较大的用户群，于是Jay Kreps就和几个早期作者一起离开了Linkedin，成立了Confluent公司，开始了Kafka和周边产品的企业化服务道路。今年(2020.4月)，Confluent公司已经获得E轮2.5亿美金融资，公司估值达到45亿美金。从Kafka诞生到现在，Jay Kreps差不多在这个产品和公司上投入了整整10年。 上图是Confluent创始人三人组，一个非常有意思的组合，一个中国人(左)，一个印度人(右)，中间的Jay Kreps是美国人。 我之所以对Kafka和Jay Kreps的印象特别深刻，是因为在2012年下半年，我在携程框架部也是专门搞大数据采集的，我还开发过一套功能类似Kafka的Log Collector + Agent产品。我记得同时期有不止4个同类型的开源产品：Facebook Scribe、Apache Chukwa、Apache Flume和Apache Kafka。现在回头看，只有Kafka走到现在发展得最好，这个和创始人的专注和持续投入是分不开的，当然背后和几个创始人的技术大格局也是分不开的。 当年我对战略性思维几乎没有概念，还处在什么技术都想学、认为各种项目做得越多越牛的阶段。搞了半年的数据采集以后，我就掉头搞其它“更有趣”的项目去了(从这个事情的侧面，也可以看出我当年的技术格局是很小的)。中间我陆续关注过Jay的一些创业动向，但是没想到他能把Confluent公司发展到目前这个规模。现在回想，其实在十年前，Jay Kreps对自己的技术成长就有比较明确的战略性规划，也具有大的技术格局和成事的一些必要特质。Jay Kreps和Kafka给我上了一堂生动的技术战略和实践课。 2.3 技术媒体大V案例介绍到这里，有些同学可能会反驳说：波波你讲的这些大牛都是学历背景好，功底扎实起点高，所以他们才更能成功。其实不然，这里我再要介绍一位技术媒体界的大V叫Brad Traversy(布拉德·特沃西)，大家可以看他的Linkedin简历，背景很一般，学历差不多是一个非正规的社区大学(相当于大专)，没有正规大厂工作经历，有限几份工作一直是在做网站外包。但是Brad Traversy目前是技术媒体领域的一个大V，当前他在Youtube上有138万多的订阅量，10年累计输出Web开发和编程相关教学视频超过800个。Brad Traversy也是Udemy上的一个成功讲师，目前已经在Udemy上累计输出课程14门，购课学生数量近25万。Brad Traversy目前是自由职业者，他的Youtube广告+Udemy课程的收入相当不错。 就是这样一位技术媒体大V，你很难想象，在年轻的时候，贴在他身上的标签是：不良少年，酗酒，抽烟，吸毒，纹身，进监狱。。。直到结婚后的第一个孩子诞生，他才开始担起责任做出改变，然后凭借对技术的一腔热情，开始在Youtube平台上持续输出免费课程。从此他找到了适合自己的战略目标，然后人生开始发生各种积极的变化。。。如果大家对Brad Traversy的过往经历感兴趣，推荐观看他在Youtube上的自述视频《My Struggles &amp; Success》。 我粗略浏览了Brad Traversy在Youtube上的所有视频，10年总计输出800+视频，平均每年80+。第一个视频提交于2010年8月，刚开始几年几乎没有订阅量，2017年1月订阅量才到50k，这中间差不多隔了6年。2017.10月订阅量猛增到200k，2018年3月订阅量到300k。当前2021.1月，订阅量达到138万。可以认为从2017开始，也就是在积累了6～7年后，他的订阅量开始出现拐点。如果把这些数据画出来，将会是一条非常漂亮的复利曲线。 2.4 案例小结Brendan Gregg，Jay Kreps和Brad Traversy三个人走的技术路线各不相同，但是他们的成功具有共性或者说模式： 找到了适合自己的长期战略目标，例如： Brendan Gregg: 成为系统性能领域顶级专家 Jay Kreps：开创基于Kafka开源消息队列的企业服务公司，并将公司做到上市 Brad Traversy: 成为技术媒体领域大V和课程讲师，并以此作为自己的职业 **专注深耕一个(或有限几个相关的)细分领域(Niche)**，保持定力，不随便切换领域： Brendan Gregg：系统性能领域 Jay Kreps: 消息中间件/实时计算领域+创业 Brad Traversy: 技术媒体/教学领域，方向Web开发 + 编程语言 长期投入，三人都持续投入了10年。 **年度细分计划+持续可量化的价值产出(Persistent &amp; Measurable Value Output)**： Brendan Gregg：除公司日常工作产出以外，每年有超过10份以上的技术文档和演讲视频产出，平均每年有2.5个开源工具产出。十年共产出书籍2本，其中《System Performance》已经更新到第二版。 Jay Kreps：总体有开源产品+公司产出，1本书产出，每年有Kafka和周边产品发版若干。 Brad Traversy: 每年有Youtube免费视频产出（平均每年80+）+Udemy收费视频课产出(平均每年1.5门)。 以终为始是牛人和普通人的一大区别。普通人通常走一步算一步，很少长远规划。牛人通常是先有远大目标，然后采用倒推法，将大目标细化到每年/月/周的详细落地计划。Brendan Gregg，Jay Kreps和Brad Traversy三人都是以终为始的典型。 上面总结了几位技术大牛的成长模式，其中一个要点是：这些大牛的成长都是通过持续有价值产出Persistent Valuable Output来驱动的。持续产出为啥如此重要，这个还要从下面的学习金字塔说起。 三、学习金字塔和刻意训练 学习金字塔是美国缅因州国家训练实验室的研究成果，它认为： 我们平时上课听讲之后，学习内容平均留存率大致只有5%左右； 书本阅读的平均留存率大致只有10%左右； 学习配上视听效果的课程，平均留存率大致在20%左右， 老师实际动手做实验演示后的平均留存率大致在30%左右； 小组讨论(尤其是辩论后)的平均留存率可以达到50%左右； 在实践中实际应用所学之后，平均留存率可以达到75%左右； 在实践的基础上，再把所学梳理出来，转而再传授给他人后，平均留存率可以达到90%左右。 上面列出的7种学习方法，前四种称为被动学习，后三种称为主动学习。拿学游泳做个类比，被动学习相当于你看别人游泳，而主动学习则是你自己要下水去游。我们知道游泳或者跑步之类的运动是要燃烧身体卡路里的，这样才能达到锻炼身体和长肌肉的效果(肌肉是卡路里燃烧的结果)。如果你只是看别人游泳，自己不实际去游，是不会长肌肉的。同样的，主动学习也是要燃烧脑部卡路里的，这样才能达到训练大脑和长脑部“肌肉”的效果。 我们也知道，燃烧身体的卡路里，通常会让人感觉不舒适，如果燃烧身体卡路里会让人感觉舒适的话，估计这个世界上应该不会有胖子这类人。同样，燃烧脑部卡路里也会让人感觉不适、紧张、出汗或语无伦次，如果燃烧脑部卡路里会让人感觉舒适的话，估计这个世界上人人都很聪明，人人都能发挥最大潜能。当然，这些不舒适是短期的，长期会使你更健康和聪明。波波一直认为，人与人之间的先天身体其实都差不多，但是后天身体素质和能力有差异，这些差异，很大程度是由后天对身体和大脑的训练质量、频度和强度所造成的。 明白这个道理之后，心智成熟和自律的人就会对自己进行持续地刻意训练。这个刻意训练包括对身体的训练，比如波波现在每天坚持跑步3km，走3km，每天做60个仰卧起坐，5分钟平板撑等，每天保持让身体燃烧一定量的卡路里。刻意训练也包括对大脑的训练，比如波波现在每天做项目写代码coding(训练脑+手)，平均每天在B站上输出十分钟免费视频(训练脑+口头表达)，另外有定期总结输出公众号文章(训练脑+文字表达)，还有每天打半小时左右的平衡球(下图)或古墓丽影游戏(训练小脑+手)，每天保持让大脑燃烧一定量的卡路里，并保持一定强度(适度不适感)。如果你对刻意训练的专业原理和方法论感兴趣，推荐看书籍《刻意练习》。 注意，如果你平时从来不做举重锻炼的，那么某天突然做举重会很不适应甚至受伤。脑部训练也是一样的，如果你从来没有做过视频输出，那么刚开始做会很不适应，做出来的视频质量会很差。不过没有关系，任何训练都是一个循序渐进，不断强化的过程。等大脑相关区域的”肌肉”长出来以后，会逐步进入正循环，后面会越来越顺畅，相关”肌肉”会越来越发达。所以，和健身一样，健脑也不能遇到困难就放弃，需要循序渐进(Incremental)+持续地(Persistent)刻意训练。 理解了学习金字塔和刻意训练以后，现在再来看Brendan Gregg，Jay Kreps和Brad Traversy这些大牛的做法，他们的学习成长都是建立在持续有价值产出的基础上的，这些产出都是刻意训练+燃烧脑部卡路里的成果。他们的产出要么是建立在实践基础上的产出，例如Jay Kreps的Kafka开源项目和Confluent公司；要么是在实践的基础上，再整理传授给其他人的产出，例如，Brendan Greeg的技术演讲ppt/视频，书籍，还有Brad Traversy的教学视频等等。换句话说，他们一直在学习金字塔的5～7层主动和高效地学习。并且，他们的学习产出还可以获得用户使用，有客户价值(Customer Value)，有用户就有反馈和度量。记住，有反馈和度量的学习，也称闭环学习，它是能够不断改进提升的；反之，没有反馈和度量的学习，无法改进提升。 现在，你也应该明白，晒个书单秀个技能图谱很简单，读个书上个课也不难。但是要你给出5～10年的总体技术成长战略，再基于这个战略给出每年的细分落地计划(尤其是产出计划)，然后再严格按计划执行，这的确是很难的事情。这需要大量的实践训练+深度思考，要燃烧大量的脑部卡路里！但这是上天设置的进化法则，成长为真正的技术大牛如同成长为一流的运动员，是需要通过燃烧与之相匹配量的卡路里来交换的。成长为真正的技术大牛，也是需要通过产出与之匹配的社会价值来交换的，只有这样社会才能正常进化。你推进了社会进化，社会才会回馈你。如果不是这样，社会就无法正常进化。 四、战略思维的诞生 一般毕业生刚进入企业工作的时候，思考大都是以天/星期/月为单位的，基本上都是今天学个什么技术，明天学个什么语言，很少会去思考一年甚至更长的目标。这是个眼前漆黑看不到的懵懂时期，捕捉到机会点的能力和概率都非常小。 工作了三年以后，悟性好的人通常会以一年为思考周期，制定和实施一些年度计划。这个时期是相信天赋和比拼能力的阶段，可以捕捉到一些小机会。 工作了五年以后，一些悟性好的人会产生出一定的胆识和眼光，他们会以3～5年为周期来制定和实施计划，开始主动布局去捕捉一些中型机会点。 工作了十年以后，悟性高的人会看到模式和规则变化，例如看出行业发展模式，还有人才的成长模式等，于是开始诞生出战略性思维。然后他们会以5～10年为周期来制定和实施自己的战略计划，开始主动布局去捕捉一些中大机会点。Brendan Gregg，Jay Kreps和Brad Traversy都是属于这个阶段的人。 当然还有很少一些更牛的时代精英，他们能够看透时代和人性，他们的思考是以一生甚至更长时间为单位的，这些超人不在本文讨论范围内。 五、建议 现在大学生毕业的年龄一般在22～23岁，那么在工作了十年后，也就是在你32～33岁的时候，你差不多也看了十年了，应该对自己和周围的世界(你的行业和领域)有一个比较深刻的领悟，需要开始为下一个十年去做战略布局了。如果你到这个年纪还懵懵懂懂，今天抓东明天抓西，那么只能说你的胆识格局是相当的低。在当前IT行业竞争这么激烈的情况下，到35岁被下岗可能就在眼前。 有了战略性思考，你就会以5～10年为周期去布局谋划你的战略。以Brendan Gregg，Jay Kreps和Brad Traversy这些大牛为例，人生若真的要干点成就出来，投入周期一般都要十年。从33岁开始，你大致有3个十年，因为到60岁以后，一般人都老眼昏花干不了大事了。如果你悟性差一点，到40岁才开始规划，那么你大致还有2个十年。如果你规划好了，这2～3个十年可以成就不小的事业。否则，你很可能一生无所作为，或者一直在帮助成全别人的事业。 考虑到人生能干事业的时间也就是2～3个十年，你会发现人生其实很短暂，这时候你会把精力都投入到实现你的十年战略上去，没有时间再浪费在比如网上的闲聊和扯皮争论上去。 “图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。”～道德经。有了十年战略方向，下一步是每年的细分落地计划，尤其是产出计划。这个计划主要应该工作在学习金字塔的5/6/7层。产出应该是刻意训练+燃烧卡路里的结果，每天让身体和大脑都保持燃烧一定量的卡路里。 产出应该有客户价值，自己能学习(自己成长进化)，对别人还有用(推动社会成长进化)，这样可以得到用户回馈和度量，形成一个闭环，可以持续改进和提升你的学习。 “少则得，多则惑”～道德经。少即是多，深耕一个(或有限几个相关的)领域。所有细分计划应该紧密围绕你的战略展开。克制内心欲望，不要贪多和分心，不要被喧嚣的世界所迷惑。 战略方向+细分计划都要写下来，定期review优化。 “曲则全、枉则直”～道德经。战略实现不是直线的，而是曲折迂回的。战略方向和细分计划通常要按需调整，尤其在早期，但是最终要收敛。如果老是变不收敛，就是缺乏战略定力，是个必须思考和解决的大问题。 别人的成长战略可以参考，但是不要刻意去模仿，你有你自己的颜色，你应该成为独一无二的你。 “合抱之木，生于毫末；九层之台，起于蔂土；千里之行，始于足下”～道德经。战略方向和细分计划明确了，接下来就是按部就班执行，十年如一日铁打不动。 做长期主义者和时间的朋友，”任何一个人，不管你的能量强弱，放眼于足够长的时间，你都可以通过长期主义这种行为模式，成为时间的朋友”～罗振宇。 最后，战略目标的实现也和种树一样是生长出来的，需要时间耐心栽培，记住慢就是快。焦虑纠结的时候，像念经一样默念王阳明《传习录》中的教诲： 立志用功，如种树然。方其根芽，犹未有干；及其有干，尚未有枝；枝而后叶，叶而后花实。初种根时，只管栽培灌溉。勿作枝想，勿作花想，勿作实想。悬想何益？但不忘栽培之功，怕没有枝叶花实？ 译文： 实现战略目标，就像种树一样。刚开始只是一个小根芽，树干还没有长出来；树干长出来了，枝叶才能慢慢长出来；枝叶长出来，然后才能开花和结果。刚开始种树的时候，只管栽培灌溉，别老是纠结枝什么时候长出来，花什么时候开，果实什么时候结出来。纠结有什么好处呢？只要你坚持投入栽培，还怕没有枝叶花实吗？","categories":[{"name":"职业发展","slug":"职业发展","permalink":"http://zhangyu.info/categories/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"}],"tags":[{"name":"职业发展","slug":"职业发展","permalink":"http://zhangyu.info/tags/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"}]},{"title":"使用nerdctl玩转containerd","slug":"nerdctl-to-containerd","date":"2021-04-11T16:00:00.000Z","updated":"2021-04-12T13:02:31.269Z","comments":true,"path":"2021/04/12/nerdctl-to-containerd/","link":"","permalink":"http://zhangyu.info/2021/04/12/nerdctl-to-containerd/","excerpt":"","text":"参考https://mp.weixin.qq.com/s/ZKoO041TqyR2guVooPegrg 从行业趋势来看，Docker 已经和 Kubernetes 社区渐行渐远，以?Containerd?为代表的实现了?CRI?接口的容器运行时将会受到 Kubernetes 的青睐。但纯粹使用 Containerd 还是有诸多困扰，比如不方便通过 CLI 来创建管理容器，有了?nerdctl?这个 CLI 工具，就就可以填补 Containerd 易用性的空缺 现有 CLI 的不足虽然 Docker 能干的事情，现在 Containerd 都能干，但 Containerd 还有一个非常明显的缺陷：CLI 不够友好。它无法像 Docker 和 Podman 一样通过一条简单的命令启动一个容器，它的两个 CLI 工具 ctr 和 crictl都无法实现这么一件非常简单的需求，而这个需求是大多数人都需要的，我总不能为了在本地测试容器而专门部署一个 Kubernetes 集群吧？ ctr 的设计对人类不太友好，例如缺少以下这些和 Docker 类似的功能： docker run -p &lt;PORT&gt; docker run --restart=always 通过凭证文件 ~/.docker/config.json 来拉取镜像 docker logs 除此之外还有一个 CLI 工具叫 crictl，和 ctr 一样不太友好。 为了解决这个痛点，Containerd 官方推出了一个新的 CLI 叫 nerdctl。nerdctl 的使用体验和 docker 一样顺滑 ##安裝nerdctl 你可以从?https://github.com/containerd/nerdctl中下载最新的可执行文件，每一个版本都有两种可用的发行版： `nerdctl--linux-amd64.tar.gz?: 只包含 nerdctl。 `nerdctl-full--linux-amd64.tar.gz?: 包含了 nerdctl 和相关依赖组件（containerd, runc, ###CNI, …）。 如果你已经安装了 Containerd，只需要选择前一个发行版，否则就选择完整版。 这里选择完整版nerdctl-full-0.7.3-linux-amd64.tar.gzcd /opt wget https://github.com/containerd/nerdctl/releases/download/v0.7.3/nerdctl-full-0.7.3-linux-amd64.tar.gz tar -C /usr/local -xzf nerdctl-full-0.7.3-linux-amd64.tar.gz mkdir -p /etc/containerd containerd config default &gt; /etc/containerd/config.toml sed -i “s#k8s.gcr.io#registry.aliyuncs.com/k8sxio#g” /etc/containerd/config.toml sed -i “s/systemd_cgroup = false/systemd_cgroup = true/g” /etc/containerd/config.toml export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com sed -i “s#https://registry-1.docker.io#${REGISTRY_MIRROR}#g&quot; /etc/containerd/config.toml mkdir -p /data/containerd-data-root sed -i “s#/var/lib/containerd#/data/containerd-data-root#g” /etc/containerd/config.toml sed -i “s#oom_score = 0#oom_score = -999#g” /etc/containerd/config.toml sed -i “/containerd.runtimes.runc.options/a\\ SystemdCgroup = true” /etc/containerd/config.toml sed -i “s#/opt/cni/bin#/usr/local/libexec/cni#g” /etc/containerd/config.toml 123456789101112添加私有仓库harbor---自定义--可选[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;harbor.testtest.com&quot;] endpoint &#x3D; [&quot;https:&#x2F;&#x2F;harbor.testtest.com&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor.testtest.com&quot;.tls] insecure_skip_verify &#x3D; true [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor.testtest.com&quot;.auth] username &#x3D; &quot;admin&quot; password &#x3D; &quot;Harbor12345&quot; systemctl daemon-reload systemctl enable –now containerd systemctl status containerd containerd –version ###普通用户Rootless 切换到普通用户 比如su - admin 执行 containerd-rootless-setuptool.sh install 强烈建议在Rootless模式下启用cgroup v2 请参阅https://rootlesscontaine.rs/getting-started/common/cgroup2/","categories":[{"name":"containerd","slug":"containerd","permalink":"http://zhangyu.info/categories/containerd/"}],"tags":[{"name":"containerd","slug":"containerd","permalink":"http://zhangyu.info/tags/containerd/"}]},{"title":"Kubernetes网络和云厂商实践浅析","slug":"Kubernetes-network","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T03:00:20.304Z","comments":true,"path":"2021/04/09/Kubernetes-network/","link":"","permalink":"http://zhangyu.info/2021/04/09/Kubernetes-network/","excerpt":"","text":"Kubernetes网络和云厂商实践浅析 原创 张向阳 云网漫步 2020-11-22https://mp.weixin.qq.com/s?src=11&amp;timestamp=1617936055&amp;ver=2997&amp;signature=czoNtYoc1oSn7KtlQknWvo*D*Q0GOWU2VI3rEKh6YtE9kSX4TOoMWQLrj8cPiKi9jDE-u4SIMSdM1iYpGw63aXBXZlz7XHUcOgrTiL335Qm9mxs0cJFYp7j1VpUx6IkF&amp;new=1 0 前言Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 源于希腊语，意为 &quot;舵手&quot; 或 &quot;飞行员&quot;，Google 在 2014 年开源了 Kubernetes 项目，Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验的基础上（Brog系统），结合了社区中最好的想法和实践。 为了能实现Kubernetes有效的管理大规模的容器，需要优秀网络技术的支撑，本文主要从Kubernetes网络的角度去介绍Kubernetes网络的需求、网络模型、实现技术、云厂商Kubernetes的网络实践。 1 Kubernetes网络系统需求集群网络系统是 Kubernetes 的核心部分，它需要解决下面四个问题。 Pod内容器间通信。 Pod 间通信。 Pod 和服务间通信。 外部和服务间通信。 Kubernetes 的宗旨就是在应用之间共享机器，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。同时动态分配端口也会给系统带来很多复杂度，每个应用都需要设置一个端口的参数，而 API服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。 与其去解决这些问题，Kubernetes 选择了其他不同的方法，下面我们介绍一下Kubernetes 网络模型。 2 Kubernetes 网络模型 Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）： 节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信。 节点上的代理（例如，系统守护进程、kubelet）可以和节点上的所有Pod通信。 每一个Pod都有它自己的IP地址，这就意味着不需要显式地在每个Pod之间创建链接，也不需要处理容器端口到主机端口之间的映射。这样将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod可以被视作虚拟机或者物理主机。同时还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容，如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP，这样就可以和其他的虚拟机进行通信，这是基本相同的模型。 3 Kubernetes 网络技术 从上文看出，每个pod有自己唯一的IP地址，可通过一个扁平的、非NAT网络和其他Pod通信。Kubernetes是如何做到这一点呢？其实，Kubernetes不负责这块，网络是有Container Network Interface（CNI）插件进行管理。CNI是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件，CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源，如下图所示。 Kubernetes网络实现模型很多，从本质上看，使用网络技术有两大类，路由方案和Overlay网络方案。 3.1 Pod3.1.1 Pod内container(容器)通信 Pod中管理着一组容器，这些容器共享同一个网络命名空间。Pod中的每个容器拥有与Pod相同的IP和port地址空间，并且由于他们在同一个网络命名空间，他们之间可以通过localhost相互访问。 每个Pod容器有一个pause容器其有独立的网络命名空间，在Pod内启动容器时候使用 –net=container就可以让当前容器加入到Pod容器拥有的网络命名空间（pause容器）。 3.1.2 同节点的Pod通信 每个Pod拥有一个ip地址，不同的Pod之间可以直接使用改ip与彼此进行通讯。在同一个Node上，从Pod的视角看，它存在于自己的网络命名空间中，并且需要与该Node上的其他网络命名空间上的Pod进行通信。 为了让多个Pod的网络命名空间链接起来，会创建一下veth pair对，veth对的一端链接到宿主机的网络命名空间，另一端链接到Pod的网络命名空间，并重新命名为eth0。 宿主机网络命名空间的接口会绑定到容器运行时配置使用的网络桥接上。从网桥的地址段中去IP地址赋值给容器的eth0接口。应用的任何运行在容器内部的程序都会发送数据到eht0网络接口，数据从宿主机命名空间的另外一个veth接口出来，然后发送给网桥。 3.1.3 不同节点的Pod通信 不同节点上的pod能够通信，需要把这些节点的网桥以某种方式连接起来，有多种连接不同节点上的网桥的方式，例如通过三层路由，或者Overlay网络（隧道技术，例如GRE和VxLAN等）。 路由方案 Overlay方案 pod通常需要对来自集群内部其他pod，以及来自集群外部的客户端的HTTP请求作出反应。pod需要一种寻找其他pod的方法来使用其他pod提供的服务。而在Kubernetes的网络中，有特殊的地方。 一个服务经常会起多个pod，你到底访问那个pod的ip呢？ pod经常会因为各种原因被调度，调度后一个pod的ip会发生变化。 pod的ip是虚拟的且局域的，在集群内部访问没有问题，但是从Kubernetes集群的外部如何访问pod的ip呢？ 为了解决第1，2的问题，Kubernetes提供了一种资源类型，服务（service）。为了解决第3个问题，Kubernetes有将服务的类型设置为NodePort，将服务的类型设置为LoadBanlance，创建一个Ingress资源。 3.2 Service Kubernetes的service（服务）是一种为一组功能相同的pod提供单一不变的接入点的资源。当服务存在时，它的ip地址和端口不会变化，客户端通过IP地址和端口号建立连接，这些连接会被路由到提供该服务的任意一个pod上。通过这种方式，客户端不需要知道每个单独的提供服务的pod的地址，这样这些pod可以在集群中随时被创建或者移除。 Kubernetes的服务需要解决两个主要问题。 服务怎么做负载均衡？ 服务怎么被发现？ 3.2.1 负载均衡 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP的形式。其实，服务并不是和pod直接相连的，它们之间是一种EndPoint资源。EndPoint资源就是暴露一个服务的IP地址和端口列表。 3.2.1.1 userspace 代理模式 kube-proxy 会监视 Kubernetes 主控节点对 Service 对象和 Endpoints 对象的添加和移除操作。对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。任何连接到“代理端口”的请求，都会被代理到 Service 的后端 Pods 中的某个上面（如 Endpoints 所报告的一样）。使用哪个后端 Pod，是 kube-proxy 基于 SessionAffinity 来确定的。 最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP） 和 Port 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。 3.2.1.2 iptables 代理模式 kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。 使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。这种方法也可能更可靠。 如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应， 则连接失败。这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败， 并会自动使用其他后端 Pod 重试。 3.2.1.3 IPVS 代理模式 在 ipvs 模式下，kube-proxy监视Kubernetes服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。与iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。IPVS提供了更多选项来平衡后端Pod的流量。 3.2.2 服务发现Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。 通过环境变量发现服务 在pod开始运行的时候，Kubernetes会初始化一系列的环境变量指向现在存在的服务 注：当您具有需要访问服务的Pod时，并且您正在使用环境变量方法将端口和群集 IP 发布到客户端 Pod 时，必须在客户端 Pod 出现 之前 创建服务。否则，这些客户端 Pod 将不会设定其环境变量。 通过DNS发现服务 支持群集的 DNS 服务器监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。如果在整个群集中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。 3.2.3 发布服务 对一些应用（如前端）的某些部分，可能希望通过外部 Kubernetes 集群外部 IP 地址暴露 Service。 Kubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP类型。 ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。 3.3 Ingress 我们也可以使用 Ingress 来暴露自己的服务。 为什么需要Ingress呢？一个重要的原因是每个LoadBalancer服务都需要自己的负载均衡器，以及独有的公用IP地址，而Ingress只需要一个公网IP就能为很多服务提供访问。例如，当客户端向Ingress发送HTTP请求时，Ingress会根据请求的主机和路径决定请求转发到的服务。 客户端先执行DNS查询，DNS服务器返回了Ingress控制器的IP地址。 客户端然后向Ingress控制器发送HTTP请求，并在Host头部中指定访问的域名。 控制器从该Host头部确认客户端尝试访问哪个服务，通过与该服务关联的Endpoint对象查看pod IP，并将客户端的请求转发给其中一个pod。 4. 云厂商Kubernetes实践4.1 AWS Kubernetes网络方案 AWS上搭建Kubernetes集群环境有两种方式，一种是使用托管服务Amazon Elastic Kubernetes Service (Amazon EKS) ，一种是自建K8S集群。可以使用Amazon VPC CNI插件管理Pod的网络地址和通信。 EKS网络架构 4.1.1 VPC CNI插件 AWS VPC CNI 为 Kubernetes 集群提供了集成的 AWS 虚拟私有云（VPC）网络，使用该 CNI 插件，可使 Kubernetes Pod 拥有与在 VPC 网络上相同的 IP 地址。CNI 将 AWS 弹性网络接口（ENI）分配给每个 Kubernetes 节点，并将每个 ENI 的辅助 IP 范围用于该节点上的 Pod 。 Kubernetes 的 Amazon VPC 容器网络接口 (CNI) 插件随每个节点一起部署，插件包含两个主要组件。 **L-IPAM 守护程序**\\-负责创建网络接口并将网络接口附加到 Amazon EC2 实例,将辅助 IP 地址分配给网络接口,并在每个节点上维护 IP 地址的地址池,以便在安排时分配到 Kubernetes Pod。 **CNI 插件** – 负责连接主机网络(例如,配置网络接口和虚拟以太网对)并向 Pod 命名空间添加正确的网络接口。 4.1.2 Pod通信 VPC 内的通信（如 Pod 到 Pod）在私有 IP 地址之间是直接通信。 4.1.2 Pod和外部通信 当流量以 VPC 外部的地址为目标时,默认情况下,Kubernetes 的 Amazon VPC CNI 插件将每个 Pod 的私有 IP 地址转换为分配给 Pod 在其上运行的 节点的主 网络接口Amazon EC2(网络接口)的主私有IP地址，有如下两种方式。 4.1.3 Ingress AWS ALB Ingress 控制器将在Kubernetes 用户声明集群上的 Ingress 资源时触发创建 ALB 以及必要的 AWS 支持资源。Ingress 资源通过 ALB 将 HTTP\\[s\\] 流量路由至集群内的不同终端节点。 控制器观察来自 API 服务器的进站事件。如果发现 Ingress 资源满足要求，则将开始创建 AWS 资源。 为 Ingress 资源创建 ALB。 为 Ingress 资源中指定的每个后端创建目标组。 为 Ingress 资源注释中指定的每个端口创建侦听器。如果未指定端口，则将使用合理的默认值（80 或 443）。 为 Ingress 资源中指定的每个路径创建规则。这将确保指向特定路径的流量将被路由至所创建的正确目标组。 4.2 GCP Kubernetes网络方案Google Kubernetes Engine (GKE) 提供了一个托管环境，可以使用 Google 基础架构在其中部署、管理和扩缩容器化应用。 4.2.1 Pod通信 4.2.2 Service 4.2.3 Loadbalancer 具体细节，参考GCP的官方文档。 https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview 4.3 阿里云Kubernetes网络方案 阿里云容器服务产品线的整体架构 本章节是介绍阿里云容器服务Kubernetes版ACK（Alibaba Cloud Container Service for Kubernetes）。 4.3.1 网络模型 容器服务将Kubernetes网络和阿里云VPC的深度集成，提供了稳定高性能的容器网络。在容器服务中，支持以下类型的互联互通。 同一个容器集群中，Pod之间相互访问。 同一个容器集群中，Pod访问Service。 同一个容器集群中，ECS访问Service。 Pod直接访问同一个VPC下的ECS。 同一个VPC下的ECS直接访问Pod。 4.3.2 阿里云Terway网络插件 Terway网络插件是阿里云容器服务的网络插件，功能上完全兼容Flannel。 支持将阿里云的弹性网卡分配给容器。 支持基于Kubernetes标准的NetworkPolicy来定义容器间的访问策略，兼容Calico的Network Policy。 在Terway网络插件中，每个Pod拥有自己网络栈和IP地址。同一台ECS内的Pod之间通信，直接通过机器内部的转发，跨ECS的Pod通信，报文通过VPC的vRouter转发。由于不需要使用VxLAN等的隧道技术封装报文，因此具有较高的通信性能。 4.4 腾讯云Kubernetes网络方案腾讯云容器服务（Tencent Kubernetes Engine ，TKE）基于原生 kubernetes 提供以容器为核心的、高度可扩展的高性能容器管理服务。 本章节主要参考以下文章，公众号：腾讯云原生 公众号文章：腾讯云容器服务TKE推出新一代零损耗容器网络 4.4.1 GlobalRouter 模式 基于 vpc 实现的全局路由模式，目前是 TKE 默认网络方案。该模式依托于 vpc 底层路由能力，不需要在节点上配置 vxlan 等 overlay 设备，就可实现容器网络 和 vpc 网络的互访，并且相比于 calico/flannel 等网络方案，没有额外的解封包，性能也会更好。 4.4.1 VPC-CNI 模式 TKE 基于 CNI 和 VPC 弹性网卡实现的容器网络能力，适用于 Pod 固定 IP，CLB 直通 Pod，Pod 直绑 EIP 等场景。该网络模式下，容器与节点分布在同一网络平面，容器 IP 为 IPAMD 组件所分配的弹性网卡 IP。 4.4.3 VPC-CNI-**独立网卡** 依托于弹性网卡，将绑定到节点的弹性网卡通过 CNI 配置到容器网络命名空间，实现容器直接独享使用弹性网卡。 4.5 其他CNI插件 参考链接 https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/ 5 总结 本文主要介绍了Kubernetes的网络实现，包括pod的通信，服务（service），Ingress的实现，也简要介绍了云厂商的CNI插件的实现方法。Kubernetes还有其他优秀的网络插件，各个插件的实现方式有所不同，不过Kubernetes网络模型是不变。 最后欢迎大家留言沟通交流。 6 参考文献Kubernetes集群网络系统 https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/ Amazon EKS https://docs.amazonaws.cn/eks/latest/userguide/external-snat.html Amazon VPC CNI https://aws.amazon.com/cn/blogs/china/use-amazon-vpc-cni-build-default-net-kubernetes-groups/ Google Kubernetes Engine (GKE) https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview kubernetes网络和CNI简介 https://www.jianshu.com/p/88062fa25083 Understanding kubernetes networking: pods https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727 containernetworking/cni https://github.com/containernetworking/cni Amazon Elastic Container Service https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html CNI - Container Network Interface（容器网络接口） https://jimmysong.io/kubernetes-handbook/concepts/cni.html containernetworking/cni https://github.com/containernetworking/cni","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"Kubernetes Ingress 控制器的技术选型技巧","slug":"Technical-selection-of-Kubernetes-Ingress-controller","date":"2021-04-07T16:00:00.000Z","updated":"2021-04-08T08:56:51.700Z","comments":true,"path":"2021/04/08/Technical-selection-of-Kubernetes-Ingress-controller/","link":"","permalink":"http://zhangyu.info/2021/04/08/Technical-selection-of-Kubernetes-Ingress-controller/","excerpt":"","text":"Kubernetes Ingress 控制器的技术选型技巧 作者：厉辉，腾讯云中间件API网关核心研发成员 在 Kubernetes 的实践、部署中，为了解决 Pod 迁移、Node Pod 端口、域名动态分配等问题，需要开发人员选择合适的 Ingress 解决方案。面对市场上众多Ingress产品，开发者该如何分辨它们的优缺点？又该如何结合自身的技术栈选择合适的技术方案呢？在本文中，腾讯云中间件核心研发工程师厉辉将为你介绍如何进行 Kubernates Ingress 控制器的技术选型。 名词解释 阅读本文需要熟悉以下基本概念： 集群：是指容器运行所需云资源的集合，包含了若干台云服务器、负载均衡器等云资源。 实例（Pod）：由相关的一个或多个容器构成一个实例，这些容器共享相同的存储和网络空间。 工作负载（Node）：Kubernetes 资源对象，用于管理 Pod 副本的创建、调度以及整个生命周期的自动控制。 服务（Service）：由多个相同配置的实例（Pod）和访问这些实例（Pod）的规则组成的微服务。 Ingress：Ingress 是用于将外部 HTTP（S）流量路由到服务（Service）的规则集合。 Kubernetes 访问现状 Kubernetes 的外部访问方式 在 Kubernetes 中，服务跟 Pod IP 主要供服务在集群内访问使用，对于集群外的应用是不可见的。怎么解决这个问题呢？为了让外部的应用能够访问 Kubernetes 集群中的服务，通常解决办法是 NodePort 和 LoadBalancer。 这两种方案其实各自都存在一些缺点： NodePort 的缺点是一个端口只能挂载一个 Service，而且为了更高的可用性，需要额外搭建一个负载均衡。 LoadBalancer 的缺点则是每个服务都必须要有一个自己的 IP，不论是内网 IP 或者外网 IP。更多情况下，为了保证 LoadBalancer 的能力，一般需要依赖于云服务商。 在Kubernetes的实践、部署中，为了解决像 Pod 迁移、Node Pod 端口、域名动态分配，或者是 Pod 后台地址动态更新这种问题，就产生了 Ingress 解决方案 Nginx Ingress 的缺点 Ingress 是Kubernetes中非常重要的外网流量入口。在Kubernetes中所推荐的默认值为Nginx Ingress，为了与后面Nginx 提供的商业版 Ingress 区分开来，我就称它为Kubernetes Ingress。 Kubernetes Ingress，顾名思义基于 Nginx 的平台，Nginx 现在是世界上最流行的 Nginx HTTP Sever，相信大家都对 Nginx 也比较熟悉，这是一个优点。它还有一个优点是 Nginx Ingress 接入 Kubernetes 集群所需的配置非常少，而且有很多文档来指引你如何使用它。这对于大部分刚接触 Kubernetes 的人或者创业公司来说，Nginx Ingress 的确是一个非常好的选择。 但是当 Nginx Ingress 在一些大环境上使用时，就会出现很多问题： 第一个问题：Nginx Ingress用了一些 OpenResty 的特性，但最终配置加载还是依赖于原有的 Nginx config reload。当路由配置非常大时，Nginx reload 会耗时很久，时间长达几秒甚至十几秒，这样就会严重影响业务，甚至造成业务中断。 第二个问题：Nginx Ingress 的插件开发非常困难。如果你认为 Nginx Ingress 本身插件不够用，需要使用一些定制化插件，这个额外的开发任务对程序员来说是十分痛苦的。因为Nginx Ingress自身的插件能力和可扩展性非常差。 Ingress 选型原则 既然发现了 Nginx Ingress 有很多问题，那是不是考虑选择其他开源的、更好用的 Ingress？市场上比 Kubernetes Ingress 好用的Ingress起码有十几家，那么如何从这么多 Ingress 中选择适合自己的呢？ Ingress 自身是基于 HTTP 网关的，市面上 HTTP 网关主要有这么几种：Nginx、Golang 原生的网关，以及新崛起的 Envoy 。但是每个开发人员所擅长的技术栈不同，所以适合的 Ingress 也会不一样。 那么问题来了，我们如何选择一个更加好用的 Ingress 呢？或者缩小点范围，熟悉 Nginx 或 OpenResty 的开发人员，应该选择哪一个 Ingress 呢？ 下面来介绍一下我对 Ingress 控制器选型的一些经验。 选型原则 1.基本特点 首先我认为Ingress 控制器应该具备以下基本功能，如果连这些功能都没有，那完全可以直接pass。 必须开源的，不开源的无法使用。 Kubernetes 中Pod 变化非常频繁，服务发现非常重要。 现在 HTTPS 已经很普及了，TLS 或者 SSL 的能力也非常重要，比如证书管理的功能。 支持 WebSocket 等常见协议，在某些情况下，可能还需要支持 HTTP2 、QUIC 等协议。 2.基础软件前面有提到，每个人擅长的技术平台不一样，所以选择自己更加熟悉的 HTTP 网关也显得至关重要。比如 Nginx、HAProxy、Envoy 或者是 Golang 原生网关。因为你熟悉它的原理，在使用中可以实现快速落地。 在生产环境上，高性能是一个很重要的特性，但比之更重要的是高可用。这意味着你选择的网关，它的可用性、稳定性一定要非常强，只有这样，服务才能稳定。 3.功能需求抛开上述两点，就是公司业务对网关的特殊需求。你选择一个开源产品，最好肯定是开箱能用的。比如你需要 GRPC 协议转换的能力，那当然希望选的网关具备这样的功能。这里简单列一下影响选择的因素： 协议：是否支持 HTTP2、HTTP3； 负载均衡算法：最基本的WRR、一致性哈希负载均衡算法是否能够满足需求，还是需要更加复杂的类似EWMA负载均衡算法。 鉴权限流：仅需要简单的鉴权，或更进阶的鉴权方式。又或者需要集成，能够快速的开发出像腾讯云 IM 的鉴权功能。Kubernetes Ingress除了前面我们提到的存在Nginx reload 耗时长、插件扩展能力差的问题，另外它还存在后端节点调整权重的能力不够灵活的问题。 选择 APISIX 相比Kubernetes Ingress，我个人更推荐 APISIX 作为Ingress ?controller。虽然它在功能上比 Kong 会少很多，但是 APISIX 很好的路由能力、灵活的插件能力，以及本身的高性能，能够弥补在 Ingress 选型上的一些缺点。对于基于 Nginx 或 Openresty 开发的程序员，如果对现在的 Ingress 不满意，我推荐你们去使用 APISIX 作为 Ingress。 如何将 APISIX 作为 Ingress 呢？我们首先要做出一个区分，Ingress 是 Kubernetes 名称的定义或者规则定义，Ingress controller 是将 Kubernetes 集群状态同步到网关的一个组件。但 APISIX 本身只是 API 网关，怎么把 APISIX 实现成 Ingress controller 呢？我们先来简要了解一下如何实现 Ingress。 实现 Ingress，本质上就只有两部分内容： 第一部分：需要将 Kubernetes 集群中的配置、或 Kubernetes 集群中的状态同步到 APISIX 集群。 第二部分：需要将 APISIX中 的一些概念，比如像服务、upstream 等概念定义为 Kubernetes 中的 CRD。 如果实现了第二部分，通过 Kubernetes Ingress 的配置，便可以很快的产生 APISIX。通过 APISIX Ingress controller 就可以产生 APISIX 相关的配置。当前为了快速的将 APISIX 落地为能够支持 Kubernetes 的 Ingress ，我们创建了一个开源项目，叫 Ingress Controller。 ingress controller 架构图 上图为Ingress controller 项目的整体架构图。左边部分为 Kubernetes 集群，这里可以导入一些 yaml 文件，对 Kubernetes 的配置进行变更。右边部分则是 APISIX 集群，以及它的控制面和数据面。从架构图中可以看出，APISIX Ingress 充当了 Kubernetes 集群以及 APISIX 集群之间的连接者。它主要负责监听 Kubernetes 集群中节点的变化，将集群中的状态同步到 APISIX 集群。另外，由于Kubernetes 倡导所有组件都要具备高可用的特性，所以在 APISIX Ingress 设计之初，我们通过双节点或多节点的模式来保证 APISIX ?Ingress Controller 的保障高可用。 总结 各类 Ingress 横向对比 相对于市面上流行的 Ingress 控制器，我们简单对比来看看 APISIX ingress 有什么优缺点。上图是外国开发人员针对 Kubernetes Ingress 选型做的一张表格。我在原来表格的基础上，结合自己的理解，将 APISIX Ingress 的功能加入了进来。我们可以看到，最左边的是APISIX，后边就是 Kubernetes Ingress 和 Kong Ingress，后面的 Traefik，就是基于 Golang 的 Ingress。HAproxy 是比较常见的，过去是比较流行的负载均衡器。Istio 和 Ambassador 是国外非常流行的两个Ingress。 接下来我们总结下这些 Ingress各自的优缺点： APISIX Ingress：APISIX Ingress 的优点前面也提到了，它具有非常强大的路由能力、灵活的插件拓展能力，在性能上表现也非常优秀。同时，它的缺点也非常明显，尽管APISIX开源后有非常多的功能，但是缺少落地案例，没有相关的文档指引大家如何使用这些功能。 Kubernetes Ingress：即 Kubernetes 推荐默认使用的 Nginx Ingress。它的主要优点为简单、易接入。缺点是Nginx reload耗时长的问题根本无法解决。另外，虽然可用插件很多，但插件扩展能力非常弱。 Nginx Ingress：主要优点是在于它完全支持 TCP 和 UDP 协议，但是缺失了鉴权方式、流量调度等其他功能。 Kong：其本身就是一个 API 网关，它也算是开创了先河，将 API 网关引入到 Kubernetes 中当 Ingress。另外相对边缘网关，Kong 在鉴权、限流、灰度部署等方面做得非常好。Kong Ingress 还有一个很大的优点：提供了一些 API、服务的定义，可以抽象成 Kubernetes 的 CRD，通过K8S Ingress 配置便可完成同步状态至 Kong 集群。缺点就是部署特别困难，另外在高可用方面，与 APISIX 相比也是相形见绌。 Traefik ：基于 Golang 的 Ingress，它本身是一个微服务网关，在 Ingress 的场景应用比较多。他的主要平台基于 Golang，自身支持的协议也非常多，总体来说是没有什么缺点。如果大家熟悉 Golang 的话，也推荐一用。 HAproxy：是一个久负盛名的负载均衡器。它主要优点是具有非常强大的负载均衡能力，其他方面并不占优势。 Istio Ingress 和 Ambassador Ingress 都是基于非常流行的 Envoy。说实话，我认为这两个 Ingress 没有什么缺点，可能唯一的缺点是他们基于 Envoy 平台，大家对这个平台都不是很熟悉，上手门槛会比较高。 综上所述，大家在了解了各个 Ingress 的优劣势后，可以结合自身情况快速选择适合自己的 Ingress。 来自 “ ITPUB博客 ” ，链接：http://blog.itpub.net/31559354/viewspace-2677027/","categories":[{"name":"Ingress","slug":"Ingress","permalink":"http://zhangyu.info/categories/Ingress/"}],"tags":[{"name":"Ingress","slug":"Ingress","permalink":"http://zhangyu.info/tags/Ingress/"}]},{"title":"Kubernetes入门-进阶实战","slug":"Kubernetes-remen","date":"2021-04-07T16:00:00.000Z","updated":"2021-04-08T09:59:59.264Z","comments":true,"path":"2021/04/08/Kubernetes-remen/","link":"","permalink":"http://zhangyu.info/2021/04/08/Kubernetes-remen/","excerpt":"","text":"Kubernetes 入门&amp;进阶实战 作者：oonamao毛江云，腾讯 CSIG 应用开发工程师 本文组织方式： 1.?K8S?是什么，即作用和目的。涉及?K8S?架构的整理，Master?和?Node?之间的关系，以及?K8S?几个重要的组件：API?Server、Scheduler、Controller、etcd?等。2.?K8S?的重要概念，即?K8S?的?API?对象，也就是常常听到的?Pod、Deployment、Service?等。3.?如何配置?kubectl，介绍kubectl工具和配置办法。4.?如何用kubectl?部署服务。5.?如何用kubectl?查看、更新/编辑、删除服务。6.?如何用kubectl?排查部署在K8S集群上的服务出现的问题 I. K8S 概览1.1 K8S 是什么？K8S 是Kubernetes的全称，官方称其是： Kubernetes is an open source system for managing containerized applications across multiple hosts. It provides basic mechanisms for deployment, maintenance, and scaling of applications. 用于自动部署、扩展和管理“容器化（containerized）应用程序”的开源系统。 翻译成大白话就是：“K8 是 S 负责自动化运维管理多个 Docker 程序的集群”。那么问题来了：Docker 运行可方便了，为什么要用 K8S，它有什么优势？ 插一句题外话： 为什么 Kubernetes 要叫 Kubernetes 呢？维基百科已经交代了（老美对星际是真的痴迷）： Kubernetes（在希腊语意为“舵手”或“驾驶员”）由 Joe Beda、Brendan Burns 和 Craig McLuckie 创立，并由其他谷歌工程师，包括 Brian Grant 和 Tim Hockin 等进行加盟创作，并由谷歌在 2014 年首次对外宣布 。该系统的开发和设计都深受谷歌的 Borg 系统的影响，其许多顶级贡献者之前也是 Borg 系统的开发者。在谷歌内部，Kubernetes 的原始代号曾经是Seven，即星际迷航中的 Borg（博格人）。Kubernetes 标识中舵轮有七个轮辐就是对该项目代号的致意。 为什么 Kubernetes 的缩写是 K8S 呢？我个人赞同Why Kubernetes is Abbreviated k8s中说的观点“嘛，写全称也太累了吧，不如整个缩写”。其实只保留首位字符，用具体数字来替代省略的字符个数的做法，还是比较常见的。 1.2 为什么是 K8S?试想下传统的后端部署办法：把程序包（包括可执行二进制文件、配置文件等）放到服务器上，接着运行启动脚本把程序跑起来，同时启动守护脚本定期检查程序运行状态、必要的话重新拉起程序。 有问题吗？显然有！最大的一个问题在于：**如果服务的请求量上来，已部署的服务响应不过来怎么办？**传统的做法往往是，如果请求量、内存、CPU 超过阈值做了告警，运维马上再加几台服务器，部署好服务之后，接入负载均衡来分担已有服务的压力。 问题出现了：从监控告警到部署服务，中间需要人力介入！那么，有没有办法自动完成服务的部署、更新、卸载和扩容、缩容呢？ 这，就是 K8S 要做的事情：自动化运维管理 Docker（容器化）程序。 1.3 K8S 怎么做？我们已经知道了 K8S 的核心功能：自动化运维管理多个容器化程序。那么 K8S 怎么做到的呢？这里，我们从宏观架构上来学习 K8S 的设计思想。首先看下图，图片来自文章Components of Kubernetes Architecture： K8S 是属于主从设备模型（Master-Slave 架构），即有 Master 节点负责核心的调度、管理和运维，Slave 节点则在执行用户的程序。但是在 K8S 中，主节点一般被称为Master Node 或者 Head Node（本文采用 Master Node 称呼方式），而从节点则被称为Worker Node 或者 Node（本文采用 Worker Node 称呼方式）。 要注意一点：Master Node 和 Worker Node 是分别安装了 K8S 的 Master 和 Woker 组件的实体服务器，每个 Node 都对应了一台实体服务器（虽然 Master Node 可以和其中一个 Worker Node 安装在同一台服务器，但是建议 Master Node 单独部署），所有 Master Node 和 Worker Node 组成了 K8S 集群，同一个集群可能存在多个 Master Node 和 Worker Node。 首先来看Master Node都有哪些组件： API Server。K8S 的请求入口服务。API Server 负责接收 K8S 所有请求（来自 UI 界面或者 CLI 命令行工具），然后，API Server 根据用户的具体请求，去通知其他组件干活。 Scheduler。K8S 所有 Worker Node 的调度器。当用户要部署服务时，Scheduler 会选择最合适的 Worker Node（服务器）来部署。 Controller Manager。K8S 所有 Worker Node 的监控器。Controller Manager 有很多具体的 Controller，在文章Components of Kubernetes Architecture中提到的有 Node Controller、Service Controller、Volume Controller 等。Controller 负责监控和调整在 Worker Node 上部署的服务的状态，比如用户要求 A 服务部署 2 个副本，那么当其中一个服务挂了的时候，Controller 会马上调整，让 Scheduler 再选择一个 Worker Node 重新部署服务。 etcd。K8S 的存储服务。etcd 存储了 K8S 的关键配置和用户配置，K8S 中仅 API Server 才具备读写权限，其他组件必须通过 API Server 的接口才能读写数据（见Kubernetes Works Like an Operating System）。 接着来看Worker Node的组件，笔者更赞同HOW DO APPLICATIONS RUN ON KUBERNETES文章中提到的组件介绍： Kubelet。Worker Node 的监视器，以及与 Master Node 的通讯器。Kubelet 是 Master Node 安插在 Worker Node 上的“眼线”，它会定期向 Worker Node 汇报自己 Node 上运行的服务的状态，并接受来自 Master Node 的指示采取调整措施。 Kube-Proxy。K8S 的网络代理。私以为称呼为 Network-Proxy 可能更适合？Kube-Proxy 负责 Node 在 K8S 的网络通讯、以及对外部网络流量的负载均衡。 Container Runtime。Worker Node 的运行环境。即安装了容器化所需的软件环境确保容器化程序能够跑起来，比如 Docker Engine。大白话就是帮忙装好了 Docker 运行环境。 Logging Layer。K8S 的监控状态收集器。私以为称呼为 Monitor 可能更合适？Logging Layer 负责采集 Node 上所有服务的 CPU、内存、磁盘、网络等监控项信息。 Add-Ons。K8S 管理运维 Worker Node 的插件组件。有些文章认为 Worker Node 只有三大组件，不包含 Add-On，但笔者认为 K8S 系统提供了 Add-On 机制，让用户可以扩展更多定制化功能，是很不错的亮点。 总结来看，K8S 的 Master Node 具备：请求入口管理（API Server），Worker Node 调度（Scheduler），监控和自动调节（Controller Manager），以及存储功能（etcd）；而 K8S 的 Worker Node 具备：状态和监控收集（Kubelet），网络和负载均衡（Kube-Proxy）、保障容器化运行环境（Container Runtime）、以及定制化功能（Add-Ons）。 到这里，相信你已经对 K8S 究竟是做什么的，有了大概认识。接下来，再来认识下 K8S 的 Deployment、Pod、Replica Set、Service 等，但凡谈到 K8S，就绕不开这些名词，而这些名词也是最让 K8S 新手们感到头疼、困惑的。 II. K8S 重要概念2.1 Pod 实例官方对于Pod的解释是： Pod是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 这样的解释还是很难让人明白究竟 Pod 是什么，但是对于 K8S 而言，Pod 可以说是所有对象中最重要的概念了！因此，我们必须首先清楚地知道“Pod 是什么”，再去了解其他的对象。 从官方给出的定义，联想下“最小的 xxx 单元”，是不是可以想到本科在学校里学习“进程”的时候，教科书上有一段类似的描述：资源分配的最小单位；还有”线程“的描述是：CPU 调度的最小单位。什么意思呢？”最小 xx 单位“要么就是事物的衡量标准单位，要么就是资源的闭包、集合。前者比如长度米、时间秒；后者比如一个”进程“是存储和计算的闭包，一个”线程“是 CPU 资源（包括寄存器、ALU 等）的闭包。 同样的，Pod 就是 K8S 中一个服务的闭包。这么说的好像还是有点玄乎，更加云里雾里了。简单来说，Pod 可以被理解成一群可以共享网络、存储和计算资源的容器化服务的集合。再打个形象的比喻，在同一个 Pod 里的几个 Docker 服务/程序，好像被部署在同一台机器上，可以通过 localhost 互相访问，并且可以共用 Pod 里的存储资源（这里是指 Docker 可以挂载 Pod 内的数据卷，数据卷的概念，后文会详细讲述，暂时理解为“需要手动 mount 的磁盘”）。笔者总结 Pod 如下图，可以看到：同一个 Pod 之间的 Container 可以通过 localhost 互相访问，并且可以挂载 Pod 内所有的数据卷；但是不同的 Pod 之间的 Container 不能用 localhost 访问，也不能挂载其他 Pod 的数据卷。 对 Pod 有直观的认识之后，接着来看 K8S 中 Pod 究竟长什么样子，具体包括哪些资源？ K8S 中所有的对象都通过 yaml 来表示，笔者从官方网站摘录了一个最简单的 Pod 的 yaml： apiVersion:?v1 kind:?Pod metadata: ??name:?memory-demo ??namespace:?mem-example spec: ??containers: ??-?name:?memory-demo-ctr ????image:?polinux/stress ????resources: ??????limits: ????????memory:?&quot;200Mi&quot; ??????requests: ????????memory:?&quot;100Mi&quot; ????command:?[&quot;stress&quot;] ????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;] ????volumeMounts: ????-?name:?redis-storage ??????mountPath:?/data/redis ??volumes: ??-?name:?redis-storage ????emptyDir:?&#123;&#125; 看不懂不必慌张，且耐心听下面的解释： apiVersion记录 K8S 的 API Server 版本，现在看到的都是v1，用户不用管。 kind记录该 yaml 的对象，比如这是一份 Pod 的 yaml 配置文件，那么值内容就是Pod。 metadata记录了 Pod 自身的元数据，比如这个 Pod 的名字、这个 Pod 属于哪个 namespace（命名空间的概念，后文会详述，暂时理解为“同一个命名空间内的对象互相可见”）。 spec记录了 Pod 内部所有的资源的详细信息，看懂这个很重要： containers记录了 Pod 内的容器信息，containers包括了：name容器名，image容器的镜像地址，resources容器需要的 CPU、内存、GPU 等资源，command容器的入口命令，args容器的入口参数，volumeMounts容器要挂载的 Pod 数据卷等。可以看到，上述这些信息都是启动容器的必要和必需的信息。 volumes记录了 Pod 内的数据卷信息，后文会详细介绍 Pod 的数据卷。 2.2 Volume 数据卷 K8S 支持很多类型的 volume 数据卷挂载，具体请参见K8S 卷。前文就“如何理解 volume”提到：“需要手动 mount 的磁盘”，此外，有一点可以帮助理解：数据卷 volume 是 Pod 内部的磁盘资源。 其实，单单就 Volume 来说，不难理解。但是上面还看到了volumeMounts，这俩是什么关系呢？ volume 是 K8S 的对象，对应一个实体的数据卷；而 volumeMounts 只是 container 的挂载点，对应 container 的其中一个参数。但是，volumeMounts 依赖于 volume，只有当 Pod 内有 volume 资源的时候，该 Pod 内部的 container 才可能有 volumeMounts。 2.3 Container 容器本文中提到的镜像 Image、容器 Container，都指代了 Pod 下的一个container。关于 K8S 中的容器，在 2.1Pod 章节都已经交代了，这里无非再啰嗦一句：一个 Pod 内可以有多个容器 container。 在 Pod 中，容器也有分类，对这个感兴趣的同学欢迎自行阅读更多资料： 标准容器 Application Container。 初始化容器 Init Container。 边车容器 Sidecar Container。 临时容器 Ephemeral Container。 一般来说，我们部署的大多是标准容器（ Application Container）。 2.4 Deployment 和 ReplicaSet（简称 RS）除了 Pod 之外，K8S 中最常听到的另一个对象就是 Deployment 了。那么，什么是 Deployment 呢？官方给出了一个要命的解释： 一个 Deployment 控制器为 Pods 和 ReplicaSets 提供声明式的更新能力。 你负责描述 Deployment 中的 _目标状态_，而 Deployment 控制器以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，并通过新的 Deployment 收养其资源。 翻译一下：Deployment 的作用是管理和控制 Pod 和 ReplicaSet，管控它们运行在用户期望的状态中。哎，打个形象的比喻，** Deployment 就是包工头 **，主要负责监督底下的工人 Pod 干活，确保每时每刻有用户要求数量的 Pod 在工作。如果一旦发现某个工人 Pod 不行了，就赶紧新拉一个 Pod 过来替换它。 新的问题又来了：那什么是 ReplicaSets 呢？ ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。 再来翻译下：ReplicaSet 的作用就是管理和控制 Pod，管控他们好好干活。但是，ReplicaSet 受控于 Deployment。形象来说，ReplicaSet 就是总包工头手下的小包工头。 笔者总结得到下面这幅图，希望能帮助理解： 新的问题又来了：如果都是为了管控 Pod 好好干活，为什么要设置 Deployment 和 ReplicaSet 两个层级呢，直接让 Deployment 来管理不可以吗？ 回答：不清楚，但是私以为是因为先有 ReplicaSet，但是使用中发现 ReplicaSet 不够满足要求，于是又整了一个 Deployment（有清楚 Deployment 和 ReplicaSet 联系和区别的小伙伴欢迎留言啊）。 但是，从 K8S 使用者角度来看，用户会直接操作 Deployment 部署服务，而当 Deployment 被部署的时候，K8S 会自动生成要求的 ReplicaSet 和 Pod。在K8S 官方文档中也指出用户只需要关心 Deployment 而不操心 ReplicaSet： This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section. 这实际上意味着您可能永远不需要操作 ReplicaSet 对象：直接使用 Deployments 并在规范部分定义应用程序。 补充说明：在 K8S 中还有一个对象 — ReplicationController（简称 RC），官方文档对它的定义是： ReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态。换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。 怎么样，和 ReplicaSet 是不是很相近？在Deployments, ReplicaSets, and pods教程中说“ReplicationController 是 ReplicaSet 的前身”，官方也推荐用 Deployment 取代 ReplicationController 来部署服务。 2.5 Service 和 Ingress吐槽下 K8S 的概念/对象/资源是真的多啊！前文介绍的 Deployment、ReplicationController 和 ReplicaSet 主要管控 Pod 程序服务；那么，Service 和 Ingress 则负责管控 Pod 网络服务。 我们先来看看官方文档中 Service 的定义： 将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。 使用 Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。 翻译下：K8S 中的服务（Service）并不是我们常说的“服务”的含义，而更像是网关层，是若干个 Pod 的流量入口、流量均衡器。 那么，为什么要 Service 呢？ 私以为在这一点上，官方文档讲解地非常清楚： Kubernetes Pod 是有生命周期的。它们可以被创建，而且销毁之后不会再启动。如果您使用 Deployment 来运行您的应用程序，则它可以动态创建和销毁 Pod。 每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。 这导致了一个问题：如果一组 Pod（称为“后端”）为群集内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用工作量的后端部分？ 补充说明：K8S 集群的网络管理和拓扑也有特别的设计，以后会专门出一章节来详细介绍 K8S 中的网络。这里需要清楚一点：K8S 集群内的每一个 Pod 都有自己的 IP（是不是很类似一个 Pod 就是一台服务器，然而事实上是多个 Pod 存在于一台服务器上，只不过是 K8S 做了网络隔离），在 K8S 集群内部还有 DNS 等网络服务（一个 K8S 集群就如同管理了多区域的服务器，可以做复杂的网络拓扑）。 此外，笔者推荐k8s 外网如何访问业务应用对于 Service 的介绍，不过对于新手而言，推荐阅读前半部分对于 service 的介绍即可，后半部分就太复杂了。我这里做了简单的总结： Service 是 K8S 服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”。举个例子，我们的一个服务 A，部署了 3 个备份，也就是 3 个 Pod；对于用户来说，只需要关注一个 Service 的入口就可以，而不需要操心究竟应该请求哪一个 Pod。优势非常明显：一方面外部用户不需要感知因为 Pod 上服务的意外崩溃、K8S 重新拉起 Pod 而造成的 IP 变更，外部用户也不需要感知因升级、变更服务带来的 Pod 替换而造成的 IP 变化，另一方面，Service 还可以做流量负载均衡。 但是，Service 主要负责 K8S 集群内部的网络拓扑。那么集群外部怎么访问集群内部呢？这个时候就需要 Ingress 了，官方文档中的解释是： Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。 翻译一下：Ingress 是整个 K8S 集群的接入层，复杂集群内外通讯。 最后，笔者把 Ingress 和 Service 的关系绘制网络拓扑关系图如下，希望对理解这两个概念有所帮助： 2.6 namespace 命名空间和前文介绍的所有的概念都不一样，namespace 跟 Pod 没有直接关系，而是 K8S 另一个维度的对象。或者说，前文提到的概念都是为了服务 Pod 的，而 namespace 则是为了服务整个 K8S 集群的。 那么，namespace 是什么呢？ 上官方文档定义： Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。这些虚拟集群被称为名字空间。 翻译一下：namespace 是为了把一个 K8S 集群划分为若干个资源不可共享的虚拟集群而诞生的。 也就是说，可以通过在 K8S 集群内创建 namespace 来分隔资源和对象。比如我有 2 个业务 A 和 B，那么我可以创建 ns-a 和 ns-b 分别部署业务 A 和 B 的服务，如在 ns-a 中部署了一个 deployment，名字是 hello，返回用户的是“hello a”；在 ns-b 中也部署了一个 deployment，名字恰巧也是 hello，返回用户的是“hello b”（要知道，在同一个 namespace 下 deployment 不能同名；但是不同 namespace 之间没有影响）。前文提到的所有对象，都是在 namespace 下的；当然，也有一些对象是不隶属于 namespace 的，而是在 K8S 集群内全局可见的，官方文档提到的可以通过命令来查看，具体命令的使用办法，笔者会出后续的实战文章来介绍，先贴下命令： `#?位于名字空间中的资源kubectl?api-resources?–namespaced=true #?不在名字空间中的资源kubectl?api-resources?–namespaced=false` 不在 namespace 下的对象有： 在 namespace 下的对象有（部分）： 2.7 其他K8S 的对象实在太多了，2.1-2.6 介绍的是在实际使用 K8S 部署服务最常见的。其他的还有 Job、CronJob 等等，在对 K8S 有了比较清楚的认知之后，再去学习更多的 K8S 对象，不是难事。 III. 配置 kubectl3.1 什么是 kubectl？官方文档中介绍 kubectl 是： Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数–kubeconfig 来指定其他位置的 kubeconfig 文件。 也就是说，可以通过 kubectl 来操作 K8S 集群，基本语法： 使用以下语法 kubectl 从终端窗口运行命令： kubectl?[command]?[TYPE]?[NAME]?[flags] 其中 command、TYPE、NAME 和 flags 分别是： command：指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 TYPE：指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果: ```shell kubectl?get?pod?pod1 kubectl?get?pods?pod1 kubectl?get?po?pod1 ``-?NAME：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息?kubectl get pods。 在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件： -?要按类型和名称指定资源：??-?要对所有类型相同的资源进行分组，请执行以下操作：TYPE1 name1 name2 name&lt;#&gt;。?例子：kubectl get pod example-pod1 example-pod2??-?分别指定多个资源类型：TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE&lt;#&gt;/name&lt;#&gt;。?例子：kubectl get pod/example-pod1 replicationcontroller/example-rc1-?用一个或多个文件指定资源：-f file1 -f file2 -f file&lt;#&gt;??-?使用?YAML?而不是?JSON?因为 YAML 更容易使用，特别是用于配置文件时。?例子：kubectl get -f ./pod.yaml-?flags:?指定可选的参数。例如，可以使用?-s?或?-server?参数指定 Kubernetes API 服务器的地址和端口。`` 就如何使用 kubectl 而言，官方文档已经说得非常清楚。不过对于新手而言，还是需要解释几句： kubectl 是 K8S 的命令行工具，并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。 kubectl 是通过本地的配置文件来连接到 K8S 集群的，默认保存在$HOME/.kube 目录下；也可以通过 KUBECONFIG 环境变量或设置命令参数–kubeconfig 来指定其他位置的 kubeconfig 文件【官方文档】。 接下来，一起看看怎么使用 kubectl 吧，切身感受下 kubectl 的使用。 请注意，如何安装 kubectl 的办法有许多非常明确的教程，比如《安装并配置 kubectl》，本文不再赘述。 1.2 怎么配置 kubectl？第一步，必须准备好要连接/使用的 K8S 的配置文件，笔者给出一份杜撰的配置： apiVersion:?v1 clusters: -?cluster: ????certificate-authority-data:?thisisfakecertifcateauthoritydata00000000000 ????server:? ??name:?cls-dev contexts: -?context: ????cluster:?cls-dev ????user:?kubernetes-admin ??name:?kubernetes-admin@test current-context:?kubernetes-admin@test kind:?Config preferences:?&#123;&#125; users: -?name:?kubernetes-admin ??user: ????token:?thisisfaketoken00000 解读如下： clusters记录了 clusters（一个或多个 K8S 集群）信息： name是这个 cluster（K8S 集群）的名称代号 server是这个 cluster（K8S 集群）的访问方式，一般为 IP+PORT certificate-authority-data是证书数据，只有当 cluster（K8S 集群）的连接方式是 https 时，为了安全起见需要证书数据 users记录了访问 cluster（K8S 集群）的账号信息： name是用户账号的名称代号 user/token是用户的 token 认证方式，token 不是用户认证的唯一方式，其他还有账号+密码等。 contexts是上下文信息，包括了 cluster（K8S 集群）和访问 cluster（K8S 集群）的用户账号等信息： name是这个上下文的名称代号 cluster是 cluster（K8S 集群）的名称代号 user是访问 cluster（K8S 集群）的用户账号代号 current-context记录当前 kubectl 默认使用的上下文信息 kind和apiVersion都是固定值，用户不需要关心 preferences则是配置文件的其他设置信息，笔者没有使用过，暂时不提。 第二步，给 kubectl 配置上配置文件。 --kubeconfig参数。第一种办法是每次执行 kubectl 的时候，都带上--kubeconfig=$&#123;CONFIG_PATH&#125;。给一点温馨小提示：每次都带这么一长串的字符非常麻烦，可以用 alias 别名来简化码字量，比如alias k=kubectl --kubeconfig=$&#123;CONFIG_PATH&#125;。 KUBECONFIG环境变量。第二种做法是使用环境变量KUBECONFIG把所有配置文件都记录下来，即export KUBECONFIG=$KUBECONFIG:$&#123;CONFIG_PATH&#125;。接下来就可以放心执行 kubectl 命令了。 $HOME/.kube/config 配置文件。第三种做法是把配置文件的内容放到$HOME/.kube/config 内。具体做法为： 如果$HOME/.kube/config 不存在，那么cp $&#123;CONFIG_PATH&#125; $HOME/.kube/config即可； 如果如果 $HOME/.kube/config已经存在，那么需要把新的配置内容加到 $HOME/.kube/config 下。单单只是cat $&#123;CONFIG_PATH&#125; &gt;&gt; $HOME/.kube/config是不行的，正确的做法是：KUBECONFIG=$HOME/.kube/config:$&#123;CONFIG_PATH&#125; kubectl config view --flatten &gt; $HOME/.kube/config 。解释下这个命令的意思：先把所有的配置文件添加到环境变量KUBECONFIG中，然后执行kubectl config view --flatten打印出有效的配置文件内容，最后覆盖$HOME/.kube/config 即可。 请注意，上述操作的优先级分别是 1&gt;2&gt;3，也就是说，kubectl 会优先检查--kubeconfig，若无则检查KUBECONFIG，若无则最后检查$HOME/.kube/config，如果还是没有，报错。但凡某一步找到了有效的 cluster，就中断检查，去连接 K8S 集群了。 第三步：配置正确的上下文 按照第二步的做法，如果配置文件只有一个 cluster 是没有任何问题的，但是对于有多个 cluster 怎么办呢？到这里，有几个关于配置的必须掌握的命令： kubectl config get-contexts。列出所有上下文信息。 kubectl config current-context。查看当前的上下文信息。其实，命令 1 线束出来的*所指示的就是当前的上下文信息。 kubectl config use-context $&#123;CONTEXT_NAME&#125;。更改上下文信息。 kubectl config set-context $&#123;CONTEXT_NAME&#125;|--current --$&#123;KEY&#125;=$&#123;VALUE&#125;。修改上下文的元素。比如可以修改用户账号、集群信息、连接到 K8S 后所在的 namespace。 关于该命令，还有几点要啰嗦的： config set-context可以修改任何在配置文件中的上下文信息，只需要在命令中指定上下文名称就可以。而–current 则指代当前上下文。 上下文信息所包括的内容有：cluster 集群（名称）、用户账号（名称）、连接到 K8S 后所在的 namespace，因此有config set-context严格意义上的用法： kubectl config set-context [NAME|--current] [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] [options] （备注：[options]可以通过 kubectl options 查看） 综上，如何操作 kubectl 配置都已交代。 IV. kubectl 部署服务K8S 核心功能就是部署运维容器化服务，因此最重要的就是如何又快又好地部署自己的服务了。本章会介绍如何部署 Pod 和 Deployment。 2.1 如何部署 Pod？通过 kubectl 部署 Pod 的办法分为两步：1). 准备 Pod 的 yaml 文件；2). 执行 kubectl 命令部署 第一步：准备 Pod 的 yaml 文件。关于 Pod 的 yaml 文件初步解释，本系列上一篇文章《K8S 系列一：概念入门》已经有了初步介绍，这里再复习下： apiVersion:?v1 kind:?Pod metadata: ??name:?memory-demo ??namespace:?mem-example spec: ??containers: ??-?name:?memory-demo-ctr ????image:?polinux/stress ????resources: ??????limits: ????????memory:?&quot;200Mi&quot; ??????requests: ????????memory:?&quot;100Mi&quot; ????command:?[&quot;stress&quot;] ????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;] ????volumeMounts: ????-?name:?redis-storage ??????mountPath:?/data/redis ??volumes: ??-?name:?redis-storage ????emptyDir:?&#123;&#125; 继续解读： metadata，对于新入门的同学来说，需要重点掌握的两个字段： name。这个 Pod 的名称，后面到 K8S 集群中查找 Pod 的关键字段。 namespace。命名空间，即该 Pod 隶属于哪个 namespace 下，关于 Pod 和 namespace 的关系，上一篇文章已经交代了。 spec记录了 Pod 内部所有的资源的详细信息，这里我们重点查看containers下的几个重要字段： name。Pod 下该容器名称，后面查找 Pod 下的容器的关键字段。 image。容器的镜像地址，K8S 会根据这个字段去拉取镜像。 resources。容器化服务涉及到的 CPU、内存、GPU 等资源要求。可以看到有limits和requests两个子项，那么这两者有什么区别吗，该怎么使用？在What’s the difference between Pod resources.limits and resources.requests in Kubernetes?回答了： limits是 K8S 为该容器至多分配的资源配额；而requests则是 K8S 为该容器至少分配的资源配额。打个比方，配置中要求了 memory 的requests为 100M，而此时如果 K8S 集群中所有的 Node 的可用内存都不足 100M，那么部署服务会失败；又如果有一个 Node 的内存有 16G 充裕，可以部署该 Pod，而在运行中，该容器服务发生了内存泄露，那么一旦超过 200M 就会因为 OOM 被 kill，尽管此时该机器上还有 15G+的内存。 command。容器的入口命令。对于这个笔者还存在很多困惑不解的地方，暂时挖个坑，有清楚的同学欢迎留言。 args。容器的入口参数。同上，有清楚的同学欢迎留言。 volumeMounts。容器要挂载的 Pod 数据卷等。请务必记住：Pod 的数据卷只有被容器挂载后才能使用！ 第二步：执行 kubectl 命令部署。有了 Pod 的 yaml 文件之后，就可以用 kubectl 部署了，命令非常简单：kubectl create -f $&#123;POD_YAML&#125;。 随后，会提示该命令是否执行成功，比如 yaml 内容不符合要求，则会提示哪一行有问题： 修正后，再次部署： 2.2 如何部署 Deployment？第一步：准备 Deployment 的 yaml 文件。首先来看 Deployment 的 yaml 文件内容： apiVersion:?extensions/v1beta1 ?kind:?Deployment ?metadata: ???name:?rss-site ???namespace:?mem-example ?spec: ???replicas:?2 ???template: ?????metadata: ???????labels: ?????????app:?web ?????spec: ??????containers: ???????-?name:?memory-demo-ctr ?????????image:?polinux/stress ?????????resources: ?????????limits: ???????????emory:?&quot;200Mi&quot; ?????????requests: ???????????memory:?&quot;100Mi&quot; ?????????command:?[&quot;stress&quot;] ?????????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;] ?????????volumeMounts: ?????????-?name:?redis-storage ???????????mountPath:?/data/redis ?????volumes: ?????-?name:?redis-storage ???????emptyDir:?&#123;&#125; 继续来看几个重要的字段： metadata同 Pod 的 yaml，这里提一点：如果没有指明 namespace，那么就是用 kubectl 默认的 namespace（如果 kubectl 配置文件中没有指明 namespace，那么就是 default 空间）。 spec，可以看到 Deployment 的spec字段是在 Pod 的spec内容外“包了一层”，那就来看 Deployment 有哪些需要注意的： metadata，新手同学先不管这边的信息。 spec，会发现这完完全全是上文提到的 Pod 的spec内容，在这里写明了 Deployment 下属管理的每个 Pod 的具体内容。 replicas。副本个数。也就是该 Deployment 需要起多少个相同的 Pod，如果用户成功在 K8S 中配置了 n（n&gt;1）个，那么 Deployment 会确保在集群中始终有 n 个服务在运行。 template。 第二步：执行 kubectl 命令部署。Deployment 的部署办法同 Pod：kubectl create -f $&#123;DEPLOYMENT_YAML&#125;。由此可见，K8S 会根据配置文件中的kind字段来判断具体要创建的是什么资源。 这里插一句题外话：部署完 deployment 之后，可以查看到自动创建了 ReplicaSet 和 Pod，如下图所示： 还有一个有趣的事情：通过 Deployment 部署的服务，其下属的 RS 和 Pod 命名是有规则的。读者朋友们自己总结发现哦。 综上，如何部署一个 Pod 或者 Deployment 就结束了。 V. kubectl 查看、更新/编辑、删除服务作为 K8S 使用者而言，更关心的问题应该是本章所要讨论的话题：如何通过 kubectl 查看、更新/编辑、删除在 K8S 上部署着的服务。 3.1 如何查看服务？请务必记得一个事情：在 K8S 中，一个独立的服务即对应一个 Pod。即，当我们说要 xxx 一个服务的就是，也就是操作一个 Pod。而与 Pod 服务相关的且需要用户关心的，有 Deployment。 通过 kubectl 查看服务的基本命令是： `$ kubectl?get|describe?${RESOURCE}?[-o?${FORMAT}]?-n=${NAMESPACE} ${RESOURCE}有:?pod、deployment、replicaset(rs)` 在此之前，还有一个需要回忆的事情是：Deployment、ReplicaSet 和 Pod 之间的关系 - 层层隶属；以及这些资源和 namespace 的关系是 - 隶属。如下图所示。 因此，要查看一个服务，也就是一个 Pod，必须首先指定 namespace！那么，如何查看集群中所有的 namespace 呢？kubectl get ns： 于是，只需要通过-n=$&#123;NAMESPACE&#125;就可以指定自己要操作的资源所在的 namespace。比如查看 Pod：kubectl get pod -n=oona-test，同理，查看 Deployment：kubectl get deployment -n=oona-test。 问题又来了：如果已经忘记自己所部属的服务所在的 namespace 怎么办？这么多 namespace，一个一个查看过来吗？ kubectl get pod --all-namespaces 这样子就可以看到所有 namespace 下面部署的 Pod 了！同理，要查找所有的命名空间下的 Deployment 的命令是：kubectl get deployment --all-namespaces。 于是，就可以开心地查看 Pod：kubectl get pod [-o wide] -n=oona-test，或者查看 Deployment：kubectl get deployment [-o wide] -n=oona-test。 哎，这里是否加-o wide有什么区别吗？实际操作下就明白了，其他资源亦然： 哎，我们看到之前部署的 Pod 服务 memory-demo 显示的“ImagePullBackOff”是怎么回事呢？先不着急，我们慢慢看下去。 3.2 如何更新/编辑服务？两种办法：1). 修改 yaml 文件后通过 kubectl 更新；2). 通过 kubectl 直接编辑 K8S 上的服务。 方法一：修改 yaml 文件后通过 kubectl 更新。我们看到，创建一个 Pod 或者 Deployment 的命令是kubectl create -f $&#123;YAML&#125;。但是，如果 K8S 集群当前的 namespace 下已经有该服务的话，会提示资源已经存在： 通过 kubectl 更新的命令是kubectl apply -f $&#123;YAML&#125;，我们再来试一试： （备注：命令kubectl apply -f $&#123;YAML&#125;也可以用于首次创建一个服务哦） 方法二：通过 kubectl 直接编辑 K8S 上的服务。命令为kubectl edit $&#123;RESOURCE&#125; $&#123;NAME&#125;，比如修改刚刚的 Pod 的命令为kubectl edit pod memory-demo，然后直接编辑自己要修改的内容即可。 但是请注意，无论方法一还是方法二，能修改的内容还是有限的，从笔者实战下来的结论是：只能修改/更新镜像的地址和个别几个字段。如果修改其他字段，会报错： The Pod “memory-demo” is invalid: spec: Forbidden: pod updates may not change fields other than spec.containers[*].image, spec.initContainers[*].image, spec.activeDeadlineSeconds or spec.tolerations (only additions to existing tolerations) 如果真的要修改其他字段怎么办呢？恐怕只能删除服务后重新部署了。 3.3 如何删除服务？在 K8S 上删除服务的操作非常简单，命令为kubectl delete $&#123;RESOURCE&#125; $&#123;NAME&#125;。比如删除一个 Pod 是：kubectl delete pod memory-demo，再比如删除一个 Deployment 的命令是：kubectl delete deployment $&#123;DEPLOYMENT_NAME&#125;。但是，请注意： 如果只部署了一个 Pod，那么直接删除该 Pod 即可； 如果是通过 Deployment 部署的服务，那么仅仅删除 Pod 是不行的，正确的删除方式应该是：先删除 Deployment，再删除 Pod。 关于第二点应该不难想象：仅仅删除了 Pod 但是 Deployment 还在的话，Deployment 定时会检查其下属的所有 Pod，如果发现失败了则会再拉起。因此，会发现过一会儿，新的 Pod 又被拉起来了。 另外，还有一个事情：有时候会发现一个 Pod 总也删除不了，这个时候很有可能要实施强制删除措施，命令为kubectl delete pod --force --grace-period=0 $&#123;POD_NAME&#125;。 VI. kubectl 排查服务问题上文说道：部署的服务 memory-demo 失败了，是怎么回事呢？本章就会带大家一起来看看常见的 K8S 中服务部署失败、服务起来了但是不正常运行都怎么排查呢？ 首先，祭出笔者最爱的一张 K8S 排查手册，来自博客《Kubernetes Deployment 故障排除图解指南》： 哈哈哈，对于新手同学来说，上图还是不够友好，下面我们简单来看两个例子： 4.1 K8S 上部署服务失败了怎么排查？请一定记住这个命令：kubectl describe $&#123;RESOURCE&#125; $&#123;NAME&#125;。比如刚刚的 Pod 服务 memory-demo，我们来看： 拉到最后看到Events部分，会显示出 K8S 在部署这个服务过程的关键日志。这里我们可以看到是拉取镜像失败了，好吧，大家可以换一个可用的镜像再试试。 一般来说，通过kubectl describe pod $&#123;POD_NAME&#125;已经能定位绝大部分部署失败的问题了，当然，具体问题还是得具体分析。大家如果遇到具体的报错，欢迎分享交流。 4.2 K8S 上部署的服务不正常怎么排查？如果服务部署成功了，且状态为running，那么就需要进入 Pod 内部的容器去查看自己的服务日志了： 查看 Pod 内部某个 container 打印的日志：kubectl log $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125;。 进入 Pod 内部某个 container：kubectl exec -it [options] $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125; [args]，嗯，这个命令的作用是通过 kubectl 执行了docker exec xxx进入到容器实例内部。之后，就是用户检查自己服务的日志来定位问题。 显然，线上可能会遇到更复杂的问题，需要借助更多更强大的命令和工具。 写在后面本文希望能够帮助对 K8S 不了解的新手快速了解 K8S。笔者一边写文章，一边查阅和整理 K8S 资料，过程中越发感觉 K8S 架构的完备、设计的精妙，是值得深入研究的，K8S 大受欢迎是有道理的。 来自 “ ITPUB博客 ” ，链接：http://blog.itpub.net/31559354/viewspace-2746071/","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"}]},{"title":"Flannel-Calico怎么选择","slug":"Flannel-Calico","date":"2021-03-23T16:00:00.000Z","updated":"2021-05-26T03:01:53.590Z","comments":true,"path":"2021/03/24/Flannel-Calico/","link":"","permalink":"http://zhangyu.info/2021/03/24/Flannel-Calico/","excerpt":"","text":"网络插件怎么选择 Kubernetes中常见的网络插件有哪些？1.flannel：能提供ip地址，但不支持网络策略 2.calico：既提供ip地址，又支持网络策略 3.canal：flannel和calico结合，通过flannel提供ip地址，calico提供网络策略 4.Cilium 着重强调网络安全，实现Kubernetes中网络的可观察性以及基本的网络隔离、故障排查等安全策略； 突破传统主机防火墙仅支持L3、L4微隔离的限制，支持基于API的网络安全过滤能力。 什么叫做网络策略？网络策略：可以达到多租户网络隔离，可以控制入网和出网流量，入网和出网ip访问的一种策略 各种CNI网络方案的差异对比参考https://helpcdn.aliyun.com/document_detail/97621.html flannel和calico网络性能分析官方指标如下 flannel host-gw = flannel vxlan-directrouting = calico bgp&gt; calico ipip &gt; flannel vxlan-vxlan&gt;flannel-udp 官方推荐使用的网络方案：所有节点在同一个网段推荐使用calico的bgp模式和flannel的host-gw模式 结论：1.如果需要多集群的跨网络分段的网络，选择Calico 2.如果需要管理网络策略，做网络隔离等，选择Calico 3.大部分公司生产环境业务不复杂的，开发测试环境就几台机器的，不存在多数据中心的。 选择用Flannel 就行了。 部署在公有云上，封装 backend 选择vxlan-directrouting。 部署在私有云上，封装 backend 选择host-gw。 参考Kubernetes集群网络规划 https://helpcdn.aliyun.com/document_detail/86500.html 使用网络策略（Network Policy） https://helpcdn.aliyun.com/document_detail/97621.html","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"}],"tags":[{"name":"flannel","slug":"flannel","permalink":"http://zhangyu.info/tags/flannel/"}]},{"title":"nginx核心知识100讲知识图谱","slug":"nginx-knowledge-graph","date":"2021-03-14T16:00:00.000Z","updated":"2021-03-15T15:07:57.974Z","comments":true,"path":"2021/03/15/nginx-knowledge-graph/","link":"","permalink":"http://zhangyu.info/2021/03/15/nginx-knowledge-graph/","excerpt":"","text":"nginx核心知识100讲知识图谱","categories":[{"name":"nginx","slug":"nginx","permalink":"http://zhangyu.info/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://zhangyu.info/tags/nginx/"}]},{"title":"性能之癫-优化你的程序","slug":"performance-optimize-your-program","date":"2021-03-14T16:00:00.000Z","updated":"2021-03-15T15:33:48.965Z","comments":true,"path":"2021/03/15/performance-optimize-your-program/","link":"","permalink":"http://zhangyu.info/2021/03/15/performance-optimize-your-program/","excerpt":"","text":"原创 码砖杂役性能之巅-优化你的程序​ outline：关注&amp;指标&amp;度量，基础理论知识，工具&amp;方法，最佳实践，参考资料性能优化关注：CPU、内存、磁盘IO、网络IO等四个方面。性能指标：吞吐率、响应时间、QPS/IOPS、TP99、资源使用率是我们经常关注的指标。时间度量：从cpu cycle到网络IO，自上到下，时间量级越大。监控、分析、优化，三部曲，以终为始，循环往复。优化性能，需要一些系统编程知识。提升处理能力、减少计算量是优化的2个根本方向。优化大师格雷格画的图，吊炸天，你应该很熟悉，gregg亲手实现了一些工具。借助工具定位性能瓶颈。gprof2dot.py可以处理多种采样输出数据 建议使用perf等非侵入式的profiling工具。perf不仅仅可以定位cpu瓶颈，还可以查看很多方面，比如缺页，分支预测失败，上下文切换等。IO瓶颈，你应该知道的知识。有关锁的知识，你应该知道的。多线程的学问很大内存管理的方方面面最佳实践，没有足够理由，你不应该违背。你应该懂得的。 关于排序，你应该知道的。这些资料不错，你值得拥有。 如果对你有帮助，请帮忙转发，让更多朋友收益。 一般性原则依据数据而不是凭空猜测忌过早优化忌过度优化深入理解业务性能优化是持久战选择合适的衡量指标、测试用例、测试环境性能优化的层次需求阶段设计阶段实现阶段一般性方法缓存并发惰性批量，合并更高效的实现缩小解空间性能优化与代码质量总结依据数据而不是凭空猜测 这是性能优化的第一原则，当我们怀疑性能有问题的时候，应该通过测试、日志、profillig来分析出哪里有问题，有的放矢，而不是凭感觉、撞运气。一个系统有了性能问题，瓶颈有可能是CPU，有可能是内存，有可能是IO（磁盘IO，网络IO），大方向的定位可以使用top以及stat系列来定位（vmstat，iostat，netstat…），针对单个进程，可以使用pidstat来分析。 在本文中，主要讨论的是CPU相关的性能问题。按照80/20定律，绝大多数的时间都耗费在少量的代码片段里面，找出这些代码唯一可靠的办法就是profile，我所知的编程语言，都有相关的profile工具，熟练使用这些profile工具是性能优化的第一步。 忌过早优化 The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.我并不十分清楚Donald Knuth说出这句名言的上下文环境，但我自己是十分认同这个观念的。在我的工作环境（以及典型的互联网应用开发）与编程模式下，追求的是快速的迭代与试错，过早的优化往往是无用功。而且，过早的优化很容易拍脑袋，优化的点往往不是真正的性能瓶颈。 忌过度优化 As performance is part of the specification of a program – a program that is unusably slow is not fit for purpose性能优化的目标是追求合适的性价比。 在不同的阶段，我们对系统的性能会有一定的要求，比如吞吐量要达到多少多少。如果达不到这个指标，就需要去优化。如果能满足预期，那么就无需花费时间精力去优化，比如只有几十个人使用的内部系统，就不用按照十万在线的目标去优化。 而且，后面也会提到，一些优化方法是“有损”的，可能会对代码的可读性、可维护性有副作用。这个时候，就更不能过度优化。 深入理解业务 代码是服务于业务的，也许是服务于最终用户，也许是服务于其他程序员。不了解业务，很难理解系统的流程，很难找出系统设计的不足之处。后面还会提及对业务理解的重要性。 性能优化是持久战 当核心业务方向明确之后，就应该开始关注性能问题，当项目上线之后，更应该持续的进行性能检测与优化。 现在的互联网产品，不再是一锤子买卖，在上线之后还需要持续的开发，用户的涌入也会带来性能问题。因此需要自动化的检测性能问题，保持稳定的测试环境，持续的发现并解决性能问题，而不是被动地等到用户的投诉。 选择合适的衡量指标、测试用例、测试环境 正因为性能优化是一个长期的行为，所以需要固定衡量指标、测试用例、测试环境，这样才能客观反映性能的实际情况，也能展现出优化的效果。 衡量性能有很多指标，比如系统响应时间、系统吞吐量、系统并发量。不同的系统核心指标是不一样的，首先要明确本系统的核心性能诉求，固定测试用例；其次也要兼顾其他指标，不能顾此失彼。 测试环境也很重要，有一次突然发现我们的QPS高了许多，但是程序压根儿没优化，查了半天，才发现是换了一个更牛逼的物理机做测试服务器。","categories":[{"name":"优化","slug":"优化","permalink":"http://zhangyu.info/categories/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"性能","slug":"性能","permalink":"http://zhangyu.info/tags/%E6%80%A7%E8%83%BD/"}]}],"categories":[{"name":"微服务","slug":"微服务","permalink":"http://zhangyu.info/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"nacos","slug":"nacos","permalink":"http://zhangyu.info/categories/nacos/"},{"name":"监控","slug":"监控","permalink":"http://zhangyu.info/categories/%E7%9B%91%E6%8E%A7/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://zhangyu.info/categories/SpringBoot/"},{"name":"API网关","slug":"API网关","permalink":"http://zhangyu.info/categories/API%E7%BD%91%E5%85%B3/"},{"name":"PaaS","slug":"PaaS","permalink":"http://zhangyu.info/categories/PaaS/"},{"name":"k8s","slug":"k8s","permalink":"http://zhangyu.info/categories/k8s/"},{"name":"日志","slug":"日志","permalink":"http://zhangyu.info/categories/%E6%97%A5%E5%BF%97/"},{"name":"公有云","slug":"公有云","permalink":"http://zhangyu.info/categories/%E5%85%AC%E6%9C%89%E4%BA%91/"},{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/categories/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"},{"name":"linux","slug":"linux","permalink":"http://zhangyu.info/categories/linux/"},{"name":"架构","slug":"架构","permalink":"http://zhangyu.info/categories/%E6%9E%B6%E6%9E%84/"},{"name":"优化","slug":"优化","permalink":"http://zhangyu.info/categories/%E4%BC%98%E5%8C%96/"},{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/categories/Scrum/"},{"name":"SRE","slug":"SRE","permalink":"http://zhangyu.info/categories/SRE/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/categories/Kubernetes/"},{"name":"消息系统","slug":"消息系统","permalink":"http://zhangyu.info/categories/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F/"},{"name":"职业发展","slug":"职业发展","permalink":"http://zhangyu.info/categories/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"},{"name":"containerd","slug":"containerd","permalink":"http://zhangyu.info/categories/containerd/"},{"name":"Ingress","slug":"Ingress","permalink":"http://zhangyu.info/categories/Ingress/"},{"name":"nginx","slug":"nginx","permalink":"http://zhangyu.info/categories/nginx/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"http://zhangyu.info/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"nacos","slug":"nacos","permalink":"http://zhangyu.info/tags/nacos/"},{"name":"监控","slug":"监控","permalink":"http://zhangyu.info/tags/%E7%9B%91%E6%8E%A7/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://zhangyu.info/tags/SpringBoot/"},{"name":"API网关","slug":"API网关","permalink":"http://zhangyu.info/tags/API%E7%BD%91%E5%85%B3/"},{"name":"PaaS","slug":"PaaS","permalink":"http://zhangyu.info/tags/PaaS/"},{"name":"k8s","slug":"k8s","permalink":"http://zhangyu.info/tags/k8s/"},{"name":"日志","slug":"日志","permalink":"http://zhangyu.info/tags/%E6%97%A5%E5%BF%97/"},{"name":"公有云","slug":"公有云","permalink":"http://zhangyu.info/tags/%E5%85%AC%E6%9C%89%E4%BA%91/"},{"name":"我假装讲-你假装看","slug":"我假装讲-你假装看","permalink":"http://zhangyu.info/tags/%E6%88%91%E5%81%87%E8%A3%85%E8%AE%B2-%E4%BD%A0%E5%81%87%E8%A3%85%E7%9C%8B/"},{"name":"linux","slug":"linux","permalink":"http://zhangyu.info/tags/linux/"},{"name":"架构","slug":"架构","permalink":"http://zhangyu.info/tags/%E6%9E%B6%E6%9E%84/"},{"name":"优化","slug":"优化","permalink":"http://zhangyu.info/tags/%E4%BC%98%E5%8C%96/"},{"name":"Scrum","slug":"Scrum","permalink":"http://zhangyu.info/tags/Scrum/"},{"name":"SRE","slug":"SRE","permalink":"http://zhangyu.info/tags/SRE/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://zhangyu.info/tags/Kubernetes/"},{"name":"Cilium","slug":"Cilium","permalink":"http://zhangyu.info/tags/Cilium/"},{"name":"Pulsar","slug":"Pulsar","permalink":"http://zhangyu.info/tags/Pulsar/"},{"name":"职业发展","slug":"职业发展","permalink":"http://zhangyu.info/tags/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"},{"name":"containerd","slug":"containerd","permalink":"http://zhangyu.info/tags/containerd/"},{"name":"Ingress","slug":"Ingress","permalink":"http://zhangyu.info/tags/Ingress/"},{"name":"flannel","slug":"flannel","permalink":"http://zhangyu.info/tags/flannel/"},{"name":"nginx","slug":"nginx","permalink":"http://zhangyu.info/tags/nginx/"},{"name":"性能","slug":"性能","permalink":"http://zhangyu.info/tags/%E6%80%A7%E8%83%BD/"}]}