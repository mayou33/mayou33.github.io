<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张天师</title>
  
  
  <link href="http://zhangyu.info/atom.xml" rel="self"/>
  
  <link href="http://zhangyu.info/"/>
  <updated>2022-03-08T17:02:32.013Z</updated>
  <id>http://zhangyu.info/</id>
  
  <author>
    <name>张天师</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>云服务器ECS选购指南及省钱法宝</title>
    <link href="http://zhangyu.info/2022/03/08/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8ECS%E9%80%89%E8%B4%AD%E6%8C%87%E5%8D%97%E5%8F%8A%E7%9C%81%E9%92%B1%E6%B3%95%E5%AE%9D/"/>
    <id>http://zhangyu.info/2022/03/08/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8ECS%E9%80%89%E8%B4%AD%E6%8C%87%E5%8D%97%E5%8F%8A%E7%9C%81%E9%92%B1%E6%B3%95%E5%AE%9D/</id>
    <published>2022-03-07T16:00:00.000Z</published>
    <updated>2022-03-08T17:02:32.013Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p> 云服务器ECS选购指南及省钱法宝（强烈建议收藏） </p><p><strong>作者 | <strong>阿**</strong></strong>里云弹性计算产品专家 马小婷**</p><p><a href="https://developer.aliyun.com/article/872102">https://developer.aliyun.com/article/872102</a></p><p>今天给大家带来的分享是如何购买云服务器ECS以及怎么买更省钱，分为四个部分：</p><p>第一部分介绍云服务器ECS的基本概念，告诉大家购买ECS实例时看哪些参数。就像小书生要买一部手机，会关注内存大小、CPU频率、屏幕分辨率、相机参数等，选购ECS实例同样可以通过参数选择来满足自己的上云需求。</p><p>第二部分介绍接云服务器ECS实例规格族，详细介绍阿里云主要的ECS产品系列。（时间紧张的情况下可以跳到第三部分）</p><p>第三部分详细讲解<strong>ECS的选型技巧</strong>，具体讲解不同场景（如大数据/数据库等）下如何选择ECS实例，或者某个ECS实例适用于怎样的生产、工作场景，重点干货部分，不容错过。</p><p>第四部分介绍如何<strong>省钱省力的来使用ECS</strong>，在满足自己需要的前提下，让你上云省钱更经济。</p><h3 id="01-云服务器ECS基础概念"><a href="#01-云服务器ECS基础概念" class="headerlink" title="01 云服务器ECS基础概念"></a><em>01</em> 云服务器ECS基础概念</h3><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/9fd8dc4675f94f29a650697d6d254890.jpg" alt="图1.jpg" title="图1.jpg"> </p><h4 id="云服务器的基础概念"><a href="#云服务器的基础概念" class="headerlink" title="云服务器的基础概念"></a>云服务器的基础概念</h4><p>第一部分会给大家介绍云服务器的一些基本概念。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/5f56e706e8b94112b7f30088ac385971.jpg" alt="图2.jpg" title="图2.jpg"> </p><p>在开始前，大家可以回想一下，我们自己购买笔记本电脑的时候会考虑哪些因素？我自己会先选择品牌，一般情况下在确定了品牌之后，接下来就会考虑硬件配置，主要是物理硬件的配置和软件的配置。</p><p>硬件配置上，我首先会考虑计算性能，像CPU和内存的大小、CPU的型号等；第二就是存储，笔记本电脑的磁盘有多大；第三部分就是网络能力，比如网卡有几个，对于玩游戏的同学来说，显卡配置也很重要。除了硬件配置外，我也会考虑电脑的操作系统是什么样的，比如Mac OS， windows或ubuntu等。而拿到电脑之后，我们首先会做一些基础应用软件的安装和配置，包括防火墙等保证我们整个应用环境的安全性。</p><p>这是我在现实生活中去购买一台物理电脑的流程，其实这些概念在云上也是适用的，比如说我们在选择一些物理硬件的参数的时候，选CPU和内存，对应在云上的话，就是选择ECS实例的 CPU 和内存大小以及 CPU 的型号。</p><p>存储这一块，磁盘在云上对应的概念就是块存储，在云上块存储其实是包含两个概念，一个概念是云盘，一个概念是本地盘。有一个跟我们现实生活中不太一样的点，是云上的块存储，我们在购买的过程中需要指定用作系统盘还是用作数据盘的。而现实生活中买了一个电脑里面是有一块磁盘，然后我们自己会把磁盘分成系统盘还是数据盘，但在云上的系统盘和数据盘是需要分开购买的，这是一点点区别。</p><p>在网络这一块其实也是类似的，云上提供弹性网卡，让用户通过访问云服务器就能够联通到网上。</p><p>除了这些物理硬件以外，要让一个云服务器真正的跑起来，跟现实生活一样，我们也需要去安装一个操作系统，这个操作系统在云上的概念就是镜像，阿里云提供多种不同的镜像版本供大家选择。</p><p>除此以外，云服务器还会有一些特殊的概念，比如安全组，本质上是通过一些规则来限定访问的流量，即被哪些应用可以访问。</p><p>我们在买一个电脑之后，这个物理机在手上，你想要什么时候使用就可以什么时候使用。在云上买完一个云服务器之后，因为这个服务器是在云端或者说在远端，我们访问云服务器的方式就跟我们平时打开一个电脑不太一样，我们需要通过阿里云的控制台或者通过远程连接的工具来登录到我们的云服务器上去。</p><p>还有一个小概念是云上的容灾备份能力，就是快照。现实生活中，如果我们的电脑磁盘出现了故障，数据出现了损坏就无能为力了，或者只能够找专业的人把数据能够找回来，但是不能够保证说所有的数据都能找回来。云上有快照这样一个概念，它的意思是说对云盘的某一个时间点的数据拍一张照，本质上就是会把磁盘上所有的数据记录下来，如果出现了问题，我们就可以通过快照，快速的回滚到某一个时间点的数据，这样能够保证在业务出现了问题的情况下，快速做灾备的恢复。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/e5984153d6e04affbd55d256bd042105.jpg" alt="图3.jpg" title="图3.jpg"> </p><p>整体介绍完云服务器的基本概念之后，接下详细介绍一下云服务器的存储和网络的概念。</p><h4 id="云上的三种存储方式"><a href="#云上的三种存储方式" class="headerlink" title="云上的三种存储方式"></a>云上的三种存储方式</h4><p>第一种是前面已经介绍的块存储的模式，用户创建了一个块存储之后，可以把块存储挂载到实例上，就跟自己使用笔记本电脑过程中，电脑自带的磁盘不够用了，去买移动硬盘来插上来类似。块存储有三种类型，包括普通的高效云盘，还有SSD云盘，以及超高性能超低延迟的ESSD云盘。</p><p>第二种存储方式是文件存储，每一个块存储只能够挂载到一个云服务器上，而每个文件存储可以被多台ECS使用。</p><p>第三种存储形态是对象存储形态OSS，这个就类似于百度云盘，使用这种存储的方式，更多的通过一个链接来做文件的读取。</p><h4 id="云上的网络"><a href="#云上的网络" class="headerlink" title="云上的网络"></a>云上的网络</h4><p>网络部分主要是两个概念，专有网络VPC和交换机。</p><p>第一个是专有网络VPC，专有网络是在云上为用户划分一个私有网络，用户通过创建VPC可以创建逻辑上彻底隔离的一个网络环境，每一个VPC都是由一个路由器以及一个以上的交换机组成的。用户一旦创建了一个VPC专有网络，阿里云会自动为用户创建一个对应的路由器，来完成VPC下所有网络的转发。同一个VPC下的实例之间的内网是互通的，即在同一个VPC下实例之间可以通过内网IP地址来互相访问。</p><p>第二个概念是交换机，前面已经介绍了，一个VPC至少有一个路由器。交换机是专有网络的基础网络设备，用来连接不同的实例资源，我们可以通过交换机，在每一个可用区创建多个交换机来划分子网，然后多个交换机之间是可以通过路由器来实现连接和转发。以上是存储和网络的一些基础的概念。</p><h4 id="云服务器ECS的使用流程"><a href="#云服务器ECS的使用流程" class="headerlink" title="云服务器ECS的使用流程"></a>云服务器ECS的使用流程</h4><p>下面我们介绍一下使用ECS的流程。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/4c54b22f8440429fb07e07add75bb064.jpg" alt="图4.jpg" title="图4.jpg"> </p><p>一个ECS的实例，我们可以把它理解成一台虚拟机，它包含内存、磁盘、网络和操作系统等软硬件。而一个ECS服务器实例是多大的规格，底层的物理硬件是什么样子的，是由对应的实例规格和实例规格族来决定的。实例规格族代表了实例适用的业务场景，它决定了CPU和内存配比，以及底层的物理硬件是什么样子的。实例规格代表的是实例的大小，比如说 CPU的数量是多少。</p><p>在确定了实例规格之后，我们还需要去选择对应的存储，因为只有CPU和内存的话，数据是没有办法存放的，所以就会有一个块存储。块存储有两种，一种是云盘，一种是本地盘。云盘其实是云上的一种三副本的存储形态，能够给用户提供高可用的能力。云盘主要用来做系统盘和数据盘，只需要像物理盘一样把它格式化就可以使用了，而本地盘可能更多的主要是用来做数据盘。</p><p>选择完了计算存储，我们接下来就要看对应的操作系统，云上的操作系统指的是镜像，目前阿里云提供多种镜像的来源，包括官方提供的这种公共镜像、第三方市场提供的镜像、用户自定义镜像，还允许不同的用户之间共享镜像。</p><p>网络方面阿里云会有一个网络带宽，用户可以直接指定。</p><p>我们把实例的计算、存储、网络以及操作系统等参数制定好之后，就可以创建一个跟我们物理的笔记本电脑一样的云服务器。</p><p>创建完之后，我们通过阿里云的控制台，或者是通过阿里云的APP，可以直接连接和访问已购买的云服务器。</p><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="02-ECS实例规格族介绍"><a href="#02-ECS实例规格族介绍" class="headerlink" title="02 ECS实例规格族介绍"></a><em>02</em> ECS实例规格族介绍</h3><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/4a6fdd8f210c444e890f7f8534a7de82.jpg" alt="图5.jpg" title="图5.jpg"> </p><p>第二部分我会给大家介绍一下ECS实例的规格族是怎么命名的，大家可能在这一块会有比较多的疑问。目前阿里云提供几百种实例规格，所以在选择的过程中会眼花缭乱，其实只要理解了ECS的实例规格族的命名方式，和它的信息布局，我们就能够很好的选型了。</p><h4 id="实例的架构类型、规格分类与详细信息"><a href="#实例的架构类型、规格分类与详细信息" class="headerlink" title="实例的架构类型、规格分类与详细信息"></a>实例的架构类型、规格分类与详细信息</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/18e3c00f905e48658fa22093e110d4f7.jpg" alt="图6.jpg" title="图6.jpg"> </p><p>在阿里云控制台的购买页面上可以看到，实例规格族的选择上分成三大模块：架构、分类、具体信息。最上面就是我们的实例规格架构的类型，有三种架构类型，分别是通用的X86的架构、异构计算（像GPU或者是FPGA、NPU等）、阿里云自研的神龙裸金属架构。</p><p>在每种架构下面会有实例规格的分类，从上图可以看到在X86的这种计算型态下，分成了7大类实例规格，不同实例规格代表了不同的硬件配置，选择任何一个实例规格的分类之后，我们可以看到对应实例规格的详细信息，这些信息主要分为四部分：</p><blockquote><p><strong>第一个就是实例规格族的详细信息</strong>，包括对应的规格族和实例规格的代称，这里可以通过点击小问号，能够看到实例规格族的一些详细的描述。<br><strong>第二部分是 CPU和内存大小的信息</strong>，这里是大家在选型的过程中会比较关注的。<br><strong>第三部分是实例的网络能力信息</strong>，包括实例内网的带宽和收发包的能力。<br><strong>第四部分是CPU的处理型号的信息</strong>，包括处理器的主频和睿频这两部分信息。</p></blockquote><h4 id="企业级实例-VS-入门级实例"><a href="#企业级实例-VS-入门级实例" class="headerlink" title="企业级实例 VS 入门级实例"></a>企业级实例 VS 入门级实例</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/f83e0c35435a4c338ee6ebde4af94042.jpg" alt="图7.jpg" title="图7.jpg"> </p><p>在控制台的购买页面上可以看到，ECS的实例规格族特别多，单纯从CPU和内存是无法判断它们的区别，所以我们需要从宏观上来看。阿里云ECS的实例规格整体是分成两大类，一类是企业级实例，一类是入门级实例。</p><p>企业级实例是阿里云在2016年9月份才推出的，其特点是vCPU是独享的，也就意味着我们创建一个企业级实例的时候，实例vCPU与我们底层物理的 CPU是绑定了的，底层的物理CPU就不可能再分配给其他的实例了，所以企业级的实例不会出现资源的争抢，因此能保证性能稳定，并且企业级实例提供了非常严格的SLA性能保证。</p><p>而入门级实例就是vCPU跟底层的物理的CPU是不绑定的，意味着可能每个vCPU是随机分配到底层的空闲的一个物理CPU上，如果同一个物理的物理服务器上有多个共享入门级实例的话，不同的实例就会出现资源的争抢，导致CPU的性能不稳定。</p><p>因为入门级实例存在性能不稳定的特性，所以阿里云现在仅仅提供一种入门级实例，就是在X86架构中的共享型实例， 而X86架构中的其他实例规格，以及异构架构和神龙架构中的所有实例，都是属于企业级实例。</p><p>由于企业级实例性能稳定，并且有严格的SLA的保证，所以它比较适合于对业务稳定性有比较高的要求的场景。入门级实例由于不能够保证性能稳定性，所以价格相对便宜，比较适合于一些对性能没有严格要求，或者在某些时段下才会有性能突发要求的场景，比如有些轻负载的应用或者是微服务。</p><h4 id="共享型实例"><a href="#共享型实例" class="headerlink" title="共享型实例"></a>共享型实例</h4><p>在介绍完ECS实例大的分类之后，我们来看一下共享型实例的具体信息。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/07c571fcad1d4dc5b1e127309342a70b.jpg" alt="图8.jpg" title="图8.jpg"> </p><p>我们前面讲到了只有X86架构下的共享型实例才是入门级实例。这类实例比前面实例在四要素以外多出一个参数，即“平均基准的CPU计算性能”，基准性能即实例能够持续提供的CPU性能。</p><p>共享型实例也就是入门级实例，分成两大类，第一类是属于标准的共享型实例， CPU是不绑定的，只提供基准CPU性能，所以当出现资源的争抢，是否能超出基准性能是没有保障。</p><p>另外一种特殊的共享型实例，名为突发性能型的共享实例，它主要就是照顾到某些应用在绝大多数的时候CPU的使用率可能都不高，负载都不高，但是在某些时候可能会有临时的突发的高性能要求，所以阿里云会提供突发性能的参数，所以您在购买共享型实例的时候，能够通过突发性性能来获得高于平均基准CPU性能的能力。</p><p>突发性能型的共享实例，如果应用实际用量低于了平均的基准性能，会获得对应的CPU的积分，如果在某些场景下性能要求突然提升之后，比如实例对应的 CPU的使用率超过了20%，会消耗之前累积的CPU的积分，去提升计算性能，让计算性能不会受到影响，这个是突发性能的共享型实例独有的特性。</p><h4 id="两个特殊的实例规格"><a href="#两个特殊的实例规格" class="headerlink" title="两个特殊的实例规格"></a>两个特殊的实例规格</h4><p>除了共享型的入门级实例以外，阿里云还有两个实例规格比较特殊，就是大数据型和本地SSD。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/e9944b2c874e4857b7eb08e12c425253.jpg" alt="图9.jpg" title="图9.jpg"> </p><p>这两种实例规格会附带一个本地存储，大数据型实例的本地存储是HDD盘，本地SSD新增的本地存储是具有非常高I/O吞吐，并且有低延迟的本地SSD盘，具体的信息大家可以在阿里云控制台查看。</p><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><p>企业级实例规格家谱</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/a73f20534b9f400b8fc66abf5f49db00.jpg" alt="图10.jpg" title="图10.jpg"> </p><p>下面介绍企业级实例规格的家谱，方便我们快速了解各个实例家族的“亲属”关系。企业级实例规格族分成三大块，第一大块是X86计算，除了共享型以外，包括通用、计算、内存、高主频、本地SSD和大数据型都属于我们的企业级实例，企业级实例每年都在不停地迭代，所以会分成不同的代系，我们在后面会详细介绍不同的代际之间的区别。异构计算里面所有的GPU和FPGA都是属于企业级的实例，裸金属和高性能计算也是一样的。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/d98d3889ae604987baac119f983ced17.jpg" alt="图11.jpg" title="图11.jpg"> </p><p>首先，我们来介绍X86的实例规格的命名方式，分成了5种：</p><p><strong>第一种实例规格是通用型，</strong>顾名思义就是什么场景都能够用，所以这种型号的代称是g系列，它的vCPU和内存的一个配比是1:4。</p><p><strong>第二种实例规格是计算型，</strong>顾名思义就是在某些场景下对CPU算力的要求会更高一点，所以它的vCPU和内存的配比是1:2，然后简称为c系列。</p><p><strong>第三种类型是内存型，</strong>提供更多的内存能力，所以它的CPU和内存的配比是1:8，也简称为r系列，r是RAM的简称</p><p><strong>第四种和第五种分别是大数据型和本地SSD型，</strong>这两种的CPU和内存的配比都是1:4，只是它们配的本地盘的类型是不一样的，导致它们的技能和适合的场景也是不一样的。所以大数据型的简称是d，本地SSD型简称是i。</p><p>在这5个基础的实例规格上面，我们会去做一些额外的能力提升，比如说在通用型、计算型和内存型这三种类型下，增加了一些高主频的能力，正常的 CPU的主频应该是2.5G赫兹，但是我们有一些可以是做到3.2G赫兹，这种加上高主频的能力就变成了高主频型，会在前面去加上一个hf这样的一个标识。</p><p>随着技术的演进，神龙架构的神龙卡也是在不断地迭代和改善，搭载了第三代的神龙卡可以整体提升通用型、计算型和内存型这三种实例规格的性能，所以就会出现一个平衡增强型。对于大数据型的话，做了计算和存储的分离，形成了大数据存储型，简称为d2，而 d2s是在大数据的基础上，做了一些网络能力的增强，就变成了一个网络增强型。</p><h4 id="实例规格的命名方式和规律"><a href="#实例规格的命名方式和规律" class="headerlink" title="实例规格的命名方式和规律"></a>实例规格的命名方式和规律</h4><p>大家通过下图能够看到阿里云实例规格的命名方式和规律。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/c31b3eacfb824b849b786f025514090b.jpg" alt="图12.jpg" title="图12.jpg"> </p><p>普通的X86实例规格名称是分成了三段，第一部分表示的是产品名称，ECS是阿里云的产品；第二部分表示了实例的规格和代系，前面已经讲过hfg表示是在通用型的基础上增加了高主频的能力，然后6代表的是什么？其实它代表的是我们产品的代系，可以根据产品的代系推算对应的产品的一个新旧，比如说6代表第6代，5代表的是第5代，这个数字越大代表它是更新的一个代系，它底层的物理硬件也会越新，它的性价比相对而言也会越高。</p><p>最后一部分是实例的规格，表示的是实例的vCPU的核数，large代表2个vCPU， xlarge代表4个 vCPU，2xlarge代表的是8个vCPU，以此类推。</p><p>了解了以上命名规律，就能通过实例规格族的名称推断出来当前这个实例的CPU是什么型号、它的是什么样的代系，以及它的 CPU的数量是多少。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/e215844161f8482ba2f9f2a1efa73852.jpg" alt="图13.jpg" title="图13.jpg"> </p><p>GPU命名规则也是类似的，只有一个不一样的点，GPU名称的的中间这一部分会提供CPU和GPU的的配比关系，因为 GPU是除了CPU以外还会提供一个额外的GPU的卡。所以我们也是直接可以通过它的规格族的格式，能够去推断出来它底层的物理的配置。</p><h3 id="03-ECS实例选型实战"><a href="#03-ECS实例选型实战" class="headerlink" title="03 ECS实例选型实战"></a><em>03</em> ECS实例选型实战</h3><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/6bd19f98419148c097f10384d76113c2.jpg" alt="图14.jpg" title="图14.jpg"> </p><p>第三部分给大家实战讲一下如何做云服务器ECS的选型。</p><h4 id="简述各种规格实例的适用场景"><a href="#简述各种规格实例的适用场景" class="headerlink" title="简述各种规格实例的适用场景"></a>简述各种规格实例的适用场景</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/45267e4f1ecf4d429293fc6c4b5b2905.jpg" alt="图15.jpg" title="图15.jpg"> </p><p>X86计算:</p><blockquote><p>• X86的通用型、计算型和存储型三种实例，CPU和内存的配比比较一致，所以比较适合做一些中小型的数据库，或者是一些数据处理的任务。<br>• c系列的话，主要是计算型，所以比较适合于做一些计算要求比较多的，比如说做一些外部应用，或者做一些批量计算，或者是一些高性能的科学计算类的。<br>• r系列的话，因为它的内存比较多，所以比较适合于做一些数据库或者数据分析的应用。<br>• 高主频实例规格也是比较适合于对CPU的主频有比较高要求的高性能科学计算。<br>• 本地SSD类型，更多的适合于做一些关系型数据库或者是NoSQL数据库的<br>• 而D系列的大数据型，可能更适合于做一些大数据集群的一个场景，比如说像这种Map Reduce这种。</p></blockquote><p>在异构这一块，分成了两大类:</p><blockquote><p>• GPU比较适合于做深度学习或者是图像视频的可视化的处理;<br>• FPGA就比较适合于做图像的转码，或者音视频的解码。</p></blockquote><p>裸金属和高性能计算:</p><blockquote><p>• 更垂直和性能要求更高的一些场景，像一些高性能的数据库或者高性能科学计算场景。</p></blockquote><p>下面我们举几个例子详细介绍一下选型方法。</p><h4 id="X86实例选型推荐"><a href="#X86实例选型推荐" class="headerlink" title="X86实例选型推荐"></a>X86实例选型推荐</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/289bafc772d5448f99ad7ba9c9d2fb9a.jpg" alt="图16.jpg" title="图16.jpg"> </p><p>我们可以把一个web应用分成以下几个层次，每个层次做对应的推荐：</p><blockquote><p>• 对于Apache和Nginx的web服务器，，因为它主要做一些计算处理，所以推荐是使用一些计算型的，比如说c5、c6这样的；<br>• 对于像 spring cloud或者说MQ这样的中间件的话，它是属于对于计算和存储的诉求都比较正常的，所以我们是推荐一些通用性的，比如说g6这样的实例规格；<br>• 而应用型因为是属于比较通用的场景，所以G6系列就能够满足；<br>• Redis和Memcache这种缓存应用，对内存的要求是比较高的，所以我们推荐使用内存型的，像r系列；<br>• 对于关系型数据库，我们是可以直接使用内存型，比如说r系列配上我们的SSD云盘；<br>• 对于NoSQL，我们推荐本地SSD型的，比如I系列；<br>• 对于大数据的话，类似于HDFS或spark的这种，我们也有专门的大数据型的，像d系列这种的来处理;<br>• 对于最底层的机器学习的，比如MXNet这种训练框架，会有对应的专门的GPU计算型。</p></blockquote><h4 id="GPU实例选型推荐"><a href="#GPU实例选型推荐" class="headerlink" title="GPU实例选型推荐"></a>GPU实例选型推荐</h4><p>GPU云服务器的场景主要分成两大类，第一大类是人工智能，或者叫机器学习，第二块是图形图像的处理。在机器学习里面也会分成两个场景，一个是训练，一个是推理。所以对于不同细分的垂直领域，我们给了一些规格的推荐，具体可见下图。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/f0033eee56684edc9270e85eba9a8446.jpg" alt="图17.jpg" title="图17.jpg"> </p><p>下面我们介绍两个相对而言比较复杂的选型场景。</p><h4 id="大数据场景实例选型实战"><a href="#大数据场景实例选型实战" class="headerlink" title="大数据场景实例选型实战"></a>大数据场景实例选型实战</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/9f77c7342d7e494f9f6be23cae355a05.jpg" alt="图18.jpg" title="图18.jpg"> </p><p>第一个复杂场景是大数据的场景，类似于Hadoop、Spark这种大数据集群搭建的时候，如果我们自己手动做搭建，会把过程分成三大块：第一大块就是集群的管理节点的实例规格选型，第二块是集群的计算节点的选型，第三块是集群的数据节点的选型。</p><blockquote><p>• 管理节点是比较通用的场景，所以直接选择g系列就能够很好地处理管理的任务；<br>• 计算节点更多的是属于比较偏正常的业务负载，所以可以把g系列作为主要的选择，搭配SSD云盘；<br>• 数据节点对存储的吞吐和网络的吞吐有比较高的要求，所以推荐使用 d系列，搭配对应的本地盘，能够完成这种数据的读取量；</p></blockquote><p>所以整体来说，在同样一个大数据的集群里面，不同的任务有不同的特征，所以会选择不同的实例规格。</p><h4 id="数据库场景实例选型实战"><a href="#数据库场景实例选型实战" class="headerlink" title="数据库场景实例选型实战"></a>数据库场景实例选型实战</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/d486f2ae916f49e08fd1f080aeb95515.jpg" alt="图19.jpg" title="图19.jpg"> </p><p>第二个复杂场景是关于数据库选型的：</p><blockquote><p>• 对于普通的业务，负载比较轻的数据库，有专门的通用型g系列，或者内存型r系列搭配高效云盘和SSD云盘就能处理，性价比会比较高。因为g系列和本地盘或者本地SSD比起来，价格还是很有优势的。高效云盘和SSD云盘的整体性能，其实也是能够满足日常数据库的场景的。<br>• 对于业务负载要求非常高的集群，推荐本地SSD的 i 系列搭载NVMe SSD的云盘，能够实现存储的高IOPS和低延时，能够满足重载数据库的性能要求。</p></blockquote><h4 id="X86-第6代vs第5代-实例价格对比"><a href="#X86-第6代vs第5代-实例价格对比" class="headerlink" title="X86 第6代vs第5代 实例价格对比"></a>X86 第6代vs第5代 实例价格对比</h4><p>除了性能以外，大家也会关注价格，这里有一个X86里面第6代和第5代的一个价格的对比。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/994835ade191495997fc4db11710ad76.jpg" alt="图20.jpg" title="图20.jpg"> </p><p>可以看到除了计算型的实例在某些区域下，第6代实例会比第5代10实例的价格会略高4%以外，通用通用型和内存型的包月价格，第6代普遍比第5代要便宜2%-12%，所以整体来说的话，第6代不仅仅是性能有20%的提升，而且绝大多数的产品会更便宜。</p><p>而按量付费的话，第6代的价格比第5代的价格会低37%-47%，这其实是一个非常大的让利的空间。所以在选择按量去购买ECS的时候，选择第6代会比第5代要便宜的要便宜的更多。</p><h4 id="选型实战总结"><a href="#选型实战总结" class="headerlink" title="选型实战总结"></a>选型实战总结</h4><p>总结选型方法，有三个法则，大家可以记在心里面，在选型的过程中运用。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/dfb454598d244a5c89a89a8fa7747538.jpg" alt="图21.jpg" title="图21.jpg"> </p><p>第一个法则是相同大小的企业级的实例比入门级的实例性能更稳定，但是入门级的实例性价比更高，因为企业级的实例它是独占了vCPU，不存在一个资源的争抢，有性能的保障，但是对于一些个人或者中小网站的应用，如果对性能的诉求并不是那么强的话，选择入门级的实例其实是一个更好的选择。</p><p>第二个法则是在相同的实例规格下，新一代的实例规格比老一代的实例规格性价比更高，这是因为新一代的实例规格，做了很多技术的演进和更新换代，能够给公有云用户释放更多的技术红利。</p><p>第三个法则是选型时不仅仅要选择合适的实例规格，而且还需要搭载合适的块存储，才能够让云上的应用达到预期的性能。云上会提供4种不同的块存储，包括高效云盘、SSD云盘、ESSD云盘和本地盘，不同的类型盘的IOPS和吞吐是不一样的，所以不仅仅要选合适的实例规格，还要选择合适的块存储，才能够形成合力，达到最佳的性能。</p><h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h3 id="04-ECS省钱省力之道"><a href="#04-ECS省钱省力之道" class="headerlink" title="04 ECS省钱省力之道"></a><em>04</em> ECS省钱省力之道</h3><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/83e745ca5d174a58a086d3f8c31124da.jpg" alt="图22.jpg" title="图22.jpg"> </p><p>在购买云服务器的时候，除了要做实例规格的选型，让选择的实例规格和业务的匹配度更高以外，我们还需要去考虑能不能更便宜，能不能够快速完成资源的交付，所以最后一部分给大家介绍一下ECS省钱省力的技巧。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/e35d8d97c17240db8de63fb455a1b4b2.jpg" alt="图23.jpg" title="图23.jpg"> </p><h4 id="省钱大法"><a href="#省钱大法" class="headerlink" title="省钱大法"></a>省钱大法</h4><p>第一个是省钱大法，省钱大法意思是选好了实例规格，还需要选择最适合的付费方式，才能够得到更好优化成本。阿里云目前提供7种付费方式，例如节省计划（Saving Plan）、包年包月、预留实例券、按量付费、抢占式实例等。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/cba5f34bc6224020bc85b70d290b21e1.jpg" alt="图24.jpg" title="图24.jpg"> </p><p>如何选择合适的付费方式呢？有一个攻略，就是我们需要根据业务的稳定性和峰谷的波动情况，来选择最适合的付费方式。像节省计划、包年包月、预留实例券就比较适合于稳定的业务负载；有状态并且是动态变化的业务负载的话，可以使用按量付费；而对于完全没有状态，并且具有很高的容灾能力的，可以使用抢占式的实例来交付，因为抢占式实例的价格是可以做到按量付费实例的10% 的。</p><h4 id="省力之道"><a href="#省力之道" class="headerlink" title="省力之道"></a>省力之道</h4><p>第二个是省力之道。在云上购买资源的时候，有时候会批量购买，阿里云会提供多种自动化的资源交付模式和工具，能够实现一次配置重复使用，从而提升整个云上部署的速度和效率。</p><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/a1e5f6b6ab7f4548831e1022d9902c55.jpg" alt="图25.jpg" title="图25.jpg"> </p><p>比如通过控制台做批量的交付；通过部署集可以完成底层具有容灾能力的算力集群的交付；通过弹性伸缩和弹性供应，能自动化地完成资源的交付；而通过资源编排，可以把多种不同的资源组合交付。</p><h4 id="上云选型四步走"><a href="#上云选型四步走" class="headerlink" title="上云选型四步走"></a>上云选型四步走</h4><p> <img src="https://ucc.alicdn.com/pic/developer-ecology/87ccb51047404e39b7d37a29455bc9cb.jpg" alt="图26.jpg" title="图26.jpg"> </p><p>总结一下，上云的过程中，我们需要走好四步：</p><blockquote><p>第一步：对自己的业务特征做一些分析，包括对性能的要求，对网络的要求，形成一个基本的判断；<br>第二步：针对业务特征来选择对应的ECS实例规格；<br>第三步：选择对应的一个付费方式，只有选择最合适的付费方式，才能够实现云上的成本最优；<br>第四步：选择合适的交付方式，帮我们省时省力地完成资源的交付。</p></blockquote><p>省钱法宝的更多分享，请参考：<a href="https://developer.aliyun.com/article/849731?spm=a2c6h.12873581.0.0.55cc472djPP6l6&groupCode=ecs">阿里云万郁香：多样付费选择构筑成本最优的弹性体验</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt; 云服务器ECS选购指南及省钱法宝（强烈建议收藏） &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者 | &lt;strong&gt;阿**&lt;/strong&gt;&lt;/strong&gt;里云弹性计算产品专家 马小婷**&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://develop</summary>
      
    
    
    
    <category term="阿里云" scheme="http://zhangyu.info/categories/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
    
    <category term="ECS" scheme="http://zhangyu.info/tags/ECS/"/>
    
  </entry>
  
  <entry>
    <title>阿里云弹性计算研发团队如何从0到1自建SRE体系</title>
    <link href="http://zhangyu.info/2022/03/08/%E9%98%BF%E9%87%8C%E4%BA%91%E5%BC%B9%E6%80%A7%E8%AE%A1%E7%AE%97%E7%A0%94%E5%8F%91%E5%9B%A2%E9%98%9F%E5%A6%82%E4%BD%95%E4%BB%8E0%E5%88%B01%E8%87%AA%E5%BB%BASRE%E4%BD%93%E7%B3%BB/"/>
    <id>http://zhangyu.info/2022/03/08/%E9%98%BF%E9%87%8C%E4%BA%91%E5%BC%B9%E6%80%A7%E8%AE%A1%E7%AE%97%E7%A0%94%E5%8F%91%E5%9B%A2%E9%98%9F%E5%A6%82%E4%BD%95%E4%BB%8E0%E5%88%B01%E8%87%AA%E5%BB%BASRE%E4%BD%93%E7%B3%BB/</id>
    <published>2022-03-07T16:00:00.000Z</published>
    <updated>2022-03-08T17:16:40.322Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/weixin_46593167/article/details/117708269">https://blog.csdn.net/weixin_46593167/article/details/117708269</a> </p></blockquote><blockquote><p>SRE 最早在十多年前 Google 提出并应用，近几年随着DevOps的发展，SRE 开始被大家熟知。而在国内，非常多的 SRE 部门与传统运维部门职责类似，本质来说负责的是互联网服务背后的技术运维工作。构建区别于传统运维的 SRE、如何在业务研发团队落地SRE，是许多企业都在攻克的难题。</p><p>本届全球运维大会 GOPS 上，阿里云弹性计算团队技术专家杨泽强以《大型研发团队SRE 探索与实践》为题，分享了在 SRE 体系建设上的思考和落地实践。</p><p><strong>本文为演讲内容整理，将从以下三部分进行介绍：</strong></p><ul><li><p>  阿里云弹性计算（ECS）自建 SRE 体系的原因；</p></li><li><p>  ECS 建设 SRE 体系的探索与实践；</p></li><li><p>  以弹性计算 SRE 体系建设的四年经验为例分享对 SRE 未来的看法。</p></li></ul><h2 id="为什么ECS要自建SRE体系？"><a href="#为什么ECS要自建SRE体系？" class="headerlink" title="为什么ECS要自建SRE体系？"></a>为什么ECS要自建SRE体系？</h2><p>ECS 团队之所以会单独建 SRE，与产品业务特性以及组织层面上的背景有关。</p><p>下图展示了 ECS 的业务特点：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/81af794d217f300c5bfb8b2f9017c2c8.png" alt="图片"></p><p>首先，从产品业务来看，ECS 是阿里云最大最核心的云产品。ECS 作为阿里巴巴经济体的以及其它部署在 ECS 上的云产品的底座，也支撑了国内外非常多的业务。阿里云全球份额排名第三，其中 ECS 的贡献毫无悬念是排名第一的，ECS 作为基础设施底座，稳定性要求特别高。</p><p>其次，由于阿里内部的经济体上云和整个云计算普及，ECS 对外的 OpenAPI 调用量每年都出现数倍增长，这意味着系统的容量每年都会面临新的挑战。</p><p>而与此同时，阿里云弹性计算启动了去 PE 的组织调整，即业务团队没有专职的运维工程师以及系统工程师，这将意味着运维类的事情、系统架构规划与横向产品需要有团队来承接。</p><h2 id="ECS-建设-SRE-体系的探索与实践"><a href="#ECS-建设-SRE-体系的探索与实践" class="headerlink" title="ECS 建设 SRE 体系的探索与实践"></a>ECS 建设 SRE 体系的探索与实践</h2><p>从 2018 年刚开始建设至今，在一路的摸索中，ECS 的 SRE 体系建设借鉴了 Google 和Netflix 的做法，并结合团队和业务的特性，最终 SRE 体系可以简单概况为下图的五个层次：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/31bcfff7174cdd149c0297d089bea526.png" alt="图片"></p><ul><li><p>  <strong>打基础。</strong>阿里云的文化主张里有一句话是“基础不牢，地动山摇”。在团队里具体的事情就是全链路稳定性治理体系以及性能容量工程，也是重要的基础。</p></li><li><p>  <strong>定标准。</strong>这在一个大型研发团队里非常重要， ECS团队主要从软件的生命完整生命周期来看，从设计-&gt;编码-&gt;CR-&gt;测试-&gt;部署-&gt;运维-&gt;下线，各个环节定义了标准。通过定期的技术培训和定期运营，先在意识上给大家普及，同时会通过小团队的试运行来看效果，如果符合预期就想办法自动化掉。</p></li><li><p>  <strong>建平台。</strong>通过建设自动化平台来不断释放SRE的人肉工作。</p></li><li><p>  <strong>做赋能。</strong>业务团队的SRE除了做好横向的基础组件和自动化平台外，还要做很多推广和协助业务研发的事情；同时SRE每天都要处理非常多的预警响应，线上问题排障以及故障响应，如何把SRE的价值最大化，我觉得最核心的是赋能。</p></li><li><p>  <strong>建团队。</strong>最后我将以弹性计算为例介绍一下SRE团队的职责，文化理念以及如何成为一名合格的SRE。</p></li></ul><h3 id="打基础"><a href="#打基础" class="headerlink" title="打基础"></a>打基础</h3><p><strong>基础框架建设与性能调优</strong></p><p>弹性计算的核心业务都是 Java 技术栈，还有少部分 golang 和 python，本质上是一个Java 研发大型分布式系统。在内部为了支撑业务规模和尽可能的抽象整合，我们自研了一系列基础框架给业务同学使用，包括轻量bpm框架、幂等框架、缓存框架、数据清理框架等，其中每个框架的抽象和设计我们都考虑了规模化容量的支撑以及小型化的输出，以工作流框架为例，我们支持了每天数亿工作流的创建运行，框架调度开销做到了5ms级别。</p><p>除了基础框架，在性能优化上针对 JVM 进行了一系列调优，比如针对IO密集型的应用开启了wisp协程，以及针对每个核心应用JVM进行调优，减少STW的时间。</p><p>另外，从服务性能角度，数据层，我们对全网超过100ms的慢SQL进行了调优；应用层，我们针对核心链路提供了多级缓存方案，可以保障最热的数据可以从内存里最快的返回；业务层，我们通过提供批量API以及异步化改造。</p><p><strong>全链路稳定性治理</strong></p><p>上图罗列了几个比较典型的点，比如预警治理，普遍问题是预警量太大了，信噪比又不高，预警能提供的信息非常有限，对于排查排障帮助比较局限。</p><p>早年间，我们也面临同样的问题，分享<strong>预警治理的两个真实故事：</strong></p><ol><li><p> 一个核心应用的数据库在晚上down了，但预警配置的通知渠道是email和旺旺，并且接收人不是当前应用owner，导致owner在发现故障问题的时候花了非常长时间定位到是数据库问题。</p></li><li><p> 之前发生了一起全链路连锁反应的故障，故障发生的起因是其中某一个业务导致的，当时我们花了两个小时来定位和恢复问题，在事后复盘才发现故障开始前5分钟，已经有相关报警，但该报警接收同学的预警量太大，漏掉了重要预警。</p></li></ol><p>所以，稳定性治理很重要的一部分就是预警治理，主要治理的方法就是监控分层、统一预警配置平台、统一预警优化配置策略，比如预警的接收人、通知方式等。</p><p>数据库稳定性治理</p><p>数据库是应用的命脉，发生在数据库上的故障往往非常致命。不论是数据的准确性或者数据的可用性受损，给业务带来的灾难通常是毁灭性的。</p><p><strong>两个难题：慢SQL和大表</strong></p><p>当在使用MySQL做数据存储的时候，最高频遇到的场景就是慢SQL和大表这两个难题。慢SQL会导致业务变慢甚至产生全链路的连锁反应导致雪崩，而大表问题和慢SQL通常也分不开，当表的数据量大到一定程度，MySQL的优化器在做索引选择的时候也会遇到一些奇怪的问题，所以在数据库的治理上基本围绕慢SQL和大表。</p><p><strong>针对慢SQL的治理方案</strong></p><p>大致的解法是把采集的慢SQL同步到SLS上，通过SLS做近实时的慢SQL分析，然后通过库表信息把慢SQL分给指定团队来修复，这个过程SRE同学会给出优化方案以及通用的基础组件，比如针对大页查询的提供bigcache以及nexttoken方案，针对热点数据的common cache以及读写分离降级能力。</p><p><strong>针对大表的治理方案</strong></p><p>针对大表问题，业界通常的解决方案是分库分表，但是其带来的研发和运维成本很高，我们内部一般业务更常用的方式是通过历史数据归档来做，在这里SRE也有统一的基础框架提供，业务方只需要给出数据归档条件以及触发频率，框架会自动将历史数据归档到离线库并把业务库数据做物理删除，这样通过腾挪数据空洞来达到空间的一定复用，保障有限的数据空间不扩容的前提来支撑业务的发展。</p><p><strong>高可用体系</strong></p><p>分布式系统的高可用可以分四个层次来看。从最底层的部署层，由下至上分别是运行时、数据层、业务层，我们的高可用体系也是按照四个层次来划分的。</p><ol><li><p> 部署层，作为ECS的研发我们对外推荐的最佳实践都是多可用区部署，理由很简单，因为容灾性更好、更柔性。</p></li><li><p> 数据层，如前面提到的数据库稳定性治理，我们数据层一方面的工作就围绕像慢SQL、热点SQL和大表的持续治理，另外一方面就是从技术架构上我们做了多读和读写自动降级框架，可以将一些大表查询自动降级到只读库，同时可以保障读写库异常情况，核心API仍然可以通过只读库提供服务，进而来保障数据库整体的高可用。</p></li><li><p> 业务层的高可用体系，一个复杂的分布式系统，难题之一就是解决依赖的复杂性，如何在依赖方不稳定的情况下仍然保障或者有损保障自身的核心业务可以玩转，这是分布式业务系统非常有挑战与有意义的一件事情。</p></li></ol><p><strong>故障案例</strong></p><p>我们曾经出过一个故障，一个核心系统被一个非常不起眼的边缘业务场景搞到雪崩。核心系统里引入了一个三方系统依赖，当依赖方服务RT变慢的时候，我们所有的HTTP请求由于设置的超时时间不合理全部阻塞，进而导致所有线程都block在等待该服务返回，结果就是所有服务RT变长，直至不再响应。</p><p>要知道在庞大的分布式系统里，没有绝对可靠的授信链，我们的设计理念就是Design For Failure，以及Failure as a service。</p><p>可参考以下思路：</p><ul><li><p>  在设计阶段时定义该依赖的性质，是强依赖还是弱依赖</p></li><li><p>  对方提供的SLO/SLA是什么，依赖方可能会出现什么问题以及对我们服务的影响是什么？</p></li><li><p>如果依赖方出现了预期/非预期的异常，我们的策略是什么？</p><p>  如何保障我们服务的最大可用性。</p><p>  最大可用性，意味着做出的响应可能有损，比如对端是弱依赖，我们可能会直接降级，返回一个mock结果，如果对端是强依赖，我们可能采取的是隔离或者熔断策略，快速失败部分请求，并尽可能记录更多信息，方便后续通过离线方式进行补偿。</p></li></ul><h3 id="定标准"><a href="#定标准" class="headerlink" title="定标准"></a>定标准</h3><p>弹性计算的研发人员大概是百人以上规模，同时还会有一些兄弟团队以及外包人员一起参与研发，自SRE建设的第一天我们就开始逐步建立各种研发标准和流程。</p><p>以UT标准案例为例，UT缺失导致的故障占比高；难度是量大，研发不重视，实际没办法收敛。解法是通过建立UT标准，和CI自动化体系，量化指标来持续改进。效果是UT缺失导致的故障大幅降低，从占比30%降低至不到0.3%。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/1472a8ba350b1292063ba8729f670cd6.png" alt="图片"></p><p><strong>研发流程体系</strong></p><p>我们从设计到发布几乎各个环境都定义了一套标准。</p><p><strong>1.设计阶段。</strong>我们统一定义了一套设计文档模版来规范和约束研发人员。从软件工程角度来看，越早引入问题带来的成本越低，所以我们的研发原则之一也是重设计。一个好的设计不仅要从业务上定义清楚问题，定义清楚UserStory和UserCase以及约束，从技术角度也要清楚的讲清楚技术方案的tradeoff以及Design for failre如何实现、如何灰度、监控回滚等等。我们希望研发多在设计阶段下功夫，少在中途返工或者发布后打补丁。同时我们的评审机制也从线下大团队评审转变为小团队线下+大团队直播方式进行，尽可能少开会，节约大家时间。</p><p><strong>2.研发阶段。</strong>我们的研发流程类似git-flow。是多feature并行开发，开发测试后合并进入develop分支，每天会有统一的流程基于develop进行daily deploy，我们基于阿里集团的Java规约做了扩展，加入了自定义的静态扫描规则，同时统一的UT框架，实现了CR后自动运行规约扫描执行静态检查，同时运行CI产出UT运行报告，只有静态扫描，CI结果主要是UT成功率和行增量覆盖率以及代码点赞数同时满足条件MR才会被合并，进入下次发布list。</p><p><strong>3.测试阶段。</strong>主要分日常测试，预发测试以及上线前的功能测试以及灰度期间的回归测试。<strong>大规模研发团队大家面临的难题就是环境怎么搞？</strong>这么快速复制以及隔离？以往模式下我们只有一套日常和预发，经常出现某个人代码问题或者多人代码冲突导致测试特别耗时。后来我们做了项目环境，简单功能可以通过容器方式快速复制全链路项目环境。针对有全链路联调需求的case，我们扩展了多套预发环境，可以做到每个业务研发团队一套预发，大家互相不争抢，这样预发的问题就解决了。</p><p>上线前的功能测试主要是针对daily deploy的，我们会在每天晚上11点自动从develop拉取分支，并部署到预发环境，同时这个时候会自动运行全量的功能测试用例来保障发布的可靠性，在日常发布如果FVT非预期失败，会导致发布取消。</p><p>最后一个测试流程是灰度期间自动回归core fvt，我们的发布是滚动发布模式，通常会灰度一个地域来做灰度验证，core fvt就是做这个的，当core fvt运行通过后可以进行后续批次发布，反之判断是否回滚。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/d1de45871ad39d9ced95f6a4f089f978.png" alt="图片"></p><p><strong>变更流程体系</strong></p><p>在SRE建设的时候，我们特意规划了变更管控流程。针对当前的变更类型做了不同的标准要求，比如数据库变更checklist+review机制，日常发布/hotfix/回滚的批次以及暂停时长，还有就是中间件的配置规范以及黑屏变更等。</p><p>有了变更流程和规范只是第一步，接着我们针对高频率的运维操作做了工具化建设，其中有部分和现有的DevOps平台合作，游离在现DevOps之外的部分我们都自己做了研发支持，比如日志清理以及进程自动重启，并开发了自动化工具可以自动化清理大文件以及重启故障进程。</p><p>举一个例子就是数据订正，数据都是通过异步编排来实现最终一致性，所以数据订正会是一个特别高频的变更，简单的一条订正SQL蕴藏的威力有时候超过我们的想象，我们之前有一个故障就是因为一条数据订正错误导致，影响非常严重，排障过程也非常困难。</p><h3 id="建平台"><a href="#建平台" class="headerlink" title="建平台"></a>建平台</h3><p><strong>SRE 自动化平台</strong></p><p>我们做SRE自动化平台就是为了将标准通过自动化方式来实现，比如研发阶段的高可用体系里的读写降级，限流等。我们在提供框架能力的同时，提供了运维接口和白屏化工具，帮助研发实现自动化/半自动化的高可用能力。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/d940d4864e25ccd5d45a522de4bbce3a.png" alt="图片"></p><p><strong>弹性计算团队的1-5-10指标</strong></p><p>后面的变更、监控、预警、诊断、恢复，对应的就是MTTR的各个细分阶段，在阿里集团有个1-5-10的指标，意思就是分钟发现问题，5分钟定位问题，和10分钟恢复问题，这是一个非常难达到的高标准。</p><p>弹性计算团队为了满足提升1-5-10指标，建立了自己的监控平台和预警平台。我们做的是预警能力的二次消费，将所有的基础监控包括系统指标cpu和mem、JVM监控以及各种中间件监控，还有非常多的业务监控做了分层，而每一个预警都会囊括各种meta信息，比如归属团队、重要性、关联的诊断场景、快恢策略，以及推荐的变更等。</p><p>这就闭环了变更、预警、诊断、快恢整个过程。当一条预警出现的时候可以自动根据TraceID分析全链路寻找局点，同时推荐相关变更，并生成影响面比如多少API、用户是哪些，以及该预警推荐的解决方案是什么，同时提供一个hook可以执行快恢动作。</p><p>举个例子，有位同学订正了一条业务，sql写的问题有问题，导致线上几个大客户的服务异常，在sql执行完的几秒内我们的监控系统就识别出了业务异常，并产生了预警信息以及预警的stack和影响面分析，同时关联的数据库变更信息也被推荐了出来，这些信息组合在一起的1分钟内我们就定位到了问题，并立马执行了回滚，业务很快就恢复了。当然该平台目前仍然有局限性，我们今年规划会做更多智能预警和诊断的事情。</p><p>最后，必须要提一下演练，即混沌工程 Chaos engineering，最早由Netflix提出。在过去的两年里，我们通过故障演练，故障注入的方式多次回放了历史故障，同时对发现线上问题也非常有帮助，故障演练现在已经作为一个常态化的事情融入到了日常。</p><p>分享完我们的SRE自动化平台体系，有了平台之后，非常重要的一个事情就是赋能。</p><p>我认为业务团队SRE最大的价值就是赋能，通过赋能来使众人行。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/4054c5851dc8155913da6fb22e531a94.png" alt="图片"></p><h3 id="做赋能"><a href="#做赋能" class="headerlink" title="做赋能"></a>做赋能</h3><p><strong>全链路SLO量化体系</strong></p><p>ECS的上下游依赖方众多，任何一个环境出现不稳定都会影响ECS出口服务的稳定性。</p><p>举个例子ECS向下对接的是虚拟化和块存储，只要虚拟化和快存储慢了体现用户层面就是ECS实例启动慢了，而这个快慢究竟如何评定呢？可能对于我们做分布式服务来说可能觉得5ms已经很慢了，但是对于虚拟化来说他认为我这个接口1s都是正常的，这个时候就需要SLA了。</p><p>做SRE的第二年，我们梳理了全链路数十个依赖以及数百个核心API与各个业务方选择最关心的指标也就是SLI，针对不同的置信空间设置了SLO值，并且建设了统一的量化平台，通过实时与离线方式持续跟进起来。通过SLO体系建立到持续运营的一年多时间，我们的依赖方可用性和时延的SLO达标率从最初的40%多治理到98%以上。这个直接产生的业务就是我们对用户API成功率的提升，用户的体感更加好了。</p><p><strong>落地SLO的四个阶段</strong></p><ol><li><p> 选取SLI，即和你的依赖方确定哪些指标是需要关注的，比如通常都关心的可用性和时延；</p></li><li><p> 和依赖方约定SLO，即明确某个API某个SLI的目标值 P99、P999分别是什么；</p></li><li><p> 计算SLO，通常的方式都是通过记录日志，将日志采集到SLS，通过数据清洗再加工计算出指标值。</p></li><li><p> 可视化，通过将SLO做成实时/离线的实时报表，来做持续跟进。</p></li></ol><p><strong>知识库</strong></p><p>SRE 很大的一部分职责在于故障排查和疑难问题处理，同时我们做了一系列框架和工具，还要非常多的运维手册以及故障复盘的资料，这些我们都按照统一模版沉淀下来，可以用来指导研发同学日常问题排查和变更使用，其中部分文档我们还共享给了阿里云其它产品。</p><p>通过知识库我们也赋能了非常多的兄弟团队，另外我们研发过程中的很多业务基础组件像工作流、幂等、缓存、降级、Dataclean 等框架也都有阿里云其它团队在使用。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/2cc02fa2048db0f75cfa99c1db7ef784.png" alt="图片"></p><p><strong>稳定性文化建设</strong></p><p>SRE 是稳定性的捍卫者，也是布道师。只有人人都意识到稳定性的重要性，我们的系统才可能真正的稳定。我们从建设 SRE 团队的第一天就开始建流程，团队内通过日报，周报，月报以及不定期的线上直播以及线下培训来宣传稳定性文化。逐步在团队形成我们特有的稳定性文化，比如 Code Review 文化，安全生产文化和事后故障复盘文化。</p><p><strong>故障复盘实践</strong></p><p>在 SRE 初期，我们开始推行故障复盘。我们对于故障的定义是所有有损业务或者人效的异常 case 都是故障。一开始，故障复盘由 SRE 主导，由业务团队配合，但整个过程非常不愉快。</p><p>随着后面 SRE 的一些自动化工具以及一些流程真正帮助了研发避免故障，以及在故障复盘过程中 SRE 的一些见解给到了研发正向反馈，慢慢故障复盘的文化在团队开始慢慢被接受，各个业务自己会写故障复盘报告，并开直播分享，其它团队的同学也会非常积极地给反馈。文化真正的深入人心后，产生的会是非常好的良性循环。</p><h3 id="建团队"><a href="#建团队" class="headerlink" title="建团队"></a>建团队</h3><p>最后分享一下SRE团队组建主要注意的几个方面。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/d098216f8aa7790967a99ef7850e2504.png" alt="图片"></p><p><strong>人</strong></p><p>在弹性计算团队，我们对 SRE 的要求是T型人才，要一专多能。</p><ul><li><p>  精通一门编程语言</p></li><li><p>  两个基本能力：抽象能力、标准化能力</p></li><li><p>  拥有全局视野，具备赋能意识</p></li></ul><p><strong>事</strong></p><p>事，基本上即是前面提到的建标准、建自动化平台、做基础服务和赋能业务团队，还有on-call支持等日常工作。</p><p>同时，我们需要在团队中建立几个共同的核心理念，我个人认为SRE几条最核心的理念：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/c4533259386b0bcaae7316fdf0e36099.png" alt="图片"></p><ul><li><p>  稳定性就是产品，稳定性不是一锤子买卖。SRE 要做的是赋予稳定性产品的灵魂，要按照产品一样去养育，去不断迭代，去持续演进。</p></li><li><p>  软件工程的方法论解决生产系统稳定性问题。SRE 区别于业务研发和运维的很大一点是，SRE 解决的是生产环境的稳定性问题，是通过软件工程的方法论来重新定义运维模型。</p></li><li><p>  自动化一切耗费团队的事情。SRE 的精髓在于软件工程定义运维，通过自动化以及赋能业务来最大化价值。</p></li></ul><h2 id="SRE-建设的回顾与个人展望"><a href="#SRE-建设的回顾与个人展望" class="headerlink" title="SRE 建设的回顾与个人展望"></a>SRE 建设的回顾与个人展望</h2><p>简单概括下弹性计算团队四年的 SRE 发展历程就是，<strong>建体系-量化-自动化-智能化。</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/dfe9ab6dd26a901c0018d865d6dd4fdd.png" alt="图片"></p><p><strong>第一年：体系化探索</strong></p><p>这一年主要的工作就是从0-1结合当前业务和团队的现状把SRE体系建设起来，比如基础框架的统一建设，稳定性治理体系。</p><p><strong>第二年：SLO体系</strong></p><p>第二年的重点主要是针对全链路数十个系统依赖，以及内部系统的核心功能定义了SLO（service level object，SLO）量化体系，并跨BU完成了整个链路的SLO量化体系建设。同时开始重点建设稳定性运营体系，以及自己的数据运营平台，将内外部核心依赖的核心API的可用性，时延的数据全部量化并持续跟进治理起来。</p><p><strong>第三年：自动化</strong></p><p>我们把研发过程中从设计、编码、测试、部署到上线后的预警响应等所有需要人工参与的事情都做了尽可能的自动化。比如在CR阶段加入UT覆盖率卡点，不符合标准的CR会自动拦截。在发布阶段接入了无人值守，根据发布期错误日志的情况来进行发布拦截，当然这里更好的方式可能是通过灰度机器的服务SLA等综合指标来判断。另外，在灰度地域发布暂停期间，我们会自动化运行corefvt来回归核心测试用例验证发布的可靠性，异常情况会自动拦截灰度，并推荐一键回滚操作。</p><p><strong>第四年：智能心智</strong></p><p>今年我们正在做的一些事情是高度自动化，比如无人发布值守，还有自动化预警根因分析等。当我布式系统规模足够大，应用复杂度足够高的时候，靠人的判断是非常困难的。所以，SRE要建设的自动化平台要有智能心智，通过系统化的能力来代替甚至超越人。</p><p><strong>对SRE未来发展的个人看法：</strong></p><ul><li><p>  稳定性即产品，我们真正是需要把稳定性当作产品来看待的，做产品意味着我们要清楚的定义问题，并产生解决方案，并且持续的迭代演讲，这不是一锤子买卖，是需要养育的。</p></li><li><p>  我觉得随着云计算的普及，SRE的技能会倾向于研发技能，当然系统工程的能力也是必须的，因为云计算作为基础设施会帮助我们屏蔽掉非常多的机房、网络、OS层面的问题，这样SRE的重点就在于用软件工程的方法论来重新定义运维，使用自动化来提高效能。</p></li><li><p>Netflix提出了一个CORE SRE的概念，NetFlix是这样解读CORE的，C就是Cloud，我们都知道Netflix是run on AWS，Cloud是基础。</p><p>  Operation就是运维了，Reliability和Enginering就不多说了。</p><p>  而我对CORE SRE的另一个解读是少量核心的SRE人员支撑并保障大规模服务的稳定性。</p></li><li><p>  少量的SRE支撑大规模服务背后的最核心理念我觉得是自动化，尽可能最大化一切可以自动化的事情，并且要智能的自动化。</p></li></ul><p>总结下稳定性就是，产品 + Dev的比重会增大 + CORE SRE + 自动化一切。</p><p>以上就是我今天想要和大家分享的内容，谢谢大家的聆听。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_46593167/article/details/117708269&quot;&gt;https://blog.csdn.net/weixin_46593167/article/deta</summary>
      
    
    
    
    <category term="SRE" scheme="http://zhangyu.info/categories/SRE/"/>
    
    
    <category term="SRE" scheme="http://zhangyu.info/tags/SRE/"/>
    
  </entry>
  
  <entry>
    <title>云上资源自动化部署新模式</title>
    <link href="http://zhangyu.info/2022/03/08/%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E6%96%B0%E6%A8%A1%E5%BC%8F/"/>
    <id>http://zhangyu.info/2022/03/08/%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E6%96%B0%E6%A8%A1%E5%BC%8F/</id>
    <published>2022-03-07T16:00:00.000Z</published>
    <updated>2022-03-08T17:15:38.066Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>从原子操作走向模板部署，详解云上资源自动化部署新模式<br><a href="https://blog.csdn.net/bjchenxu/article/details/117794680">https://blog.csdn.net/bjchenxu/article/details/117794680</a></p></blockquote><blockquote><p>5 月 29 日，阿里[云开发]者大会的《应用开发的基础设施云上优化》分论坛上，阿里云技术专家王斌鑫发表了主题为《云上资源自动化部署新模式》的分享，详细阐述了云上资源自动化部署新模式——基于资源编排、Terraform托管、ROS CDK的自动化部署最佳实践。 </p><p>本文为根据王斌鑫的演讲整理成文。</p><p><strong>当前云上资源部署模式</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/24b9fc89a519dc332fe03e0b28385183.png"></p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/7e4c9dadd6be2d88862ad013689be882.png"> 云上资源传统部署模式的挑战</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/c23409c9c64b612469505d70ec996224.png"></p><p>大部分用户一般是通过控制台/API/SDK等传统模式进行云上资源的部署，这种部署模式会面临规模、效率、规范和成本四个方面的挑战：</p><p>1. 规模上，随着业务的发展需要管理的资源规模不断上升，部署和管理种类繁多的资源带来挑战。</p><p>2. 效率上，随着规模上升，手动批量部署变得难以为继。</p><p>3. 规范上，如何确保对基础设施的变更均符合组织管理规范。</p><p>4. 成本上，手动部署的方式无法极致地利用云上弹性能力，其成本仍有优化空间。</p><p><strong>阿里云自动化部署模式</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/9df336d655e3cdf6f5b9ab8a8246db39.png"></p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/c522f57920978ac9dc843a0bce5e3eda.png"> 资源编排服务（ROS）的核心价值</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/540b08d8eddbc42017d33fcf59dea10a.png"></p><p>面对传统部署模式的挑战，我们在想是否能够帮助客户对云上资源进行自动化部署？因此有了阿里云资源编排服务（ROS），它基于基础设施即代码（IaC）的理念，让开发者和管理员使用模版的方式，编排云上的多种资源，进行自动化部署。</p><p>对比手动部署，使用资源编排服务ROS进行自动化部署会带来如下好处：</p><p>• 效率提升，针对诸如SAP这样复杂的解决方案能有效提升部署效率，也能够帮助MSP、ISV、onECS服务提升部署效率；</p><p>• 架构优化，ROS提供了种类丰富的阿里云最佳实践模板，用户无需丰富的架构经验即可部署解决方案级别的架构；</p><p>• 流程管控，由IT管理员统一管理基础设施以避免各类风险，且可基于模板进行审核再进一步结合CI/CD以规范化IT管理流程；</p><p>• 节省成本，自动化部署方式可以按需部署和释放资源，从而极致地使用云上弹性能力来降低成本。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/709d01bc7735abb89a16fe4a2fdb11d0.png"> ROS 的使用流程和核心功能</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/15eef602c39d93cad3e2c7884843db03.png"></p><p>使用 ROS 进行自动化部署的过程非常简单：</p><p>1. 按照ROS 模板语法编写模板，定义想要创建的各类云上资源。</p><p>2. 在 ROS 控制台 使用模板创建资源栈，以执行部署。其中，资源栈是一组资源的集合，这些资源均是模板中定义的资源。</p><p>3. 在 ROS 控制台 查看资源栈，可以查看栈中各种资源的创建情况，并可以跳转到对应资源的控制台。</p><p>值得一提的是，ROS 服务本身完全免费，集成了身份认证和安全审计的功能，资源创建结果是可视化的，且能够进行多账号跨地域地部署，支持资源栈和实际资源的差异检测并进行修正。</p><p>除了直接使用 ROS 模板来做自动化部署，是否还有别的方式呢？</p><p><strong>新模式一：Terraform 托管</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/e796ee7e140d7f9826418a65889d1dbe.png"></p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/289bdc006e85574c97571e01220f9ba8.png"> Terraform 是什么</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/286749609a5e7f83db5e76e1f6d01ea1.png"></p><p>Terraform 同 ROS 一样，也是基于基础设施即代码（IaC）的理念的自动化编排工具。它使用一种特定的配置语言（HCL, Hashicorp Configuration Language）来描述基础设施资源，语法样例如上图所示。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/067e98d9e2b4e79a91f95602126a4abc.png"> Terraform &amp; ROS</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/d678cab3d14ea3784164e49ca49627bb.png"></p><p>既然 Terraform 和 ROS 都是基于相同的理念的自动化编排工具，那它们的目标也是一致的，都是为用户打造良好的云上部署体验。</p><p>两者有很多相同之处，比方说 Terraform 的配置文件相当于ROS的模板，Terraform 的状态相当于 ROS 的资源栈，Terraform 的 CLI 程序则相当于 ROS 的编排引擎。</p><p>两者也各有优势，Terraform 的语法更简洁，对多云支持地很完善；而 ROS 则提供免费的服务托管，且有云原生的鉴权和审计能力。</p><p>那么是否能够将两者的优势结合呢？因此就有了 Terraform 托管能力。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/52542d43be98a0f5958dae5799770642.png"> Terraform 托管</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/7960f35892c01e3e993aa75558933106.png"></p><p>用户直接在本地使用 Terraform 时，需要根据当前的操作系统下载对应的 Terraform CLI，编写模板，管理所使用的各类 Provider 的版本，且要管理状态等文件。</p><p>而使用 Terraform 托管功能时，只需在 ROS 的控制台编写 Terraform 模板便可直接部署，后续则通过资源栈来管理模板中定义的资源。底层的各类管理都交给 ROS。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/22080bda7054cfb486c535ad80afed77.png"></p><p>在使用原理上，ROS 控制台会将 Terraform 模板组合成符合 ROS 语法规范的模板，ROS 服务端会其进行语法校验，生成租户信息，调度到 ROS 的 Terraform 服务进行资源的部署。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/193c29ec55cac1a44dbd923e0241a89d.png"> 定时与多云场景实践</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/4f3f8f74e52ef3ec3329b65cf7508075.png"></p><p>我们可以在很多场景中使用 Terraform 托管的功能。</p><p><strong>场景一：定时部署资源</strong></p><p>假设我们需要通过 Terraform 定时部署资源，传统方式下需要本地创建定时任务，执行 Terraform CLI 来做。而在云上，我们可以：</p><p>• 事先编写一个 Terraform 模板，声明想要部署的云资源；</p><p>• 事先编写一个 OOS 运维模板来声明由它调用 ROS 进行资源部署；</p><p>• 在 OOS 中设置为定时执行，OOS 会定时触发 ROS，ROS 则会使用 Terraform 托管功能进行资源部署。</p><p><strong>场景二：多云管理</strong></p><p>如果我们既想要对多种云平台（如阿里云、AWS等）的资源进行部署，又想有可视化的结果反馈，可以直接编写 Terraform 的模板来声明各个云上资源，并使用 ROS 的 Terraform 托管功能来进行部署。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/e961a4ee3bc4582eb6dd00476c896e80.png"> Terraofrm 托管总结</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/ca11506c44b156269ba51e99f688de95.png"></p><p>Terraform 托管功能能让用户在云上直接使用 Terraform，和直接使用 ROS 模板部署有一致的控制台体验，且兼容了 ROS 原生的API，同时兼备了统一的身份认证和权限控制。相比于本地使用 Terraform，不再需要管理多种 Provider 和多个 Terraform CLI版本。</p><p><strong>新模式二：ROS CDK</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/8b8488afd1dc0fa0018d4beb74361a98.png"></p><p>现有资源定义方式的不足</p><p><img src="https://img-blog.csdnimg.cn/img_convert/e0a8043d524e99c41110ddd35e69cd3b.png"></p><p>事实上，通过直接编写 ROS 模板，或者通过可视化编辑器生成模板，然后进行资源部署的方式是能够大大提升资源的部署效率的，但是也有一些不足之处：</p><p>• 缺少对过程式的支持</p><p>• 复杂场景的编写效率较低</p><p>• 对程序的友好性较低</p><p>• 动态性支持较差</p><p>针对这些问题，是否可以更进一步，在模板之上解决这些不足呢？</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/999d4c3c955c789527cdc10294611ca2.png"> ROS CDK 是什么</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/8b24a382fda35494e2fef14ed9cf4454.png"></p><p>ROS CDK 是资源编排（ROS）提供的一种命令行工具和多语言SDK，利用面向对象的高级抽象模式对云资源进行标准定义，从而快速构建云资源。</p><p>ROS CDK 以应用作为资源管理的入口，一个应用管理多个资源栈，而每个资源栈中则可以有多个构件。构件可以理解为云上资源的组件，能包含一个或多个资源。</p><p>我们可以选择自己熟悉的编程语言（TypeScript/JavaScript/Java/Python/C#）编写应用代码声明想要部署的资源，ROS CDK 会将项目代码转换成 ROS 模板，然后使用该模板进行自动化部署。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/1a673c55b4faed9d2766520819c42994.png"> 使用步骤和项目生命周期</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/51ef8420238d988a392751231fd1c8e9.png"></p><p>ROS CDK 的使用步骤也很简单：</p><p>1. 首先，就是初始化项目，配置阿里云的访问凭证（AccessKey）</p><p>2. 其次，就是编写资源代码和测试用例进行本地测试</p><p>3. 最后，通过CDK CLI或者直接程序进行资源部署，并管理资源栈</p><p>在进行部署的阶段，CDK会根据用户编写的资源代码进行构造，实例化出各种资源对象；然后在准备阶段做终态前的调整（通常由框架自动完成）；进而验证各种资源属性，确保能够正确部署；最终合成出一个 ROS 模板，并使用该模板部署为资源栈。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/8b263adbeff900ebd508a06ae6b62a03.png"> 代码、模板示例</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/2fcc1dca6a8b2b40013a900e91a0613d.png"></p><p>上图中，左边是 ROS CDK的资源代码，其中声明了一个 VPC，并使用循环动态生成3个 VSwitch。而右边则是由 ROS CDK 生成的 ROS 模板。由此可以看出针对动态生成的场景，ROS CDK 可以大大简化模板编写的复杂度。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/43a6ae4020d382aeb6b588a46daf5dc4.png"> 应用程序集成CDK实现持续部署场景</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/79917e2930ff3965e3005a46c6629938.png"></p><p>假设我们需要实现一个CI/CD系统，能够部署这样的资源架构：使用API网关中提供API，使用函数计算的函数提供业务逻辑，要分别部署测试、预发、线上环境的资源，并且支持从测试发布到预发，从预发部署到线上。</p><p>针对这样的资源架构，在直接使用 ROS 模板的方式中，需要分别为三个环境准备三个模板，而环境间的部署则还需要动态拼接模板，对应用程序来说并不友好。这里就建议使用ROS CDK，这样应用程序可以根据环境的不同指定对应的变量，生产对应的资源，从而满足环境的动态性部署。</p><p> <strong><img src="https://img-blog.csdnimg.cn/img_convert/c14256d12bb210176c8d641ceedded8b.png"> ROS CDK 总结</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/e04fd164b14ab98cf4009335a379b47b.png"></p><p>相较于直接使用 ROS 模板部署，使用 ROS CDK 允许开发者选择自己熟悉的编程语言，并能借助其动态特性来实现复杂的编排效果。ROS CDK 能够非常容易地集成到应用程序中，从而能够方便地在程序中进行资源部署。</p><p><strong>总结</strong></p><p><img src="https://img-blog.csdnimg.cn/img_convert/a3f31815d8657df380b9e7a311e445ae.png"></p><p><img src="https://img-blog.csdnimg.cn/img_convert/98abf46ef0f4214e97696676528d1f32.png"></p><p>企业上云规模逐渐增大，企业云上资源的部署方式从人工开始走向自动，从单云走向多云，从<a href="https://so.csdn.net/so/search?q=%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C&spm=1001.2101.3001.7020">原子操作</a>走向模板部署。随着基础设施即代码的理念而兴起，资源的部署模式也因场景的不同而不同，总体来说有以下四个部署模式的建议：</p><p>1. 作为入门级用户，只需管理有限几个资源，直接使用控制台的方式是最为简单直观的；</p><p>2. 作为企业IT管理员，需管理规模较大的云上资源，使用 ROS 模板管理基础设施会是最有效率的选择；</p><p>3. 作为运维研发人员，需要在业务系统中实现资源部署逻辑，那么 ROS CDK 会是最佳选择；</p><p>4. 作为多云管理员，需可视化管理阿里云、AWS、Azure等多种云的资源，使用 ROS Terraform 托管功能是不二之选。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;从原子操作走向模板部署，详解云上资源自动化部署新模式&lt;br&gt;&lt;a href=&quot;https://blog.csdn.net/bjchenxu/article/details/117794680&quot;&gt;https://blog.csdn.net/bjche</summary>
      
    
    
    
    <category term="SRE" scheme="http://zhangyu.info/categories/SRE/"/>
    
    
    <category term="SRE" scheme="http://zhangyu.info/tags/SRE/"/>
    
  </entry>
  
  <entry>
    <title>开启shareProcessNamespace后容器异常</title>
    <link href="http://zhangyu.info/2021/05/31/cotainer-init/"/>
    <id>http://zhangyu.info/2021/05/31/cotainer-init/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-31T12:41:15.290Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://qingwave.github.io/cotainer-init/">https://qingwave.github.io/cotainer-init/</a></p><blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>目前k8s不支持容器启动顺序，部分业务通过开启<code>shareProcessNamespace</code>监控某些进程状态。当开启共享pid后，有用户反馈某个容器主进程退出，但是容器并没有重启，执行<code>exec</code>会卡住，现象参考<a href="https://qingwave.github.io/cotainer-init/3">issue</a></p><h2 id="复现"><a href="#复现" class="headerlink" title="复现"></a><a href="https://qingwave.github.io/cotainer-init/#%E5%A4%8D%E7%8E%B0" title="复现"></a>复现</h2><ol><li> 创建deployment</li></ol><ul><li><p>apiVersion: apps/v1<br>  kind: Deployment<br>  metadata:<br>   labels:<br>   app: nginx<br>   name: nginx<br>  spec:<br>   selector:<br>   matchLabels:<br>   app: nginx<br>   template:<br>   metadata:<br>   labels:<br>   app: nginx<br>   name: nginx<br>   spec:<br>   shareProcessNamespace: true<br>   containers:  </p><ul><li>image: nginx:alpine<br>name: nginx  </li></ul></li><li><p>查看进程信息<br>  由于开启了<code>shareProcessNamespace</code>, <code>pause</code>变为<code>pid 1</code>, <code>nginx daemon</code>pid为<code>6</code>, ppid为<code>containerd-shim</code></p></li><li><p># 查看容器内进程<br>  / # ps -efo “pid,ppid,comm,args”<br>  PID   PPID  COMMAND          COMMAND<br>   1     0 pause            /pause<br>   6     0 nginx            nginx: master process nginx -g daemon off;<br>   11     6 nginx            nginx: worker process<br>   12     6 nginx            nginx: worker process<br>   13     6 nginx            nginx: worker process<br>   14     6 nginx            nginx: worker process<br>   15     0 sh               sh<br>   47    15 ps               ps -efo pid,ppid,comm,args  </p></li><li><p>删除主进程<br>  子进程被<code>pid 1</code>回收, 有时也会被<code>containerd-shim</code>回收</p></li></ul><ol><li><p>/ # kill -9 6<br> / #<br> / # ps -efo “pid,ppid,comm,args”<br> PID   PPID  COMMAND          COMMAND<br>  1     0 pause            /pause<br>  11     1 nginx            nginx: worker process<br>  12     1 nginx            nginx: worker process<br>  13     1 nginx            nginx: worker process<br>  14     1 nginx            nginx: worker process<br>  15     0 sh               sh<br>  48    15 ps               ps -efo pid,ppid,comm,args  </p></li><li><p>docker hang<br> 此时对此容器执行docker命令(<code>inspect, logs, exec</code>)将卡住， 同样通过<code>kubectl</code>执行会超时。</p></li></ol><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a><a href="https://qingwave.github.io/cotainer-init/#%E5%88%86%E6%9E%90" title="分析"></a>分析</h2><p>在未开启<code>shareProcessNamespace</code>的容器中，主进程退出<code>pid 1</code>, 此pid namespace销毁，系统会<code>kill</code>其下的所有进程。开启后，<code>pid 1</code>为<code>pause</code>进程，容器主进程退出，由于共享pid namespace，其他进程没有退出变成孤儿进程。此时调用docker相关接口去操作容器，docker首先去找主进程，但主进程已经不存在了，导致异常(待确认)。</p><p>清理掉这些孤儿进程容器便会正常退出，可以<code>kill</code>掉这些进程或者<code>kill</code>pause进程，即可恢复。</p><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a><a href="https://qingwave.github.io/cotainer-init/#%E6%96%B9%E6%A1%88" title="方案"></a>方案</h2><p>有没有优雅的方式解决此种问题，如果主进程退出子进程也一起退出便符合预期，这就需要进程管理工具来实现，在宿主机中有<code>systemd</code>、<code>god</code>，容器中也有类似的工具即<code>init进程</code>(传递信息，回收子进程)，常见的有</p><ol><li> <code>docker init</code>, docker自带的init进程(即<code>tini</code>)</li><li> <a href="https://github.com/krallin/tini"><code>tini</code></a>, 可回收孤儿进程/僵尸进程，<code>kill</code>进程组等</li><li> <a href="https://github.com/Yelp/dumb-init"><code>dumb-init</code></a>, 可管理进程，重写信号等</li></ol><p>经过测试，<code>tini</code>进程只能回收前台程序，对于后台程序则无能为力(例如<code>nohup</code>, <code>&amp;</code>启动的程序)，<code>dumb-init</code>在主进程退出时，会传递信号给子进程，符合预期。</p><p>开启<code>dumb-init</code>进程的<code>dockerfile</code>如下，<code>tini</code>也类似  </p><p>FROM nginx:alpine  </p><p># tini<br># RUN apk add –no-cache tini<br># ENTRYPOINT [“/sbin/tini”, “-s”, “-g”, “–”]  </p><p># dumb-init<br>RUN wget -O /usr/bin/dumb-init <a href="https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init/_1.2.2/_amd64">https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init\_1.2.2\_amd64</a><br>RUN chmod +x /usr/bin/dumb-init<br>ENTRYPOINT [“/usr/bin/dumb-init”, “-v”, “–”]  </p><p>CMD [“nginx”, “-g”, “daemon off;”]  </p><p>init方式对于此问题是一种临时的解决方案，需要docker从根本上解决此种情况。容器推荐单进程运行，但某些情况必须要运行多进程，如果不想处理处理传递回收进程等，可以通过<code>init</code>进程，无需更改代码即可实现。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a><a href="https://qingwave.github.io/cotainer-init/#%E5%8F%82%E8%80%83" title="参考"></a>参考</h2><p>[1] <a href="https://github.com/Yelp/dumb-init">https://github.com/Yelp/dumb-init</a><br>[2] <a href="https://github.com/krallin/tini">https://github.com/krallin/tini</a><br>[3] <a href="https://github.com/kubernetes/kubernetes/issues/92214">https://github.com/kubernetes/kubernetes/issues/92214</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://qingwave.github.io/cotainer-init/&quot;&gt;https://qingwave.github.io/cotainer-init/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>k8s中shell脚本启动如何传递信号</title>
    <link href="http://zhangyu.info/2021/05/31/docker-shell-signal/"/>
    <id>http://zhangyu.info/2021/05/31/docker-shell-signal/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-31T12:43:18.387Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://qingwave.github.io/docker-shell-signal/">https://qingwave.github.io/docker-shell-signal/</a></p><blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在k8s或docker中，有时候我们需要通过shell来启动程序，但是默认shell不会传递信号（sigterm）给子进程，当在pod终止时应用无法优雅退出，直到最大时间时间后强制退出（<code>kill -9</code>）。</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a><a href="https://qingwave.github.io/docker-shell-signal/#%E5%88%86%E6%9E%90" title="分析"></a>分析</h2><p>普通情况下，大多业务的启动命令如下  </p><p>command: [“binary”, “-flags”, …]  </p><p>主进程做为1号进程会收到<code>sigterm</code>信号，优雅退出(需要程序捕获信号); 而通过脚本启动时，<code>shell</code>作为1号进程，不会显示传递信号给子进程，造成子进程无法优雅退出，直到最大退出时间后强制终止。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a><a href="https://qingwave.github.io/docker-shell-signal/#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88" title="解决方案"></a>解决方案</h2><h3 id="exec"><a href="#exec" class="headerlink" title="exec"></a><a href="https://qingwave.github.io/docker-shell-signal/#exec" title="exec"></a>exec</h3><p>如何只需一个进程收到信号，可通过<code>exec</code>，<code>exec</code>会替换当前shell进程，即<code>pid</code>不变  </p><p># do something<br>exec binay -flags …  </p><p>正常情况测试命令如下，使用sleep来模拟应用<code>sh -c &#39;echo &quot;start&quot;; sleep 100&#39;</code>：<br><code>pstree</code>展示如下，<code>sleep</code>进程会生成一个子进程  </p><p>bash(28701)───sh(24588)───sleep(24589)  </p><p>通过<code>exec</code>运行后，命令<code>sh -c &#39;echo &quot;start&quot;; exec sleep 100&#39;</code>  </p><p>bash(28701)───sleep(24664)  </p><p>加入<code>exec</code>后，<code>sleep</code>进程替换了shell进程，没有生成子进程</p><p>此种方式可以收到信号，但只适用于一个子进程的情况</p><h3 id="trap"><a href="#trap" class="headerlink" title="trap"></a><a href="https://qingwave.github.io/docker-shell-signal/#trap" title="trap"></a>trap</h3><p>在shell中可以显示通过<code>trap</code>捕捉信号传递给子进程  </p><p>echo “start”<br>binary -flags… &amp;<br>pid=”$!”  </p><p>_kill() {<br> echo “receive sigterm”<br> kill $pid #传递给子进程<br> wait $pid<br> exit 0<br>}  </p><p>trap _kill SIGTERM #捕获信号<br>wait #等待子进程退出  </p><p>此种方式需要改动启动脚本，显示传递信号给子进程</p><h2 id="docker-init"><a href="#docker-init" class="headerlink" title="docker-init"></a><a href="https://qingwave.github.io/docker-shell-signal/#docker-init" title="docker-init"></a>docker-init</h2><p><a href="https://docs.docker.com/engine/reference/run/#specify-an-init-process">docker-init</a>即在docker启动时加入<code>--init</code>参数，docker-int会作为一号进程，会向子进程传递信号并且会回收僵尸进程。</p><p>遗憾的是k8s并不支持<code>--init</code>参数，用户可在镜像中声明init进程，更多可参考<a href="https://qingwave.github.io/docker-shell-signal/container-init.md">container-init</a>  </p><p>RUN wget -O /usr/bin/dumb-init <a href="https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init/_1.2.2/_amd64">https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init\_1.2.2\_amd64</a><br>RUN chmod +x /usr/bin/dumb-init<br>ENTRYPOINT [“/usr/bin/dumb-init”, “-v”, “–”]  </p><p>CMD [“nginx”, “-g”, “daemon off;”]</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://qingwave.github.io/docker-shell-signal/&quot;&gt;https://qingwave.github.io/docker-shell-signal/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&quot;背景</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>在Kubernetes中实施审计策略</title>
    <link href="http://zhangyu.info/2021/05/31/enforce-audit-policy-in-k8s/"/>
    <id>http://zhangyu.info/2021/05/31/enforce-audit-policy-in-k8s/</id>
    <published>2021-05-30T16:00:00.000Z</published>
    <updated>2021-05-31T12:49:59.507Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://mp.weixin.qq.com/s/samvVSGqTlW0DJ9RGvZckQ">在Kubernetes 中实施审计策略</a></p><blockquote><blockquote><p>作者：Vinod Kumar Nair</p><p>翻译：Bach (K8sMeetup)</p><p>校对：星空下的文仔</p></blockquote><p>如果我们想检查 Kubernetes 生产环境中的以下活动：</p><ol><li><p> 谁登录了 Kubernetes 集群？</p></li><li><p> 哪个服务帐户或用户访问了集群中的哪些资源？</p></li><li><p> 谁创建了 secret 或 configmap？</p></li><li><p> 谁看了 ETCD 的 secrets ，或者其他更多？</p></li></ol><p><strong>那么在 Kubernetes 中执行审计策略（Audit Policy）是非常正确的选择。</strong></p><p><strong>典型的 Kubernetes 环境</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1e9ia4YcKpMMibwFg0B7oGbtf3HFMvdN1R3wm3HPPHB13HXNUiaTK0UU19nKicDP1u2Er5Pmfx4TZfWTicYjSc80jKQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>没有审计策略的 Kubernetes  </p><p><strong>启用后，审计记录将在 kube-apiserver 组件内开始其生命周期。</strong>每个请求在其执行的每个阶段都会生成一个审计事件，然后根据特定策略对其进行预处理，并写入后端。该策略确定记录的内容，后端将保留记录。当前的后端实现包括日志文件和 webhooks。</p><p><strong>每个请求都可以记录一个关联的阶段（stage）。</strong>定义的阶段有：</p><ul><li><p>  <code>RequestReceived</code> - 此阶段对应审计处理器接收到请求后，并且在委托给其余处理器之前生成的事件。</p></li><li><p>  <code>ResponseStarted</code> - 在响应消息的头部发送后，响应消息体发送前生成的事件。只有长时间运行的请求（例如 watch）才会生成这个阶段。</p></li><li><p>  <code>ResponseComplete</code> - 当响应消息体完成并且没有更多数据需要传输的时候。</p></li><li><p>  <code>Panic</code> - 当 panic 发生时生成。</p></li></ul><p><strong>审计策略规则和级别</strong></p><p>审计策略定义了有关应该记录哪些事件以及应包含哪些数据的规则。审核策略对象结构在 <code>audit.k8s.io</code>API 组中定义。<strong>处理事件时，会按顺序将其与规则列表进行比较。第一个匹配规则设置事件的级别（audit levels）。</strong>定义的审核级别有：</p><ul><li><p>  <code>None</code> - 符合这条规则的日志将不会记录。</p></li><li><p>  <code>Metadata</code> - 记录请求的元数据（请求的用户、时间戳、资源、动词等等），但是不记录请求或者响应的消息体。</p></li><li><p>  <code>Request</code> - 记录事件的元数据和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。</p></li><li><p>  <code>RequestResponse</code> - 记录事件的元数据，请求和响应的消息体。这不适用于非资源类型的请求。</p></li></ul><p>下面是一个典型的审计策略文件：</p><h1 id="Log-all-requests-at-the-Metadata-level"><a href="#Log-all-requests-at-the-Metadata-level" class="headerlink" title="Log all requests at the Metadata level."></a>Log all requests at the Metadata level.</h1><p>apiVersion: audit.k8s.io/v1<br>kind: Policy<br>rules:<br>- level: Metadata  </p><p>复杂一点就是：</p><p>apiVersion: audit.k8s.io/v1<br>kind: Policy<br>omitStages:  </p><ul><li><p>“RequestReceived”<br>rules:  </p></li><li><p>level: RequestResponse<br>resources:  </p><ul><li>group: “”<br>resources: [“pods”]  </li></ul></li><li><p>level: Metadata<br>resources:  </p><ul><li>group: “”<br>resources: [“pods/log”, “pods/status”]  </li></ul></li><li><p>level: None<br>resources:  </p><ul><li>group: “”<br>resources: [“configmaps”]<br>resourceNames: [“controller-leader”]  </li></ul></li><li><p>level: None<br>users: [“system:kube-proxy”]<br>verbs: [“watch”]<br>resources:  </p><ul><li>group: “” # core API group<br>resources: [“endpoints”, “services”]  </li></ul></li><li><p>level: None<br>userGroups: [“system:authenticated”]<br>nonResourceURLs:  </p><ul><li>“/api*“ # Wildcard matching.  </li><li>“/version”  </li></ul></li><li><p>level: Request<br>resources:  </p><ul><li>group: “” # core API group<br>resources: [“configmaps”]<br>namespaces: [“kube-system”]  </li></ul></li><li><p>level: Metadata<br>resources:  </p><ul><li>group: “” # core API group<br>resources: [“secrets”, “configmaps”]  </li></ul></li><li><p>level: Request<br>resources:  </p><ul><li>group: “” # core API group  </li><li>group: “extensions” # Version of group should NOT be included.  </li></ul></li><li><p>level: Metadata<br>omitStages:  </p><ul><li>“RequestReceived”</li></ul></li></ul><p><strong>架构-Kubernetes 中的审计策略</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1e9ia4YcKpMMibwFg0B7oGbtf3HFMvdN1R9KOuibkzD2FAb7zpSibyBIBwx7Qibuc8ftHXbuS2AVicpkHViarttwApmgg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>Kubernetes 启用了审计策略  </p><p>我们可以使用 Webhooks 将审核日志发送到文件或远程 Web API。</p><p>在本文中，我们将强制 kube api-server 将审核日志发送到文件。</p><p><strong>在 Kubernetes 中启用审计策略（对于审计日志文件）</strong></p><ol><li> <strong>创建审计策略 YAML 文件</strong>：前往 Kubernetes 集群，并使用以下规则创建 audit-policy.yaml：</li></ol><p>apiVersion: audit.k8s.io/v1<br>kind: Policy<br>rules:   </p><h1 id="Log-the-request-body-of-configmap-changes-in-kube-system"><a href="#Log-the-request-body-of-configmap-changes-in-kube-system" class="headerlink" title="Log the request body of configmap changes in kube-system."></a>Log the request body of configmap changes in kube-system.</h1><ul><li><p>level: Request<br>resources:  </p><ul><li>group: “” # core API group<br>resources: [“configmaps”]<br>namespaces: [“kube-system”]  </li></ul><h1 id="Log-configmap-and-secret-changes-in-all-other-namespaces-at-the-Metadata-level"><a href="#Log-configmap-and-secret-changes-in-all-other-namespaces-at-the-Metadata-level" class="headerlink" title="Log configmap and secret changes in all other namespaces at the Metadata level."></a>Log configmap and secret changes in all other namespaces at the Metadata level.</h1></li><li><p>level: Metadata<br>resources:  </p><ul><li>group: “” # core API group<br>resources: [“secrets”, “configmaps”]  </li></ul><h1 id="A-catch-all-rule-to-log-all-other-requests-at-the-Metadata-level"><a href="#A-catch-all-rule-to-log-all-other-requests-at-the-Metadata-level" class="headerlink" title="A catch-all rule to log all other requests at the Metadata level."></a>A catch-all rule to log all other requests at the Metadata level.</h1></li><li><p>level: Metadata<br>omitStages:  </p><ul><li>“RequestReceived”  </li></ul></li></ul><ol start="2"><li> <strong>更新 kube api-server 清单文件：</strong></li></ol><p>- kube-apiserver<br>    - –advertise-address=10.156.0.6<br>    - –audit-policy-file=/etc/kubernetes/audit-policy.yaml<br>    - –audit-log-path=/var/log/audit.log   </p><hr><ul><li>mountPath: /etc/kubernetes/audit-policy.yaml<br>name: audit<br>readOnly: true<br>— mountPath: /var/log/audit.log<br>name: audit-log<br>readOnly: false<br>-–<br>volumes:  </li><li>name: audit<br>hostPath:<br> path: /etc/kubernetes/audit-policy.yaml<br> type: File  </li><li>name: audit-log<br>hostPath:<br>  path: /var/log/audit.log<br>  type: FileOrCreate  </li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1e9ia4YcKpMMibwFg0B7oGbtf3HFMvdN1RkAmtrPeKpNCZictbkKYRVQL0TfMRsqAocOzULjSjS2KEn9Py3cJTLAw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>k8s-api-server —清单文件  </p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1e9ia4YcKpMMibwFg0B7oGbtf3HFMvdN1RLXZkqic7qsIwsxib441A8nJRKY8hPsS22hriawx2fyGQLQk9C4IBPhdFA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>k8s-api-server —清单文件  </p><p>就这样，转到生成的审计日志文件。</p><p>在这个案例中，这是 audit.log。我们可以看到在阶段级别捕获的有关 Kubernetes 集群的审计日志信息，如以下示例中所示：</p><p>{<br> “kind”:”Event”,<br> “apiVersion”:”audit.k8s.io/v1”,<br> “level”:”Metadata”,<br> “auditID”:”a42fa658-f143–43d8-b5e6–4e101d3e15ea”,<br> “stage”:”ResponseComplete”,<br> “requestURI”:”/api/v1/namespaces/default/secrets?fieldManager=kubectl-create”,<br> “verb”:”create”,<br> “user”:{<br> “username”:”kubernetes-admin”,<br> “groups”:[<br> “system:masters”,<br> “system:authenticated”<br> ]<br> },<br> “sourceIPs”:[<br> “10.156.0.2”<br> ],<br> “userAgent”:”kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19”,<br> “objectRef”:{<br> “resource”:”secrets”,<br> “namespace”:”default”,<br> “name”:”test-secret”,<br> “apiVersion”:”v1”<br> },<br> “responseStatus”:{<br> “metadata”:{  </p><p> },<br> “code”:201<br> },<br> “requestReceivedTimestamp”:”2021–04–03T13:50:37.009656Z”,<br> “stageTimestamp”:”2021–04–03T13:50:38.040874Z”,<br> “annotations”:{<br> “authorization.k8s.io/decision”:”allow”,<br> “authorization.k8s.io/reason”:””<br> }<br>}<br>{<br> “kind”:”Event”,<br> “apiVersion”:”audit.k8s.io/v1”,<br> “level”:”Metadata”,<br> “auditID”:”f1466b01–9b68–45ec-b3bb-b440397f6481”,<br> “stage”:”ResponseComplete”,<br> “requestURI”:”/api/v1/namespaces/default/secrets/test-secret”,<br> “verb”:”get”,<br> “user”:{<br> “username”:”kubernetes-admin”,<br> “groups”:[<br> “system:masters”,<br> “system:authenticated”<br> ]<br> },<br> “sourceIPs”:[<br> “10.156.0.2”<br> ],<br> “userAgent”:”kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19”,<br> “objectRef”:{<br> “resource”:”secrets”,<br> “namespace”:”default”,<br> “name”:”test-secret”,<br> “apiVersion”:”v1”<br> },<br> “responseStatus”:{<br> “metadata”:{  </p><p> },<br> “code”:200<br> },<br> “requestReceivedTimestamp”:”2021–04–03T13:51:08.603724Z”,<br> “stageTimestamp”:”2021–04–03T13:51:08.607716Z”,<br> “annotations”:{<br> “authorization.k8s.io/decision”:”allow”,<br> “authorization.k8s.io/reason”:””<br> }<br>}<br>{<br> “kind”:”Event”,<br> “apiVersion”:”audit.k8s.io/v1”,<br> “level”:”Metadata”,<br> “auditID”:”30be8c70-fda6–44de-8a83–3fe56161d44e”,<br> “stage”:”ResponseComplete”,<br> “requestURI”:”/api/v1/namespaces/default/secrets/test-secret”,<br> “verb”:”get”,<br> “user”:{<br> “username”:”kubernetes-admin”,<br> “groups”:[<br> “system:masters”,<br> “system:authenticated”<br> ]<br> },<br> “sourceIPs”:[<br> “10.156.0.2”<br> ],<br> “userAgent”:”kubectl/v1.20.2 (linux/amd64) kubernetes/faecb19”,<br> “objectRef”:{<br> “resource”:”secrets”,<br> “namespace”:”default”,<br> “name”:”test-secret”,<br> “apiVersion”:”v1”<br> },<br> “responseStatus”:{<br> “metadata”:{  </p><p> },<br> “code”:200<br> },<br> “requestReceivedTimestamp”:”2021–04–03T13:54:57.867317Z”,<br> “stageTimestamp”:”2021–04–03T13:54:57.871369Z”,<br> “annotations”:{<br> “authorization.k8s.io/decision”:”allow”,<br> “authorization.k8s.io/reason”:””<br> }<br>}  </p><p>此外， 我们可以使用以下 kube-apiserver 标志配置 Log 审计后端，来更改审计日志文件的状态：</p><ul><li><p>  <code>--audit-log-maxage</code> 定义保留旧审计日志文件的最大天数。</p></li><li><p>  <code>--audit-log-maxbackup</code> 定义要保留的审计日志文件的最大数量。</p></li><li><p>  <code>--audit-log-maxsize</code> 定义审计日志文件的最大大小（兆字节）。</p></li></ul><p><strong>总结</strong></p><p><strong>审计策略会检查 Kubernetes 集群中发生的所有请求、响应。</strong>这是一个最佳实践，应在早期阶段就启用。在本文示例中，和大家展示了如何将审计数据发送到文件。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/samvVSGqTlW0DJ9RGvZckQ&quot;&gt;在Kubernetes 中实施审计策略&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：Vinod Kumar Nair&lt;</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Cilium网络概述</title>
    <link href="http://zhangyu.info/2021/05/26/Cilium-Network-Overview/"/>
    <id>http://zhangyu.info/2021/05/26/Cilium-Network-Overview/</id>
    <published>2021-05-25T16:00:00.000Z</published>
    <updated>2021-05-26T02:49:26.327Z</updated>
    
    <content type="html"><![CDATA[<p>Cilium 对系统的要求比较高，例如内核的版本要求Linux kernel &gt;= 4.9.17</p><p>受限于eBPF比较新，且需要的内核版本较高，因此目前还没有被kubernetes大规模推广，但该网络方案是一个大趋势。</p><p>目前calico已经支持eBPF模式(不建议生产使用)，且阿里云的Terway插件也是基于eBPF。</p><p>Google 声明GKE将选择 Cilium作为 GKE 网络的数据面V2以便增加其容器安全性和可观测性。</p><p>GKE 使用 Cilium 的声明: <a href="https://cloud.google.com/blog/products/containers-kubernetes/bringing-ebpf-and-cilium-to-google-kubernetes-engine">https://cloud.google.com/blog/products/containers-kubernetes/bringing-ebpf-and-cilium-to-google-kubernetes-engine</a></p><blockquote><p>原文链接：<a href="https://mp.weixin.qq.com/s/NrlxI5uMqQQ3sDrrPSKhZA">https://mp.weixin.qq.com/s/NrlxI5uMqQQ3sDrrPSKhZA</a></p></blockquote><blockquote><p><a href="https://cilium.io/">Cilium</a>是一种开源网络实现方案，与其他网络方案不同的是，Cilium着重强调了其在网络安全上的优势，可以透明的对Kubernetes等容器管理平台上的应用程序服务之间的网络连接进行安全防护。  </p><p>Cilium在设计和实现上，基于Linux的一种新的内核技术<a href="https://mp.weixin.qq.com/s/pPDO4NpDoIblh4taJXVuzw">eBPF</a>，可以在Linux内部动态插入强大的安全性、可见性和网络控制逻辑，相应的安全策略可以在不修改应用程序代码或容器配置的情况下进行应用和更新。  </p><p>Cilium在其官网上对产品的定位称为“API-aware Networking and Security”，因此可以看出，其特性主要包括这三方面：  </p><ol><li> 提供Kubernetes中基本的网络互连互通的能力，实现容器集群中包括Pod、Service等在内的基础网络连通功能；</li><li> 依托eBPF，实现Kubernetes中网络的可观察性以及基本的网络隔离、故障排查等安全策略；</li><li> 依托eBPF，突破传统主机防火墙仅支持L3、L4微隔离的限制，支持基于API的网络安全过滤能力。Cilium提供了一种简单而有效的方法来定义和执行基于容器/Pod身份（Identity Based）的网络层和应用层（比如HTTP/gRPC/Kafka等）安全策略。</li></ol><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>Cilium官方给出了如下的<a href="https://cilium.readthedocs.io/en/stable/concepts/overview/">参考架构</a>，Cilium位于容器编排系统和Linux Kernel之间，向上可以通过编排平台为容器进行网络以及相应的安全配置，向下可以通过在Linux内核挂载eBPF程序，来控制容器网络的转发行为以及安全策略执行。  </p><p><a href="http://dockone.io/uploads/article/20200711/859f9434d6a416bfd931e42490650aae.png"><img src="http://dockone.io/uploads/article/20200711/859f9434d6a416bfd931e42490650aae.png" alt="1.png" title="1.png"></a></p><p><em>图1 Cilium架构</em>  </p><p>在Cilium的架构中，除了Key-Value数据存储之外，主要组件包括Cilium Agent和Cilium Operator，还有一个客户端的命令行工具Cilium CLI。  </p><p>Cilium Agent作为整个架构中最核心的组件，通过DaemonSet的方式，以特权容器的模式，运行在集群的每个主机上。Cilium Agent作为用户空间守护程序，通过插件与容器运行时和容器编排系统进行交互，进而为本机上的容器进行网络以及安全的相关配置。同时提供了开放的API，供其他组件进行调用。  </p><p>Cilium Agent在进行网络和安全的相关配置时，采用eBPF程序进行实现。Cilium Agent结合容器标识和相关的策略，生成eBPF程序，并将eBPF程序编译为字节码，将它们传递到Linux内核。  </p><p><a href="http://dockone.io/uploads/article/20200711/fe83d4296fd13bb110abc64da30ed48b.png"><img src="http://dockone.io/uploads/article/20200711/fe83d4296fd13bb110abc64da30ed48b.png" alt="2.png" title="2.png"></a></p><p><em>图2 Cilium部署架构</em>  </p><p>Cilium Operator 主要负责管理集群中的任务，尽可能的保证以集群为单位，而不是单独的以节点为单位进行任务处理。主要包括，通过etcd为节点之间同步资源信息、确保Pod的DNS可以被Cilium管理、集群NetworkPolicy的管理和更新等。  </p><h3 id="组网模式"><a href="#组网模式" class="headerlink" title="组网模式"></a>组网模式</h3><p>Cilium提供多种组网模式，默认采用基于VXLAN的Overlay组网。除此之外，还包括：  </p><ol><li> 通过BGP路由的方式，实现集群间Pod的组网和互联；</li><li> 在AWS的ENI（Elastic Network Interfaces）模式下部署使用Cilium；</li><li> Flannel和Cilium的集成部署；</li><li> 采用基于ipvlan的组网，而不是默认的基于veth；</li><li> Cluster Mesh组网，实现跨多个Kubernetes集群的网络连通和安全性等多种<a href="https://cilium.readthedocs.io/en/stable/gettingstarted/#advanced-networking">组网模式</a>。</li></ol><p>本文将针对默认的基于vxlan的overlay组网，进行深度的原理和数据包路径分析。  </p><h3 id="Overlay组网"><a href="#Overlay组网" class="headerlink" title="Overlay组网"></a>Overlay组网</h3><p>使用官方给出的yaml文件，通过下述命令，实现Cilium的快速部署。  </p><p>root@u18-161:~# kubectl create -f <a href="https://raw.githubusercontent.com/cilium/cilium/v1.6.5/install/kubernetes/quick-install.yaml">https://raw.githubusercontent.com/cilium/cilium/v1.6.5/install/kubernetes/quick-install.yaml</a>  </p><p>部署成功后，我们可以发现，在集群的每个主机上，启动了一个Cilium Agent（cilium-k54qt，cilium-v7fx4），整个集群启动了一个Cilium Operator（cilium-operator-cdb4d8bb6-8mj5w）。  </p><p>root@u18-161:~# kubectl get pods –all-namespaces -o wide | grep cilium<br>NAMESPACE              NAME                                              READY  STATUS     RESTARTS    AGE           IP                       NODE<br>kube-system    cilium-k54qt                                       1/1     Running     0              80d     192.168.19.161    u18-161<br>kube-system    cilium-v7fx4                                       1/1     Running     0              80d     192.168.19.162    u18-162<br>kube-system    cilium-operator-cdb4d8bb6-8mj5w     1/1    Running     1              80d     192.168.19.162    u18-162      </p><p>在这种默认的组网情况下，主机上的网络发生了以下变化：在主机的root命名空间，新增了如下图所示的四个虚拟网络接口，其中cilium_vxlan主要是处理对数据包的vxlan隧道操作，采用metadata模式，并不会为这个接口分配ip地址；cilium_host作为主机上该子网的一个网关，并且在node-161为其自动分配了IP地址10.244.0.26/32，cilium_net和cilium_host作为一对veth而创建，还有一个lxc_health。  </p><p>在每个主机上，可以进入Cilium Agent，查看其隧道配置。比如进入主机node-161上的Cilium Agent cilium-k54qt，运行cilium bpf tunnel list，可以看到，其为集群中的另一台主机node-162（192.168.19.162）上的虚拟网络10.244.1.0创建了一个隧道。同样在node-162上也有一条这样的隧道配置。  </p><p><a href="http://dockone.io/uploads/article/20200711/73c789ae538f1bba67988459142b6344.jpg"><img src="http://dockone.io/uploads/article/20200711/73c789ae538f1bba67988459142b6344.jpg" alt="3.jpg" title="3.jpg"></a></p><p><em>图3 Cilium默认Overlay组网</em>  </p><p>接下来创建Pod1和Pod2运行于node-161，Pod3和Pod4运行于node-162。其与主机的root命名空间，通过veth-pair连接，如下图所示。  </p><p><a href="http://dockone.io/uploads/article/20200711/2ebac6da02d4feea46447d6f9830708f.jpg"><img src="http://dockone.io/uploads/article/20200711/2ebac6da02d4feea46447d6f9830708f.jpg" alt="4.jpg" title="4.jpg"></a></p><p><em>图4 测试环境组网示例</em>  </p><p>进入Pod1，可以发现，Cilium已经为其分配了IP地址，并且设置了默认的路由，默认路由指向了本机的cilium_host。初始状态Pod内的arp表为空。  </p><p>root@u18-161:~# kubectl exec -it test-1-7cd5798f46-vzf9s  -n test-1 bash<br>root@test-1-7cd5798f46-vzf9s:/# route -n<br>Kernel IP routing table<br>Destination          Gateway                  Genmask                  Flags       Metric    Ref    Use   Iface<br>0.0.0.0                10.244.0.26           0.0.0.0                      UG               0            0        0   eth0<br>10.244.0.26          0.0.0.0                255.255.255.255       UH              0            0        0    eth0<br>root@test-1-7cd5798f46-vzf9s:/# arp<br>root@test-1-7cd5798f46-vzf9s:/#           </p><p>在Pod1中ping Pod2，通过抓包可以发现，Pod发出的ARP请求，其对应的ARP响应直接通过其对端的veth-pair 接口返回（52:c6:5e:ef:6e:97和5e:2d:20:9d:b1:a8是Pod1对应的veth-pair）。这个ARP响应是通过Cilium Agent通过挂载的eBPF程序实现的自动应答，并且将veth-pair对端的MAC地址返回，避免了虚拟网络中的ARP广播问题。  </p><p>No.  Time             Source                       Destination         Protocol  Length           Info<br>133  39.536478  52:c6:5e:ef:6e:97  5e:2d:20:9d:b1:a8    ARP          42       Who has 10.244.0.26  Tell 10.244.0.71<br>134  39.536617  5e:2d:20:9d:b1:a8  52:c6:5e:ef:6e:97    ARP          42       10.244.0.26 is at 5e:2d:20:9d:b1:a8  </p><h4 id="主机内Pod通信"><a href="#主机内Pod通信" class="headerlink" title="主机内Pod通信"></a>主机内Pod通信</h4><p>分析完组网状态之后，那么同一个主机内，两个Pod间通信的情况，就很容易理解了。例如，Pod1向Pod2发包，其数据通路如下图所示Pod1 –&gt; eth0 –&gt; lxc909734ef58f7 –&gt; lxc7c0fcdd49dd0 –&gt; eth0 –&gt; Pod2。  </p><p><a href="http://dockone.io/uploads/article/20200711/262700242ac65b3f7790badcecd3f060.jpg"><img src="http://dockone.io/uploads/article/20200711/262700242ac65b3f7790badcecd3f060.jpg" alt="5.jpg" title="5.jpg"></a></p><p><em>图5 主机内Pod通信路径</em>  </p><h4 id="跨主机Pod通信"><a href="#跨主机Pod通信" class="headerlink" title="跨主机Pod通信"></a>跨主机Pod通信</h4><p>在这种Overlay组网模式下，Pod跨节点之间的通信，通过vxlan实现隧道的封装，其数据路径如下图所示pod1 –&gt; eth0 –&gt; lxc909734ef58f7 –&gt; cilium_vxlan –&gt; eth0(node-161) –&gt; eth0(node-162) –&gt; cilium_vxlan –&gt; lxc2df34a40a888 –&gt; eth0 –&gt; pod3。  </p><p><a href="http://dockone.io/uploads/article/20200711/3d306943aeb25f4a42028f3fafcc8dd1.jpg"><img src="http://dockone.io/uploads/article/20200711/3d306943aeb25f4a42028f3fafcc8dd1.jpg" alt="6.jpg" title="6.jpg"></a></p><p><em>图6 跨主机节点Pod通信路径</em>  </p><p>我们在cilium_vxlan虚拟网络接口上抓包，如下所示。从抓包分析可以看出，Linux内核将Pod1发出的原始数据包发送到cilium_vxlan进行隧道相关的封包、解包处理，然后再将其送往主机的物理网卡eth0。  </p><p><a href="http://dockone.io/uploads/article/20200711/125ed1e32f02eef93549893f8a220eec.png"><img src="http://dockone.io/uploads/article/20200711/125ed1e32f02eef93549893f8a220eec.png" alt="7.png" title="7.png"></a></p><p><em>图7 cilium_vxlan抓包</em>  </p><p>在物理网卡eth0抓包可以发现，Pod1出发的数据包经过cilium_vxlan的封装处理之后，其源目的地址已经变成物理主机node-161和node-162，这是经典的overlay封装。同时，还可以发现，cilium_vxlan除了对数据包进行了隧道封装之外，还将原始数据包进行了TLS加密处理，保障了数据包在主机外的物理网络中的安全性。  </p><p><a href="http://dockone.io/uploads/article/20200711/d0c918edc075885b35df262586d82b63.png"><img src="http://dockone.io/uploads/article/20200711/d0c918edc075885b35df262586d82b63.png" alt="8.png" title="8.png"></a></p><p><em>图8 node-161 eth0抓包</em>  </p><h3 id="API感知的安全性"><a href="#API感知的安全性" class="headerlink" title="API感知的安全性"></a>API感知的安全性</h3><h4 id="安全可视化与分析"><a href="#安全可视化与分析" class="headerlink" title="安全可视化与分析"></a>安全可视化与分析</h4><p>Cilium在1.17版本之后，推出并开源了其网络可视化组件<a href="https://cilium.io/blog/2019/11/19/announcing-hubble/">Hubble</a>，Hubble是建立在Cilium和eBPF之上，以一种完全透明的方式，提供网络基础设施通信以及应用行为的深度可视化，是一个应用于云原生工作负载，完全分布式的网络和安全可观察性平台。  </p><p>Hubble能够利用Cilium提供的eBPF数据路径，获得对Kubernetes应用和服务网络流量的深度可见性。这些网络流量信息可以对接Hubble CLI、UI工具，可以通过交互式的方式快速发现诊断相关的网络问题与安全问题。Hubble除了自身的监控工具，还可以对接像Prometheus、Grafana等主流的云原生监控体系，实现可扩展的监控策略。  </p><p><a href="http://dockone.io/uploads/article/20200711/a8058c725e572c0919972f1e811aecab.png"><img src="http://dockone.io/uploads/article/20200711/a8058c725e572c0919972f1e811aecab.png" alt="9.png" title="9.png"></a></p><p><em>图9 Hubble架构图</em>  </p><p>从上图的架构以及Hubble部署可以看出，Hubble在Cilium Agent之上，以DaemonSet的方式运行自己的Agent，笔者这里的部署示例采用Hubble UI来操作和展示相关的网络以及安全数据。  </p><p>root@u18-163:~# kubectl get pods –all-namespaces -o wide | grep hubble<br>kube-system  hubble-5tvzc                           1/1  Running  16  66d  10.244.1.209  u18-164  <none>  <none><br>kube-system  hubble-k9ft8                           1/1  Running  0   34m  10.244.0.198  u18-163  <none>  <none><br>kube-system  hubble-ui-5f9fc85849-x7lnl  1/1  Running  4   67d  10.244.0.109  u18-163  <none>  <none>  </p><p>依托于Hubble深入的对网络数据和行为的可观察性，其可以为网络和安全运维人员提供以下相关能力：  </p><p>服务依赖关系和通信映射拓扑：比如，可以知道哪些服务之间在相互通信？这些服务通信的频率是多少？服务依赖关系图是什么样的？正在进行什么HTTP调用？服务正在消费或生产哪些Kafka的Topic等。  </p><p>运行时的网络监控和告警：比如，可以知道是否有网络通信失败了？为什么通信会失败？是DNS的问题？还是应用程序得问题？还是网络问题？是在第4层(TCP)或第7层(HTTP)的发生的通信中断等；哪些服务在过去5分钟内遇到了DNS解析的问题？哪些服务最近经历了TCP连接中断或看到连接超时 TCP SYN请求的未回答率是多少 等等。  </p><p>应用程序的监控：比如，可以知道针对特定的服务或跨集群服务，HTTP 4xx或者5xx响应码速率是多少？在我的集群中HTTP请求和响应之间的第95和第99百分位延迟是多少 哪些服务的性能最差 两个服务之间的延迟是什么 等等这些问题。  </p><p>安全可观察性：比如，可以知道哪些服务的连接因为网络策略而被阻塞？从集群外部访问了哪些服务？哪些服务解析了特定的DNS名称？等等。  </p><p><a href="http://dockone.io/uploads/article/20200711/3a5e7effd086ed4d3f395dc461ade074.png"><img src="http://dockone.io/uploads/article/20200711/3a5e7effd086ed4d3f395dc461ade074.png" alt="10.png" title="10.png"></a></p><p><em>图10 Hubble界面功能</em>  </p><p>从上图Hubble的界面，我们可以简单的看出其部分功能和数据，比如，可以直观的显示出网路和服务之间的通信关系，可以查看Flows的多种详细数据指标，可以查看对应的安全策略情况，可以通过namespace对观测结果进行过滤等等。  </p><h4 id="微隔离"><a href="#微隔离" class="headerlink" title="微隔离"></a>微隔离</h4><p>默认情况下，Cilium与其他网络插件一样，提供了整个集群网络的完全互联互通，用户需要根据自己的应用服务情况设定相应的安全隔离策略。如下图所示，每当用户新创建一个Pod，或者新增加一条安全策略，Cilium Agent会在主机对应的虚拟网卡驱动加载相应的eBPF程序，实现网络连通以及根据安全策略对数据包进行过滤。比如，可以通过采用下面的NetworkPolicy实现一个基本的L3/L4层网络安全策略。  </p><p>apiVersion: “cilium.io/v2”<br>kind: CiliumNetworkPolicy<br>description: “L3-L4 policy to restrict deathstar access to empire ships only”<br>metadata:<br>name: “rule1”<br>spec:<br>endpointSelector:<br>    matchLabels:<br>        org: empire<br>     class: deathstar<br>ingress:  </p><ul><li>fromEndpoints:    </li><li>matchLabels:  <pre><code>     org: empire  </code></pre>toPorts:  </li><li>ports:  <ul><li>port: “80”<br> protocol: TCP       </li></ul></li></ul><p><a href="http://dockone.io/uploads/article/20200711/2d97601bb58407968f11799252dbd6a4.png"><img src="http://dockone.io/uploads/article/20200711/2d97601bb58407968f11799252dbd6a4.png" alt="11.png" title="11.png"></a></p><p><em>图11 Cilium网络隔离方案示意图</em>  </p><p>然而，在微服务架构中，一个基于微服务的应用程序通常被分割成一些独立的服务，这些服务通过API（使用HTTP、gRPC、Kafka等轻量级协议）实现彼此的通信。因此，仅实现在L3/L4层的网络安全策略，缺乏对于微服务层的可见性以及对API的细粒度隔离访问控制，在微服务架构中是不够的。  </p><p>我们可以看如下这个例子，Job Postings这个服务暴露了其服务的健康检查、以及一些增、删、改、查的API。Gordon作为一个求职者，需要访问Job Postings提供的Jobs相关信息。按照传统的L3/L4层的隔离方法，可以通过iptables -s 10.1.1.1 -p tcp –dport 80 -j ACCEPT，允许Gordon来访问Job Postings在80端口提供的HTTP服务。但是这样的网络规则，导致Gordon同样可以访问包括发布信息、修改信息、甚至是删除信息等其他接口。这样的情况肯定是我们的服务设计者所不希望发生的，同时也存在着严重的安全隐患。  </p><p><a href="http://dockone.io/uploads/article/20200711/3b6fe1bb1ad7433a96315e782b4b51fa.png"><img src="http://dockone.io/uploads/article/20200711/3b6fe1bb1ad7433a96315e782b4b51fa.png" alt="12.png" title="12.png"></a></p><p><em>图12 L7微隔离示例</em>  </p><p>因此，实现微服务间的L7层隔离，实现其对应的API级别的访问控制，是微服务网络微隔离的一个重要部分。Cilium在为Docker和Kubernetes等基于Linux的容器框架提供了支持API层面的网络安全过滤能力。通过使用eBPF，Cilium提供了一种简单而有效的方法来定义和执行基于容器/pod身份的网络层和应用层安全策略。我们可以通过采用下面的NetworkPolicy实现一个L7层网络安全策略。  </p><p><a href="http://dockone.io/uploads/article/20200711/989a29fe05e37ace4af2547f7bac62f3.png"><img src="http://dockone.io/uploads/article/20200711/989a29fe05e37ace4af2547f7bac62f3.png" alt="13.png" title="13.png"></a></p><p><em>图13 Cilium实现微服务安全</em>  </p><p>apiVersion: “cilium.io/v2”<br>kind: CiliumNetworkPolicy<br>description: “L7 policy to restrict access to specific HTTP call”<br>metadata:<br>name: “rule1”<br>spec:<br> endpointSelector:<br>     matchLabels:<br>      org: empire<br>     class: deathstar<br>ingress:  </p><ul><li>fromEndpoints:  <ul><li>matchLabels:<br>   org: empire<br>toPorts:  </li><li>ports:  <ul><li>port: “80”<br>protocol: TCP<br>rules:<br>   http:  <ul><li>method: “POST”<br> path: “/v1/request-landing”  </li></ul></li></ul></li></ul></li></ul><p>Cilium还提供了一种基于Proxy的实现方式，可以更方便的对L7协议进行扩展。如下图所示，Cilium Agent采用eBPF实现对数据包的重定向，将需要进行过滤的数据包首先转发至Proxy代理，Proxy代理根据其相应的过滤规则，对收到的数据包进行过滤，然后再将其发回至数据包的原始路径，而Proxy代理进行过滤的规则，则通过Cilium Agent进行下发和管理。  </p><p>当需要扩展协议时，只需要在Proxy代理中，增加对新协议的处理解析逻辑以及规则处置逻辑，即可实现相应的过滤能力。  </p><p><a href="http://dockone.io/uploads/article/20200711/4f823d0a351a00b903d25e113f10add9.png"><img src="http://dockone.io/uploads/article/20200711/4f823d0a351a00b903d25e113f10add9.png" alt="14.png" title="14.png"></a></p><p><em>图14 L7层访问控制协议扩展原理图</em>  </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Cilium是一个基于eBPF和XDP的高性能网络方案，本文着重介绍了其原理以及默认的overlay组网通信。除了基本的网络通信能力外，Cilium还包含了基于eBPF的负载均衡能力，L3/L4/L7的安全策略能力等相关的内容，后续会进行更详细的实践分析。  </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Cilium 对系统的要求比较高，例如内核的版本要求Linux kernel &amp;gt;= 4.9.17&lt;/p&gt;
&lt;p&gt;受限于eBPF比较新，且需要的内核版本较高，因此目前还没有被kubernetes大规模推广，但该网络方案是一个大趋势。&lt;/p&gt;
&lt;p&gt;目前calico已经支</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Cilium" scheme="http://zhangyu.info/tags/Cilium/"/>
    
  </entry>
  
  <entry>
    <title>快速生成k8s的yaml配置的4种方法</title>
    <link href="http://zhangyu.info/2021/05/19/generating-yaml-for-k8s/"/>
    <id>http://zhangyu.info/2021/05/19/generating-yaml-for-k8s/</id>
    <published>2021-05-18T16:00:00.000Z</published>
    <updated>2021-05-19T07:34:35.208Z</updated>
    
    <content type="html"><![CDATA[<p>[快速生成kubernetes(k8s)的yaml配置的4种方法](<a href="https://www.toutiao.com/i6952422377816965639/">https://www.toutiao.com/i6952422377816965639/</a> wid=1621396706552)</p><blockquote><h1 id="快速生成k8s的yaml配置的4种方法"><a href="#快速生成k8s的yaml配置的4种方法" class="headerlink" title="快速生成k8s的yaml配置的4种方法"></a>快速生成k8s的yaml配置的4种方法</h1><p>1、通过kubectl命令行快速生成一个deployment及service的yaml标准配置</p><pre><code>#我们在后面加上`--dry-run -o yaml`--dry-run代表这条命令不会实际在K8s执行，-o yaml是会将试运行结果以yaml的格式打印出来，这样我们就能轻松获得yaml配置了# kubectl create deployment nginx --image=nginx --dry-run -o yaml       apiVersion: apps/v1     # &lt;---  apiVersion 是当前配置格式的版本kind: Deployment     #&lt;--- kind 是要创建的资源类型，这里是 Deploymentmetadata:        #&lt;--- metadata 是该资源的元数据，name 是必需的元数据项  creationTimestamp: null  labels:    app: nginx  name: nginxspec:        #&lt;---    spec 部分是该 Deployment 的规格说明  replicas: 1        #&lt;---  replicas 指明副本数量，默认为 1  selector:    matchLabels:      app: nginx  strategy: &#123;&#125;  template:        #&lt;---   template 定义 Pod 的模板，这是配置文件的重要部分    metadata:        #&lt;---     metadata 定义 Pod 的元数据，至少要定义一个 label。label 的 key 和 value 可以任意指定      creationTimestamp: null      labels:        app: nginx    spec:           #&lt;---  spec 描述 Pod 的规格，此部分定义 Pod 中每一个容器的属性，name 和 image 是必需的      containers:      - image: nginx        name: nginx        resources: &#123;&#125;status: &#123;&#125;# 基于上面的deployment服务生成service的yaml配置# kubectl expose deployment nginx --port=80 --target-port=80 --dry-run -o yamlapiVersion: v1kind: Servicemetadata:  creationTimestamp: null  labels:    app: nginx  name: nginxspec:  ports:  - port: 80    protocol: TCP    targetPort: 80  selector:    app: nginxstatus:  loadBalancer: &#123;&#125;</code></pre><p>2、利用helm查看各种官方标准复杂的yaml配置以供参考</p><pre><code># 以查看rabbitmq集群安装的配置举例# 首先添加chart仓库helm repo add aliyun-apphub https://apphub.aliyuncs.comhelm repo update# 这里我们在后面加上 --dry-run --debug 就是模拟安装并且打印输出所有的yaml配置helm install -n rq rabbitmq-ha aliyun-apphub/rabbitmq-ha --dry-run --debug </code></pre><p>3、将docker-compose转成k8s的yaml格式配置</p><pre><code># 下载二进制包# https://github.com/kubernetes/kompose/releases# 开始转发yaml配置./kompose-linux-amd64 -f docker-compose.yml convert</code></pre><p>4、docker命令输出转换成对应的yaml文件示例</p><pre><code>这里以 Prometheus Node Exporter 为例演示如何运行自己的 DaemonSet。Prometheus 是流行的系统监控方案，Node Exporter 是 Prometheus 的 agent，以 Daemon 的形式运行在每个被监控节点上。如果是直接在 Docker 中运行 Node Exporter 容器，命令为：docker run -d \    -v &quot;/proc:/host/proc&quot; \    -v &quot;/sys:/host/sys&quot; \      -v &quot;/:/rootfs&quot; \    --net=host \      prom/node-exporter \      --path.procfs /host/proc \      --path.sysfs /host/sys \      --collector.filesystem.ignored-mount-points &quot;^/(sys|proc|dev|host|etc)($|/)&quot;将其转换为 DaemonSet 的 YAML 配置文件 node_exporter.yml：apiVersion: extensions/v1beta1kind: DaemonSetmetadata:  name: node-exporter-daemonsetspec:  template:    metadata:      labels:        app: prometheus    spec:      hostNetwork: true      # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 1 直接使用 Host 的网络      containers:      - name: node-exporter        image: prom/node-exporter        imagePullPolicy: IfNotPresent        command:             # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 2 设置容器启动命令        - /bin/node_exporter        - --path.procfs        - /host/proc        - --path.sysfs        - /host/sys        - --collector.filesystem.ignored-mount-points        - ^/(sys|proc|dev|host|etc)($|/)        volumeMounts:        # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 3 通过Volume将Host路径/proc、/sys 和 / 映射到容器中        - name: proc          mountPath: /host/proc        - name: sys          mountPath: /host/sys        - name: root          mountPath: /rootfs      volumes:      - name: proc        hostPath:          path: /proc      - name: sys        hostPath:          path: /sys      - name: root        hostPath:          path: /</code></pre></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[快速生成kubernetes(k8s)的yaml配置的4种方法](&lt;a href=&quot;https://www.toutiao.com/i6952422377816965639/&quot;&gt;https://www.toutiao.com/i6952422377816965639/&lt;/</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>使用fklek搭建Kubernetes日志收集工具栈</title>
    <link href="http://zhangyu.info/2021/04/28/fklek-to-k8s-log/"/>
    <id>http://zhangyu.info/2021/04/28/fklek-to-k8s-log/</id>
    <published>2021-04-27T16:00:00.000Z</published>
    <updated>2021-04-28T05:58:46.365Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>使用 Fluentd+Kafka+Logstash+Elasticsearch+Kibana搭建 Kubernetes 日志收集工具栈</p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s/lPeYavvFJ6GdivkT0iwTGw">https://mp.weixin.qq.com/s/lPeYavvFJ6GdivkT0iwTGw</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;使用 Fluentd+Kafka+Logstash+Elasticsearch+Kibana搭建 Kubernetes 日志收集工具栈&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.we</summary>
      
    
    
    
    <category term="日志" scheme="http://zhangyu.info/categories/%E6%97%A5%E5%BF%97/"/>
    
    
    <category term="日志" scheme="http://zhangyu.info/tags/%E6%97%A5%E5%BF%97/"/>
    
  </entry>
  
  <entry>
    <title>只有黑话，才能拯救互联网人</title>
    <link href="http://zhangyu.info/2021/04/27/%E5%8F%AA%E6%9C%89%E9%BB%91%E8%AF%9D%E6%89%8D%E8%83%BD%E6%8B%AF%E6%95%91%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA/"/>
    <id>http://zhangyu.info/2021/04/27/%E5%8F%AA%E6%9C%89%E9%BB%91%E8%AF%9D%E6%89%8D%E8%83%BD%E6%8B%AF%E6%95%91%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA/</id>
    <published>2021-04-26T16:00:00.000Z</published>
    <updated>2021-04-27T05:40:09.445Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>只有黑话，才能拯救互联网人 </p></blockquote><blockquote><p>原创 东半球第二正经の 牛顿顿顿 4月8日 </p></blockquote><blockquote><p>最近，一鸣同学在字节年会上怒斥了公司报告中的 “ 互联网黑话 “ 现象，我看了一眼他举的例子，整个人当场就被生态化反了：</p><p>给大家念一段我用咱们双月会材料里摘出来的词，拼凑出来的一段话：</p><p>过去我们主要依靠推荐技术赋予的信息分发能力、跨端联动抖头西、分多个产品自研，实现深度共建，形成组合拳，打造内容生态闭环，以此赋能客户用户创造价值。未来我们要增加横向不同场景价值，延长服 务链路。同时纵深满足用户需求，借助人类年龄的自然势能，在小中青多个年龄用户深度渗透。另外通过加强基建投入，多种阵地相关产品完善经营价值链路，建立对外用户持久影响力。</p><p>黑话不是没见过，但这种级别的，堪称 “ 互联网黑神话 “，玉皇大帝来了也只能喊一声：</p><p>你这个颗粒度的输出，很难击穿我的心智啊。</p><p>为了发力，拆解，抨击这个现象，我和阿半紧张的孵化了一整宿，通过增强耦合性，我们一起进行了串联，拉通，输出，为这篇稿子赋能。</p><p><img src="http://zkres1.myzaker.com/202104/606e7dfd8e9f09581f743dbc_1024.jpg"></p><p>我对阿半个说，写完稿子咱们就联动，我喜欢在后面发力，你最好给我一个抓手，我要牢抓你的两个突出点，两手抓两手都要硬，我们对齐水位，聚焦痛点，打通要害，完成闭环，你要提高感知度，我们一起达到引爆点。</p><p>阿半当时脸就红了，发出了灵魂的拷问：</p><p>那个务实、奋进、用结果说话，承载着年轻人光荣与梦想的互联网死哪儿去了？</p><p><img src="http://zkres2.myzaker.com/202104/606e7dfd8e9f09581f743dbd_1024.jpg"></p><p><strong>1</strong></p><p><strong>为什么这些互联网人在职场不讲人话，整天搞这些 “ 务虚 “ 的八股文 “ 黑话 “？</strong></p><p><strong>因为，黑话是最好的 “ 职场遮羞布 “。</strong></p><p>大厂今天 OKR，明天 361，大搞周报文化，甚至早有早报，晚有晚报，月有月报，仔细一看，全是福报。</p><p>原来是为了汇报工作写 PPT，现在为了写 PPT 写 PPT。</p><p>入行互联网，别的没学会，先成了 PPT 精装师，模板搬运工。</p><p>但问题是，大部分人的工作价值感是很低的，一周搞不出什么东西来，很多人既没有 Power 也没有 Point。</p><p>那么如何让自己平庸的，低效的无价值工作，听上去，稍微有那么点价值和深度呢？</p><p>这时候就需要点专业的 “ 黑话 “ 来装点门面了。</p><p>我今天虽然别的没干，<strong>找人修了个打印机</strong>，但只要用黑话包装一下，就成了：</p><h2 id="“-联动协同多部门，多维度发力支撑办公场景，打造硬件故障紧急处理-SOP，提前布局对接团队，以易用性组合拳为抓手，提高团队运作效率。”"><a href="#“-联动协同多部门，多维度发力支撑办公场景，打造硬件故障紧急处理-SOP，提前布局对接团队，以易用性组合拳为抓手，提高团队运作效率。”" class="headerlink" title="“ 联动协同多部门，多维度发力支撑办公场景，打造硬件故障紧急处理 SOP，提前布局对接团队，以易用性组合拳为抓手，提高团队运作效率。”"></a>“ 联动协同多部门，多维度发力支撑办公场景，打造硬件故障紧急处理 SOP，提前布局对接团队，以易用性组合拳为抓手，提高团队运作效率。”</h2><p>装神弄鬼，故弄玄虚，把简单的东西讲复杂还不容易吗？</p><p>2</p><p>大厂里那些大大小小的领导，嘴里的黑话也越来越高了。</p><p>为什么？</p><p>别觉得当上了领导，就有真本事，输出个 “ 洞察 “：</p><p>这几年的互联网人，尤其是大厂的中层，草包含量越来越高。</p><p>越是废物，越热衷拼凑这些，晦涩难懂，佶屈聱牙，专业色彩浓厚的 “ 术语 “ 为自己 “ 赋魅 “。</p><p>他们自己都没意识到，能当上领导，完全不是因为能力强，只是运气好，沾上了时代尿频一样洒下的红利。</p><p>很多中层高 P，无非就是出生早一点，毕业早一点，在早期混进了大厂。</p><p>大厂也有年功序列制，公司高速成长，新的管理岗缺口自然就会出现，哪怕做不出什么成绩来，只要你苟的聪明，同届有本事的人跳出去闯了，那就是剩者为王。</p><p>老人们的职场，就像坐电梯往上升，就算你是条狗，跟着走，也能干成保卫科科长。</p><p><strong>这就是时代红利滋中了你。</strong></p><p><img src="http://zkres1.myzaker.com/202104/606e7dfd8e9f09581f743dbf_1024.jpg"></p><p>不管是时代的红利也好，空降兵也好，外行领导内行也好，越来越多的草包领导们面临同一个困境：</p><p>不够屌，怎么才能装逼呢？</p><p>如果完全不专业，那至少也要想办法让自己 “ 看起来 “ 专业。</p><p>赋魅的本质，就是包装。</p><p>搓澡不叫搓澡，叫人体表皮组织研究；</p><p>搬砖不叫搬砖，叫研究物质空间位置转移的科研项目；</p><p>贴膜不叫贴膜，叫智能数字通讯设备表面高分子化合物平面处理；</p><p><strong>互联网黑话是对酒囊饭袋最好的包装。</strong></p><p><img src="http://zkres1.myzaker.com/202104/606e7dfd8e9f09581f743dc0_1024.jpg"></p><p>3</p><p><strong>更重要的是：务虚是最安全的。</strong></p><p>这几年一个趋势，大佬们也不爱讲真话了。</p><p>其实，早些年，很多互联网大佬其实还是讲人话的，不仅讲，还十分爱讲。</p><p>一鸣同学当年喜欢出来谈延迟满足，王兴在写了一万多条饭否传经布道，黄峥甚至开公众号写连载，当年，讲的都是大实话。</p><p>然后呢？</p><p>“ 悔创阿里 “” 不知妻美 “” 一无所有 “” 普通家庭 “” 名下无房 “” 顺便挣钱 “</p><p>实话很快遭到了反噬，各路媒体围观解读，挖坟打脸。</p><p>难看吗？很难看。</p><p>这个时候，老一辈的生存哲学才发出了闪闪的光芒。</p><p>《是，大臣》里面有句名台词 “ 如果人们不知道你在干什么，他们就不知道你做错了什么。”</p><p><img src="http://zkres2.myzaker.com/202104/606e7dfd8e9f09581f743dc1_1024.jpg"></p><p>多说多错，少说少错，不说不错。</p><p>非要你讲呢？那就尽量让说了和没说一样。</p><p>黑话，闪闪发光。</p><p>互联网黑话只是一种职场文化，或者说是生存哲学吗？</p><p>不，黑话的背后，隐藏着一个冰冷的现实：</p><p><strong>互联网来到了 “ 守成时代 “。</strong></p><p>群雄逐鹿，尘埃将落。</p><p>各个赛道上的大厂已经划完了地盘，站稳了脚跟，那些最暴利的业务早就被大厂吃干抹净。</p><p>甚至连不怎么挣钱，曾经让他们不屑一顾的边缘业务都成了大厂的爪牙们争夺的焦点。</p><p>扩张逼近边界，格局固化，增量见顶。</p><p><strong>那个曾经改变无数普通年轻人命运窗口要关闭了，一个充满光荣与梦想的时代要落幕了。</strong></p><p>我国的互联网普及率，用了不到 15 年的时间，就从 2006 年的 10% 增长到了 70%。</p><p>在这 15 年里，无数怀着理想的年轻人涌进来。</p><p>互联网一度是年轻人的理想国。</p><p>你讨厌传统行业里的蝇营狗苟你可以来这里；</p><p>现实里一无所有，迷惘无依，你可以来这里；</p><p>不需要你有一个区长父亲，不需要你有一个企业家母亲，有一腔热血，这里就有梦可以给你做。</p><p>然后一代年轻人用青春和热血敲下一行行代码，开发出一个又一个改变我们生活的应用，用 996 和青春饲喂出一个又一个世界级的互联网巨鳄和独角兽。</p><p>然后，没有了然后。</p><p>存量和守成的时代，是一个不说人话和逆向淘汰的时代。</p><p><strong>“Talk is cheap， Show me the code” 的时代结束了。</strong></p><p><strong>那个普通人可以闪闪发光的时代也结束了。</strong></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;只有黑话，才能拯救互联网人 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;原创 东半球第二正经の 牛顿顿顿 4月8日 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;最近，一鸣同学在字节年会上怒斥了公司</summary>
      
    
    
    
    <category term="胡说八道" scheme="http://zhangyu.info/categories/%E8%83%A1%E8%AF%B4%E5%85%AB%E9%81%93/"/>
    
    
    <category term="黑话" scheme="http://zhangyu.info/tags/%E9%BB%91%E8%AF%9D/"/>
    
  </entry>
  
  <entry>
    <title>漫画一个NB互联网项目的上线过程</title>
    <link href="http://zhangyu.info/2021/04/27/%E6%BC%AB%E7%94%BB%E4%B8%80%E4%B8%AANB%E4%BA%92%E8%81%94%E7%BD%91%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%B8%8A%E7%BA%BF%E8%BF%87%E7%A8%8B/"/>
    <id>http://zhangyu.info/2021/04/27/%E6%BC%AB%E7%94%BB%E4%B8%80%E4%B8%AANB%E4%BA%92%E8%81%94%E7%BD%91%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%B8%8A%E7%BA%BF%E8%BF%87%E7%A8%8B/</id>
    <published>2021-04-26T16:00:00.000Z</published>
    <updated>2022-03-08T15:16:48.955Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>##漫画 一个NB互联网项目的上线过程 </p></blockquote><blockquote><p>作者 | 苏南</p><p>来源 | 前端布道师（ID：honeyBadger8）</p><p><img src="https://n.sinaimg.cn/sinakd2021425s/581/w900h481/20210425/e8b5-kphwumq8343170.png"></p><p>今天这篇漫画讲述的是一个大型项目，从需求诞生到进入实际开发、对外发布的过程…</p><p>本期漫画情节纯属虚构</p><p>如有雷同，纯属巧合.</p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/b70e-kphwumq8343249.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/27d1-kphwumq8343246.png"></p><p><img src="https://n.sinaimg.cn/sinakd20210425s/684/w1080h2004/20210425/d0d7-kphwumq8344663.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/8a74-kphwumq8343377.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/2d60-kphwumq8343759.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/676/w1080h1996/20210425/8b99-kphwumq8343760.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/d91d-kphwumq8343841.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/eefa-kphwumq8343844.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/682/w1080h2002/20210425/0ba7-kphwumq8343932.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/9062-kphwumq8343931.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/680/w1080h2000/20210425/bb28-kphwumq8344127.png"></p><p><img src="https://n.sinaimg.cn/sinakd2021425s/34/w1080h554/20210425/5de5-kphwumq8344119.png"></p><p>我们经常会看到某某知名互联网公司开产品发布会，又或者某些分享会上，演讲者常常会提到我们公司的项目致力于解决用户的某一痛点，并且采用了当下最顶尖的云服务部署、微服务架构、模块化开发、大数据精准分析等互联网黑话，但这些华丽的外表背后真的如此吗？</p><p>可能也只有身在其中的程序员才最清楚了吧，当然也不排除部分公司确实做的非常好。但是绝大部分的公司都很难做到流程标准化，项目长远规划，都只是一味的追求快，快、意味着时间的减少，而程序员前期的规划、架构也随之被抛在脑后，为了完成需求编码而编码，压根没有时间去思考项目长远的开发问题…</p><p>话说，你工作中的项目状态是如画中的王大拿这样的吗？</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;##漫画 一个NB互联网项目的上线过程 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;作者 | 苏南&lt;/p&gt;
&lt;p&gt;来源 | 前端布道师（ID：honeyBadger8）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://n</summary>
      
    
    
    
    <category term="胡说八道" scheme="http://zhangyu.info/categories/%E8%83%A1%E8%AF%B4%E5%85%AB%E9%81%93/"/>
    
    
    <category term="上线" scheme="http://zhangyu.info/tags/%E4%B8%8A%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>Pulsar vs Kafka，CTO 如何抉择</title>
    <link href="http://zhangyu.info/2021/04/24/comparing-pulsar-and-kafka-from-a-ctos-point-of-view/"/>
    <id>http://zhangyu.info/2021/04/24/comparing-pulsar-and-kafka-from-a-ctos-point-of-view/</id>
    <published>2021-04-23T16:00:00.000Z</published>
    <updated>2021-04-25T08:20:42.115Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi">https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi</a></p></blockquote><blockquote><p>作者 | Jesse Anderson<br>译者 | Sijia<br><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1619338804&amp;ver=3029&amp;signature=LstPwE5cJ7yKmBlTCfazpUH3EqJ3tPgHscCjhRBoc1lJ1IHTlI57lOKONxCKmfLeWlqDAKdaKutZXVdxm3XEVFDvKt2z3fNX6eoKV0qMt">https://mp.weixin.qq.com/s?src=11&amp;timestamp=1619338804&amp;ver=3029&amp;signature=LstPwE5cJ7yKmBlTCfazpUH3EqJ3tPgHscCjhRBoc1lJ1IHTlI57lOKONxCKmfLeWlqDAKdaKutZXVdxm3XEVFDvKt2z3fNX6eoKV0qMt</a><br>在评估新技术时，高层管理人员的视角通常与中层管理人员、架构师、数据工程师等有所不同。高层管理人员不仅要关注基准测试结果、产品支持的特性，还要从长远角度考虑新技术的可靠性，新技术能够为企业带来哪些竞争优势，以及是否可以缩短上市时间、节约开销。</p><p>我是 Big Data Institute 的常务董事，技术评估是我的一项主要工作。我们帮助企业根据业务需求选择并落地最合适的技术。我们不与供应商合作，因此客户尤为看中我们能够客观地评估不同的技术。</p><p>在本文中，我将从 CTO 的视角出发，对比 Apache Pulsar 和 Apache Kafka。只进行理论上的对比空洞无效，也不能帮助我们作出决策，实际用例才真正值得参考。所以，在本文中，我会通过一些常见的实际使用场景来对比 Pulsar 和 Kafka，即简单消息使用场景、复杂消息使用场景和高级消息使用场景。在这些实际使用场景下，Pulsar 和 Kafka 的表现能够帮助我们更好地理解二者的性能和优势，进而作出决策。</p><h1 id="简单消息使用场景"><a href="#简单消息使用场景" class="headerlink" title="简单消息使用场景"></a>简单消息使用场景</h1><p>假设有一个企业，之前从未使用过消息系统，现在需要通过一个简单的消息系统，将消息从位置 A 发送到位置 B，但不需要复制消息。</p><p>数据架构师团队在深入研究 Pulsar 和 Kafka 的业务案例后，得出如下结论：在这一使用场景中，Pulsar 和 Kafka 都没有绝对优势。并且，他们认为在短时间内，该使用场景基本不会发生改变。</p><p>对于类似这样的简单消息使用场景而言，我也赞同 Pulsar 和 Kafka 都没有绝对优势。仅从技术角度出发，Pulsar 和 Kafka 这一回合打成平局，那么我们只能考虑成本。二者的运营成本、员工培训成本分别是多少？我打算根据 Kafka 或 Pulsar 的服务提供商的收费标准进行对比。对比开销时，选好服务提供商也可以在一定程度上减少运营成本和员工培训成本。Kafka 的云服务提供商，我参考了使用 <a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview">Kafka API</a>（Azure）的 <a href="https://www.confluent.io/confluent-cloud/">Confluent Cloud</a>、<a href="https://aws.amazon.com/msk/">MSK</a>（AWS）和 <a href="https://azure.microsoft.com/en-us/services/event-hubs/">Event Hubs</a>。Pulsar 的云服务提供商，我选择 <a href="https://streamnative.io/zh/cloud/hosted">StreamNative Cloud</a>。</p><h2 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h2><p>出于稳妥考虑，我们决定选择 Kafka API。目前，已有多种技术支持非 Kafka broker 使用 Kafka API 或传输协议。使用 Kafka API，非 Kafka broker 可通过添加新库支持 Kafka 的传输协议，保证对 Kafka API 的兼容性，从而最大化技术选择的多样性。例如，可以通过修改 Kafka API 的实现重新编译或通过 Pulsar broker 解析 Kafka 的协议（KOP），将 Pulsar 用作 Kafka 的后端。</p><p>我们在对比单位成本后，选择了成本效益高的一方。Kafka API 可以保证后端质量，用户在后端之间的数据移动不会受到影响，有效规避风险。即使社区不活跃，技术热度不高，我们的使用也不会受到影响。</p><h1 id="复杂消息使用场景"><a href="#复杂消息使用场景" class="headerlink" title="复杂消息使用场景"></a>复杂消息使用场景</h1><p>假设一个公司需要复杂消息系统。由于需要处理世界各地的数据，必须支持跨地域复制。该企业一直在使用消息系统，因此对实时系统的复杂性有一定的了解，也发现了当前消息系统的不足之处。因此该企业对消息系统的要求是能够处理高级的消息传递和复杂的消息特性。</p><p>数据架构师团队和股东以及业务部门详细讨论了当前和未来需求。最后得出的结论是，Pulsar 和 Kafka 各有优势。同时，他们认为随着时间的推移，该使用场景和数据量都会有所增长。</p><p>在这种情况下，Pulsar 和 Kafka 难分胜负。要想作出正确决策，必须深入研究二者的使用场景。</p><h2 id="跨地域复制"><a href="#跨地域复制" class="headerlink" title="跨地域复制"></a>跨地域复制</h2><p>Kafka 既提供私有的（价格高）跨地域复制，也提供开源的（附加服务）跨地域复制解决方案。私有的跨地域复制解决方案为其内置特性，但价格高昂。开源的解决方案（MirrorMaker）实际上就是数据复制，但由于不是其内置特性，会增加运营开销。</p><p>Pulsar 提供开源内置的跨地域复制特性，支持复杂的复制策略。对于使用场景和数据量都在增加的企业而言，显然，支持内置跨地域复制策略的 Pulsar 完胜。</p><p>就跨地域复制而言，我们选择 Pulsar。</p><h2 id="复杂消息"><a href="#复杂消息" class="headerlink" title="复杂消息"></a>复杂消息</h2><p>由于企业正在向新消息平台迁移，消息系统最好可以处理新使用场景。数据架构师团队一直在了解各个平台，尝试寻找最佳解决方案。在当前使用的消息系统中，一旦出现处理错误，必须重新生成消息，再手动重试，因此最好还可以引入消息延迟发送。另外，当前消息系统的 schema 实施功能也有待加强，各个团队选择不同的 schema 实现时，团队合作的难度显著增加。</p><p>Kafka 没有内置死信队列特性，一旦消息处理失败，必须手动处理，或修改代码重试。Kafka 也没有延迟发送消息的内置机制，延迟发送消息流程复杂、工作量大。另外，Kafka 没有内置 schema 实施机制，导致云服务提供商分别提供了不同的 schema 解决方案。</p><p>Pulsar 内置死信队列特性，当消息处理失败，收到否认 ack 时，Pulsar 可以自动重试，但次数有限。Pulsar 也支持延迟发送消息，可以设定延迟时间。对于 Pulsar 而言，schema 级别高，因此 Pulsar 有内置 schema 注册，Pulsar API 也原生支持 schema。</p><p>就复杂消息而言，我们选择 Pulsar。</p><h2 id="高级消息传递"><a href="#高级消息传递" class="headerlink" title="高级消息传递"></a>高级消息传递</h2><p>随着对架构的深入了解，我们发现为了确保均匀分配资源，需要循环发送同一 topic 上的数据，并且需要通过排序确保消息有序排列。</p><p>Kafka 不能分发消息给指定的 consumer。当 consumer 接收到不属于它消费的消息时，要保证这些消息被正确消费，我们只能重新发送这些消息到额外的 topic 中，但这样会造成数据冗余，增加使用成本。因此，我们需要可以制定路由规则发送给指定 consumer 的产品。</p><p>Pulsar broker 可以通过制定的路由规则，把一个 topic 的不同消息根据路由规则发送到指定的 consumer 中。Pulsar broker 轻松实现了我们的目标，无需任何额外工作。</p><p>就高级消息传递而言，我们选择 Pulsar。</p><h2 id="部署和社区"><a href="#部署和社区" class="headerlink" title="部署和社区"></a>部署和社区</h2><p>为了全面比较 Pulsar 和 Kafka，我们还需要看一下二者的部署数量和社区概况。</p><p>从服务市场来看，Kafka 的提供商更多，销售和支持 Kafka 产品的团队也更多。Kafka 和 Pulsar 的开源社区都积极活跃，但 Kafka 的社区规模更大。</p><p>从使用市场来看，Kafka 和 Pulsar 都已部署在大公司的大型生产环境中。在生产环境中部署 Kafka 的公司在数量上更胜一筹。</p><p>从用户数量来看，Kafka 的用户更多。但是，数据工程师团队认为， Kafka 的使用者可以轻松学习 Pulsar。</p><p>就服务支持和社区而言，我们选择 Kafka。但值得一提的是，Pulsar 社区正在迅速发展。</p><h2 id="对比结果-1"><a href="#对比结果-1" class="headerlink" title="对比结果"></a>对比结果</h2><p>由于 Pulsar 和 Kafka 在这一使用场景中都有明显的优劣势，决策难度大大增加。</p><p>Pulsar 可以在社区和部署上奋起直追，Kafka 则可以努力丰富产品特性。</p><p>在作出决策前，我们先来总结一下，该企业在技术上最看重哪方面；在技术方面，我们是否需要做最保守的选择。根据以往的经验，新的开源技术会带来更多惊喜，因此我们更倾向于选择 Pulsar。</p><p>如果选择 Kafka，我们需要承担向业务赞助商坦诚“我们无法处理这一使用场景”的风险。甚至，即使支付大笔资金购买跨地域复制许可，也无法保证顺利实现客户的需求。业务团队最终可能需要花大量时间（甚至几个月）来编写、完善、测试他们的工作方案。</p><p>如果选择 Pulsar，我们可以告诉业务赞助商“一切尽在掌握中”。由于 Pulsar 的各项内置特性都已经过测试，使用团队可以在短时间内完成部署。</p><p>在这种情况下，因为我们不需要 Kafka API 的独有特性，所以我们没有使用支持 Kafka 协议（KOP）的 Pulsar Broker，而是选择 Pulsar API，因为 Pulsar API 支持所有我们需要 Kafka API 提供的功能。</p><p>决策如下：选择 Pulsar，可以优先处理业务请求，开发团队只专注编写代码，而不是解决其他问题。选择 Pulsar 的同时，也关注 Pulsar 社区和提供商的动态。</p><p>如果采取保守决策选择 Kafka，需要接受可能无法实现某些使用场景的事实。对于相似的使用场景，我们采取相应解决方案。调整项目时间规划，增加实行预期解决方案的时间。联系运营团队，确保可以承受执行预期解决方案的开销。</p><h1 id="高级消息使用场景"><a href="#高级消息使用场景" class="headerlink" title="高级消息使用场景"></a>高级消息使用场景</h1><p>假设一个公司已经在使用多种消息和队列系统。从运营、架构和开销的角度来看，我们认为有必要迁移到单个系统。同时，我们也希望降低运营成本。</p><p>数据架构师团队在和股东以及业务部门详细讨论了当前和未来需求后，给出的结论是，Pulsar 和 Kafka 各有优势。</p><h2 id="队列和消息"><a href="#队列和消息" class="headerlink" title="队列和消息"></a>队列和消息</h2><p>最大的难题是 RabbitMQ 系统。我们使用 RabbitMQ 发送太多消息，RabbitMQ 已经无法满足需求。我们调整了 RabbitMQ 的代码，将消息缓冲在内存中，并继续创建新集群来处理负载。但是我们需要的不是变通方法，而是一个能够处理大规模消息的系统。</p><p>数据架构师在研究这一使用场景时，得出结论：新系统必须可以同时处理消息流模型和队列模型。我们不仅需要继续使用 RabbitMQ 处理消息，也需要更高级的消息技术。</p><p>Kafka 擅长消息传递，也可以处理大规模消息流，但是无法处理队列。开发团队可以尝试一些解决方案，但这样就不能实现使用单个系统的预期目标。要处理队列使用场景，就同时需要 Kafka 集群和 RabbitMQ 集群。Kafka 集群更像一个缓冲区，可以有效防止 RabbitMQ 集群过载。但是 Kafka 不支持原生 RabbitMQ，我们需要与提供商合作或自己编写代码，才可以实现在 Kafka 和 RabbitMQ 之间移动数据。</p><p>Pulsar 可以在同一集群中处理队列和消息，还支持扩展集群。Pulsar 可以将所有消息流模型和队列模型的使用场景整合到一个集群中。用户可以继续使用 RabbitMQ 代码，Pulsar 支持 RabbitMQ 连接器，或者在 broker 中使用 StreamNative 开发的 <a href="https://github.com/streamnative/aop">AoP（AMQP 协议处理插件）</a>，该插件已获得 Apache 许可。</p><p>如果不想继续使用 RabbitMQ 代码，则可以使用 Pulsar API。Pulsar API 具有和 RabbitMQ 相同的队列功能。用户需要对代码进行相应修改，工作量取决于原代码的结构和细节，修改代码后，还需要对代码进行评估测试。</p><p>就队列模型和消息流模型而言，我们选择 Pulsar。</p><h2 id="高级保留"><a href="#高级保留" class="headerlink" title="高级保留"></a>高级保留</h2><p>数据架构师分析了数据使用情况，发现 99.99% 的数据在首次使用后就未被读取。但是，他们决定采取保守策略，保留消息一周。虽然决定存储数据一周，但我们不希望增加太多运营成本。分层存储可以保存数据到本地，然后卸载其他数据到 S3，降低长期保存数据的成本。</p><p>Kafka 团队正在开发分层存储，但 Kafka 目前还不支持这一特性。一些服务商提供私有分层存储，但我们不确定是否可以直接用于生产环境中。</p><p>分层存储是 Pulsar 的原生特性，可以直接用于生产环境。目前已有多个企业在生产环境中部署该特性。</p><p>就分层存储而言，我们选择 Pulsar。Kafka 正在全力开发分层存储，这一特性的重要性不言而喻。</p><h2 id="路由-Topic"><a href="#路由-Topic" class="headerlink" title="路由 Topic"></a>路由 Topic</h2><p>由于我们使用多个 topic 来分解数据，我们期待新系统可以创建大量 topic。数据架构师认为，我们起初需要 10 万个 topic，随着时间的推移，这个数字将会涨到 50 万。</p><p>Kafka 集群支持创建的分区数量有限且每个 topic 至少需要一个分区。Kafka 正在增加可支持 topic 的数量，但新特性尚未发布。另外，Kafka 没有命名空间和多租户，因此无法基于 topic 对资源进行分片，十万个 topic 需要存储在同一个命名空间中。</p><p>一些企业的确在使用 Kafka 集群存储甚至更多的 topic，同时进行了资源分片。但他们放弃使用单一集群，同时还需要为此支付费用。</p><p>Pulsar 支持存储数百万个 topic，这一功能早已发布并投入生产环境。Pulsar 支持命名空间和多租户，用户可以为每个 topic 设置资源配额，进而节约开销。</p><p>就 topic 而言，我们选择 Pulsar。</p><h2 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h2><p>由于我们假设该企业曾经使用 RabbitMQ，在设计上，一般通过 broker 路由机制把 topic 上的数据转发到不同的 topic 中。例如，有一个用于存储世界范围数据的 topic，而 RabbitMQ broker 把它处理成以国家为单位的 topic。</p><p>数据架构师团队深入研究了如何在消息系统中使用单一 topic 存储世界范围的数据。他们发现当接收数据量增大时，下游 consumer 无法继续处理数据。对每个下游系统进行反序列化、查看数据，再丢弃数据的流程繁杂，且费时费力。</p><p>Kafka 将所有数据存储在单一 topic 中，但是，当 consumer 需要过滤的数据量增加或集群过载时，这个方法不可行。我们通常需要进行水平缩放，增加 consumer 数量，才可以读取全局 topic 并做进一步处理。用户只能选择：编写自定义 consumer / producer，编写 Kafka Streams 程序，或使用专有 KSQL。</p><p>Pulsar 支持使用 Pulsar Functions 或自定义 consumer / producer 进行路由，因此可以先读取全局 topic，再将数据保存到以国家为单位的特定 topic 上。使用独立 topic，consumer 可以按需订阅 topic，只接收相关消息。</p><p>就路由而言，我们选择 Pulsar。</p><h2 id="最终决策"><a href="#最终决策" class="headerlink" title="最终决策"></a>最终决策</h2><p>时间是影响最终决策的主要原因。我们是否有时间让 Kafka 赶上 Pulsar？我们是否有时间让数据工程师来实现 Kafka 的解决方案？等待会让公司错失良机，延缓增加新的使用场景，影响业务发展。</p><p>最终决策：我们选择 Pulsar。</p><p>时间充足情况下的决策：延迟使用新架构。给 Kafka 半年时间，看 Kafka 是否可以在性能上赶超 Pulsar。如果可以，我们将在生产环境中测试这些新特性，评估稳定性。如果 Kafka 不能让人眼前一亮，我们仍然会选择 Pulsar。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>本文涉及的三个使用场景都是我在实际工作中遇到的，希望本文给出的解决方案可以为您提供参考，帮助您根据具体使用场景进行技术评估。</p><p>原文链接：</p><p><a href="https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi">https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://dzone.com/articles/comparing-pulsar-and-kafka-from-a-ctos-point-of-vi&quot;&gt;https://dzone.com/articles/comparing</summary>
      
    
    
    
    <category term="消息系统" scheme="http://zhangyu.info/categories/%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="Pulsar" scheme="http://zhangyu.info/tags/Pulsar/"/>
    
  </entry>
  
  <entry>
    <title>生产环境中的Kubernetes最佳实践</title>
    <link href="http://zhangyu.info/2021/04/21/kubernetes-best-practices-in-production/"/>
    <id>http://zhangyu.info/2021/04/21/kubernetes-best-practices-in-production/</id>
    <published>2021-04-20T16:00:00.000Z</published>
    <updated>2021-04-21T08:50:20.907Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>生产环境中的Kubernetes最佳实践<br><a href="https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&amp;mid=2649721992&amp;idx=1&amp;sn=31b9a4352a147a1fa585b8d8ef4c68b0&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&amp;mid=2649721992&amp;idx=1&amp;sn=31b9a4352a147a1fa585b8d8ef4c68b0&amp;scene=21#wechat_redirect</a><br>（翻译：易理林）<br><a href="https://my.oschina.net/u/1787735/blog/4870582">https://my.oschina.net/u/1787735/blog/4870582</a></p></blockquote><blockquote><p>DevOps从提出到现在，已经走过了一段很长的路。包括Docker和Kubernetes在内的多种平台也已经帮助企业用前所未有的速度实现了软件应用的交付。同时，随着应用的容器化构建和发布比率不断上升，作为事实上的容器编排工具，Kubernetes在企业用户中备受欢迎和广泛认可。</p><p>Kubernetes具有支持伸缩、零中断部署、服务发现、自动更迭和自动回滚等卓越功能特性。在管理大规模容器部署方面，Kubernetes因支持资源和工作负载的灵活分配能力，而成为了企业的必选工具，在生产环境中广泛应用。但与此同时，Kubernetes的应用需要操作人员花许多时间来熟悉和掌握它，存在一定技术门槛。鉴于目前许多公司都希望在生产中使用Kubernetes，因此有必要率先梳理这方面的最佳实践。在本文中，我们将介绍Kubernetes在生产环境中的一些最佳实践。</p><p><strong>生产环境中Kubernetes表现</strong></p><p>根据Garner的预测，到2022年时，全球超过75%的组织将在生产环境中运行容器化应用。这个比率在当前还不足30%，而预计到2025年时，这个比率将在2022年的基础上，继续增长到85%。快速增长的一个主要原因是云原生的软件应用在基础设施自动化、DevOps、专业操作技能方面的需求越来越强烈，而且这些工具和技术在企业的IT组织中往往很难找到。</p><p>其次，业界普遍认为在生产环境中运行容器并不容易，需要大量的计算资源和相关工作投入。目前市场上有多款容器编排平台产品可供选择，但已经获得了主要云提供商的支持和认可的平台只有Kubernetes。</p><p>再次，Kubernetes、容器化和微服务给企业用户带来的技术受益的同时，也带来了新的安全挑战。Kubernetes的Pod具备在所有基础设施类之间快速切换的能力，从而导致更多的内部流量和与之相关的安全风险，加上Kubernetes被攻击面往往比我们预期的更大，以及Kubernetes的高度动态和临时的环境与原有安全工具的融合差距等因素，可以预测使用Kubernetes并非是一件容易的事情。</p><p>最后，Kubernetes丰富的功能导致它的学习曲线复杂而陡峭，在生产环境中的操作需应尽可能小心和谨慎。企业如果没有熟悉这方面的专业人员，可以考虑外购Kubernetes-as-a-service（KaaS）提供商的服务，获取Kubernetes最佳实践。但假设用户是完全依靠自己的能力，管理生产环境中的Kubernetes集群，在这种情况下，理解和实现Kubernetes最佳实践尤其重要，特别是在可观察性、日志记录、集群监控和安全配置等方面。</p><p>综上所述，非常有必要开发一套Kubernetes管理策略，以实现在安全性、监视、网络、容器生命周期管理和平台选择等方面应用最佳实践。如下是Kubernetes应用管理需要重点考虑的措施。</p><p><strong>使用服务状态探针进行健康检查</strong></p><p>管理大型分布式系统是一件复杂的工作，尤其是出现问题的时候。因此为了确保应用的实例工作正常，配置Kubernetes健康检查至关重要。通过创建自定义运行状况检查，可以更好地满足用户的环境和应用的检测需要。服务状态探针包括服务就绪探针和服务活性探针。</p><p><img src="http://dockone.io/uploads/article/20210127/aed7959e8fadf0a4f96b584e7577fd8f.png" alt="图片"></p><p>Readiness-就绪探针：目的是让Kubernetes知道应用程序是否准备好提供服务。Kubernetes始终会在确认准备就绪探针通过检测后，然后才允许向POD发送服务请求流量。</p><p>Liveness-存活探针：目的是帮助用户确认应用程序是否正常存活，如果应用出现了异常，Kubernetes将启动新的Pod，替换异常的Pod。</p><p><strong>资源管理</strong></p><p>为单个容器指定资源需求和资源限制是一个很好的实践。<br><img src="https://oscimg.oschina.net/oscnet/42e6edf0-9c4f-4638-9deb-25e40e7178e3.png" alt="图片"><br>另一个好的实践是为不同团队、部门、应用程序和客户端，划分独立的Kubernetes命名空间环境。提供相对独立的运行资源环境，减少资源使用冲突。</p><p><img src="http://dockone.io/uploads/article/20210127/f43a6c5bcf0edb9c25a938a39c773307.png" alt="图片"></p><p><strong>资源使用</strong></p><p>Kubernetes资源使用情况掌握了生产环境中容器/Pod的资源数量使用情况。因此，密切关注Pod和容器的资源使用情况非常重要，资源使用越多，运行成本就越高。</p><p><strong>资源利用</strong></p><p>运维团队通常致力于优化和最大化Pod分配资源的利用百分比。资源使用情况往往也是Kubernetes优化程度的重要指标之一。可以说，优化最好的Kubernetes环境，内部运行容器的平均CPU利用率也是最优的。</p><p><strong>开启RBAC策略</strong></p><p>基于角色的访问控制（RBAC）是系统或网络中限制用户和应用程序的接入或访问的一种控制方法。</p><p><img src="http://dockone.io/uploads/article/20210127/47adf722dfc933233803d8337b0babae.png" alt="图片"></p><p>Kubernetes 从1.8版本开始，引入了RBAC访问控制技术，使用rbac.authorization.k8s.io程序API创建授权策略。RBAC的授权使用包括开启访问用户或帐户、添加/删除权限、设置规则等。它为Kubernetes集群添加了一个额外的安全层，限制哪些访问可以到达Kubernetes集群的生产环境。</p><p> <strong>集群配置和负载均衡</strong></p><p>生产级Kubernetes基础设施通常需要具备高可用性，具备多控制节点、多etcd集群等关键特性。此类集群特性的配置实现通常需要借助如Terraform或Ansible等工具实现。</p><p><img src="https://oscimg.oschina.net/oscnet/337e69d0-be7a-434e-84ed-fa6e3a3e502a.png" alt="图片"></p><p><img src="http://dockone.io/uploads/article/20210127/a07975057cb8dc3917f4ea965f7b0c6e.png" alt="图片"></p><p>通常情况下，当集群的所有配置都完成，并创建了Pod时，此时的Pod基本都会配置有负载均衡器，用于将流量路由到适当的应用服务。但这其中的负载均衡器并不是Kubernetes项目的默认配置，而是由Kubernetes Ingress控制器的扩展集成工具提供的。</p><p><strong>标注Kubernetes对象</strong></p><p>为Kubernetes的Pod等对象打上键/值对类型的标签，通常可以用来标记重要的对象属性，特别是对用户意义重大的属性。因此，在生产环境中使用Kubernetes时，不能忽视的重要实践就是利用标签功能，它们可以帮助实现Kubernetes对象的批量查询和批量操作。同时，标签还具有将Kubernetes对象组织成集群的独特作用，这样做的一个最佳实践应用就是能够根据应用对Pod进行分组管理。除此之外，标签没有数量和内容的限制，运维团队可以任意创建和使用。</p><p><img src="http://dockone.io/uploads/article/20210127/b8f53ed28367a64527d6de80cc831b5d.png" alt="图片"></p><p><strong>设置网络策略</strong></p><p>网络策略设置对于生产环境中的Kubernetes平台非常重要。</p><p><img src="http://dockone.io/uploads/article/20210127/40b02da5de6eecef731258ca11ea0f5f.png" alt="图片"></p><p>网络策略本质上也是一种对象，让用户能够声明和决定哪些流量是允许或禁止传输的。Kubernetes能够阻止所有不需要的和不合规的流量。因此，强烈建议Kubernetes将网络策略配置作为基本和必要的安全措施之一，执行定义和限制集群中的网络流量。</p><p>Kubernetes中的每条网络策略都被定义成一个授权连接列表。无论何时创建的网络策略，平台全部的Pod都有权利建立或接受该连接列表。简单来说，网络策略其实就是授权和允许连接的请求白名单，无论是“输入”还是“输出”到Pod，在至少有一条网络策略允许的情况下，到该Pod流量才被允许通行。</p><p><strong>集群监控与日志</strong></p><p>监控对于运行状态的Kubernetes至关重要，它直接影响到平台配置、性能和流量的安全。能够帮助用户及时掌握平台状态，执行问题诊断、确保运行合规，是平台运行的必要功能部署。在开启集群监视时，必须在平台的每一层都开启日志记录，让产生的日志能够执行安全、审计和性能分析。</p><p><img src="http://dockone.io/uploads/article/20210127/e7ac8a580bba4ded6fa5e07b0605f84d.png" alt="图片"></p><p><strong>采用无状态应用</strong></p><p>虽然这种观念正随着Kubernetes应用组织的增加在不断改变，但管理和运行无状态应用要比有状态应用要容易很多。事实上，对于刚接触Kubernetes的团队，建议一开始就采用无状态应用的设计。同时，还建议采用无状态的后端程序，从而让开发人员更有效地部署应用程序，实现服务的零停机时间。但前提是需要开发团队确保后端没有长时间运行的连接，不会影响到运行环境的弹性扩展。无状态应用还被认为具备根据业务需要进行简便迁移和快速扩展的能力。</p><p><strong>启用自动扩展</strong></p><p><img src="http://dockone.io/uploads/article/20210127/ec53085e44c3028b5df74032ab7864a1.png" alt="图片"></p><p>Kubernetes的服务部署拥有3个自动扩展能力：Pod水平自动扩展（HPA），Pod垂直自动扩展（VPA）和集群自动扩展。</p><p>Pod水平自动扩展能够基于CPU的利用率，自动扩展运行应用的Pod数量，调整副本控制器、副本集或状态配置。</p><p>Pod垂直自动扩展建议为应用设定适当的CPU，内存的需求值和上限值。VPA能够根据情况，自动伸缩配置适当的资源数量。</p><p>集群自动扩展能够伸缩工作节点的资源池规模，从而根据当前的资源使用情况，自动调整Kubernetes集群的大小。</p><p><strong>控制镜像拉取来源</strong></p><p>如果允许Pod从公共库中拉取镜像，而不知道其真正运行内容的时候，用户应该控制所运行容器集群的资源，以避免资源使用的失控。而如果是从受信任的注册节点提取镜像，则可以在注册节点上采用控制策略，限制只允许提取安全且经过认证的镜像。</p><p><strong>保持持续学习</strong></p><p>对应用程序的状态不断评估、学习和改进。例如，通过查看容器的历史内存使用情况，确定可以分配更少的内存来节省成本。</p><p><strong>重点保护核心服务</strong></p><p>使用Pod优先级功能，可以为不同的服务设置重要度。例如，可以配置RabbitMQ Pod的优先级高于应用程序Pod，以获得更好的稳定性。或为输入控制器Pod配置比数据处理Pod更高的重要度，以保持服务的可用性。</p><p><strong>保证服务零停机</strong></p><p>服务的零停机能力可以通过全方位HA架构，支持集群和服务的零停机升级。从而为客户获得更高的服务可用性提供了保证。使用Pod反亲和性配置，确保多个副本Pod被调度到不同的节点上，从而保证计划和非计划的集群节点停机不会影响服务的可用性，或使用Pod中断预备能力，确保在可用成本内，保留最少的副本数量。</p><p><strong>为失败指定计划</strong></p><p>借用一句名言来理解如果应对硬件故障。硬件最终会失败，软件最终会运行。–（迈克尔·哈顿） “Hardware eventually fails. Software eventually works.”（Michael Hartung）。</p><p><strong>结论</strong></p><p>业界共知的Kubernetes，实际上已经是DevOps的标配编配平台。生产环境中运行的Kubernetes环境必须具备可用性、可伸缩性、安全性、弹性、资源管理和监控等功能和性能特征。由于许多公司都在生产中使用Kubernetes，因此建议遵循上面提到的Kubernetes最佳实践，以便顺利、可靠地运维和管理应用程序。</p><p>原文链接：<a href="https://containerjournal.com/topics/container-management/kubernetes-best-practices-in-production/">https://containerjournal.com/topics/container-management/kubernetes-best-practices-in-production/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;生产环境中的Kubernetes最佳实践&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA5OTAyNzQ2OA==&amp;amp;mid=2649721992&amp;amp;idx=1&amp;amp;sn=31b9a</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>你的技术成长战略是什么</title>
    <link href="http://zhangyu.info/2021/04/19/jishuchengzhang/"/>
    <id>http://zhangyu.info/2021/04/19/jishuchengzhang/</id>
    <published>2021-04-18T16:00:00.000Z</published>
    <updated>2021-04-19T16:03:49.781Z</updated>
    
    <content type="html"><![CDATA[<p>你的技术成长战略是什么?_架构师波波的专栏-CSDN博客</p><p><a href="https://blog.csdn.net/yang75108/article/details/112511324?spm=1001.2014.3001.5502">https://blog.csdn.net/yang75108/article/details/112511324?spm=1001.2014.3001.5502</a>)</p><blockquote><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>在波波的微信技术交流群里头，经常有学员问关于技术人该如何学习成长的问题，虽然是微信交流，但我依然可以感受到小伙伴们焦虑的心情。</p><p>技术人为啥焦虑？恕我直言，说白了是胆识不足格局太小。胆就是胆量，焦虑的人一般对未来的不确定性怀有恐惧。识就是见识，焦虑的人一般看不清楚周围世界，也看不清自己和适合自己的道路。格局也称志向，容易焦虑的人通常视野窄志向小。如果从战略和管理的视角来看，就是对自己和周围世界的认知不足，没有一个清晰和长期的学习成长战略，也没有可执行的阶段性目标计划+严格的执行。</p><p>因为问此类问题的学员很多，让我感觉有点烦了，为了避免重复回答，所以我专门总结梳理了这篇长文，试图统一来回答这类问题。如果后面还有学员问类似问题，我会引导他们来读这篇文章，然后让他们用三个月、一年甚至更长的时间，去思考和回答这样一个问题：<strong>你的技术成长战略究竟是什么</strong>？如果你想清楚了这个问题，有清晰和可落地的答案，那么恭喜你，你只需按部就班执行就好，根本无需焦虑，你实现自己的战略目标并做出成就只是一个时间问题；否则，你仍然需要通过不断磨炼+思考，务必去搞清楚这个人生的大问题！！！</p><p>下面我们来看一些行业技术大牛是怎么做的。</p><h2 id="二、跟技术大牛学成长战略"><a href="#二、跟技术大牛学成长战略" class="headerlink" title="二、跟技术大牛学成长战略"></a>二、跟技术大牛学成长战略</h2><p>我们知道软件设计是有设计模式(Design Pattern)的，其实技术人的成长也是有成长模式(Growth Pattern)的。波波经常在Linkedin上看一些技术大牛的成长履历，探究其中的成长模式，从而启发制定自己的技术成长战略。</p><p>当然，很少有技术大牛会清晰地告诉你他们的技术成长战略，以及每一年的细分落地计划。但是，这并不妨碍我们通过他们的过往履历和产出成果，去溯源他们的技术成长战略。实际上，<strong>越是牛逼的技术人，他们的技术成长战略和路径越是清晰，我们越容易从中探究出一些成功的模式</strong>。</p><h3 id="2-1-系统性能专家案例"><a href="#2-1-系统性能专家案例" class="headerlink" title="2.1 系统性能专家案例"></a>2.1 系统性能专家案例</h3><p>国内的开发者大都热衷于系统性能优化，有些人甚至三句话离不开高性能/高并发，但真正能深入这个领域，做到专家级水平的却寥寥无几。</p><p>我这边要特别介绍的这个技术大牛叫Brendan Gregg(布兰登·格雷格)，他是系统性能领域经典书《System Performance: Enterprise and the Cloud》(中文版<a href="https://www.amazon.cn/dp/B08GC261P9">《性能之巅：洞悉系统、企业和云计算》</a>)的作者，也是著名的性能分析利器<a href="https://github.com/brendangregg/FlameGraph">火焰图(Flame Graph)</a>的作者。Brendan Gregg目前是Netflix公司的高级性能架构师，已经在Netflix工作近7年，之前他是Joynet公司的Lead Performance Engineer。总体上，他已经在系统性能领域深耕超过10年，<a href="https://www.linkedin.com/in/brendangregg/">Brendan Gregg的过往履历</a>可以在linkedin上看到。在这10年间，除了书籍以外，Brendan Gregg还产出了超过上百份和系统性能相关的技术文档，演讲视频/ppt，还有各种工具软件，相关内容都整整齐齐地分享在<a href="http://www.brendangregg.com/">他的技术博客</a>上，可以说他是一个非常高产的技术大牛。<br><img src="https://img-blog.csdnimg.cn/20210112110253707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>上图来自Brendan Gregg的新书《BPF Performance Tools: Linux System and Application Observability》，其中红色标注的是他开发的各种性能工具。从这个图可以看出，Brendan Gregg对系统性能领域的掌握程度，已经深挖到了硬件、操作系统和应用的每一个角落，可以说是360度无死角，整个计算机系统对他来说几乎都是透明的。波波认为，Brendan Gregg是名副其实的，世界级的，系统性能领域的大神级人物。</p><h3 id="2-2-从开源到企业案例"><a href="#2-2-从开源到企业案例" class="headerlink" title="2.2 从开源到企业案例"></a>2.2 从开源到企业案例</h3><p>我要分享的第二个技术大牛是Jay Kreps(杰·克雷普斯)，他是知名的开源消息中间件Kafka的创始人/架构师，也是Confluent公司的联合创始人和CEO，Confluent公司是围绕Kafka开发企业级产品和服务的技术公司。</p><p>从<a href="https://www.linkedin.com/in/jaykreps/">Linkedin的履历</a>上我们可以看出，Jay Kreps之前在Linkedin工作了7年多(2007.6 ~ 2014. 9)，从高级工程师、工程主管，一直做到首席资深工程师。Kafka大致是在2010年，Jay Kreps在Linkedin发起的一个项目，解决Linkedin内部的大数据采集、存储和消费问题。之后，他和他的团队一直专注Kafka的打磨，开源(2011年初)和社区生态的建设。到2014年底，Kafka在社区已经非常成功，有了一个比较大的用户群，于是Jay Kreps就和几个早期作者一起离开了Linkedin，<a href="https://tech.163.com/14/1107/18/AAFG92LD00094ODU.html">成立了Confluent公司</a>，开始了Kafka和周边产品的企业化服务道路。今年(2020.4月)，Confluent公司已经获得E轮2.5亿美金融资，公司估值达到45亿美金。从Kafka诞生到现在，Jay Kreps差不多在这个产品和公司上投入了整整10年。<br><img src="https://img-blog.csdnimg.cn/202101121103167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>上图是Confluent创始人三人组，一个非常有意思的组合，一个中国人(左)，一个印度人(右)，中间的Jay Kreps是美国人。</p><p>我之所以对Kafka和Jay Kreps的印象特别深刻，是因为在2012年下半年，我在携程框架部也是专门搞大数据采集的，我还开发过一套功能类似Kafka的Log Collector + Agent产品。我记得同时期有不止4个同类型的开源产品：Facebook Scribe、Apache Chukwa、Apache Flume和Apache Kafka。现在回头看，只有Kafka走到现在发展得最好，这个和创始人的专注和持续投入是分不开的，当然背后和几个创始人的技术大格局也是分不开的。</p><p>当年我对战略性思维几乎没有概念，还处在<strong>什么技术都想学、认为各种项目做得越多越牛的阶段</strong>。搞了半年的数据采集以后，我就掉头搞其它“更有趣”的项目去了(从这个事情的侧面，也可以看出我当年的技术格局是很小的)。中间我陆续关注过Jay的一些创业动向，但是没想到他能把Confluent公司发展到目前这个规模。现在回想，其实在十年前，Jay Kreps对自己的技术成长就有比较明确的战略性规划，也具有大的技术格局和成事的一些必要特质。Jay Kreps和Kafka给我上了一堂生动的技术战略和实践课。</p><h3 id="2-3-技术媒体大V案例"><a href="#2-3-技术媒体大V案例" class="headerlink" title="2.3 技术媒体大V案例"></a>2.3 技术媒体大V案例</h3><p>介绍到这里，有些同学可能会反驳说：波波你讲的这些大牛都是学历背景好，功底扎实起点高，所以他们才更能成功。其实不然，这里我再要介绍一位技术媒体界的大V叫Brad Traversy(布拉德·特沃西)，大家可以看<a href="https://www.linkedin.com/in/bradtraversy/">他的Linkedin简历</a>，背景很一般，学历差不多是一个非正规的社区大学(相当于大专)，没有正规大厂工作经历，有限几份工作一直是在做网站外包。但是Brad Traversy目前是技术媒体领域的一个大V，当前<a href="https://www.youtube.com/c/TraversyMedia">他在Youtube上</a>有138万多的订阅量，10年累计输出Web开发和编程相关教学视频超过800个。<a href="https://www.udemy.com/user/brad-traversy/">Brad Traversy也是Udemy上的一个成功讲师</a>，目前已经在Udemy上累计输出课程14门，购课学生数量近25万。Brad Traversy目前是自由职业者，他的Youtube广告+Udemy课程的收入相当不错。</p><p>就是这样一位技术媒体大V，你很难想象，在年轻的时候，贴在他身上的标签是：不良少年，酗酒，抽烟，吸毒，纹身，进监狱。。。直到结婚后的第一个孩子诞生，他才开始担起责任做出改变，然后凭借对技术的一腔热情，开始在Youtube平台上持续输出免费课程。从此他找到了适合自己的战略目标，然后人生开始发生各种积极的变化。。。如果大家对Brad Traversy的过往经历感兴趣，推荐观看他在Youtube上的<a href="https://www.youtube.com/watch?v=zA9krklwADI">自述视频《My Struggles &amp; Success》</a>。<br><img src="https://img-blog.csdnimg.cn/20210112110337364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>我粗略浏览了<a href="https://www.youtube.com/c/TraversyMedia/videos">Brad Traversy在Youtube上的所有视频</a>，10年总计输出800+视频，平均每年80+。第一个视频提交于2010年8月，刚开始几年几乎没有订阅量，2017年1月订阅量才到50k，这中间差不多隔了6年。2017.10月订阅量猛增到200k，2018年3月订阅量到300k。当前2021.1月，订阅量达到138万。可以认为从2017开始，也就是在积累了6～7年后，他的订阅量开始出现拐点。<strong>如果把这些数据画出来，将会是一条非常漂亮的复利曲线</strong>。</p><h3 id="2-4-案例小结"><a href="#2-4-案例小结" class="headerlink" title="2.4 案例小结"></a>2.4 案例小结</h3><p>Brendan Gregg，Jay Kreps和Brad Traversy三个人走的技术路线各不相同，但是他们的成功具有共性或者说模式：</p><ol><li>找到了适合自己的<strong>长期战略目标</strong>，例如：<ul><li>  Brendan Gregg: 成为系统性能领域顶级专家</li><li>  Jay Kreps：开创基于Kafka开源消息队列的企业服务公司，并将公司做到上市</li><li>  Brad Traversy: 成为技术媒体领域大V和课程讲师，并以此作为自己的职业</li></ul></li><li>**专注深耕一个(或有限几个相关的)细分领域(Niche)**，保持定力，不随便切换领域：<ul><li>  Brendan Gregg：系统性能领域</li><li>  Jay Kreps: 消息中间件/实时计算领域+创业</li><li>  Brad Traversy: 技术媒体/教学领域，方向Web开发 + 编程语言</li></ul></li><li> <strong>长期投入</strong>，三人都持续投入了<strong>10年</strong>。</li><li>**年度细分计划+持续可量化的价值产出(Persistent &amp; Measurable Value Output)**：<ul><li>  Brendan Gregg：除公司日常工作产出以外，每年有超过10份以上的技术文档和演讲视频产出，平均每年有2.5个开源工具产出。十年共产出书籍2本，其中《System Performance》已经更新到第二版。</li><li>  Jay Kreps：总体有开源产品+公司产出，1本书产出，每年有Kafka和周边产品发版若干。</li><li>  Brad Traversy: 每年有Youtube免费视频产出（平均每年80+）+Udemy收费视频课产出(平均每年1.5门)。</li></ul></li><li><strong>以终为始</strong>是牛人和普通人的一大区别。普通人通常走一步算一步，很少长远规划。牛人通常是先有远大目标，然后采用<strong>倒推法</strong>，将大目标细化到每年/月/周的详细落地计划。Brendan Gregg，Jay Kreps和Brad Traversy三人都是以终为始的典型。<br> <img src="https://img-blog.csdnimg.cn/20210116212459921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ol><p>上面总结了几位技术大牛的成长模式，其中一个要点是：这些大牛的成长都是通过<strong>持续有价值产出Persistent Valuable Output</strong>来驱动的。持续产出为啥如此重要，这个还要从下面的学习金字塔说起。</p><h2 id="三、学习金字塔和刻意训练"><a href="#三、学习金字塔和刻意训练" class="headerlink" title="三、学习金字塔和刻意训练"></a>三、学习金字塔和刻意训练</h2><p><img src="https://img-blog.csdnimg.cn/20210112110407342.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>学习金字塔是美国缅因州国家训练实验室的研究成果，它认为：</p><blockquote><ol><li> 我们平时上课听讲之后，学习内容平均留存率大致只有5%左右；</li><li> 书本阅读的平均留存率大致只有10%左右；</li><li> 学习配上视听效果的课程，平均留存率大致在20%左右，</li><li> 老师实际动手做实验演示后的平均留存率大致在30%左右；</li><li> 小组讨论(尤其是辩论后)的平均留存率可以达到50%左右；</li><li> 在实践中实际应用所学之后，平均留存率可以达到75%左右；</li><li> 在实践的基础上，再把所学梳理出来，转而再传授给他人后，平均留存率可以达到90%左右。</li></ol></blockquote><p>上面列出的7种学习方法，前四种称为<strong>被动学习</strong>，后三种称为<strong>主动学习</strong>。拿学游泳做个类比，被动学习相当于你看别人游泳，而主动学习则是你自己要下水去游。我们知道游泳或者跑步之类的运动是要燃烧身体卡路里的，这样才能达到锻炼身体和长肌肉的效果(肌肉是卡路里燃烧的结果)。如果你只是看别人游泳，自己不实际去游，是不会长肌肉的。同样的，主动学习也是要燃烧<strong>脑部卡路里</strong>的，这样才能达到训练大脑和长脑部“肌肉”的效果。</p><p>我们也知道，燃烧身体的卡路里，通常会让人感觉不舒适，如果燃烧身体卡路里会让人感觉舒适的话，估计这个世界上应该不会有胖子这类人。同样，燃烧脑部卡路里也会让人感觉不适、紧张、出汗或语无伦次，如果燃烧脑部卡路里会让人感觉舒适的话，估计这个世界上人人都很聪明，人人都能发挥最大潜能。当然，这些不舒适是短期的，长期会使你更健康和聪明。波波一直认为，人与人之间的先天身体其实都差不多，但是后天身体素质和能力有差异，这些差异，很大程度是由后天对身体和大脑的训练质量、频度和强度所造成的。</p><p>明白这个道理之后，心智成熟和自律的人就会对自己进行持续地<strong>刻意训练</strong>。这个刻意训练包括对身体的训练，比如波波现在每天坚持跑步3km，走3km，每天做60个仰卧起坐，5分钟平板撑等，每天保持让身体燃烧一定量的卡路里。刻意训练也包括对大脑的训练，比如波波现在每天做项目写代码coding(训练脑+手)，平均每天在B站上输出十分钟免费视频(训练脑+口头表达)，另外有定期总结输出公众号文章(训练脑+文字表达)，还有每天打半小时左右的平衡球(下图)或古墓丽影游戏(训练小脑+手)，每天保持让大脑燃烧一定量的卡路里，并保持一定强度(适度不适感)。<br><img src="https://img-blog.csdnimg.cn/20210112110438932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果你对刻意训练的专业原理和方法论感兴趣，推荐看书籍《刻意练习》。<br><img src="https://img-blog.csdnimg.cn/20210116212846695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>注意，如果你平时从来不做举重锻炼的，那么某天突然做举重会很不适应甚至受伤。脑部训练也是一样的，如果你从来没有做过视频输出，那么刚开始做会很不适应，做出来的视频质量会很差。不过没有关系，任何训练都是一个循序渐进，不断强化的过程。等大脑相关区域的”肌肉”长出来以后，会逐步进入正循环，后面会越来越顺畅，相关”肌肉”会越来越发达。所以，和健身一样，健脑也不能遇到困难就放弃，需要循序渐进(Incremental)+持续地(Persistent)刻意训练。</p><p>理解了学习金字塔和刻意训练以后，现在再来看Brendan Gregg，Jay Kreps和Brad Traversy这些大牛的做法，他们的学习成长都是建立在持续有价值产出的基础上的，这些产出都是刻意训练+燃烧脑部卡路里的成果。他们的产出要么是建立在实践基础上的产出，例如Jay Kreps的Kafka开源项目和Confluent公司；要么是在实践的基础上，再整理传授给其他人的产出，例如，Brendan Greeg的技术演讲ppt/视频，书籍，还有Brad Traversy的教学视频等等。换句话说，他们一直在学习金字塔的5～7层主动和高效地学习。并且，他们的学习产出还可以获得用户使用，有客户价值(Customer Value)，有用户就有反馈和度量。记住，<strong>有反馈和度量的学习，也称闭环学习，它是能够不断改进提升的</strong>；反之，<strong>没有反馈和度量的学习，无法改进提升</strong>。</p><p>现在，你也应该明白，晒个书单秀个技能图谱很简单，读个书上个课也不难。但是要你给出5～10年的总体技术成长战略，再基于这个战略给出每年的细分落地计划(尤其是产出计划)，然后再严格按计划执行，这的确是很难的事情。这需要大量的实践训练+深度思考，要燃烧大量的脑部卡路里！<strong>但这是上天设置的进化法则，成长为真正的技术大牛如同成长为一流的运动员，是需要通过燃烧与之相匹配量的卡路里来交换的。成长为真正的技术大牛，也是需要通过产出与之匹配的社会价值来交换的，只有这样社会才能正常进化。你推进了社会进化，社会才会回馈你。如果不是这样，社会就无法正常进化</strong>。</p><h2 id="四、战略思维的诞生"><a href="#四、战略思维的诞生" class="headerlink" title="四、战略思维的诞生"></a>四、战略思维的诞生</h2><p><img src="https://img-blog.csdnimg.cn/20210112110505523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一般毕业生刚进入企业工作的时候，思考大都是以天/星期/月为单位的，基本上都是今天学个什么技术，明天学个什么语言，很少会去思考一年甚至更长的目标。这是个眼前漆黑看不到的懵懂时期，捕捉到机会点的能力和概率都非常小。</p><p>工作了三年以后，悟性好的人通常会以一年为思考周期，制定和实施一些年度计划。这个时期是相信天赋和比拼能力的阶段，可以捕捉到一些小机会。</p><p>工作了五年以后，一些悟性好的人会产生出一定的胆识和眼光，他们会以3～5年为周期来制定和实施计划，开始主动布局去捕捉一些中型机会点。</p><p>工作了十年以后，悟性高的人会看到模式和规则变化，例如看出行业发展模式，还有人才的成长模式等，于是开始<strong>诞生出战略性思维</strong>。然后他们会以5～10年为周期来制定和实施自己的战略计划，开始主动布局去捕捉一些中大机会点。Brendan Gregg，Jay Kreps和Brad Traversy都是属于这个阶段的人。</p><p>当然还有很少一些更牛的时代精英，他们能够看透时代和人性，他们的思考是以一生甚至更长时间为单位的，这些超人不在本文讨论范围内。</p><h2 id="五、建议"><a href="#五、建议" class="headerlink" title="五、建议"></a>五、建议</h2><ol><li><p> 现在大学生毕业的年龄一般在22～23岁，那么在工作了十年后，也就是在你32～33岁的时候，你差不多也看了十年了，应该对自己和周围的世界(你的行业和领域)有一个比较深刻的领悟，需要开始为下一个十年去做战略布局了。<strong>如果你到这个年纪还懵懵懂懂，今天抓东明天抓西，那么只能说你的胆识格局是相当的低</strong>。在当前IT行业竞争这么激烈的情况下，到35岁被下岗可能就在眼前。</p></li><li><p> 有了战略性思考，你就会以5～10年为周期去布局谋划你的战略。以Brendan Gregg，Jay Kreps和Brad Traversy这些大牛为例，<strong>人生若真的要干点成就出来，投入周期一般都要十年</strong>。从33岁开始，你大致有3个十年，因为到60岁以后，一般人都老眼昏花干不了大事了。如果你悟性差一点，到40岁才开始规划，那么你大致还有2个十年。如果你规划好了，这2～3个十年可以成就不小的事业。否则，你很可能一生无所作为，或者一直在帮助成全别人的事业。</p></li><li><p> 考虑到人生能干事业的时间也就是2～3个十年，你会发现人生其实很短暂，这时候你会把精力都投入到实现你的十年战略上去，没有时间再浪费在比如网上的闲聊和扯皮争论上去。</p></li><li><p> “图难于其易，为大于其细。天下难事必作于易，天下大事必作于细。是以圣人终不为大，故能成其大。”～道德经。有了十年战略方向，下一步是每年的细分落地计划，尤其是产出计划。这个计划主要应该工作在学习金字塔的5/6/7层。<strong>产出应该是刻意训练+燃烧卡路里的结果，每天让身体和大脑都保持燃烧一定量的卡路里</strong>。</p></li><li><p> 产出应该有客户价值，自己能学习(自己成长进化)，对别人还有用(推动社会成长进化)，这样可以得到<strong>用户回馈和度量</strong>，形成一个闭环，可以持续改进和提升你的学习。</p></li><li><p> “少则得，多则惑”～道德经。<strong>少即是多</strong>，深耕一个(或有限几个相关的)领域。所有细分计划应该紧密围绕你的战略展开。克制内心欲望，不要贪多和分心，不要被喧嚣的世界所迷惑。</p></li><li><p> 战略方向+细分计划都要写下来，定期review优化。</p></li><li><p> “曲则全、枉则直”～道德经。战略实现不是直线的，而是曲折迂回的。战略方向和细分计划通常要按需调整，尤其在早期，但是最终要收敛。如果老是变不收敛，就是缺乏战略定力，是个必须思考和解决的大问题。</p></li><li><p> 别人的成长战略可以参考，但是不要刻意去模仿，你有你自己的颜色，<strong>你应该成为独一无二的你</strong>。</p></li><li><p> “合抱之木，生于毫末；九层之台，起于蔂土；千里之行，始于足下”～道德经。战略方向和细分计划明确了，接下来就是按部就班执行，十年如一日铁打不动。</p></li><li><p> 做<strong>长期主义者和时间的朋友</strong>，”任何一个人，不管你的能量强弱，放眼于足够长的时间，你都可以通过长期主义这种行为模式，成为时间的朋友”～罗振宇。</p></li><li><p>最后，战略目标的实现也和种树一样是生长出来的，需要时间耐心栽培，记住<strong>慢就是快</strong>。焦虑纠结的时候，像念经一样默念王阳明《传习录》中的教诲：</p><blockquote><p>立志用功，如种树然。方其根芽，犹未有干；及其有干，尚未有枝；枝而后叶，叶而后花实。初种根时，只管栽培灌溉。勿作枝想，勿作花想，勿作实想。悬想何益？但不忘栽培之功，怕没有枝叶花实？</p><p>译文：</p><p>实现战略目标，就像种树一样。刚开始只是一个小根芽，树干还没有长出来；树干长出来了，枝叶才能慢慢长出来；枝叶长出来，然后才能开花和结果。刚开始种树的时候，只管栽培灌溉，别老是纠结枝什么时候长出来，花什么时候开，果实什么时候结出来。纠结有什么好处呢？只要你坚持投入栽培，还怕没有枝叶花实吗？</p></blockquote></li></ol></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你的技术成长战略是什么?_架构师波波的专栏-CSDN博客&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/112511324?spm=1001.2014.3001.5502&quot;&gt;https://b</summary>
      
    
    
    
    <category term="职业发展" scheme="http://zhangyu.info/categories/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"/>
    
    
    <category term="职业发展" scheme="http://zhangyu.info/tags/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/"/>
    
  </entry>
  
  <entry>
    <title>使用nerdctl玩转containerd</title>
    <link href="http://zhangyu.info/2021/04/12/nerdctl-to-containerd/"/>
    <id>http://zhangyu.info/2021/04/12/nerdctl-to-containerd/</id>
    <published>2021-04-11T16:00:00.000Z</published>
    <updated>2021-04-12T13:02:31.269Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考<br><a href="https://mp.weixin.qq.com/s/ZKoO041TqyR2guVooPegrg">https://mp.weixin.qq.com/s/ZKoO041TqyR2guVooPegrg</a></p><p>从行业趋势来看，Docker 已经和 Kubernetes 社区渐行渐远，以?Containerd?为代表的实现了?CRI?接口的容器运行时将会受到 Kubernetes 的青睐。<br>但纯粹使用 Containerd 还是有诸多困扰，比如不方便通过 CLI 来创建管理容器，有了?nerdctl?这个 CLI 工具，就就可以填补 Containerd 易用性的空缺</p><h2 id="现有-CLI-的不足"><a href="#现有-CLI-的不足" class="headerlink" title="现有 CLI 的不足"></a>现有 CLI 的不足</h2><p>虽然 Docker 能干的事情，现在 <code>Containerd</code> 都能干，但 <code>Containerd</code> 还有一个非常明显的缺陷：<strong>CLI 不够友好</strong>。它无法像 Docker 和 Podman 一样通过一条简单的命令启动一个容器，它的两个 CLI 工具 ctr 和 crictl都无法实现这么一件非常简单的需求，而这个需求是大多数人都需要的，我总不能为了在本地测试容器而专门部署一个 Kubernetes 集群吧？</p><p>ctr 的设计对人类不太友好，例如缺少以下这些和 Docker 类似的功能：</p><ul><li><p>  <code>docker run -p &lt;PORT&gt;</code></p></li><li><p>  <code>docker run --restart=always</code></p></li><li><p>  通过凭证文件 <code>~/.docker/config.json</code> 来拉取镜像</p></li><li><p>  <code>docker logs</code></p></li></ul><p>除此之外还有一个 CLI 工具叫 <code>crictl</code>，和 <code>ctr</code> 一样不太友好。</p><p>为了解决这个痛点，Containerd 官方推出了一个新的 CLI 叫 nerdctl。nerdctl 的使用体验和 docker 一样顺滑</p><p>##安裝nerdctl</p><p>你可以从?<a href="https://github.com/containerd/nerdctl%E4%B8%AD%E4%B8%8B%E8%BD%BD%E6%9C%80%E6%96%B0%E7%9A%84%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%EF%BC%8C%E6%AF%8F%E4%B8%80%E4%B8%AA%E7%89%88%E6%9C%AC%E9%83%BD%E6%9C%89%E4%B8%A4%E7%A7%8D%E5%8F%AF%E7%94%A8%E7%9A%84%E5%8F%91%E8%A1%8C%E7%89%88%EF%BC%9A">https://github.com/containerd/nerdctl中下载最新的可执行文件，每一个版本都有两种可用的发行版：</a></p><ul><li><p>  `nerdctl-<VERSION>-linux-amd64.tar.gz?: 只包含 nerdctl。</p></li><li><p>  `nerdctl-full-<VERSION>-linux-amd64.tar.gz?: 包含了 nerdctl 和相关依赖组件（containerd, runc, ###CNI, …）。</p></li></ul><p>如果你已经安装了 Containerd，只需要选择前一个发行版，否则就选择完整版。</p><p>这里选择完整版nerdctl-full-0.7.3-linux-amd64.tar.gz<br>cd /opt </p><p>wget <a href="https://github.com/containerd/nerdctl/releases/download/v0.7.3/nerdctl-full-0.7.3-linux-amd64.tar.gz">https://github.com/containerd/nerdctl/releases/download/v0.7.3/nerdctl-full-0.7.3-linux-amd64.tar.gz</a></p><p>tar -C /usr/local -xzf nerdctl-full-0.7.3-linux-amd64.tar.gz</p><p>mkdir -p /etc/containerd</p><p>containerd config default &gt; /etc/containerd/config.toml</p><p>sed -i “s#k8s.gcr.io#registry.aliyuncs.com/k8sxio#g”  /etc/containerd/config.toml</p><p>sed -i “s/systemd_cgroup = false/systemd_cgroup = true/g” /etc/containerd/config.toml </p><p>export REGISTRY_MIRROR=<a href="https://registry.cn-hangzhou.aliyuncs.com/">https://registry.cn-hangzhou.aliyuncs.com</a></p><p>sed -i “s#<a href="https://registry-1.docker.io/#${REGISTRY_MIRROR}#g&quot;">https://registry-1.docker.io#${REGISTRY_MIRROR}#g&quot;</a>  /etc/containerd/config.toml</p><p>mkdir -p /data/containerd-data-root</p><p>sed -i “s#/var/lib/containerd#/data/containerd-data-root#g”  /etc/containerd/config.toml</p><p>sed -i “s#oom_score = 0#oom_score = -999#g”  /etc/containerd/config.toml</p><p>sed -i “/containerd.runtimes.runc.options/a\            SystemdCgroup = true”  /etc/containerd/config.toml</p><p>sed -i “s#/opt/cni/bin#/usr/local/libexec/cni#g”  /etc/containerd/config.toml</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">添加私有仓库harbor---自定义--可选</span><br><span class="line"></span><br><span class="line">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;harbor.testtest.com&quot;]</span><br><span class="line">      endpoint &#x3D; [&quot;https:&#x2F;&#x2F;harbor.testtest.com&quot;]</span><br><span class="line">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor.testtest.com&quot;.tls]</span><br><span class="line">      insecure_skip_verify &#x3D; true</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;harbor.testtest.com&quot;.auth]</span><br><span class="line">      username &#x3D; &quot;admin&quot;</span><br><span class="line">      password &#x3D; &quot;Harbor12345&quot;</span><br></pre></td></tr></table></figure><blockquote><p>systemctl daemon-reload</p><p>systemctl enable –now containerd</p><p>systemctl status containerd</p><p>containerd –version</p><p>###普通用户Rootless</p><p>切换到普通用户</p><p>比如su - admin</p><p>执行  containerd-rootless-setuptool.sh install</p><p>强烈建议在Rootless模式下启用cgroup v2</p><p>请参阅<a href="https://rootlesscontaine.rs/getting-started/common/cgroup2/">https://rootlesscontaine.rs/getting-started/common/cgroup2/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;参考&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/ZKoO041TqyR2guVooPegrg&quot;&gt;https://mp.weixin.qq.com/s/ZKoO041TqyR2guVooPegrg&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="containerd" scheme="http://zhangyu.info/categories/containerd/"/>
    
    
    <category term="containerd" scheme="http://zhangyu.info/tags/containerd/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes网络和云厂商实践浅析</title>
    <link href="http://zhangyu.info/2021/04/09/Kubernetes-network/"/>
    <id>http://zhangyu.info/2021/04/09/Kubernetes-network/</id>
    <published>2021-04-08T16:00:00.000Z</published>
    <updated>2021-04-09T03:00:20.304Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Kubernetes网络和云厂商实践浅析"><a href="#Kubernetes网络和云厂商实践浅析" class="headerlink" title="Kubernetes网络和云厂商实践浅析"></a>Kubernetes网络和云厂商实践浅析</h3><blockquote><p>原创 张向阳 云网漫步 2020-11-22<br><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1617936055&amp;ver=2997&amp;signature=czoNtYoc1oSn7KtlQknWvo*D*Q0GOWU2VI3rEKh6YtE9kSX4TOoMWQLrj8cPiKi9jDE-u4SIMSdM1iYpGw63aXBXZlz7XHUcOgrTiL335Qm9mxs0cJFYp7j1VpUx6IkF&amp;new=1">https://mp.weixin.qq.com/s?src=11&amp;timestamp=1617936055&amp;ver=2997&amp;signature=czoNtYoc1oSn7KtlQknWvo*D*Q0GOWU2VI3rEKh6YtE9kSX4TOoMWQLrj8cPiKi9jDE-u4SIMSdM1iYpGw63aXBXZlz7XHUcOgrTiL335Qm9mxs0cJFYp7j1VpUx6IkF&amp;new=1</a></p><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a><strong>0 前言</strong></h2><pre><code>Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 源于希腊语，意为 &quot;舵手&quot; 或 &quot;飞行员&quot;，Google 在 2014 年开源了 Kubernetes 项目，Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验的基础上（Brog系统），结合了社区中最好的想法和实践。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f261zzyleNpCL2WIkPNGbxQ0jibk6XS5uYnAtXQvE50m30lWT9IXSxH5la46UxcZCgEJyklzENV94A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><pre><code>    为了能实现Kubernetes有效的管理大规模的容器，需要优秀网络技术的支撑，本文主要从Kubernetes网络的角度去介绍Kubernetes网络的需求、网络模型、实现技术、云厂商Kubernetes的网络实践。</code></pre><h2 id="1-Kubernetes网络系统需求"><a href="#1-Kubernetes网络系统需求" class="headerlink" title="1 Kubernetes网络系统需求"></a><strong>1 Kubernetes网络系统需求</strong></h2><h2 id="集群网络系统是-Kubernetes-的核心部分，它需要解决下面四个问题。"><a href="#集群网络系统是-Kubernetes-的核心部分，它需要解决下面四个问题。" class="headerlink" title="集群网络系统是 Kubernetes 的核心部分，它需要解决下面四个问题。"></a>集群网络系统是 Kubernetes 的核心部分，它需要解决下面四个问题。</h2><ol><li><p> <strong>Pod内容器间通信。</strong></p></li><li><p> <strong>Pod 间通信。</strong></p></li><li><p> <strong>Pod 和服务间通信。</strong></p></li><li><p>  <strong>外部和服务间通信。</strong></p></li></ol><pre><code>     Kubernetes 的宗旨就是在应用之间共享机器，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。同时动态分配端口也会给系统带来很多复杂度，每个应用都需要设置一个端口的参数，而 API服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。    与其去解决这些问题，Kubernetes 选择了其他不同的方法，下面我们介绍一下Kubernetes 网络模型。</code></pre><h2 id="2-Kubernetes-网络模型"><a href="#2-Kubernetes-网络模型" class="headerlink" title="2 Kubernetes 网络模型"></a><strong>2 Kubernetes 网络模型</strong></h2><pre><code>    Kubernetes 对所有网络设施的实施，都需要满足以下的基本要求（除非有设置一些特定的网络分段策略）：</code></pre><ol><li><p> <strong>节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信。</strong></p></li><li><p> <strong>节点上的代理（例如，系统守护进程、kubelet）可以和节点上的所有Pod通信。</strong></p></li></ol><pre><code>    每一个Pod都有它自己的IP地址，这就意味着不需要显式地在每个Pod之间创建链接，也不需要处理容器端口到主机端口之间的映射。这样将创建一个干净的、向后兼容的模型，在这个模型里，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod可以被视作虚拟机或者物理主机。同时还和 Kubernetes 的实现廉价的从虚拟机向容器迁移的初衷相兼容，如果你的工作开始是在虚拟机中运行的，你的虚拟机有一个 IP，这样就可以和其他的虚拟机进行通信，这是基本相同的模型。</code></pre><h2 id="3-Kubernetes-网络技术"><a href="#3-Kubernetes-网络技术" class="headerlink" title="3 Kubernetes 网络技术"></a><strong>3 Kubernetes 网络技术</strong></h2><pre><code>    从上文看出，每个pod有自己唯一的IP地址，可通过一个扁平的、非NAT网络和其他Pod通信。Kubernetes是如何做到这一点呢？其实，Kubernetes不负责这块，网络是有Container Network Interface（CNI）插件进行管理。CNI是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件，CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源，如下图所示。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgULfibdfDNTQxQtxSVQA52UI7DIPH7PK2FZicib12kwBLIqOZRXvMZMHC5w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><pre><code>    Kubernetes网络实现模型很多，从本质上看，使用网络技术有两大类，路由方案和Overlay网络方案。</code></pre><h4 id="3-1-Pod"><a href="#3-1-Pod" class="headerlink" title="3.1 Pod"></a><strong>3.1 Pod</strong></h4><h4 id="3-1-1-Pod内container-容器-通信"><a href="#3-1-1-Pod内container-容器-通信" class="headerlink" title="3.1.1 Pod内container(容器)通信"></a><strong>3.1.1 Pod内container(容器)通信</strong></h4><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUtVCibQVH9053CaVWyxD0JrbM62AEVFLAEmzJHCjDWvUvM6lNRfibicFmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>  Pod中管理着一组容器，这些容器共享同一个网络命名空间。Pod中的每个容器拥有与Pod相同的IP和port地址空间，并且由于他们在同一个网络命名空间，他们之间可以通过localhost相互访问。</p><p> 每个Pod容器有一个pause容器其有独立的网络命名空间，在Pod内启动容器时候使用 –net=container就可以让当前容器加入到Pod容器拥有的网络命名空间（pause容器）。</p><h4 id="3-1-2-同节点的Pod通信"><a href="#3-1-2-同节点的Pod通信" class="headerlink" title="3.1.2 同节点的Pod通信"></a><strong>3.1.2 同节点的Pod通信</strong></h4><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUJIdYRGhYl4eGrBfcD7JQNBhBxjqu6TMZEicTice4CsroVyym55EE8YhA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p> 每个Pod拥有一个ip地址，不同的Pod之间可以直接使用改ip与彼此进行通讯。在同一个Node上，从Pod的视角看，它存在于自己的网络命名空间中，并且需要与该Node上的其他网络命名空间上的Pod进行通信。</p><p> 为了让多个Pod的网络命名空间链接起来，会创建一下veth pair对，veth对的一端链接到宿主机的网络命名空间，另一端链接到Pod的网络命名空间，并重新命名为eth0。</p><p> 宿主机网络命名空间的接口会绑定到容器运行时配置使用的网络桥接上。从网桥的地址段中去IP地址赋值给容器的eth0接口。应用的任何运行在容器内部的程序都会发送数据到eht0网络接口，数据从宿主机命名空间的另外一个veth接口出来，然后发送给网桥。</p><p><strong>3.1.3 不同节点的Pod通信</strong>  </p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUpDyRviakVvT8ystj4YDe9vvn7bq9dNicS6C82hLDs6ibiap0Lh1lafyIpQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">        不同节点上的pod能够通信，需要把这些节点的网桥以某种方式连接起来，有多种连接不同节点上的网桥的方式，例如通过三层路由，或者Overlay网络（隧道技术，例如GRE和VxLAN等）。</p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUhjFcOVS1hG35ktRxKibz8xia5YYyy5FtevGD3NQaf5dFfBa5CD1IuSibA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>路由方案</p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUTDEVN5CJzuMhr917QJ5tpdc6Mys0Fbl7Bel5WjN4zXyIfkk7UiabQHg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>Overlay方案</p><pre><code>    pod通常需要对来自集群内部其他pod，以及来自集群外部的客户端的HTTP请求作出反应。pod需要一种寻找其他pod的方法来使用其他pod提供的服务。而在Kubernetes的网络中，有特殊的地方。</code></pre><ol><li><p> <strong>一个服务经常会起多个pod，你到底访问那个pod的ip呢？</strong></p></li><li><p> <strong>pod经常会因为各种原因被调度，调度后一个pod的ip会发生变化。</strong></p></li><li><p> <strong>pod的ip是虚拟的且局域的，在集群内部访问没有问题，但是从Kubernetes集群的外部如何访问pod的ip呢？</strong></p></li></ol><pre><code>    为了解决第1，2的问题，Kubernetes提供了一种资源类型，服务（service）。为了解决第3个问题，Kubernetes有将服务的类型设置为NodePort，将服务的类型设置为LoadBanlance，创建一个Ingress资源。</code></pre><p><strong>3.2  Service</strong></p><pre><code>    Kubernetes的service（服务）是一种为一组功能相同的pod提供单一不变的接入点的资源。当服务存在时，它的ip地址和端口不会变化，客户端通过IP地址和端口号建立连接，这些连接会被路由到提供该服务的任意一个pod上。通过这种方式，客户端不需要知道每个单独的提供服务的pod的地址，这样这些pod可以在集群中随时被创建或者移除。</code></pre><p>Kubernetes的服务需要解决两个主要问题。</p><ol><li><p> <strong>服务怎么做负载均衡？</strong></p></li><li><p> <strong>服务怎么被发现？</strong></p></li></ol><h4 id="3-2-1-负载均衡"><a href="#3-2-1-负载均衡" class="headerlink" title="3.2.1 负载均衡"></a><strong>3.2.1 负载均衡</strong></h4><pre><code>    在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP的形式。其实，服务并不是和pod直接相连的，它们之间是一种EndPoint资源。EndPoint资源就是暴露一个服务的IP地址和端口列表。</code></pre><p> <strong>3.2.1.1  userspace 代理模式</strong></p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUe9bhy1brf2NIRCYd9MQcuy1MPcuApWiaEadEZVwtjWuZV8eI7c0fLfg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><pre><code>    kube-proxy 会监视 Kubernetes 主控节点对 Service 对象和 Endpoints 对象的添加和移除操作。对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。任何连接到“代理端口”的请求，都会被代理到 Service 的后端 Pods 中的某个上面（如 Endpoints 所报告的一样）。使用哪个后端 Pod，是 kube-proxy 基于 SessionAffinity 来确定的。    最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP） 和 Port 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。</code></pre><p><strong>3.2.1.2 iptables 代理模式</strong></p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUrBsj95TT1HZ9DtssFL0QP9G8K2xVYdsbMyHuhPcjoCXYtVQiajzq9fQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><pre><code>    kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。    使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。这种方法也可能更可靠。    如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应， 则连接失败。这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败， 并会自动使用其他后端 Pod 重试。</code></pre><p><strong>3.2.1.3 IPVS 代理模式</strong></p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUN8GZxWOLAqT1dYjxlxxlcQTzvZA8EUlZMnfp2IhZj7QuCLsDzwicO0A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><pre><code>    在 ipvs 模式下，kube-proxy监视Kubernetes服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。     IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。与iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。IPVS提供了更多选项来平衡后端Pod的流量。</code></pre><h4 id="3-2-2-服务发现"><a href="#3-2-2-服务发现" class="headerlink" title="3.2.2  服务发现"></a><strong>3.2.2  服务发现</strong></h4><h4 id="Kubernetes-支持两种基本的服务发现模式-——-环境变量和-DNS。"><a href="#Kubernetes-支持两种基本的服务发现模式-——-环境变量和-DNS。" class="headerlink" title="Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。"></a>Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS。</h4><p> <strong>通过环境变量发现服务</strong></p><pre><code>    在pod开始运行的时候，Kubernetes会初始化一系列的环境变量指向现在存在的服务    注：当您具有需要访问服务的Pod时，并且您正在使用环境变量方法将端口和群集 IP 发布到客户端 Pod 时，必须在客户端 Pod 出现 之前 创建服务。否则，这些客户端 Pod 将不会设定其环境变量。</code></pre><p> <strong>通过DNS发现服务</strong></p><pre><code>    支持群集的 DNS 服务器监视 Kubernetes API 中的新服务，并为每个服务创建一组 DNS 记录。如果在整个群集中都启用了 DNS，则所有 Pod 都应该能够通过其 DNS 名称自动解析服务。</code></pre><h4 id="3-2-3-发布服务"><a href="#3-2-3-发布服务" class="headerlink" title="3.2.3 发布服务"></a><strong>3.2.3 发布服务</strong></h4><pre><code>    对一些应用（如前端）的某些部分，可能希望通过外部 Kubernetes 集群外部 IP 地址暴露 Service。    Kubernetes ServiceTypes 允许指定一个需要的类型的 Service，默认是 ClusterIP类型。    ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。    NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个 NodePort 服务。    LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。</code></pre><p><strong>3.3  Ingress</strong>  </p><pre><code>    我们也可以使用 Ingress 来暴露自己的服务。    为什么需要Ingress呢？一个重要的原因是每个LoadBalancer服务都需要自己的负载均衡器，以及独有的公用IP地址，而Ingress只需要一个公网IP就能为很多服务提供访问。例如，当客户端向Ingress发送HTTP请求时，Ingress会根据请求的主机和路径决定请求转发到的服务。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgU8WZwuhU9a2RlDJQXQX9YJS0PhLr4IvLzlh1hH6E9kgczX2usrPA5NA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ol><li><p> <strong>客户端先执行DNS查询，DNS服务器返回了Ingress控制器的IP地址。</strong></p></li><li><p> <strong>客户端然后向Ingress控制器发送HTTP请求，并在Host头部中指定访问的域名。</strong></p></li><li><p> <strong>控制器从该Host头部确认客户端尝试访问哪个服务，通过与该服务关联的Endpoint对象查看pod IP，并将客户端的请求转发给其中一个pod。</strong></p></li></ol><h2 id="4-云厂商Kubernetes实践"><a href="#4-云厂商Kubernetes实践" class="headerlink" title="4.  云厂商Kubernetes实践"></a><strong>4.  云厂商Kubernetes实践</strong></h2><h3 id="4-1-AWS-Kubernetes网络方案"><a href="#4-1-AWS-Kubernetes网络方案" class="headerlink" title="4.1  AWS Kubernetes网络方案"></a><strong>4.1  AWS Kubernetes网络方案</strong></h3><p> AWS上搭建Kubernetes集群环境有两种方式，一种是使用托管服务Amazon Elastic Kubernetes Service (Amazon EKS) ，一种是自建K8S集群。可以使用Amazon VPC CNI插件管理Pod的网络地址和通信。</p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUTZ7JtxunyxplCvicQ1gtXGBtENOXwdiaevHdicricqjtyg8OPLXiboV8obg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>EKS网络架构</p><p><strong>4.1.1 VPC CNI插件</strong>  </p><pre><code>    AWS VPC CNI 为 Kubernetes 集群提供了集成的 AWS 虚拟私有云（VPC）网络，使用该 CNI 插件，可使 Kubernetes Pod 拥有与在 VPC 网络上相同的 IP 地址。CNI 将 AWS 弹性网络接口（ENI）分配给每个 Kubernetes 节点，并将每个 ENI 的辅助 IP 范围用于该节点上的 Pod 。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUyAZ6pd6GG2POVwMMZaz4EP4vF6SGo6x23fhnITk0l65AD5kOjicFfyg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p> Kubernetes 的 Amazon VPC 容器网络接口 (CNI) 插件随每个节点一起部署，插件包含两个主要组件。</p><pre><code>    **L-IPAM 守护程序**\-负责创建网络接口并将网络接口附加到 Amazon EC2 实例,将辅助 IP 地址分配给网络接口,并在每个节点上维护 IP 地址的地址池,以便在安排时分配到 Kubernetes Pod。    **CNI 插件** – 负责连接主机网络(例如,配置网络接口和虚拟以太网对)并向 Pod 命名空间添加正确的网络接口。</code></pre><p><strong>4.1.2 Pod通信</strong></p><pre><code>    VPC 内的通信（如 Pod 到 Pod）在私有 IP 地址之间是直接通信。</code></pre><p><strong>4.1.2 Pod和外部通信</strong></p><pre><code>     当流量以 VPC 外部的地址为目标时,默认情况下,Kubernetes 的 Amazon VPC CNI 插件将每个 Pod 的私有 IP 地址转换为分配给 Pod 在其上运行的 节点的主 网络接口Amazon EC2(网络接口)的主私有IP地址，有如下两种方式。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUGxw0WichBrbV8Es7GVCk7tj7YmbzYktpGRXKLwKMicz0WKo1EAz86bEA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUN3lbSsYya6mI9JHHNK98UEqEAO1o0NCp5jSClibyPLe6mqZ9rHIyu7w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.1.3  Ingress</strong>  </p><pre><code>    AWS ALB Ingress 控制器将在Kubernetes 用户声明集群上的 Ingress 资源时触发创建 ALB 以及必要的 AWS 支持资源。Ingress 资源通过 ALB 将 HTTP\[s\] 流量路由至集群内的不同终端节点。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUDyTdcdJrlTZnC4ALQyODOibkH2icMmGibwXSqnVAQXfLhYfWibv2HgdJUg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><ol><li><p> <strong>控制器观察来自 API 服务器的进站事件。如果发现 Ingress 资源满足要求，则将开始创建 AWS 资源。</strong></p></li><li><p> <strong>为 Ingress 资源创建 ALB。</strong></p></li><li><p> <strong>为 Ingress 资源中指定的每个后端创建目标组。</strong></p></li><li><p> <strong>为 Ingress 资源注释中指定的每个端口创建侦听器。如果未指定端口，则将使用合理的默认值（80 或 443）。</strong></p></li><li><p> <strong>为 Ingress 资源中指定的每个路径创建规则。这将确保指向特定路径的流量将被路由至所创建的正确目标组。</strong></p></li></ol><h3 id="4-2-GCP-Kubernetes网络方案"><a href="#4-2-GCP-Kubernetes网络方案" class="headerlink" title="4.2 GCP Kubernetes网络方案"></a><strong>4.2 GCP Kubernetes网络方案</strong></h3><p>Google Kubernetes Engine (GKE) 提供了一个托管环境，可以使用 Google 基础架构在其中部署、管理和扩缩容器化应用。</p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUfdXyndSpPjsvT84iaYfOvXdUcLicTQ2PT2dqAaLcScqGZh3EcmuYjJ0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.2.1 Pod通信</strong>  </p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUwf3ct5tcjJ4vibRVjA0wRHV7iaDAAkjLThLHibKMA74RVZnL1ic9HtyhWw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.2.2 Service</strong></p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUG7bSXeDhs3J6ZFLeLCj5ZiaCBichFAzia4sUUQyZCtRfMyopj4lfZe8Nw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.2.3 Loadbalancer</strong></p><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUMGmR29EMJ3W9zUrXmRey5aKxwCerkx8DmxxJ5IAdY5KzGqdC5uSoJw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>具体细节，参考GCP的官方文档。</p><p><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview">https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview</a></p><h3 id="4-3-阿里云Kubernetes网络方案"><a href="#4-3-阿里云Kubernetes网络方案" class="headerlink" title="4.3 阿里云Kubernetes网络方案"></a><strong>4.3 阿里云Kubernetes网络方案</strong></h3><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUKMC3xnbLMl6bFGnKL3QPRA0iaRWu9RrBNNBLyPyEiclp7ot4tAgkgDtg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>阿里云容器服务产品线的整体架构</p><p> 本章节是介绍阿里云容器服务Kubernetes版ACK（Alibaba Cloud Container Service for Kubernetes）。</p><p><strong>4.3.1 网络模型</strong>  </p><pre><code>    容器服务将Kubernetes网络和阿里云VPC的深度集成，提供了稳定高性能的容器网络。在容器服务中，支持以下类型的互联互通。</code></pre><ol><li><p> <strong>同一个容器集群中，Pod之间相互访问。</strong></p></li><li><p> <strong>同一个容器集群中，Pod访问Service。</strong></p></li><li><p> <strong>同一个容器集群中，ECS访问Service。</strong></p></li><li><p> <strong>Pod直接访问同一个VPC下的ECS。</strong></p></li><li><p> <strong>同一个VPC下的ECS直接访问Pod。</strong></p></li></ol><p> <strong>4.3.2  阿里云Terway网络插件</strong></p><pre><code>    Terway网络插件是阿里云容器服务的网络插件，功能上完全兼容Flannel。        支持将阿里云的弹性网卡分配给容器。        支持基于Kubernetes标准的NetworkPolicy来定义容器间的访问策略，兼容Calico的Network Policy。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUuB3SQOe6nYhGDt3GyicMKxf1IMRMxfztRjKicvKH9MnvxviamOR1EkkcA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><pre><code>    在Terway网络插件中，每个Pod拥有自己网络栈和IP地址。同一台ECS内的Pod之间通信，直接通过机器内部的转发，跨ECS的Pod通信，报文通过VPC的vRouter转发。由于不需要使用VxLAN等的隧道技术封装报文，因此具有较高的通信性能。</code></pre><h3 id="4-4-腾讯云Kubernetes网络方案"><a href="#4-4-腾讯云Kubernetes网络方案" class="headerlink" title="4.4 腾讯云Kubernetes网络方案"></a><strong>4.4 腾讯云Kubernetes网络方案</strong></h3><p>腾讯云容器服务（Tencent Kubernetes Engine ，TKE）基于原生 kubernetes 提供以容器为核心的、高度可扩展的高性能容器管理服务。</p><p>本章节主要参考以下文章，公众号：腾讯云原生 公众号文章：<strong>腾讯云容器服务TKE推出新一代零损耗容器网络</strong></p><p><strong>4.4.1  GlobalRouter</strong> <strong>模式</strong></p><pre><code>    基于 vpc 实现的全局路由模式，目前是 TKE 默认网络方案。该模式依托于 vpc 底层路由能力，不需要在节点上配置 vxlan 等 overlay 设备，就可实现容器网络 和 vpc 网络的互访，并且相比于 calico/flannel 等网络方案，没有额外的解封包，性能也会更好。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUjbGyBAZx0cOF2vSSZoRUmj34T5XxWEAjXnRueh4XMf56hNich1MUibrg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.4.1 VPC-CNI</strong> <strong>模式</strong></p><pre><code>    TKE 基于 CNI 和 VPC 弹性网卡实现的容器网络能力，适用于 Pod 固定 IP，CLB 直通 Pod，Pod 直绑 EIP 等场景。该网络模式下，容器与节点分布在同一网络平面，容器 IP 为 IPAMD 组件所分配的弹性网卡 IP。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgUnDwWHibDstkUaAN3f4OyX1Pico2ZEXP52Q9yNBKQI6XPQRKDAZASkibfA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.4.3  VPC-CNI-**</strong>独立网卡**</p><pre><code>    依托于弹性网卡，将绑定到节点的弹性网卡通过 CNI 配置到容器网络命名空间，实现容器直接独享使用弹性网卡。</code></pre><p><img src="https://img01.sogoucdn.com/net/a/04/link?appid=100520033&url=https://mmbiz.qpic.cn/mmbiz_png/iaSemHs116f35bkesgaQdjQ0vSte7nRgU8Hfx2iaCJaxSorgetVfjAmqcX3OltIXpjK5K6fX9m5OOWQHzb8dbBRw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p><strong>4.5 其他CNI插件</strong>  </p><p>参考链接</p><p><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/">https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/</a></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5  总结"></a><strong>5  总结</strong></h2><pre><code> 本文主要介绍了Kubernetes的网络实现，包括pod的通信，服务（service），Ingress的实现，也简要介绍了云厂商的CNI插件的实现方法。Kubernetes还有其他优秀的网络插件，各个插件的实现方式有所不同，不过Kubernetes网络模型是不变。</code></pre><p> 最后欢迎大家留言沟通交流。</p><h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6 参考文献"></a><strong>6 参考文献</strong></h2><p>Kubernetes集群网络系统</p><p><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/">https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/</a></p><p>Amazon EKS</p><p><a href="https://docs.amazonaws.cn/eks/latest/userguide/external-snat.html">https://docs.amazonaws.cn/eks/latest/userguide/external-snat.html</a></p><p>Amazon VPC CNI</p><p><a href="https://aws.amazon.com/cn/blogs/china/use-amazon-vpc-cni-build-default-net-kubernetes-groups/">https://aws.amazon.com/cn/blogs/china/use-amazon-vpc-cni-build-default-net-kubernetes-groups/</a></p><p>Google Kubernetes Engine (GKE) </p><p><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview">https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview</a></p><p>kubernetes网络和CNI简介</p><p><a href="https://www.jianshu.com/p/88062fa25083">https://www.jianshu.com/p/88062fa25083</a></p><p>Understanding kubernetes networking: pods</p><p><a href="https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727">https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727</a></p><p>containernetworking/cni</p><p><a href="https://github.com/containernetworking/cni">https://github.com/containernetworking/cni</a></p><p>Amazon Elastic Container Service</p><p><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html</a></p><p>CNI - Container Network Interface（容器网络接口）</p><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/cni.html">https://jimmysong.io/kubernetes-handbook/concepts/cni.html</a></p><p>containernetworking/cni</p><p><a href="https://github.com/containernetworking/cni">https://github.com/containernetworking/cni</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Kubernetes网络和云厂商实践浅析&quot;&gt;&lt;a href=&quot;#Kubernetes网络和云厂商实践浅析&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes网络和云厂商实践浅析&quot;&gt;&lt;/a&gt;Kubernetes网络和云厂商实践浅析&lt;/h3&gt;&lt;</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Ingress 控制器的技术选型技巧</title>
    <link href="http://zhangyu.info/2021/04/08/Technical-selection-of-Kubernetes-Ingress-controller/"/>
    <id>http://zhangyu.info/2021/04/08/Technical-selection-of-Kubernetes-Ingress-controller/</id>
    <published>2021-04-07T16:00:00.000Z</published>
    <updated>2021-04-08T08:56:51.700Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="Kubernetes-Ingress-控制器的技术选型技巧"><a href="#Kubernetes-Ingress-控制器的技术选型技巧" class="headerlink" title="Kubernetes Ingress 控制器的技术选型技巧"></a>Kubernetes Ingress 控制器的技术选型技巧</h3></blockquote><blockquote><p>作者：厉辉，腾讯云中间件API网关核心研发成员  </p><blockquote><p>在 Kubernetes 的实践、部署中，为了解决 Pod 迁移、Node Pod 端口、域名动态分配等问题，需要开发人员选择合适的 Ingress 解决方案。面对市场上众多Ingress产品，开发者该如何分辨它们的优缺点？又该如何结合自身的技术栈选择合适的技术方案呢？在本文中，腾讯云中间件核心研发工程师厉辉将为你介绍如何进行 Kubernates Ingress 控制器的技术选型。</p></blockquote><p><strong><strong>名词解释</strong></strong></p><p>阅读本文需要熟悉以下基本概念：</p><ul><li><p>  集群：是指容器运行所需云资源的集合，包含了若干台云服务器、负载均衡器等云资源。</p></li><li><p>  实例（Pod）：由相关的一个或多个容器构成一个实例，这些容器共享相同的存储和网络空间。</p></li><li><p>  工作负载（Node）：Kubernetes 资源对象，用于管理 Pod 副本的创建、调度以及整个生命周期的自动控制。</p></li><li><p>  服务（Service）：由多个相同配置的实例（Pod）和访问这些实例（Pod）的规则组成的微服务。</p></li><li><p>  Ingress：Ingress 是用于将外部 HTTP（S）流量路由到服务（Service）的规则集合。</p></li></ul><p><strong><strong>Kubernetes 访问现状</strong></strong></p><p><img src="http://img.blog.itpub.net/blog/2020/02/24/02cb35c80ce70416.jpeg?x-oss-process=style/bb"></p><p> Kubernetes 的外部访问方式</p><p>在 Kubernetes 中，服务跟 Pod IP 主要供服务在集群内访问使用，对于集群外的应用是不可见的。怎么解决这个问题呢？为了让外部的应用能够访问 Kubernetes 集群中的服务，通常解决办法是 NodePort 和 LoadBalancer。</p><p>这两种方案其实各自都存在一些缺点：</p><ul><li>  NodePort 的缺点是一个端口只能挂载一个 Service，而且为了更高的可用性，需要额外搭建一个负载均衡。</li></ul><ul><li>  LoadBalancer 的缺点则是每个服务都必须要有一个自己的 IP，不论是内网 IP 或者外网 IP。更多情况下，为了保证 LoadBalancer 的能力，一般需要依赖于云服务商。</li></ul><p>在Kubernetes的实践、部署中，为了解决像 Pod 迁移、Node Pod 端口、域名动态分配，或者是 Pod 后台地址动态更新这种问题，就产生了 Ingress 解决方案</p><p><strong><strong>Nginx Ingress 的缺点</strong></strong></p><p>Ingress 是Kubernetes中非常重要的外网流量入口。在Kubernetes中所推荐的默认值为Nginx Ingress，为了与后面Nginx 提供的商业版 Ingress 区分开来，我就称它为Kubernetes Ingress。</p><p>Kubernetes Ingress，顾名思义基于 Nginx 的平台，Nginx 现在是世界上最流行的 Nginx HTTP Sever，相信大家都对 Nginx 也比较熟悉，这是一个优点。它还有一个优点是 Nginx Ingress 接入 Kubernetes 集群所需的配置非常少，而且有很多文档来指引你如何使用它。这对于大部分刚接触 Kubernetes 的人或者创业公司来说，Nginx Ingress 的确是一个非常好的选择。</p><p>但是当 Nginx Ingress 在一些大环境上使用时，就会出现很多问题：</p><ul><li>  第一个问题：Nginx Ingress用了一些 OpenResty 的特性，但最终配置加载还是依赖于原有的 Nginx config reload。当路由配置非常大时，Nginx reload 会耗时很久，时间长达几秒甚至十几秒，这样就会严重影响业务，甚至造成业务中断。</li></ul><ul><li>  第二个问题：Nginx Ingress 的插件开发非常困难。如果你认为 Nginx Ingress 本身插件不够用，需要使用一些定制化插件，这个额外的开发任务对程序员来说是十分痛苦的。因为Nginx Ingress自身的插件能力和可扩展性非常差。</li></ul><p><strong><strong>Ingress 选型原则</strong></strong></p><p>既然发现了 Nginx Ingress 有很多问题，那是不是考虑选择其他开源的、更好用的 Ingress？市场上比 Kubernetes Ingress 好用的Ingress起码有十几家，那么如何从这么多 Ingress 中选择适合自己的呢？</p><p>Ingress 自身是基于 HTTP 网关的，市面上 HTTP 网关主要有这么几种：Nginx、Golang 原生的网关，以及新崛起的 Envoy 。但是每个开发人员所擅长的技术栈不同，所以适合的 Ingress 也会不一样。</p><p>那么问题来了，我们如何选择一个更加好用的 Ingress 呢？或者缩小点范围，熟悉 Nginx 或 OpenResty 的开发人员，应该选择哪一个 Ingress 呢？</p><p>下面来介绍一下我对 Ingress 控制器选型的一些经验。</p><p><img src="http://img.blog.itpub.net/blog/2020/02/24/95f705c188d9885d.jpeg?x-oss-process=style/bb"></p><p>选型原则</p><p><strong>1.基本特点</strong>  </p><p>首先我认为Ingress 控制器应该具备以下基本功能，如果连这些功能都没有，那完全可以直接pass。</p><ul><li><p>  必须开源的，不开源的无法使用。</p></li><li><p>  Kubernetes 中Pod 变化非常频繁，服务发现非常重要。</p></li><li><p>  现在 HTTPS 已经很普及了，TLS 或者 SSL 的能力也非常重要，比如证书管理的功能。</p></li><li><p>  支持 WebSocket 等常见协议，在某些情况下，可能还需要支持 HTTP2 、QUIC 等协议。</p></li></ul><h3 id="2-基础软件"><a href="#2-基础软件" class="headerlink" title="2.基础软件"></a><strong>2.基础软件</strong></h3><p>前面有提到，每个人擅长的技术平台不一样，所以选择自己更加熟悉的 HTTP 网关也显得至关重要。比如 Nginx、HAProxy、Envoy 或者是 Golang 原生网关。因为你熟悉它的原理，在使用中可以实现快速落地。</p><p>在生产环境上，高性能是一个很重要的特性，但比之更重要的是高可用。这意味着你选择的网关，它的可用性、稳定性一定要非常强，只有这样，服务才能稳定。</p><h3 id="3-功能需求"><a href="#3-功能需求" class="headerlink" title="3.功能需求"></a><strong>3.功能需求</strong></h3><p>抛开上述两点，就是公司业务对网关的特殊需求。你选择一个开源产品，最好肯定是开箱能用的。比如你需要 GRPC 协议转换的能力，那当然希望选的网关具备这样的功能。这里简单列一下影响选择的因素：  </p><ul><li><p>  协议：是否支持 HTTP2、HTTP3；</p></li><li><p>  负载均衡算法：最基本的WRR、一致性哈希负载均衡算法是否能够满足需求，还是需要更加复杂的类似EWMA负载均衡算法。</p></li><li><p>  鉴权限流：仅需要简单的鉴权，或更进阶的鉴权方式。又或者需要集成，能够快速的开发出像腾讯云 IM 的鉴权功能。Kubernetes Ingress除了前面我们提到的存在Nginx reload 耗时长、插件扩展能力差的问题，另外它还存在后端节点调整权重的能力不够灵活的问题。</p></li></ul><p><strong><strong>选择 APISIX</strong></strong></p><p>相比Kubernetes Ingress，我个人更推荐 APISIX 作为Ingress ?controller。虽然它在功能上比 Kong 会少很多，但是 APISIX 很好的路由能力、灵活的插件能力，以及本身的高性能，能够弥补在 Ingress 选型上的一些缺点。对于基于 Nginx 或 Openresty 开发的程序员，如果对现在的 Ingress 不满意，我推荐你们去使用 APISIX 作为 Ingress。  </p><p>如何将 APISIX 作为 Ingress 呢？我们首先要做出一个区分，Ingress 是 Kubernetes 名称的定义或者规则定义，Ingress controller 是将 Kubernetes 集群状态同步到网关的一个组件。但 APISIX 本身只是 API 网关，怎么把 APISIX 实现成 Ingress controller 呢？我们先来简要了解一下如何实现 Ingress。</p><p>实现 Ingress，本质上就只有两部分内容：</p><ul><li><p>  第一部分：需要将 Kubernetes 集群中的配置、或 Kubernetes 集群中的状态同步到 APISIX 集群。</p></li><li><p>  第二部分：需要将 APISIX中 的一些概念，比如像服务、upstream 等概念定义为 Kubernetes 中的 CRD。</p></li></ul><p>如果实现了第二部分，通过 Kubernetes Ingress 的配置，便可以很快的产生 APISIX。通过 APISIX Ingress controller 就可以产生 APISIX 相关的配置。当前为了快速的将 APISIX 落地为能够支持 Kubernetes 的 Ingress ，我们创建了一个开源项目，叫 Ingress Controller。</p><p><img src="http://img.blog.itpub.net/blog/2020/02/24/be165da4dc6dd12e.jpeg?x-oss-process=style/bb"></p><p>ingress controller 架构图</p><p>上图为Ingress controller 项目的整体架构图。左边部分为 Kubernetes 集群，这里可以导入一些 yaml 文件，对 Kubernetes 的配置进行变更。右边部分则是 APISIX 集群，以及它的控制面和数据面。从架构图中可以看出，APISIX Ingress 充当了 Kubernetes 集群以及 APISIX 集群之间的连接者。它主要负责监听 Kubernetes 集群中节点的变化，将集群中的状态同步到 APISIX 集群。另外，由于Kubernetes 倡导所有组件都要具备高可用的特性，所以在 APISIX Ingress 设计之初，我们通过双节点或多节点的模式来保证 APISIX ?Ingress Controller 的保障高可用。</p><p><strong><strong>总结</strong></strong></p><p> <img src="http://img.blog.itpub.net/blog/2020/02/24/9754e33944740096.jpeg?x-oss-process=style/bb"> 各类 Ingress 横向对比  </p><p>相对于市面上流行的 Ingress 控制器，我们简单对比来看看 APISIX ingress 有什么优缺点。上图是外国开发人员针对 Kubernetes Ingress 选型做的一张表格。我在原来表格的基础上，结合自己的理解，将 APISIX Ingress 的功能加入了进来。我们可以看到，最左边的是APISIX，后边就是 Kubernetes Ingress 和 Kong Ingress，后面的 Traefik，就是基于 Golang 的 Ingress。HAproxy 是比较常见的，过去是比较流行的负载均衡器。Istio 和 Ambassador 是国外非常流行的两个Ingress。</p></blockquote><blockquote><p>接下来我们总结下这些 Ingress各自的优缺点：</p><ul><li><p>  APISIX Ingress：APISIX Ingress 的优点前面也提到了，它具有非常强大的路由能力、灵活的插件拓展能力，在性能上表现也非常优秀。同时，它的缺点也非常明显，尽管APISIX开源后有非常多的功能，但是缺少落地案例，没有相关的文档指引大家如何使用这些功能。</p></li><li><p>  Kubernetes Ingress：即 Kubernetes 推荐默认使用的 Nginx Ingress。它的主要优点为简单、易接入。缺点是Nginx reload耗时长的问题根本无法解决。另外，虽然可用插件很多，但插件扩展能力非常弱。</p></li><li><p>  Nginx Ingress：主要优点是在于它完全支持 TCP 和 UDP 协议，但是缺失了鉴权方式、流量调度等其他功能。</p></li><li><p>  Kong：其本身就是一个 API 网关，它也算是开创了先河，将 API 网关引入到 Kubernetes 中当 Ingress。另外相对边缘网关，Kong 在鉴权、限流、灰度部署等方面做得非常好。Kong Ingress 还有一个很大的优点：提供了一些 API、服务的定义，可以抽象成 Kubernetes 的 CRD，通过K8S Ingress 配置便可完成同步状态至 Kong 集群。缺点就是部署特别困难，另外在高可用方面，与 APISIX 相比也是相形见绌。</p></li><li><p>  Traefik ：基于 Golang 的 Ingress，它本身是一个微服务网关，在 Ingress 的场景应用比较多。他的主要平台基于 Golang，自身支持的协议也非常多，总体来说是没有什么缺点。如果大家熟悉 Golang 的话，也推荐一用。</p></li><li><p>  HAproxy：是一个久负盛名的负载均衡器。它主要优点是具有非常强大的负载均衡能力，其他方面并不占优势。</p></li><li><p>  Istio Ingress 和 Ambassador Ingress 都是基于非常流行的 Envoy。说实话，我认为这两个 Ingress 没有什么缺点，可能唯一的缺点是他们基于 Envoy 平台，大家对这个平台都不是很熟悉，上手门槛会比较高。</p></li></ul><p>综上所述，大家在了解了各个 Ingress 的优劣势后，可以结合自身情况快速选择适合自己的 Ingress。</p><p>来自 “ ITPUB博客 ” ，链接：<a href="http://blog.itpub.net/31559354/viewspace-2677027/">http://blog.itpub.net/31559354/viewspace-2677027/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h3 id=&quot;Kubernetes-Ingress-控制器的技术选型技巧&quot;&gt;&lt;a href=&quot;#Kubernetes-Ingress-控制器的技术选型技巧&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes Ingress 控制</summary>
      
    
    
    
    <category term="Ingress" scheme="http://zhangyu.info/categories/Ingress/"/>
    
    
    <category term="Ingress" scheme="http://zhangyu.info/tags/Ingress/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes入门-进阶实战</title>
    <link href="http://zhangyu.info/2021/04/08/Kubernetes-remen/"/>
    <id>http://zhangyu.info/2021/04/08/Kubernetes-remen/</id>
    <published>2021-04-07T16:00:00.000Z</published>
    <updated>2021-04-08T09:59:59.264Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="Kubernetes-入门-amp-进阶实战"><a href="#Kubernetes-入门-amp-进阶实战" class="headerlink" title="Kubernetes 入门&amp;进阶实战"></a>Kubernetes 入门&amp;进阶实战</h3></blockquote><blockquote><p>作者：oonamao毛江云，腾讯 CSIG 应用开发工程师</p><p>本文组织方式：</p><p>1.?K8S?是什么，即作用和目的。涉及?K8S?架构的整理，Master?和?Node?之间的关系，以及?K8S?几个重要的组件：API?Server、Scheduler、Controller、etcd?等。<br>2.?K8S?的重要概念，即?K8S?的?API?对象，也就是常常听到的?Pod、Deployment、Service?等。<br>3.?如何配置?kubectl，介绍kubectl工具和配置办法。<br>4.?如何用kubectl?部署服务。<br>5.?如何用kubectl?查看、更新/编辑、删除服务。<br>6.?如何用kubectl?排查部署在K8S集群上的服务出现的问题</p><h3 id="I-K8S-概览"><a href="#I-K8S-概览" class="headerlink" title="I. K8S 概览"></a>I. K8S 概览</h3><h5 id="1-1-K8S-是什么？"><a href="#1-1-K8S-是什么？" class="headerlink" title="1.1 K8S 是什么？"></a>1.1 K8S 是什么？</h5><p>K8S 是Kubernetes的全称，官方称其是：</p><blockquote><p>Kubernetes is an open source system for managing containerized applications across multiple hosts. It provides basic mechanisms for deployment, maintenance, and scaling of applications.</p><p>用于自动部署、扩展和管理“容器化（containerized）应用程序”的开源系统。</p></blockquote><p>翻译成大白话就是：“<strong>K8 是 S 负责自动化运维管理多个 Docker 程序的集群</strong>”。那么问题来了：Docker 运行可方便了，为什么要用 K8S，它有什么优势？</p><p>插一句题外话：</p><ul><li><p>为什么 Kubernetes 要叫 Kubernetes 呢？维基百科已经交代了（老美对星际是真的痴迷）：</p><blockquote><p>Kubernetes（在希腊语意为“舵手”或“驾驶员”）由 Joe Beda、Brendan Burns 和 Craig McLuckie 创立，并由其他谷歌工程师，包括 Brian Grant 和 Tim Hockin 等进行加盟创作，并由谷歌在 2014 年首次对外宣布 。该系统的开发和设计都深受谷歌的 Borg 系统的影响，其许多顶级贡献者之前也是 Borg 系统的开发者。在谷歌内部，Kubernetes 的原始代号曾经是Seven，即星际迷航中的 Borg（博格人）。Kubernetes 标识中舵轮有七个轮辐就是对该项目代号的致意。</p></blockquote></li><li><p>  为什么 Kubernetes 的缩写是 K8S 呢？我个人赞同Why Kubernetes is Abbreviated k8s中说的观点“嘛，写全称也太累了吧，不如整个缩写”。其实只保留首位字符，用具体数字来替代省略的字符个数的做法，还是比较常见的。</p></li></ul><h5 id="1-2-为什么是-K8S"><a href="#1-2-为什么是-K8S" class="headerlink" title="1.2 为什么是 K8S?"></a>1.2 为什么是 K8S?</h5><p>试想下传统的后端部署办法：把程序包（包括可执行二进制文件、配置文件等）放到服务器上，接着运行启动脚本把程序跑起来，同时启动守护脚本定期检查程序运行状态、必要的话重新拉起程序。</p><p>有问题吗？显然有！最大的一个问题在于：**如果服务的请求量上来，已部署的服务响应不过来怎么办？**传统的做法往往是，如果请求量、内存、CPU 超过阈值做了告警，运维马上再加几台服务器，部署好服务之后，接入负载均衡来分担已有服务的压力。</p><p>问题出现了：从监控告警到部署服务，中间需要人力介入！那么，<strong>有没有办法自动完成服务的部署、更新、卸载和扩容、缩容呢？</strong></p><p><strong>这，就是 K8S 要做的事情：自动化运维管理 Docker（容器化）程序</strong>。</p><h5 id="1-3-K8S-怎么做？"><a href="#1-3-K8S-怎么做？" class="headerlink" title="1.3 K8S 怎么做？"></a>1.3 K8S 怎么做？</h5><p>我们已经知道了 K8S 的核心功能：自动化运维管理多个容器化程序。那么 K8S 怎么做到的呢？这里，我们从宏观架构上来学习 K8S 的设计思想。首先看下图，图片来自文章Components of Kubernetes Architecture：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/ca2fc2b718b3e342.jpeg?x-oss-process=style/bb"></p><p>K8S 是属于<strong>主从设备模型（Master-Slave 架构）</strong>，即有 Master 节点负责核心的调度、管理和运维，Slave 节点则在执行用户的程序。但是在 K8S 中，主节点一般被称为<strong>Master Node 或者 Head Node</strong>（本文采用 Master Node 称呼方式），而从节点则被称为<strong>Worker Node 或者 Node</strong>（本文采用 Worker Node 称呼方式）。</p><p>要注意一点：Master Node 和 Worker Node 是分别安装了 K8S 的 Master 和 Woker 组件的实体服务器，每个 Node 都对应了一台实体服务器（虽然 Master Node 可以和其中一个 Worker Node 安装在同一台服务器，但是建议 Master Node 单独部署），<strong>所有 Master Node 和 Worker Node 组成了 K8S 集群</strong>，同一个集群可能存在多个 Master Node 和 Worker Node。</p><p>首先来看<strong>Master Node</strong>都有哪些组件：</p><ul><li><p>  <strong>API Server</strong>。<strong>K8S 的请求入口服务</strong>。API Server 负责接收 K8S 所有请求（来自 UI 界面或者 CLI 命令行工具），然后，API Server 根据用户的具体请求，去通知其他组件干活。</p></li><li><p>  <strong>Scheduler</strong>。<strong>K8S 所有 Worker Node 的调度器</strong>。当用户要部署服务时，Scheduler 会选择最合适的 Worker Node（服务器）来部署。</p></li><li><p>  <strong>Controller Manager</strong>。<strong>K8S 所有 Worker Node 的监控器</strong>。Controller Manager 有很多具体的 Controller，在文章Components of Kubernetes Architecture中提到的有 Node Controller、Service Controller、Volume Controller 等。Controller 负责监控和调整在 Worker Node 上部署的服务的状态，比如用户要求 A 服务部署 2 个副本，那么当其中一个服务挂了的时候，Controller 会马上调整，让 Scheduler 再选择一个 Worker Node 重新部署服务。</p></li><li><p>  <strong>etcd</strong>。<strong>K8S 的存储服务</strong>。etcd 存储了 K8S 的关键配置和用户配置，K8S 中仅 API Server 才具备读写权限，其他组件必须通过 API Server 的接口才能读写数据（见Kubernetes Works Like an Operating System）。</p></li></ul><p>接着来看<strong>Worker Node</strong>的组件，笔者更赞同HOW DO APPLICATIONS RUN ON KUBERNETES文章中提到的组件介绍：</p><ul><li><p>  <strong>Kubelet</strong>。<strong>Worker Node 的监视器，以及与 Master Node 的通讯器</strong>。Kubelet 是 Master Node 安插在 Worker Node 上的“眼线”，它会定期向 Worker Node 汇报自己 Node 上运行的服务的状态，并接受来自 Master Node 的指示采取调整措施。</p></li><li><p>  <strong>Kube-Proxy</strong>。<strong>K8S 的网络代理</strong>。私以为称呼为 Network-Proxy 可能更适合？Kube-Proxy 负责 Node 在 K8S 的网络通讯、以及对外部网络流量的负载均衡。</p></li><li><p>  <strong>Container Runtime</strong>。<strong>Worker Node 的运行环境</strong>。即安装了容器化所需的软件环境确保容器化程序能够跑起来，比如 Docker Engine。大白话就是帮忙装好了 Docker 运行环境。</p></li><li><p>  <strong>Logging Layer</strong>。<strong>K8S 的监控状态收集器</strong>。私以为称呼为 Monitor 可能更合适？Logging Layer 负责采集 Node 上所有服务的 CPU、内存、磁盘、网络等监控项信息。</p></li><li><p>  <strong>Add-Ons</strong>。<strong>K8S 管理运维 Worker Node 的插件组件</strong>。有些文章认为 Worker Node 只有三大组件，不包含 Add-On，但笔者认为 K8S 系统提供了 Add-On 机制，让用户可以扩展更多定制化功能，是很不错的亮点。</p></li></ul><p>总结来看，<strong>K8S 的 Master Node 具备：请求入口管理（API Server），Worker Node 调度（Scheduler），监控和自动调节（Controller Manager），以及存储功能（etcd）；而 K8S 的 Worker Node 具备：状态和监控收集（Kubelet），网络和负载均衡（Kube-Proxy）、保障容器化运行环境（Container Runtime）、以及定制化功能（Add-Ons）。</strong></p><p>到这里，相信你已经对 K8S 究竟是做什么的，有了大概认识。接下来，再来认识下 K8S 的 Deployment、Pod、Replica Set、Service 等，但凡谈到 K8S，就绕不开这些名词，而这些名词也是最让 K8S 新手们感到头疼、困惑的。</p><h3 id="II-K8S-重要概念"><a href="#II-K8S-重要概念" class="headerlink" title="II. K8S 重要概念"></a>II. K8S 重要概念</h3><h5 id="2-1-Pod-实例"><a href="#2-1-Pod-实例" class="headerlink" title="2.1 Pod 实例"></a>2.1 Pod 实例</h5><p>官方对于<strong>Pod</strong>的解释是：</p><blockquote><p><strong>Pod</strong>是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。</p></blockquote><p>这样的解释还是很难让人明白究竟 Pod 是什么，但是对于 K8S 而言，Pod 可以说是所有对象中最重要的概念了！因此，我们<strong>必须首先清楚地知道“Pod 是什么”</strong>，再去了解其他的对象。</p><p>从官方给出的定义，联想下“最小的 xxx 单元”，是不是可以想到本科在学校里学习“进程”的时候，教科书上有一段类似的描述：资源分配的最小单位；还有”线程“的描述是：CPU 调度的最小单位。什么意思呢？<strong>”最小 xx 单位“要么就是事物的衡量标准单位，要么就是资源的闭包、集合</strong>。前者比如长度米、时间秒；后者比如一个”进程“是存储和计算的闭包，一个”线程“是 CPU 资源（包括寄存器、ALU 等）的闭包。</p><p>同样的，<strong>Pod 就是 K8S 中一个服务的闭包</strong>。这么说的好像还是有点玄乎，更加云里雾里了。简单来说，<strong>Pod 可以被理解成一群可以共享网络、存储和计算资源的容器化服务的集合</strong>。再打个形象的比喻，在同一个 Pod 里的几个 Docker 服务/程序，好像被部署在同一台机器上，可以通过 localhost 互相访问，并且可以共用 Pod 里的存储资源（这里是指 Docker 可以挂载 Pod 内的数据卷，数据卷的概念，后文会详细讲述，暂时理解为“需要手动 mount 的磁盘”）。笔者总结 Pod 如下图，可以看到：<strong>同一个 Pod 之间的 Container 可以通过 localhost 互相访问，并且可以挂载 Pod 内所有的数据卷；但是不同的 Pod 之间的 Container 不能用 localhost 访问，也不能挂载其他 Pod 的数据卷</strong>。</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/699bc2906b2d2360.jpeg?x-oss-process=style/bb"></p><p>对 Pod 有直观的认识之后，接着来看 K8S 中 Pod 究竟长什么样子，具体包括哪些资源？</p><p>K8S 中所有的对象都通过 yaml 来表示，笔者从官方网站摘录了一个最简单的 Pod 的 yaml：</p><p><code>apiVersion:?v1   kind:?Pod   metadata:   ??name:?memory-demo   ??namespace:?mem-example   spec:   ??containers:   ??-?name:?memory-demo-ctr   ????image:?polinux/stress   ????resources:   ??????limits:   ????????memory:?&quot;200Mi&quot;   ??????requests:   ????????memory:?&quot;100Mi&quot;   ????command:?[&quot;stress&quot;]   ????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;]   ????volumeMounts:   ????-?name:?redis-storage   ??????mountPath:?/data/redis   ??volumes:   ??-?name:?redis-storage   ????emptyDir:?&#123;&#125;   </code></p><p>看不懂不必慌张，且耐心听下面的解释：</p><ul><li><p>  <code>apiVersion</code>记录 K8S 的 API Server 版本，现在看到的都是<code>v1</code>，用户不用管。</p></li><li><p>  <code>kind</code>记录该 yaml 的对象，比如这是一份 Pod 的 yaml 配置文件，那么值内容就是<code>Pod</code>。</p></li><li><p>  <code>metadata</code>记录了 Pod 自身的元数据，比如这个 Pod 的名字、这个 Pod 属于哪个 namespace（命名空间的概念，后文会详述，暂时理解为“同一个命名空间内的对象互相可见”）。</p></li><li><p>  <code>spec</code>记录了 Pod 内部所有的资源的详细信息，看懂这个很重要：</p></li></ul><ul><li><p>  <code>containers</code>记录了 Pod 内的容器信息，<code>containers</code>包括了：<code>name</code>容器名，<code>image</code>容器的镜像地址，<code>resources</code>容器需要的 CPU、内存、GPU 等资源，<code>command</code>容器的入口命令，<code>args</code>容器的入口参数，<code>volumeMounts</code>容器要挂载的 Pod 数据卷等。可以看到，<strong>上述这些信息都是启动容器的必要和必需的信息</strong>。</p></li><li><p>  <code>volumes</code>记录了 Pod 内的数据卷信息，后文会详细介绍 Pod 的数据卷。</p></li></ul><p><strong>2.2 Volume 数据卷</strong></p><p>K8S 支持很多类型的 volume 数据卷挂载，具体请参见K8S 卷。前文就“如何理解 volume”提到：“<strong>需要手动 mount 的磁盘</strong>”，此外，有一点可以帮助理解：<strong>数据卷 volume 是 Pod 内部的磁盘资源</strong>。</p><p>其实，单单就 Volume 来说，不难理解。但是上面还看到了<code>volumeMounts</code>，这俩是什么关系呢？</p><p><strong>volume 是 K8S 的对象，对应一个实体的数据卷；而 volumeMounts 只是 container 的挂载点，对应 container 的其中一个参数</strong>。但是，<strong>volumeMounts 依赖于 volume</strong>，只有当 Pod 内有 volume 资源的时候，该 Pod 内部的 container 才可能有 volumeMounts。</p><h5 id="2-3-Container-容器"><a href="#2-3-Container-容器" class="headerlink" title="2.3 Container 容器"></a>2.3 Container 容器</h5><p>本文中提到的镜像 Image、容器 Container，都指代了 Pod 下的一个<code>container</code>。关于 K8S 中的容器，在 2.1Pod 章节都已经交代了，这里无非再啰嗦一句：<strong>一个 Pod 内可以有多个容器 container</strong>。</p><p>在 Pod 中，容器也有分类，对这个感兴趣的同学欢迎自行阅读更多资料：</p><ul><li><p>  <strong>标准容器 Application Container</strong>。</p></li><li><p>  <strong>初始化容器 Init Container</strong>。</p></li><li><p>  <strong>边车容器 Sidecar Container</strong>。</p></li><li><p>  <strong>临时容器 Ephemeral Container</strong>。</p></li></ul><p>一般来说，我们部署的大多是<strong>标准容器（ Application Container）</strong>。</p><h5 id="2-4-Deployment-和-ReplicaSet（简称-RS）"><a href="#2-4-Deployment-和-ReplicaSet（简称-RS）" class="headerlink" title="2.4 Deployment 和 ReplicaSet（简称 RS）"></a>2.4 Deployment 和 ReplicaSet（简称 RS）</h5><p>除了 Pod 之外，K8S 中最常听到的另一个对象就是 Deployment 了。那么，什么是 Deployment 呢？官方给出了一个要命的解释：</p><blockquote><p>一个 <em>Deployment</em> 控制器为 Pods 和 ReplicaSets 提供声明式的更新能力。</p><p>你负责描述 Deployment 中的 _目标状态_，而 Deployment 控制器以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，并通过新的 Deployment 收养其资源。</p></blockquote><p>翻译一下：<strong>Deployment 的作用是管理和控制 Pod 和 ReplicaSet，管控它们运行在用户期望的状态中</strong>。哎，打个形象的比喻，** Deployment 就是包工头 **，主要负责监督底下的工人 Pod 干活，确保每时每刻有用户要求数量的 Pod 在工作。如果一旦发现某个工人 Pod 不行了，就赶紧新拉一个 Pod 过来替换它。</p><p>新的问题又来了：那什么是 ReplicaSets 呢？</p><blockquote><p>ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。</p></blockquote><p>再来翻译下：ReplicaSet 的作用就是管理和控制 Pod，管控他们好好干活。但是，ReplicaSet 受控于 Deployment。形象来说，<strong>ReplicaSet 就是总包工头手下的小包工头</strong>。</p><p>笔者总结得到下面这幅图，希望能帮助理解：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/84b954d509a70147.jpeg?x-oss-process=style/bb"></p><p>新的问题又来了：<strong>如果都是为了管控 Pod 好好干活，为什么要设置 Deployment 和 ReplicaSet 两个层级呢，直接让 Deployment 来管理不可以吗？</strong></p><p>回答：不清楚，但是私以为是因为先有 ReplicaSet，但是使用中发现 ReplicaSet 不够满足要求，于是又整了一个 Deployment（<strong>有清楚 Deployment 和 ReplicaSet 联系和区别的小伙伴欢迎留言啊</strong>）。</p><p>但是，从 K8S 使用者角度来看，用户会直接操作 Deployment 部署服务，而当 Deployment 被部署的时候，K8S 会自动生成要求的 ReplicaSet 和 Pod。在K8S 官方文档中也指出用户只需要关心 Deployment 而不操心 ReplicaSet：</p><blockquote><p>This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.</p><p>这实际上意味着您可能永远不需要操作 ReplicaSet 对象：直接使用 Deployments 并在规范部分定义应用程序。</p></blockquote><p>补充说明：在 K8S 中还有一个对象 — <strong>ReplicationController（简称 RC）</strong>，官方文档对它的定义是：</p><blockquote><p><em>ReplicationController</em> 确保在任何时候都有特定数量的 Pod 副本处于运行状态。换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。</p></blockquote><p>怎么样，和 ReplicaSet 是不是很相近？在Deployments, ReplicaSets, and pods教程中说“ReplicationController 是 ReplicaSet 的前身”，官方也推荐用 Deployment 取代 ReplicationController 来部署服务。</p><h5 id="2-5-Service-和-Ingress"><a href="#2-5-Service-和-Ingress" class="headerlink" title="2.5 Service 和 Ingress"></a>2.5 Service 和 Ingress</h5><p>吐槽下 K8S 的概念/对象/资源是真的多啊！<strong>前文介绍的 Deployment、ReplicationController 和 ReplicaSet 主要管控 Pod 程序服务；那么，Service 和 Ingress 则负责管控 Pod 网络服务</strong>。</p><p>我们先来看看官方文档中 Service 的定义：</p><blockquote><p>将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。</p><p>使用 Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。</p></blockquote><p>翻译下：K8S 中的服务（Service）并不是我们常说的“服务”的含义，而更像是网关层，是若干个 Pod 的流量入口、流量均衡器。</p><p>那么，<strong>为什么要 Service 呢</strong>？</p><p>私以为在这一点上，官方文档讲解地非常清楚：</p><blockquote><p>Kubernetes Pod 是有生命周期的。它们可以被创建，而且销毁之后不会再启动。如果您使用 Deployment 来运行您的应用程序，则它可以动态创建和销毁 Pod。</p><p>每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。</p><p>这导致了一个问题：如果一组 Pod（称为“后端”）为群集内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用工作量的后端部分？</p></blockquote><p>补充说明：K8S 集群的网络管理和拓扑也有特别的设计，以后会专门出一章节来详细介绍 K8S 中的网络。这里需要清楚一点：K8S 集群内的每一个 Pod 都有自己的 IP（是不是很类似一个 Pod 就是一台服务器，然而事实上是多个 Pod 存在于一台服务器上，只不过是 K8S 做了网络隔离），在 K8S 集群内部还有 DNS 等网络服务（一个 K8S 集群就如同管理了多区域的服务器，可以做复杂的网络拓扑）。</p><p>此外，笔者推荐k8s 外网如何访问业务应用对于 Service 的介绍，不过对于新手而言，推荐阅读前半部分对于 service 的介绍即可，后半部分就太复杂了。我这里做了简单的总结：</p><p><strong>Service 是 K8S 服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”</strong>。举个例子，我们的一个服务 A，部署了 3 个备份，也就是 3 个 Pod；对于用户来说，只需要关注一个 Service 的入口就可以，而不需要操心究竟应该请求哪一个 Pod。优势非常明显：<strong>一方面外部用户不需要感知因为 Pod 上服务的意外崩溃、K8S 重新拉起 Pod 而造成的 IP 变更，外部用户也不需要感知因升级、变更服务带来的 Pod 替换而造成的 IP 变化，另一方面，Service 还可以做流量负载均衡</strong>。</p><p>但是，Service 主要负责 K8S 集群内部的网络拓扑。那么集群外部怎么访问集群内部呢？这个时候就需要 Ingress 了，官方文档中的解释是：</p><blockquote><p>Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。</p><p>Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。</p></blockquote><p>翻译一下：Ingress 是整个 K8S 集群的接入层，复杂集群内外通讯。</p><p>最后，笔者把 Ingress 和 Service 的关系绘制网络拓扑关系图如下，希望对理解这两个概念有所帮助：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/6d5aba4f055deac4.jpeg?x-oss-process=style/bb"></p><h5 id="2-6-namespace-命名空间"><a href="#2-6-namespace-命名空间" class="headerlink" title="2.6 namespace 命名空间"></a>2.6 namespace 命名空间</h5><p>和前文介绍的所有的概念都不一样，namespace 跟 Pod 没有直接关系，而是 K8S 另一个维度的对象。或者说，前文提到的概念都是为了服务 Pod 的，而 namespace 则是为了服务整个 K8S 集群的。</p><p>那么，namespace 是什么呢？</p><p>上官方文档定义：</p><blockquote><p>Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。这些虚拟集群被称为名字空间。</p></blockquote><p>翻译一下：<strong>namespace 是为了把一个 K8S 集群划分为若干个资源不可共享的虚拟集群而诞生的</strong>。</p><p>也就是说，<strong>可以通过在 K8S 集群内创建 namespace 来分隔资源和对象</strong>。比如我有 2 个业务 A 和 B，那么我可以创建 ns-a 和 ns-b 分别部署业务 A 和 B 的服务，如在 ns-a 中部署了一个 deployment，名字是 hello，返回用户的是“hello a”；在 ns-b 中也部署了一个 deployment，名字恰巧也是 hello，返回用户的是“hello b”（要知道，在同一个 namespace 下 deployment 不能同名；但是不同 namespace 之间没有影响）。前文提到的所有对象，都是在 namespace 下的；当然，也有一些对象是不隶属于 namespace 的，而是在 K8S 集群内全局可见的，官方文档提到的可以通过命令来查看，具体命令的使用办法，笔者会出后续的实战文章来介绍，先贴下命令：</p><p>`#?位于名字空间中的资源<br>kubectl?api-resources?–namespaced=true  </p><p>#?不在名字空间中的资源<br>kubectl?api-resources?–namespaced=false<br>`</p><p>不在 namespace 下的对象有：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/ff89644c91845045.jpeg?x-oss-process=style/bb"></p><p>在 namespace 下的对象有（部分）：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/a552642c6e79015d.jpeg?x-oss-process=style/bb"></p><h5 id="2-7-其他"><a href="#2-7-其他" class="headerlink" title="2.7 其他"></a>2.7 其他</h5><p>K8S 的对象实在太多了，2.1-2.6 介绍的是在实际使用 K8S 部署服务最常见的。其他的还有 Job、CronJob 等等，在对 K8S 有了比较清楚的认知之后，再去学习更多的 K8S 对象，不是难事。</p><h3 id="III-配置-kubectl"><a href="#III-配置-kubectl" class="headerlink" title="III. 配置 kubectl"></a>III. 配置 kubectl</h3><h5 id="3-1-什么是-kubectl？"><a href="#3-1-什么是-kubectl？" class="headerlink" title="3.1 什么是 kubectl？"></a>3.1 什么是 kubectl？</h5><p>官方文档中介绍 kubectl 是：</p><blockquote><p>Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数–kubeconfig 来指定其他位置的 kubeconfig 文件。</p></blockquote><p>也就是说，可以通过 kubectl 来操作 K8S 集群，基本语法：</p><blockquote><p>使用以下语法 <code>kubectl</code> 从终端窗口运行命令：</p><p><code>kubectl?[command]?[TYPE]?[NAME]?[flags]   </code></p><p>其中 <code>command</code>、<code>TYPE</code>、<code>NAME</code> 和 <code>flags</code> 分别是：</p><ul><li><p>  <code>command</code>：指定要对一个或多个资源执行的操作，例如 <code>create</code>、<code>get</code>、<code>describe</code>、<code>delete</code>。</p></li><li><p>  <code>TYPE</code>：指定资源类型。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果:</p></li></ul><p><code>```shell   kubectl?get?pod?pod1   kubectl?get?pods?pod1   kubectl?get?po?pod1  </code></p><p>``<br>-?<code>NAME</code>：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息?<code>kubectl get pods</code>。  </p><p>在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：  </p><p>-?要按类型和名称指定资源：<br>??-?要对所有类型相同的资源进行分组，请执行以下操作：<code>TYPE1 name1 name2 name&lt;#&gt;</code>。<br>?例子：<code>kubectl get pod example-pod1 example-pod2</code><br>??-?分别指定多个资源类型：<code>TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE&lt;#&gt;/name&lt;#&gt;</code>。<br>?例子：<code>kubectl get pod/example-pod1 replicationcontroller/example-rc1</code><br>-?用一个或多个文件指定资源：<code>-f file1 -f file2 -f file&lt;#&gt;</code><br>??-?<a href="#general-config-tips">使用?YAML?而不是?JSON</a>?因为 YAML 更容易使用，特别是用于配置文件时。<br>?例子：<code>kubectl get -f ./pod.yaml</code><br>-?<code>flags</code>:?指定可选的参数。例如，可以使用?<code>-s</code>?或?<code>-server</code>?参数指定 Kubernetes API 服务器的地址和端口。<br>``</p></blockquote><p>就如何使用 kubectl 而言，官方文档已经说得非常清楚。不过对于新手而言，还是需要解释几句：</p><ol><li><p> kubectl 是 K8S 的命令行工具，并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。</p></li><li><p> kubectl 是通过本地的配置文件来连接到 K8S 集群的，默认保存在$HOME/.kube 目录下；也可以通过 KUBECONFIG 环境变量或设置命令参数–kubeconfig 来指定其他位置的 kubeconfig 文件【官方文档】。</p></li></ol><p>接下来，一起看看怎么使用 kubectl 吧，切身感受下 kubectl 的使用。</p><p>请注意，如何安装 kubectl 的办法有许多非常明确的教程，比如《安装并配置 kubectl》，本文不再赘述。</p><h5 id="1-2-怎么配置-kubectl？"><a href="#1-2-怎么配置-kubectl？" class="headerlink" title="1.2 怎么配置 kubectl？"></a>1.2 怎么配置 kubectl？</h5><p><strong>第一步，必须准备好要连接/使用的 K8S 的配置文件</strong>，笔者给出一份杜撰的配置：</p><p><code>apiVersion:?v1   clusters:   -?cluster:   ????certificate-authority-data:?thisisfakecertifcateauthoritydata00000000000   ????server:?   ??name:?cls-dev   contexts:   -?context:   ????cluster:?cls-dev   ????user:?kubernetes-admin   ??name:?kubernetes-admin@test   current-context:?kubernetes-admin@test   kind:?Config   preferences:?&#123;&#125;   users:   -?name:?kubernetes-admin   ??user:   ????token:?thisisfaketoken00000   </code></p><p>解读如下：</p><ul><li>  <code>clusters</code>记录了 clusters（一个或多个 K8S 集群）信息：</li></ul><ul><li><p>  <code>name</code>是这个 cluster（K8S 集群）的名称代号</p></li><li><p>  <code>server</code>是这个 cluster（K8S 集群）的访问方式，一般为 IP+PORT</p></li><li><p>  <code>certificate-authority-data</code>是证书数据，只有当 cluster（K8S 集群）的连接方式是 https 时，为了安全起见需要证书数据</p></li></ul><ul><li>  <code>users</code>记录了访问 cluster（K8S 集群）的账号信息：</li></ul><ul><li><p>  <code>name</code>是用户账号的名称代号</p></li><li><p>  <code>user/token</code>是用户的 token 认证方式，token 不是用户认证的唯一方式，其他还有账号+密码等。</p></li></ul><ul><li>  <code>contexts</code>是上下文信息，包括了 cluster（K8S 集群）和访问 cluster（K8S 集群）的用户账号等信息：</li></ul><ul><li><p>  <code>name</code>是这个上下文的名称代号</p></li><li><p>  <code>cluster</code>是 cluster（K8S 集群）的名称代号</p></li><li><p>  <code>user</code>是访问 cluster（K8S 集群）的用户账号代号</p></li></ul><ul><li><p>  <code>current-context</code>记录当前 kubectl 默认使用的上下文信息</p></li><li><p>  <code>kind</code>和<code>apiVersion</code>都是固定值，用户不需要关心</p></li><li><p>  <code>preferences</code>则是配置文件的其他设置信息，笔者没有使用过，暂时不提。</p></li></ul><p><strong>第二步，给 kubectl 配置上配置文件</strong>。</p><ol><li><p> <strong><code>--kubeconfig</code>参数</strong>。第一种办法是每次执行 kubectl 的时候，都带上<code>--kubeconfig=$&#123;CONFIG_PATH&#125;</code>。给一点温馨小提示：每次都带这么一长串的字符非常麻烦，可以用 alias 别名来简化码字量，比如<code>alias k=kubectl --kubeconfig=$&#123;CONFIG_PATH&#125;</code>。</p></li><li><p> <strong><code>KUBECONFIG</code>环境变量</strong>。第二种做法是使用环境变量<code>KUBECONFIG</code>把所有配置文件都记录下来，即<code>export KUBECONFIG=$KUBECONFIG:$&#123;CONFIG_PATH&#125;</code>。接下来就可以放心执行 kubectl 命令了。</p></li><li><p> <strong>$HOME/.kube/config 配置文件</strong>。第三种做法是把配置文件的内容放到$HOME/.kube/config 内。具体做法为：</p></li></ol><ol><li><p> 如果$HOME/.kube/config 不存在，那么<code>cp $&#123;CONFIG_PATH&#125; $HOME/.kube/config</code>即可；</p></li><li><p> 如果如果 $HOME/.kube/config已经存在，那么需要把新的配置内容加到 $HOME/.kube/config 下。单单只是<code>cat $&#123;CONFIG_PATH&#125; &gt;&gt; $HOME/.kube/config</code>是不行的，正确的做法是：<code>KUBECONFIG=$HOME/.kube/config:$&#123;CONFIG_PATH&#125; kubectl config view --flatten &gt; $HOME/.kube/config</code> 。解释下这个命令的意思：先把所有的配置文件添加到环境变量<code>KUBECONFIG</code>中，然后执行<code>kubectl config view --flatten</code>打印出有效的配置文件内容，最后覆盖$HOME/.kube/config 即可。</p></li></ol><p>请注意，上述操作的优先级分别是 1&gt;2&gt;3，也就是说，kubectl 会优先检查<code>--kubeconfig</code>，若无则检查<code>KUBECONFIG</code>，若无则最后检查$HOME/.kube/config，如果还是没有，报错。但凡某一步找到了有效的 cluster，就中断检查，去连接 K8S 集群了。</p><p><strong>第三步：配置正确的上下文</strong></p><p>按照第二步的做法，如果配置文件只有一个 cluster 是没有任何问题的，但是对于有多个 cluster 怎么办呢？到这里，有几个关于配置的必须掌握的命令：</p><ul><li><p><code>kubectl config get-contexts</code>。列出所有上下文信息。</p><p>  <img src="http://img.blog.itpub.net/blog/2020/12/29/81e0be1ac5829f3a.jpeg?x-oss-process=style/bb"></p></li><li><p><code>kubectl config current-context</code>。查看当前的上下文信息。其实，命令 1 线束出来的*所指示的就是当前的上下文信息。</p><p>  <img src="http://img.blog.itpub.net/blog/2020/12/29/0992a4a267e2508e.jpeg?x-oss-process=style/bb"></p></li><li><p><code>kubectl config use-context $&#123;CONTEXT_NAME&#125;</code>。更改上下文信息。</p><p>  <img src="http://img.blog.itpub.net/blog/2020/12/29/2bd15b22f0369a1a.jpeg?x-oss-process=style/bb"></p></li><li><p><code>kubectl config set-context $&#123;CONTEXT_NAME&#125;|--current --$&#123;KEY&#125;=$&#123;VALUE&#125;</code>。修改上下文的元素。比如可以修改用户账号、集群信息、连接到 K8S 后所在的 namespace。</p><p>  <img src="http://img.blog.itpub.net/blog/2020/12/29/995ca816a3c20dfe.jpeg?x-oss-process=style/bb"></p><p>  关于该命令，还有几点要啰嗦的：</p></li></ul><ul><li><p>  <code>config set-context</code>可以修改任何在配置文件中的上下文信息，只需要在命令中指定上下文名称就可以。而–current 则指代当前上下文。</p></li><li><p>上下文信息所包括的内容有：cluster 集群（名称）、用户账号（名称）、连接到 K8S 后所在的 namespace，因此有<code>config set-context</code>严格意义上的用法：</p><p>  <code>kubectl config set-context [NAME|--current] [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] [options]</code></p><p>  （备注：[options]可以通过 kubectl options 查看）</p></li></ul><p>综上，如何操作 kubectl 配置都已交代。</p><h3 id="IV-kubectl-部署服务"><a href="#IV-kubectl-部署服务" class="headerlink" title="IV. kubectl 部署服务"></a>IV. kubectl 部署服务</h3><p>K8S 核心功能就是部署运维容器化服务，因此最重要的就是如何又快又好地部署自己的服务了。本章会介绍如何部署 Pod 和 Deployment。</p><h5 id="2-1-如何部署-Pod？"><a href="#2-1-如何部署-Pod？" class="headerlink" title="2.1 如何部署 Pod？"></a>2.1 如何部署 Pod？</h5><p>通过 kubectl 部署 Pod 的办法分为两步：1). 准备 Pod 的 yaml 文件；2). 执行 kubectl 命令部署</p><p><strong>第一步：准备 Pod 的 yaml 文件</strong>。关于 Pod 的 yaml 文件初步解释，本系列上一篇文章《K8S 系列一：概念入门》已经有了初步介绍，这里再复习下：</p><p><code>apiVersion:?v1   kind:?Pod   metadata:   ??name:?memory-demo   ??namespace:?mem-example   spec:   ??containers:   ??-?name:?memory-demo-ctr   ????image:?polinux/stress   ????resources:   ??????limits:   ????????memory:?&quot;200Mi&quot;   ??????requests:   ????????memory:?&quot;100Mi&quot;   ????command:?[&quot;stress&quot;]   ????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;]   ????volumeMounts:   ????-?name:?redis-storage   ??????mountPath:?/data/redis   ??volumes:   ??-?name:?redis-storage   ????emptyDir:?&#123;&#125;   </code></p><p>继续解读：</p><ul><li>  <code>metadata</code>，对于新入门的同学来说，需要重点掌握的两个字段：</li></ul><ul><li><p>  <code>name</code>。这个 Pod 的名称，后面到 K8S 集群中查找 Pod 的关键字段。</p></li><li><p>  <code>namespace</code>。命名空间，即该 Pod 隶属于哪个 namespace 下，关于 Pod 和 namespace 的关系，上一篇文章已经交代了。</p></li></ul><ul><li>  <code>spec</code>记录了 Pod 内部所有的资源的详细信息，这里我们重点查看<code>containers</code>下的几个重要字段：</li></ul><ul><li><p>  <code>name</code>。Pod 下该容器名称，后面查找 Pod 下的容器的关键字段。</p></li><li><p>  <code>image</code>。容器的镜像地址，K8S 会根据这个字段去拉取镜像。</p></li><li><p><code>resources</code>。容器化服务涉及到的 CPU、内存、GPU 等资源要求。可以看到有<code>limits</code>和<code>requests</code>两个子项，那么这两者有什么区别吗，该怎么使用？在What’s the difference between Pod resources.limits and resources.requests in Kubernetes?回答了：</p><p>  <strong><code>limits</code>是 K8S 为该容器至多分配的资源配额；而<code>requests</code>则是 K8S 为该容器至少分配的资源配额</strong>。打个比方，配置中要求了 memory 的<code>requests</code>为 100M，而此时如果 K8S 集群中所有的 Node 的可用内存都不足 100M，那么部署服务会失败；又如果有一个 Node 的内存有 16G 充裕，可以部署该 Pod，而在运行中，该容器服务发生了内存泄露，那么一旦超过 200M 就会因为 OOM 被 kill，尽管此时该机器上还有 15G+的内存。</p></li><li><p>  <code>command</code>。容器的入口命令。对于这个笔者还存在很多困惑不解的地方，暂时挖个坑，有清楚的同学欢迎留言。</p></li><li><p>  <code>args</code>。容器的入口参数。同上，有清楚的同学欢迎留言。</p></li><li><p>  <code>volumeMounts</code>。容器要挂载的 Pod 数据卷等。请务必记住：<strong>Pod 的数据卷只有被容器挂载后才能使用</strong>！</p></li></ul><p><strong>第二步：执行 kubectl 命令部署</strong>。有了 Pod 的 yaml 文件之后，就可以用 kubectl 部署了，命令非常简单：<code>kubectl create -f $&#123;POD_YAML&#125;</code>。</p><p>随后，会提示该命令是否执行成功，比如 yaml 内容不符合要求，则会提示哪一行有问题：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/328f52b359badeff.jpeg?x-oss-process=style/bb"></p><p>修正后，再次部署：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/7adb153cf3477539.jpeg?x-oss-process=style/bb"></p><h5 id="2-2-如何部署-Deployment？"><a href="#2-2-如何部署-Deployment？" class="headerlink" title="2.2 如何部署 Deployment？"></a>2.2 如何部署 Deployment？</h5><p><strong>第一步：准备 Deployment 的 yaml 文件</strong>。首先来看 Deployment 的 yaml 文件内容：</p><p> <code>apiVersion:?extensions/v1beta1   ?kind:?Deployment   ?metadata:   ???name:?rss-site   ???namespace:?mem-example   ?spec:   ???replicas:?2   ???template:   ?????metadata:   ???????labels:   ?????????app:?web   ?????spec:   ??????containers:   ???????-?name:?memory-demo-ctr   ?????????image:?polinux/stress   ?????????resources:   ?????????limits:   ???????????emory:?&quot;200Mi&quot;   ?????????requests:   ???????????memory:?&quot;100Mi&quot;   ?????????command:?[&quot;stress&quot;]   ?????????args:?[&quot;--vm&quot;,?&quot;1&quot;,?&quot;--vm-bytes&quot;,?&quot;150M&quot;,?&quot;--vm-hang&quot;,?&quot;1&quot;]   ?????????volumeMounts:   ?????????-?name:?redis-storage   ???????????mountPath:?/data/redis   ?????volumes:   ?????-?name:?redis-storage   ???????emptyDir:?&#123;&#125;</code></p><p>继续来看几个重要的字段：</p><ul><li><p>  <code>metadata</code>同 Pod 的 yaml，这里提一点：如果没有指明 namespace，那么就是用 kubectl 默认的 namespace（如果 kubectl 配置文件中没有指明 namespace，那么就是 default 空间）。</p></li><li><p>  <code>spec</code>，可以看到 Deployment 的<code>spec</code>字段是在 Pod 的<code>spec</code>内容外“包了一层”，那就来看 Deployment 有哪些需要注意的：</p></li></ul><ul><li><p>  <code>metadata</code>，新手同学先不管这边的信息。</p></li><li><p>  <code>spec</code>，会发现这完完全全是上文提到的 Pod 的<code>spec</code>内容，在这里写明了 Deployment 下属管理的每个 Pod 的具体内容。</p></li><li><p>  <code>replicas</code>。副本个数。也就是该 Deployment 需要起多少个相同的 Pod，<strong>如果用户成功在 K8S 中配置了 n（n&gt;1）个，那么 Deployment 会确保在集群中始终有 n 个服务在运行</strong>。</p></li><li><p>  <code>template</code>。</p></li></ul><p><strong>第二步：执行 kubectl 命令部署</strong>。Deployment 的部署办法同 Pod：<code>kubectl create -f $&#123;DEPLOYMENT_YAML&#125;</code>。由此可见，<strong>K8S 会根据配置文件中的<code>kind</code>字段来判断具体要创建的是什么资源</strong>。</p><p>这里插一句题外话：<strong>部署完 deployment 之后，可以查看到自动创建了 ReplicaSet 和 Pod</strong>，如下图所示：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/a2815e26ce216800.jpeg?x-oss-process=style/bb"></p><p>还有一个有趣的事情：<strong>通过 Deployment 部署的服务，其下属的 RS 和 Pod 命名是有规则的</strong>。读者朋友们自己总结发现哦。</p><p>综上，如何部署一个 Pod 或者 Deployment 就结束了。</p><h3 id="V-kubectl-查看、更新-编辑、删除服务"><a href="#V-kubectl-查看、更新-编辑、删除服务" class="headerlink" title="V. kubectl 查看、更新/编辑、删除服务"></a>V. kubectl 查看、更新/编辑、删除服务</h3><p>作为 K8S 使用者而言，更关心的问题应该是本章所要讨论的话题：如何通过 kubectl 查看、更新/编辑、删除在 K8S 上部署着的服务。</p><h5 id="3-1-如何查看服务？"><a href="#3-1-如何查看服务？" class="headerlink" title="3.1 如何查看服务？"></a>3.1 如何查看服务？</h5><p>请务必记得一个事情：<strong>在 K8S 中，一个独立的服务即对应一个 Pod。即，当我们说要 xxx 一个服务的就是，也就是操作一个 Pod</strong>。而与 Pod 服务相关的且需要用户关心的，有 Deployment。</p><p>通过 kubectl 查看服务的基本命令是：</p><p>`$ kubectl?get|describe?${RESOURCE}?[-o?${FORMAT}]?-n=${NAMESPACE}  </p><h1 id="RESOURCE-有-pod、deployment、replicaset-rs"><a href="#RESOURCE-有-pod、deployment、replicaset-rs" class="headerlink" title="${RESOURCE}有:?pod、deployment、replicaset(rs)"></a>${RESOURCE}有:?pod、deployment、replicaset(rs)</h1><p>`</p><p>在此之前，还有一个需要回忆的事情是：Deployment、ReplicaSet 和 Pod 之间的关系 - 层层隶属；以及这些资源和 namespace 的关系是 - 隶属。如下图所示。</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/c20ccd600a5a85c9.jpeg?x-oss-process=style/bb"></p><p>因此，<strong>要查看一个服务，也就是一个 Pod，必须首先指定 namespace</strong>！那么，如何查看集群中所有的 namespace 呢？<code>kubectl get ns</code>：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/1f8eb4628827fa9e.jpeg?x-oss-process=style/bb"></p><p>于是，只需要通过<code>-n=$&#123;NAMESPACE&#125;</code>就可以指定自己要操作的资源所在的 namespace。比如查看 Pod：<code>kubectl get pod -n=oona-test</code>，同理，查看 Deployment：<code>kubectl get deployment -n=oona-test</code>。</p><p>问题又来了：<strong>如果已经忘记自己所部属的服务所在的 namespace 怎么办？这么多 namespace，一个一个查看过来吗？</strong></p><p><code>kubectl get pod --all-namespaces</code></p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/146e7c546d85fda3.jpeg?x-oss-process=style/bb"></p><p>这样子就可以看到所有 namespace 下面部署的 Pod 了！同理，要查找所有的命名空间下的 Deployment 的命令是：<code>kubectl get deployment --all-namespaces</code>。</p><p>于是，就可以开心地查看 Pod：<code>kubectl get pod [-o wide] -n=oona-test</code>，或者查看 Deployment：<code>kubectl get deployment [-o wide] -n=oona-test</code>。</p><p>哎，这里是否加<code>-o wide</code>有什么区别吗？实际操作下就明白了，其他资源亦然：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/4a9eac475dccf50a.jpeg?x-oss-process=style/bb"></p><p>哎，我们看到之前部署的 Pod 服务 memory-demo 显示的“ImagePullBackOff”是怎么回事呢？先不着急，我们慢慢看下去。</p><h5 id="3-2-如何更新-编辑服务？"><a href="#3-2-如何更新-编辑服务？" class="headerlink" title="3.2 如何更新/编辑服务？"></a>3.2 如何更新/编辑服务？</h5><p>两种办法：1). 修改 yaml 文件后通过 kubectl 更新；2). 通过 kubectl 直接编辑 K8S 上的服务。</p><p><strong>方法一：修改 yaml 文件后通过 kubectl 更新</strong>。我们看到，创建一个 Pod 或者 Deployment 的命令是<code>kubectl create -f $&#123;YAML&#125;</code>。但是，如果 K8S 集群当前的 namespace 下已经有该服务的话，会提示资源已经存在：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/32e031b17dfe8f90.jpeg?x-oss-process=style/bb"></p><p>通过 kubectl 更新的命令是<code>kubectl apply -f $&#123;YAML&#125;</code>，我们再来试一试：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/c4f0f8ce0878a2d3.jpeg?x-oss-process=style/bb"></p><p>（备注：命令<code>kubectl apply -f $&#123;YAML&#125;</code>也可以用于首次创建一个服务哦）</p><p><strong>方法二：通过 kubectl 直接编辑 K8S 上的服务</strong>。命令为<code>kubectl edit $&#123;RESOURCE&#125; $&#123;NAME&#125;</code>，比如修改刚刚的 Pod 的命令为<code>kubectl edit pod memory-demo</code>，然后直接编辑自己要修改的内容即可。</p><p>但是请注意，无论方法一还是方法二，能修改的内容还是有限的，从笔者实战下来的结论是：<strong>只能修改/更新镜像的地址和个别几个字段</strong>。如果修改其他字段，会报错：</p><blockquote><p>The Pod “memory-demo” is invalid: spec: Forbidden: pod updates may not change fields other than <code>spec.containers[*].image</code>, <code>spec.initContainers[*].image</code>, <code>spec.activeDeadlineSeconds</code> or <code>spec.tolerations</code> (only additions to existing tolerations)</p></blockquote><p>如果真的要修改其他字段怎么办呢？恐怕只能删除服务后重新部署了。</p><h5 id="3-3-如何删除服务？"><a href="#3-3-如何删除服务？" class="headerlink" title="3.3 如何删除服务？"></a>3.3 如何删除服务？</h5><p>在 K8S 上删除服务的操作非常简单，命令为<code>kubectl delete $&#123;RESOURCE&#125; $&#123;NAME&#125;</code>。比如删除一个 Pod 是：<code>kubectl delete pod memory-demo</code>，再比如删除一个 Deployment 的命令是：<code>kubectl delete deployment $&#123;DEPLOYMENT_NAME&#125;</code>。但是，请注意：</p><ul><li><p><strong>如果只部署了一个 Pod，那么直接删除该 Pod 即可</strong>；</p><p>  <img src="http://img.blog.itpub.net/blog/2020/12/29/2613ca4a3927a019.jpeg?x-oss-process=style/bb"></p></li><li><p><strong>如果是通过 Deployment 部署的服务，那么仅仅删除 Pod 是不行的，正确的删除方式应该是：先删除 Deployment，再删除 Pod</strong>。</p><p>  <img src="http://img.blog.itpub.net/blog/2020/12/29/71ade0a4ee1d9024.jpeg?x-oss-process=style/bb"></p></li></ul><p>关于第二点应该不难想象：仅仅删除了 Pod 但是 Deployment 还在的话，Deployment 定时会检查其下属的所有 Pod，如果发现失败了则会再拉起。因此，会发现过一会儿，新的 Pod 又被拉起来了。</p><p>另外，还有一个事情：有时候会发现一个 Pod 总也删除不了，这个时候很有可能要实施强制删除措施，命令为<code>kubectl delete pod --force --grace-period=0 $&#123;POD_NAME&#125;</code>。</p><h3 id="VI-kubectl-排查服务问题"><a href="#VI-kubectl-排查服务问题" class="headerlink" title="VI. kubectl 排查服务问题"></a>VI. kubectl 排查服务问题</h3><p>上文说道：部署的服务 memory-demo 失败了，是怎么回事呢？本章就会带大家一起来看看常见的 K8S 中服务部署失败、服务起来了但是不正常运行都怎么排查呢？</p><p>首先，祭出笔者最爱的一张 K8S 排查手册，来自博客《Kubernetes Deployment 故障排除图解指南》：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/68c4bac155425108.jpeg?x-oss-process=style/bb"></p><p>哈哈哈，对于新手同学来说，上图还是不够友好，下面我们简单来看两个例子：</p><h5 id="4-1-K8S-上部署服务失败了怎么排查？"><a href="#4-1-K8S-上部署服务失败了怎么排查？" class="headerlink" title="4.1 K8S 上部署服务失败了怎么排查？"></a>4.1 K8S 上部署服务失败了怎么排查？</h5><p>请一定记住这个命令：<code>kubectl describe $&#123;RESOURCE&#125; $&#123;NAME&#125;</code>。比如刚刚的 Pod 服务 memory-demo，我们来看：</p><p><img src="http://img.blog.itpub.net/blog/2020/12/29/153699f04df7ac55.jpeg?x-oss-process=style/bb"></p><p>拉到最后看到<code>Events</code>部分，会显示出 K8S 在部署这个服务过程的关键日志。这里我们可以看到是拉取镜像失败了，好吧，大家可以换一个可用的镜像再试试。</p><p>一般来说，通过<code>kubectl describe pod $&#123;POD_NAME&#125;</code>已经能定位绝大部分部署失败的问题了，当然，具体问题还是得具体分析。大家如果遇到具体的报错，欢迎分享交流。</p><h5 id="4-2-K8S-上部署的服务不正常怎么排查？"><a href="#4-2-K8S-上部署的服务不正常怎么排查？" class="headerlink" title="4.2 K8S 上部署的服务不正常怎么排查？"></a>4.2 K8S 上部署的服务不正常怎么排查？</h5><p>如果服务部署成功了，且状态为<code>running</code>，那么就需要进入 Pod 内部的容器去查看自己的服务日志了：</p><ul><li><p>  查看 Pod 内部某个 container 打印的日志：<code>kubectl log $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125;</code>。</p></li><li><p>  进入 Pod 内部某个 container：<code>kubectl exec -it [options] $&#123;POD_NAME&#125; -c $&#123;CONTAINER_NAME&#125; [args]</code>，嗯，这个命令的作用是通过 kubectl 执行了<code>docker exec xxx</code>进入到容器实例内部。之后，就是用户检查自己服务的日志来定位问题。</p></li></ul><p>显然，线上可能会遇到更复杂的问题，需要借助更多更强大的命令和工具。</p><h3 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h3><p>本文希望能够帮助对 K8S 不了解的新手快速了解 K8S。笔者一边写文章，一边查阅和整理 K8S 资料，过程中越发感觉 K8S 架构的完备、设计的精妙，是值得深入研究的，K8S 大受欢迎是有道理的。</p></blockquote><blockquote><p>来自 “ ITPUB博客 ” ，链接：<a href="http://blog.itpub.net/31559354/viewspace-2746071/">http://blog.itpub.net/31559354/viewspace-2746071/</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;h3 id=&quot;Kubernetes-入门-amp-进阶实战&quot;&gt;&lt;a href=&quot;#Kubernetes-入门-amp-进阶实战&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 入门&amp;amp;进阶实战&quot;&gt;&lt;/a&gt;Kuberne</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Flannel-Calico怎么选择</title>
    <link href="http://zhangyu.info/2021/03/24/Flannel-Calico/"/>
    <id>http://zhangyu.info/2021/03/24/Flannel-Calico/</id>
    <published>2021-03-23T16:00:00.000Z</published>
    <updated>2021-05-26T03:01:53.590Z</updated>
    
    <content type="html"><![CDATA[<p>网络插件怎么选择<br><img src="https://www.talkwithtrend.com/club/attachments/month_2009/2009081142a1e2e6b6a2db712a.png"></p><h3 id="Kubernetes中常见的网络插件有哪些？"><a href="#Kubernetes中常见的网络插件有哪些？" class="headerlink" title="Kubernetes中常见的网络插件有哪些？"></a>Kubernetes中常见的网络插件有哪些？</h3><p>1.flannel：能提供ip地址，但不支持网络策略</p><p>2.calico：既提供ip地址，又支持网络策略</p><p>3.canal：flannel和calico结合，通过flannel提供ip地址，calico提供网络策略</p><p>4.Cilium  着重强调网络安全，实现Kubernetes中网络的可观察性以及基本的网络隔离、故障排查等安全策略； 突破传统主机防火墙仅支持L3、L4微隔离的限制，支持基于API的网络安全过滤能力。 </p><h3 id="什么叫做网络策略？"><a href="#什么叫做网络策略？" class="headerlink" title="什么叫做网络策略？"></a>什么叫做网络策略？</h3><p>网络策略：可以达到多租户网络隔离，可以控制入网和出网流量，入网和出网ip访问的一种策略</p><h3 id="各种CNI网络方案的差异对比"><a href="#各种CNI网络方案的差异对比" class="headerlink" title="各种CNI网络方案的差异对比"></a>各种CNI网络方案的差异对比</h3><p>参考<br><a href="https://helpcdn.aliyun.com/document_detail/97621.html">https://helpcdn.aliyun.com/document_detail/97621.html</a></p><h3 id="flannel和calico网络性能分析"><a href="#flannel和calico网络性能分析" class="headerlink" title="flannel和calico网络性能分析"></a>flannel和calico网络性能分析</h3><p>官方指标如下</p><p>flannel host-gw = flannel  vxlan-directrouting = calico bgp&gt; calico ipip &gt; flannel vxlan-vxlan&gt;flannel-udp</p><p>官方推荐使用的网络方案：<br>所有节点在同一个网段推荐使用calico的bgp模式和flannel的host-gw模式</p><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>1.如果需要多集群的跨网络分段的网络，选择Calico</p><p>2.如果需要管理网络策略，做网络隔离等，选择Calico</p><p>3.大部分公司生产环境业务不复杂的，开发测试环境就几台机器的，不存在多数据中心的。</p><p>选择用Flannel 就行了。</p><pre><code> 部署在公有云上，封装 backend 选择vxlan-directrouting。 部署在私有云上，封装 backend 选择host-gw。</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>Kubernetes集群网络规划</p><p><a href="https://helpcdn.aliyun.com/document_detail/86500.html">https://helpcdn.aliyun.com/document_detail/86500.html</a></p><p>使用网络策略（Network Policy）</p><p><a href="https://helpcdn.aliyun.com/document_detail/97621.html">https://helpcdn.aliyun.com/document_detail/97621.html</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网络插件怎么选择&lt;br&gt;&lt;img src=&quot;https://www.talkwithtrend.com/club/attachments/month_2009/2009081142a1e2e6b6a2db712a.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Kubernetes中常</summary>
      
    
    
    
    <category term="Kubernetes" scheme="http://zhangyu.info/categories/Kubernetes/"/>
    
    
    <category term="flannel" scheme="http://zhangyu.info/tags/flannel/"/>
    
  </entry>
  
  <entry>
    <title>nginx核心知识100讲知识图谱</title>
    <link href="http://zhangyu.info/2021/03/15/nginx-knowledge-graph/"/>
    <id>http://zhangyu.info/2021/03/15/nginx-knowledge-graph/</id>
    <published>2021-03-14T16:00:00.000Z</published>
    <updated>2021-03-15T15:07:57.974Z</updated>
    
    <content type="html"><![CDATA[<p>nginx核心知识100讲知识图谱</p><p><img src="https://img-blog.csdnimg.cn/20190116152033544.jpg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;nginx核心知识100讲知识图谱&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20190116152033544.jpg&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="nginx" scheme="http://zhangyu.info/categories/nginx/"/>
    
    
    <category term="nginx" scheme="http://zhangyu.info/tags/nginx/"/>
    
  </entry>
  
</feed>
