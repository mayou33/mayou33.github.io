<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大雨哥</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhangyu8.me/"/>
  <updated>2020-07-13T09:14:41.549Z</updated>
  <id>http://zhangyu8.me/</id>
  
  <author>
    <name>大雨哥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GitOps</title>
    <link href="http://zhangyu8.me/2020/07/13/GitOps/"/>
    <id>http://zhangyu8.me/2020/07/13/GitOps/</id>
    <published>2020-07-13T07:00:00.000Z</published>
    <updated>2020-07-13T09:14:41.549Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h1 id="GitOps-在-Kubernetes-中进行-DevOps-的方式"><a href="#GitOps-在-Kubernetes-中进行-DevOps-的方式" class="headerlink" title="GitOps - 在 Kubernetes 中进行 DevOps 的方式"></a>GitOps - 在 Kubernetes 中进行 DevOps 的方式</h1></blockquote><blockquote><h1 id="什么是-GitOps？"><a href="#什么是-GitOps？" class="headerlink" title="什么是 GitOps？"></a>什么是 GitOps？</h1><p>GitOps 是一个概念，将软件的端到端描述放置到 Git 中，然后尝试着让集群状态和 Git 仓库持续同步，其中有两个概念需要说明下。</p><ol><li><strong>软件的描述表示</strong>：Kubernetes、应用和底层基础架构之间的关系是一种声明式的，我们用声明式方式（YAML）来描述我们需要的基础架构。这些 YAML 的实现细节被底层的 Kubernetes 集群的 Controller、Schedulers、CoreDNS、Operator 等等抽象出来，这使得我们可以从传统的<strong>基础架构即代码</strong>转向<strong>基础架构即数据，</strong>我们也可以从 GitHub 上了解到更多相关的信息。这里的关键是，你需要的每一个应用声明的角色（应用开发者/应用运维/集群运维）都会被用到持续交互流水线的 YAML 中，最后被推送到 GitOps 仓库中去。</li></ol><ol start="2"><li><strong>持续同步</strong>：持续同步的意思是不断地检查 Git 仓库，将任何状态变化都反映到 Kubernetes 集群中。这种思路来自于 Flux 工具，Flux 使用 Kubernetes Operator 将自动化部署方式从 Kubernetes 集群外转移到集群内部来。</li></ol><p>GitOps 由以下4个主要的组件组成：</p><ol><li><strong>Git 仓库</strong>：用来存储我们的应用程序的声明式定义的 YAML 文件的源代码仓库。</li></ol><ol start="2"><li><strong>Kubernetes 集群</strong>：用于部署我们应用程序的底层集群。</li></ol><ol start="3"><li><strong>同步代理</strong>：Kubernetes Operator 扩展，它的工作是将 Git 仓库和应用状态持续同步到集群中。</li></ol><ol start="4"><li><strong>CD Pipeline</strong>：持续部署流水线，用来编排整个流程的持续部署流水线。</li></ol><p>关于这些组件如何协同工作来创建 GitOps 流程的架构图如下所示。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cezgWwU6NE50RPybSiauPxOMibG7ycygsHBhlesTN2Cw5JY3D38I5hlRg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>在上面的架构图中，YAML 文件的创建和修改分为应用开发、应用运维和集群运维三部分。根据我们的组织团队架构、集群多租户等需求，可以选择分一到两步进行。接下来我们来看看为什么需要使用 GitOps？</p><h1 id="为什么需要-GitOps？"><a href="#为什么需要-GitOps？" class="headerlink" title="为什么需要 GitOps？"></a>为什么需要 GitOps？</h1><p>GitOps 可以在很多方面都产生价值，下面我们来看看其中的一些关键的价值。</p><p><strong>应用交付速度</strong></p><p>持续的 GitOps 可以通过以下几个方面来提高产品交付速度。</p><ol><li>可以比较最终的 YAML 和集群状态的能力，这也可以作为批准发布的决策指南。</li></ol><ol start="2"><li>借助 Prometheus 的应用程序指标，通过自动化的蓝绿部署，非常容易进行部署。</li></ol><ol start="3"><li>根据策略自动更新容器镜像，例如，Istio sidecar 次要版本的发布是向后兼容的，可以自动更新。</li></ol><ol start="4"><li>GitOps 将以运维和开发为中心，提高效率。</li></ol><ol start="5"><li>应用团队可以接管一些运维工作，而运维团队则可以更加专注于平台建设。</li></ol><ol start="6"><li>GitOps 仓库可以绕过完整的持续部署流程进行紧急发布。</li></ol><p><strong>端到端的自动化</strong></p><p>在 GitOps 中，所有和应用开发、应用运维和集群运维相关的声明都通过 git 嵌入到 YAML 文件中，实现了端到端的自动化。</p><p><strong>安全、审计和合规性</strong></p><p>零手动更改到集群中应用的策略将大大增加集群的安全性，由于集群中的所有配置都在 git 中，我们将拥有一个完整的审计日志，记录集群中发生的事情。</p><p><strong>集群可观测性</strong></p><p>有了完整的审计日志，我们就可以很容易获得集群中发生的变化，来帮助调试一些问题。</p><p><strong>关注点分离和迁移</strong></p><p>GitOps 将应用开发者、应用运维和集群运维之间的关注点进行分离，这些团队中的依赖关系以声明式的方式注入到 git 中，这将大大缓解我们对底层 K8S 集群、治理策略等工具的迁移。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cEibKKfVx9n7ziaTLss4Ee8kpnAtaEg2iamLYNF7ecZOCtw0y4p67w5cHA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h1 id="如何采用-GitOps？"><a href="#如何采用-GitOps？" class="headerlink" title="如何采用 GitOps？"></a>如何采用 GitOps？</h1><p>我们将通过以下四个不同的方面进行阐述来帮助我们实现 GitOps 这一目标。</p><ul><li>GitOps 工作流的实现</li></ul><ul><li>管理声明式的 YAML</li></ul><ul><li>工具</li></ul><ul><li>从什么地方开始？</li></ul><h2 id="GitOps-工作流的实现"><a href="#GitOps-工作流的实现" class="headerlink" title="GitOps 工作流的实现"></a>GitOps 工作流的实现</h2><p>以下三个工作流程是我们在开始使用 GitOps 时要采用的比较流行的工作流程。</p><p><strong>工作流1</strong>：标准的 GitOps 流程</p><p>这是标准的 GitOps 工作流，我们将应用程序的 YAML 描述推送到 GitOps 仓库中，GitOps Agent 就会自动同步状态变化。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cCpLKt5qIvNNOM1x2c1iazNoYPV2WKme8Zmo2lZKrc9TMCxAHuQm2CcQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>工作流2</strong>：镜像自动更新</p><p>在这个工作流中，GitOps Agent 会根据指定的策略从容器镜像仓库中自动更新新版本的容器镜像，例如，我们可以设置这样的策略，如果镜像有一个小版本变化，我们就可以自动更新，因为它们是向后兼容的。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cq6k4LwFZwMibibjr5QsEOpw7dckGPTvbovB08OC57dwXwnSQP17NbdGw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>工作流3</strong>：自动化金丝雀部署</p><p>该工作流非常强大，我们可以在这里实现金丝雀自动化部署。有了这个，我们在用 Prometheus 测量 HTTP 请求成功率、请求平均持续时间和 Pods 健康状态等关键性能指标的同时，可以逐步将流量迁移到金丝雀实例上。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5crKDdUgNZDpCOyWicuISxVibCScHxLDXRDkMlPjibBHU8wUFDMGOSRSoBQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="管理声明式的-YAML"><a href="#管理声明式的-YAML" class="headerlink" title="管理声明式的 YAML"></a>管理声明式的 YAML</h2><p>假设我们有一个电子商务购物车应用，完整的应用程序定义如下所示。</p><ol><li>应用程序镜像。</li></ol><ol start="2"><li>Pod、Deployment、Service、Volume 和 ConfigMap 的 YAML 文件。</li></ol><ol start="3"><li>连接数据的一些 sealed secrets 对象。</li></ol><ol start="4"><li>标记将 Istio 作为默认的集群服务治理策略网格。</li></ol><ol start="5"><li>环境治理策略，比如 staging 环境1个副本，生产环境3个副本。</li></ol><ol start="6"><li>标记添加节点亲和性和容忍用于高可用的节点调度。</li></ol><ol start="7"><li>基于 Pod 标签的网络安全策略 YAML 声明。</li></ol><p>要构建一个最终的应用 YAML 描述文件，我们需要应用开发者、应用运维和集群运维人员的一些输入。</p><p>假设我们是一个比较小的团队并且管理了很多的 Pod，下面的流程就可以来表示如何构建一个持续部署的自动化流水线。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cb9oVNicve4A2ribtmX4CE3xC1DFUjqjqSNwibsp1qxBazPCxHiaIq6MyPw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>这种关注点分离的方式深受 OAM（开放应用模型）的影响，该模型试图为云原生应用开发提供一个完善的框架。</p><p>OAM(<a href="https://oam.dev/" target="_blank" rel="noopener">https://oam.dev/</a>) 描述了一种模式：</p><ul><li><strong>开发人员</strong>负责定义应用组件。</li></ul><ul><li><strong>应用运维人员</strong>负责创建这些组件的实例，并为其分配应用配置。</li></ul><ul><li><strong>基础设施运维人员</strong>负责声明、安装和维护平台上的可用的底层服务。</li></ul><p>通过使用 OAM 框架，会将 YAML 的贡献者责任进行分离，当然这可能会根据你的团队组织结构和使用的 Kubernetes 集群类型有所变化。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cjySamdPt5BtuuUstOzIm8C6uAOrkVcU0C67JQoyh3mppdlYnonPeEg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>如果你比较赞同 GitOps 的理念，那么下面就可以来选择一些需要用到的工具了。有很多工具可以支持我们去实现 GitOps 的不同功能。接下来我们简单介绍一些工具及其使用方法。</p><p><strong>Git</strong>：这是我们使用 GitOps 来存储 YAML 清单的基础。</p><p><strong>Helm &amp; Kustomize</strong>：这是一个强大的组合，可以帮助我们生成声明式的 YAML 资源清单文件，我们可以使用 Helm 打包应用程序和它的依赖关系。然后 Kustomize 会帮助我们自定义和修补 YAML 文件，而不需要去改变原来的 YAML 文件。单独使用 Helm 是不够的，特别是用于区分不同环境的资源清单的时候，我们还需要结合 Kustomize。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5c2mpOqEica4h1icq3G7crcqbDH0llawQMfOS7nuNXLCzDbPQvgsiaZYHRg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>Argo CD</strong>：这是一个 GitOps 持续交付工具，它可以作为一个 Agent，将 GitOps 仓库中的改动同步到 Kubernetes 集群中。</p><p><strong>Flux</strong>：这是另外一个 GitOps 持续交付的工具，功能和 Argo CD 类似。</p><p><strong>Flagger</strong>：这个工具和 Flux 配合使用，可以很好地实现金丝雀部署。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cpMDSbOoen7N0BhnC7OAarv386DMCagr4f285nibvpL7M7abQFRD6rZg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="如何入手？"><a href="#如何入手？" class="headerlink" title="如何入手？"></a>如何入手？</h2><p>如果你正准备开始一个新的项目，那么从一开始就采用 GitOps 是比较容易的，我们所要做的就是选择我们的 CI/CD、声明式 YAML 文件管理以及 GitOps Agent 等工具来启动即可。</p><p>如果你想在现有的项目中实施 GitOps，下面的几点可以帮助你实施：</p><ol><li>你可以一次只选择一个应用，用成功的案例来推动其他应用的改造。</li></ol><ol start="2"><li>当选择第一个应用的时候，可以选择变化比较频繁的应用，这将有助于我们为成功案例建立一些可靠的指标说明。</li></ol><ol start="3"><li>选择一个经常出问题的应用，使用了 GitOps 过后，这些应用的问题频率应该会下降不少，当它出现问题的时候，应该有更好的可观测性了。</li></ol><ol start="4"><li>优先选择业务应用，而不是像 Istio、RBAC 集成的等运维复杂的应用。</li></ol><ol start="5"><li>如果需要的话可以暂时引入人工审批的步骤。</li></ol><p>  转发来源  阳明 k8s技术圈 </p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect</a></p><blockquote><p>原文链接：<a href="https://itnext.io/continuous-gitops-the-way-to-do-devops-in-kubernetes-896b0ea1d0fb" target="_blank" rel="noopener">https://itnext.io/continuous-gitops-the-way-to-do-devops-in-kubernetes-896b0ea1d0fb</a></p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h1 id=&quot;GitOps-在-Kubernetes-中进行-DevOps-的方式&quot;&gt;&lt;a href=&quot;#GitOps-在-Kubernetes-中进行-DevOps-的方式&quot; class=&quot;headerlink&quot; title=&quot;GitOps - 在 
      
    
    </summary>
    
      <category term="devops" scheme="http://zhangyu8.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhangyu8.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>使用GitLab-CI与Argo-CD进行GitOps实践</title>
    <link href="http://zhangyu8.me/2020/07/13/%E4%BD%BF%E7%94%A8GitLab-CI%E4%B8%8EArgo-CD%E8%BF%9B%E8%A1%8CGitOps%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zhangyu8.me/2020/07/13/使用GitLab-CI与Argo-CD进行GitOps实践/</id>
    <published>2020-07-13T06:00:00.000Z</published>
    <updated>2020-07-13T09:14:52.452Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h1 id="使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践"><a href="#使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践" class="headerlink" title="使用 GitLab CI 与 Argo CD 进行 GitOps 实践"></a>使用 GitLab CI 与 Argo CD 进行 GitOps 实践</h1></blockquote><blockquote><p>在现在的云原生世界里面 GitOps 不断的被提及，这种持续交付的模式越来越受到了大家的青睐，<a href="http://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">我们前面也有文章详细讲解了 GitOps 的相关概念</a>，在网上也可以找到很多关于它的资源，但是关于 GitOps 相关的工作流实践的示例却并不多见，我们这里就将详细介绍一个使用示例，希望对大家实践 GitOps 有所帮助。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUk1FGGtWPmb7xyeiaOicS5Zwk9fyZJVgqVwL44SHpBOJw3u459iaYicAyfSg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitOps Workflow</p><p>上图是当前示例中的 GitOps 工作流程。GitLab 和 Argo CD 是两个主要的核心组件：</p><p><strong>Argo CD</strong> 是一个声明式、GitOps 持续交付的 Kubernetes 工具，它的配置和使用非常简单，并且自带一个简单易用的 Dashboard 页面，更重要的是 Argo CD 支持 kustomzie、helm、ksonnet 等多种工具。应用程序可以通过 Argo CD 提供的 CRD 资源对象进行配置，可以在指定的目标环境中自动部署所需的应用程序。关于 Argo CD 更多的信息可以查看官方文档了解更多。</p><p><strong>GitLab CI</strong> 是 GitLab 的持续集成和持续交付的工具，也是非常流行的 CI/CD 工具，相比 Jenkins 更加轻量级，更重要的是和 GitLab 天然集成在一起的，所以非常方便。</p><h2 id="Argo-CD-安装"><a href="#Argo-CD-安装" class="headerlink" title="Argo CD 安装"></a>Argo CD 安装</h2><p>当前前提条件是有一个可用的 Kubernetes 集群，通过 kubectl 可以正常访问集群，为了访问 Argo CD 的 Dashboard 页面，我们可以通过 Ingress 来暴露服务，为此需要在 Kubernetes 中安装一个 Ingress Controller，我这里已经提前安装了 <code>ingress-nginx</code>，接下来我们将 Helm3 来安装 Argo CD，关于 Helm 以及 ingress-nginx 的使用我们前面的文章中已经多次提到，这里就不再详细介绍他们的使用了。</p><p>首先创建一个 argocd 的命名空间：</p><pre><code>$ kubectl create ns argocd</code></pre><p>然后添加 argocd 的 chart 仓库地址：</p><pre><code>$ helm repo add argo https://argoproj.github.io/argo-helm</code></pre><p>接下来我们就可以使用 Helm 安装 Argo CD 了：</p><pre><code>$ helm install argocd -n argocd argo/argo-cd --values values.yaml</code></pre><p>其中 values.yaml 文件如下所示，用来定制安装的 Argo CD：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server:</span><br><span class="line">  ingress:</span><br><span class="line">    enabled: true</span><br><span class="line">    annotations:</span><br><span class="line">      kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</span><br><span class="line">    hosts:</span><br><span class="line">    - argocd.k8s.local</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>执行上面的安装命令后，Argo CD 就会被安装在 argocd 命名空间之下，可以在本地 <code>/etc/hosts</code> 中添加一个映射，将 argocd.k8s.local 映射到 ingress-nginx 所在的节点即可：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls -n argocd</span><br><span class="line">NAME    NAMESPACE       REVISION        UPDATED                                 STATUS    CHART            APP VERSION</span><br><span class="line">argocd  argocd          2               2020-07-10 15:26:38.259258 +0800 CST    deployed  argo-cd-2.5.0    1.6.1</span><br><span class="line">$ kubectl get pods -n argocd</span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">argocd-application-controller-85c4788ffc-p2m4c   1/1     Running   0          49m</span><br><span class="line">argocd-dex-server-cc65c7546-x78bj                1/1     Running   0          49m</span><br><span class="line">argocd-redis-5f45875bc7-mnx8b                    1/1     Running   0          49m</span><br><span class="line">argocd-repo-server-7bcf647588-h8gtq              1/1     Running   0          49m</span><br><span class="line">argocd-server-7877ff8889-zp7tq                   1/1     Running   0          49m</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>当所有 Pod 变成 Running 状态后，我们就可以通过浏览器访问 Argo CD 的 Dashboard 页面了：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkDyG5dBqSjrkqaHc0ic1ib5BdR9mEMM6iamffu3PxxlJ5mFxpib0bAAx2Tw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>默认的用户名为 admin，密码为 server Pod 的名称，可以通过如下所示的命令来获取：</p><p>$ kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d’/‘ -f 2</p><p>用上面的用户名和密码即可登录成功，接下来我们在 GitLab 中来创建示例项目。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkJNQ7dCTBtUALC3dBo76DHkQZCD0iceFNTVoWJiaClFiafhZZjdEEUibDVg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="GitLab-项目配置"><a href="#GitLab-项目配置" class="headerlink" title="GitLab 项目配置"></a>GitLab 项目配置</h2><p>我们这里使用的示例项目是一个 Golang 程序，在页面上显示一个文本信息和 Pod 名称，代码地址：<a href="https://github.com/cnych/gitops-webapp-demo。我们可以将该项目代码上传到我们自己的" target="_blank" rel="noopener">https://github.com/cnych/gitops-webapp-demo。我们可以将该项目代码上传到我们自己的</a> GitLab 上面去，我这里的 GitLab 安装在 Kubernetes 之上，通过配置域名 git.k8s.local 进行访问，调整过后我们本地的代码仓库地址为：<a href="http://git.k8s.local/course/gitops-webapp" target="_blank" rel="noopener">http://git.k8s.local/course/gitops-webapp</a> 。</p><p>接下来需要添加一些在 GitLab CI 流水线中用到的环境变量（Settings → CI/CD → Variables）：</p><ul><li>CI_REGISTRY - 镜像仓库地址，值为：<a href="https://index.docker.io/v1/" target="_blank" rel="noopener">https://index.docker.io/v1/</a></li></ul><ul><li>CI_REGISTRY_IMAGE - 镜像名称，值为：cnych/gitops-webapp</li></ul><ul><li>CI_REGISTRY_USER - Docker Hub 仓库用户名，值为 cnych</li></ul><ul><li>CI_REGISTRY_PASSWORD - Docker Hub 仓库密码</li></ul><ul><li>CI_PASSWORD - Git 仓库访问密码</li></ul><ul><li>CI_USERNAME - Git 仓库访问用户名</li></ul><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkYIDTCMRrglaBa0mHtYVSsMaZRj2GpXXxZ6cROTs1eAPjYqyzjvyk6A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Argo-CD-配置"><a href="#Argo-CD-配置" class="headerlink" title="Argo CD 配置"></a>Argo CD 配置</h2><p>现在我们可以开始使用 GitOps 来配置我们的 Kubernetes 中的应用了。Argo CD 自带了一套 CRD 对象，可以用来进行声明式配置，这当然也是推荐的方式，把我们的基础设施作为代码来进行托管，下面是我们为开发和生产两套环境配置的资源清单：</p> <figure class="highlight plain"><figcaption><span>gitops-demo-app.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: web-app-dev</span><br><span class="line">  namespace: argocd</span><br><span class="line">spec:</span><br><span class="line">  project: default</span><br><span class="line">  source:</span><br><span class="line">    repoURL: http://git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">    path: deployment/dev</span><br><span class="line">  destination:</span><br><span class="line">    server: https://kubernetes.default.svc</span><br><span class="line">    namespace: dev</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: true</span><br><span class="line">---</span><br><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: web-app-prod</span><br><span class="line">  namespace: argocd</span><br><span class="line">spec:</span><br><span class="line">  project: default</span><br><span class="line">  source:</span><br><span class="line">    repoURL: http://git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">    path: deployment/prod</span><br><span class="line">  destination:</span><br><span class="line">    server: https://kubernetes.default.svc</span><br><span class="line">    namespace: prod</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: true</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>上面定义的 Application 这个资源，就是 Argo CD 用于描述应用的 CRD 对象：</p><ul><li>name：Argo CD 应用程序的名称</li></ul><ul><li>project：应用程序将被配置的项目名称，这是在 Argo CD 中应用程序的一种组织方式</li></ul><ul><li>repoURL：源代码的仓库地址</li></ul><ul><li>targetRevision：想要使用的 git 分支</li></ul><ul><li>path：Kubernetes 资源清单在仓库中的路径</li></ul><ul><li>destination：Kubernetes 集群中的目标</li></ul><p>然后同样使用 kubectl 工具直接部署上面的资源对象即可，将会创建两个 Application 类型的对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f gitops-demo-app.yaml</span><br><span class="line">application.argoproj.io/web-app-dev created</span><br><span class="line">application.argoproj.io/web-app-prod created</span><br><span class="line">$ kubectl get application -n argocd</span><br><span class="line">NAME           AGE</span><br><span class="line">web-app-dev    25s</span><br><span class="line">web-app-prod   24s</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>此时我们再去 Argo CD 的 Dashboard 首页同样将会看到两个 Application 的信息：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkEnwJ6x3ic9sOvqtJ4a9omlxSKic74ADdIciaDBIriaKCmh3Nj3qwYOUaZQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>点击其中一个就可以看到关于应用的详细信息，我们可以在 gitops-webapp 代码仓库的 <code>deployment/&lt;env&gt;</code> 目录里面找到资源对象。我们可以看到，在每个文件夹下面都有一个 <code>kustomization.yaml</code> 文件，Argo CD 可以识别它，不需要任何其他的设置就可以使用。</p><p>由于我们这里的代码仓库是私有的 GitLab，所以我们还需要配置对应的仓库地址，在页面上 Settings → Repositories，点击 <code>Connect Repo using HTTPS</code> 按钮：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkwbI136LplOtAo1PFmJTt9ic46ZFrambxuy5rmhQkrpoq0DoaXD6ug5g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>添加我们的代码仓库认证信息：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkHxTR6pFhDytdTh8AkzV67x7In3HiaOQ7ibZyAaw6EjGLNlXrEkc8gw9w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>需要注意的是这里默认使用的是 HTTPS，所以我们需要勾选下方的 <code>Skip server verification</code>，然后点击上方的 <code>CONNECT</code> 按钮添加即可。然后重新同步上面的两个 Application，就可以看到正常的状态了。</p><h2 id="GitLab-CI-流水线"><a href="#GitLab-CI-流水线" class="headerlink" title="GitLab CI 流水线"></a>GitLab CI 流水线</h2><p>接下来我们需要为应用程序创建流水线，自动构建我们的应用程序，推送到镜像仓库，然后更新 Kubernetes 的资源清单文件。</p><p>下面的示例并不是一个多么完美的流水线，但是基本上可以展示整个 GitOps 的工作流。开发人员在自己的分支上开发代码，他们分支的每一次提交都会触发一个阶段性的构建，当他们将自己的修改和主分支合并时，完整的流水线就被触发。将构建应用程序，打包成 Docker 镜像，将镜推送到 Docker 仓库，并自动更新 Kubernetes 资源清单，此外，一般情况下将应用部署到生产环境需要手动操作。</p><p>GitLab CI 中的流水线默认定义在代码仓库根目录下的 <code>.gitlab-ci.yml</code> 文件中，在该文件的最上面定义了一些构建阶段和环境变量、镜像以及一些前置脚本：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stages:</span><br><span class="line">- build</span><br><span class="line">- publish</span><br><span class="line">- deploy-dev</span><br><span class="line">- deploy-prod</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>接下来是阶段的定义和所需的任务声明。我们这里的构建过程比较简单，只需要在一个 golang 镜像中执行一个构建命令即可，然后将编译好的二进制文件保存到下一个阶段处理，这一个阶段适合分支的任何变更：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">build:</span><br><span class="line">  stage: build</span><br><span class="line">  image:</span><br><span class="line">    name: golang:1.13.1</span><br><span class="line">  script:</span><br><span class="line">    - go build -o main main.go</span><br><span class="line">  artifacts:</span><br><span class="line">    paths:</span><br><span class="line">      - main</span><br><span class="line">  variables:</span><br><span class="line">    CGO_ENABLED: 0</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>然后就是构建镜像并推送到镜像仓库，这里我们使用 Kaniko，当然也可以使用 DinD 模式进行构建，只是安全性不高，这里我们可以使用 GIT 提交的 commit 哈希值作为镜像 tag，关于 Docker 镜像仓库的认证和镜像地址信息可以通过项目的参数来进行传递，不过这个阶段只在主分支发生变化时才会触发：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">publish:</span><br><span class="line">  stage: publish</span><br><span class="line">  image:</span><br><span class="line">    name: cnych/kaniko-executor:v0.22.0</span><br><span class="line">    entrypoint: [&quot;&quot;]</span><br><span class="line">  script:</span><br><span class="line">    - echo &quot;&#123;\&quot;auths\&quot;:&#123;\&quot;$CI_REGISTRY\&quot;:&#123;\&quot;username\&quot;:\&quot;$CI_REGISTRY_USER\&quot;,\&quot;password\&quot;:\&quot;$CI_REGISTRY_PASSWORD\&quot;&#125;&#125;&#125;&quot; &gt; /kaniko/.docker/config.json</span><br><span class="line">    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile ./Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA</span><br><span class="line">  dependencies:</span><br><span class="line">    - build</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>下一个阶段就是将应用程序部署到开发环境中，在 GitOps 中就意味着需要更新 Kubernetes 的资源清单，这样 Argo CD 就可以拉取更新的版本来部署应用。这里我们使用了为项目定义的环境变量，包括用户名和 TOKEN，此外在提交消息里面增加 <code>[skip ci]</code> 这样的关键字，这样流水线就不会被触发：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">deploy-dev:</span><br><span class="line">  stage: deploy-dev</span><br><span class="line">  image: cnych/kustomize:v1.0</span><br><span class="line">  before_script:</span><br><span class="line">    - git remote set-url origin http://$&#123;CI_USERNAME&#125;:$&#123;CI_PASSWORD&#125;@git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    - git config --global user.email &quot;gitlab@git.k8s.local&quot;</span><br><span class="line">    - git config --global user.name &quot;GitLab CI/CD&quot;</span><br><span class="line">  script:</span><br><span class="line">    - git checkout -B master</span><br><span class="line">    - cd deployment/dev</span><br><span class="line">    - kustomize edit set image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span><br><span class="line">    - cat kustomization.yaml</span><br><span class="line">    - git commit -am &apos;[skip ci] DEV image update&apos;</span><br><span class="line">    - git push origin master</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>最后添加一个部署到 prod 环境的阶段，和前面非常类似，只是添加了一个手动操作的流程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">deploy-prod:</span><br><span class="line">  stage: deploy-prod</span><br><span class="line">  image: cnych/kustomize:v1.0</span><br><span class="line">  before_script:</span><br><span class="line">    - git remote set-url origin http://$&#123;CI_USERNAME&#125;:$&#123;CI_PASSWORD&#125;@git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    - git config --global user.email &quot;gitlab@git.k8s.local&quot;</span><br><span class="line">    - git config --global user.name &quot;GitLab CI/CD&quot;</span><br><span class="line">  script:</span><br><span class="line">    - git checkout -B master</span><br><span class="line">    - git pull origin master</span><br><span class="line">    - cd deployment/prod</span><br><span class="line">    - kustomize edit set image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span><br><span class="line">    - cat kustomization.yaml</span><br><span class="line">    - git commit -am &apos;[skip ci] PROD image update&apos;</span><br><span class="line">    - git push origin master</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br><span class="line">  when: manual</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>这样我们就完成了整个流水线的定义。</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>接下来我们来看看它们是如何一起工作的。我们将开发和线上两个环境的应用分别部署在了 dev 和 prod 命名空间之下，通过 Ingress 暴露服务，同样需要将两个应用的域名 <a href="http://webapp.dev.k8s.local/" target="_blank" rel="noopener">http://webapp.dev.k8s.local/</a> 与 <a href="http://webapp.prod.k8s.local/" target="_blank" rel="noopener">http://webapp.prod.k8s.local/</a> 在本地 <code>/etc/hosts</code> 中添加映射。</p><p>如果一切正常的话现在我们可以在浏览器中来查看我们部署的 web 应用程序了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkylSD4SV0tgeuyviclKwuKicCJ6LyPqRsZ9oV9fvd7NuxFUNnZ5bf6Q0A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt>Dev web app</p><p>然后我们来尝试修改下代码，编辑 main.go 文件，将变量 welcome 中的 <code>GITOPS</code> 修改为 <code>GITOPS-K8S</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">   welcome := Welcome&#123;&quot;GITOPS-K8S&quot;, time.Now().Format(time.Stamp), os.Getenv(&quot;HOSTNAME&quot;)</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>然后提交代码到 master 分支，然后进入 GitLab 项目 -&gt; CI/CD -&gt; Pipelines，就可以看到一个新的流水线开始构建了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkEz8Po0y2e3IqlSNH8u1WmgBHfokg8zoKXkALuE0Xhac19cXaLymHfA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>等待一会儿，正常情况下会执行到 dev 的部署阶段，然后变成 <code>skipped</code> 的状态，此时流水线已经将代码中的 dev 下的资源清单文件已经更新了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkCyPfBKa2csl9PfKNYkN64sYIBjllnhicEu4jcV6ianUaTiab38K0T0LNw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitLab CI/CD Pipeline</p><p>然后 Argo CD 在自动同步模式下在一分钟内变会更新 Kubernetes 的资源对象，我们也可以在 Argo CD 的页面中看到进度。当 Argo CD 中同步完成后我们再去查看 DEV 环境的应用，就可以看到页面上面的信息已经变成了 <code>GITOPS-K8S</code> 了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUk5aXjc4XibnmywxOzt1dBr9aR3gPMOqUFU4E7lxXzIRQSia8Nrs8t1yjQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt>Update Dev Web APP</p><p>最后如果需要部署到 prod 环境，我们只需要在 GitLab 的流水线中手动触发即可，之后，prod 中的镜像也会被更新。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkInCosUJFMHtjiaO1quRoQohfICvaa1QO7aZyKs15yKOR4GUyUI0EwYQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitLab CI/CD Prod deployment</p><p>下面是同步时 Argo CD 更新的页面状态变化图。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkFgM9ujWCgKRLHVJCJxQDcFwAO4BictzbeuO52PTlH3OWIOf4o7lpricw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>Argo CD Sync Workflow</p><p>到这里，我们就使用 GitOps 成功的将我们的应用部署到了开发和生产环境之中了。</p></blockquote><blockquote><p> 转发来源  阳明 k8s技术圈 <a href="https://mp.weixin.qq.com/s/tCY86QZ_K3STiN5ZvfhjpQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/tCY86QZ_K3STiN5ZvfhjpQ</a></p></blockquote><blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://www.weave.works/technologies/gitops/" target="_blank" rel="noopener">https://www.weave.works/technologies/gitops/</a></li></ul><ul><li><a href="https://argoproj.github.io/argo-cd/" target="_blank" rel="noopener">https://argoproj.github.io/argo-cd/</a></li></ul><ul><li><a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">https://docs.gitlab.com/ee/ci/yaml/</a></li></ul><ul><li><a href="https://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6" target="_blank" rel="noopener">https://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6</a></li></ul><ul><li><a href="https://github.com/cnych/gitops-webapp-demo" target="_blank" rel="noopener">https://github.com/cnych/gitops-webapp-demo</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h1 id=&quot;使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践&quot;&gt;&lt;a href=&quot;#使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践&quot; class=&quot;headerlink&quot; title=&quot;使用 GitLab
      
    
    </summary>
    
      <category term="devops" scheme="http://zhangyu8.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhangyu8.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>理解Kubernetes架构</title>
    <link href="http://zhangyu8.me/2020/07/03/%E7%90%86%E8%A7%A3Kubernetes%E6%9E%B6%E6%9E%84/"/>
    <id>http://zhangyu8.me/2020/07/03/理解Kubernetes架构/</id>
    <published>2020-07-03T03:00:00.000Z</published>
    <updated>2020-07-03T03:22:12.479Z</updated>
    
    <content type="html"><![CDATA[<p>架构师波波</p><p><a href="https://blog.csdn.net/yang75108/article/details/100215486" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/100215486</a> </p><blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>理解K8s的架构是运用好K8s的基础，本文波波帮助大家梳理一下K8s的架构。我们先会对K8s的架构进行一个概览，然后分别剖析Master和Worker节点的组件构成，然后把这些组件再集成起来，通过一个发布样例展示这些组件是如何配合工作的，最后展示K8s集群的总体架构。</p><h2 id="架构概览"><a href="#架构概览" class="headerlink" title="架构概览"></a>架构概览</h2><p><img src="https://img-blog.csdnimg.cn/20190902113048702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图是K8s架构的概览。K8s集群中主要有两类角色，一类是Master节点，另外一类是Worker节点，简单讲，Master节点主要用来管理和调度集群资源的，而Worker节点则是提供资源的。在一个高可用的K8s集群中，Master和Worker一般都有多个节点构成，这些节点可以是物理机，也可以是虚拟机。</p><p>Worker节点提供的资源单位称为Pod，简单理解，Pod就是K8s云平台提供的虚拟机。Pod里头住的是应用容器，比如Docker容器，容器是CPU/Mem资源隔离单位。大部分场景下，一个Pod只住一个应用容器，但是也有一些场景，一个Pod里头可以住多个容器，其中一个是主容器，其它则是辅助容器。一个Pod里头的容器共享Pod的网络栈和存储资源。</p><p>K8s主要解决集群资源调度的问题。简单讲，就是当有应用发布请求过来的时候，K8s需要根据集群资源空闲现状，将这个应用的Pods合理的分配到空闲的Worker节点上去。同时，K8s需要时刻监控集群，如果有节点或者Pods挂了，它要能够重新协调和启动Pods，保证应用高可用，这个术语叫自愈。还有，K8s需要管理集群网络，保证Pod/服务之间可以互通互联。</p><h2 id="Master节点组件"><a href="#Master节点组件" class="headerlink" title="Master节点组件"></a>Master节点组件</h2><p><img src="https://img-blog.csdnimg.cn/2019090211310821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>Master节点是K8s集群大脑，它由如下组件构成：</p><ol><li><strong>Etcd</strong>： 它是K8s的集中状态存储，所有的集群状态数据，例如节点，Pods，发布，配置等等，最终都存储在Etcd中。Etcd是一个分布式KV数据库，采用Raft分布式一致性算法。Etcd高可用部署一般需要至少三个节点。Etcd集群可以独立部署，也可以和Master节点住在一起。</li><li><strong>API server</strong>： 它是K8s集群的接口和通讯总线。用户通过kubectl，dashboard或者sdk等方式操作K8s，背后都通过API server和集群进行交互。集群内的其它组件，例如Kubelet/Kube-Proxy/Scheduler/Controller-Manager等，都通过API server和集群进行交互。API server可以认为是Etcd的一个代理Proxy，它是唯一能够访问操作Etcd数据库的组件，其它组件，都必须通过API server间接操作Etcd。API server不仅接受其它组件的API请求，它还是集群的事件总线，其它组件可以订阅在API server上，当有新事件发生时候，API server会将相关事件通知到感兴趣的组件。</li><li><strong>Scheduler</strong>： 它是K8s集群负责调度决策的组件。Scheduler掌握当前的集群资源使用情况，当有新的应用发布请求被提交到K8s集群，它负责决策相应的Pods应该分布到哪些空闲节点上去。K8s中的调度决策算法是可以扩展的。</li><li><strong>Controller Manager</strong>： 它是保证集群状态最终一致的组件。它通过API server监控集群状态，确保实际状态和预期状态最终一致，如果一个应用要求发布十个Pods，Controller Manager保证这个应用最终启动十个Pods，如果中间有Pods挂了，Controller Manager会负责协调重启Pods，如果Pods启多了，Controller Manager会负责协调关闭多余Pods。也即是说，K8s采用最终一致调度策略，它是集群自愈的背后实现机制。</li></ol><h2 id="Worker节点组件"><a href="#Worker节点组件" class="headerlink" title="Worker节点组件"></a>Worker节点组件</h2><p><img src="https://img-blog.csdnimg.cn/20190902113122457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>Worker节点是K8s集群资源的提供者，它由如下组件构成：</p><ol><li><strong>Kubelet</strong>: 它是Worker节点资源的管理者，相当于一个Agent角色。它监听API server的事件，根据Master节点的指示启动或者关闭Pod等资源，也将本节点状态数据汇报给Master节点。如果说Master节点是K8s集群的大脑，那么Kubelet就是Worker节点的小脑。</li><li><strong>Container Runtime</strong>: 它是节点容器资源的管理者，如果采用Docker容器，那么它就是Docker Engine。Kubelet并不直接管理节点的容器资源，它委托Container Runtime进行管理，比如启动或者关闭容器，收集容器状态等。Container Runtime在启动容器时，如果本地没有镜像缓存，则需要到Docker Registry(或Docker Hub)去拉取相应镜像，然后缓存本地。</li><li><strong>Kube-Proxy</strong>: 它是管理K8s中的服务(Service)网络的组件。Pod在K8s中是ephemeral的概念，也就是不固定的，PodIP可能会变(包括预期和非预期的)。为了屏蔽PodIP的可能的变化，K8s中引入了Servie概念，它可以屏蔽应用的PodIP，并且在调用时进行负载均衡。Kube-Proxy是实现K8s服务(Service)网络的背后机制。另外，当需要把K8s中的服务(Service)暴露给外网时，也需要通过Kube-Proxy进行代理转发。</li></ol><h2 id="流程样例"><a href="#流程样例" class="headerlink" title="流程样例"></a>流程样例</h2><p><img src="https://img-blog.csdnimg.cn/20190902113135346.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>如果我们把Master节点和Worker节点集成起来，就构成上图所示的K8s集群。下面我们通过一个发布流程，展示上面介绍的这些组件是如何配合工作的。</p><ol><li>假设管理员要发布一个新应用，他通过Kubectl命令行工具将发布请求提交到API server，API server将请求存储到Etcd数据库中。</li><li>Scheduler通过API server监听到有新的应用发布请求，它通过调度算法决策，选择若干可发布的空闲节点，并将发布决策更新到API server。</li><li>被选中的Worker节点上的Kubelet通过API server监听到有给自己的新发布任务，它根据任务指示在本地启动相应的Pods(间接通过Container Runtime启动容器)，并将任务执行成功情况报告给API server。</li><li>所有Worker节点上Kube-Proxy通过API server监听到有新的发布，它获取应用的PodIP/ClusterIP/端口等相关数据，更新本地的iptables表规则，让本地的Pods可以通过iptables转发方式，访问到新发布应用的Pods。</li><li>Controller Manager通过API server，时刻监控新发应用的健康状况，保证实际状态和预期状态最终一致。</li></ol><h2 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h2><p><img src="https://img-blog.csdnimg.cn/20190902113147939.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图是一个K8s集群的总体架构。实际K8s集群中还有一个覆盖(Overlay)网络，集群中的Pods通过覆盖网络可以实现IP寻址和通讯。实现覆盖网络的技术有很多，例如Flannel/VxLan/Calico/Weave-Net等等。外网流量如果要访问K8s集群内部的服务，一般要走负载均衡器(Load Balancer)，背后流量会通过Kube-Proxy间接转发到服务Pods上。</p><p>除了上述组件，K8s外围一般还有存储，监控，日志和分析等配套支持服务。</p><h2 id="总结和课程推荐"><a href="#总结和课程推荐" class="headerlink" title="总结和课程推荐"></a>总结和课程推荐</h2><p>下表我把本文讲到的一些K8s关键组件的作用做一个梳理总结，方便大家理解记忆。</p><p><img src="https://img-blog.csdnimg.cn/20190902113204818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;架构师波波&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/100215486&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/yan
      
    
    </summary>
    
      <category term="Kubernetes" scheme="http://zhangyu8.me/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://zhangyu8.me/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>我为啥暂不看好ServiceMesh</title>
    <link href="http://zhangyu8.me/2020/06/28/%E6%88%91%E4%B8%BA%E5%95%A5%E6%9A%82%E4%B8%8D%E7%9C%8B%E5%A5%BDServiceMesh/"/>
    <id>http://zhangyu8.me/2020/06/28/我为啥暂不看好ServiceMesh/</id>
    <published>2020-06-28T03:00:00.000Z</published>
    <updated>2020-06-28T08:26:44.713Z</updated>
    
    <content type="html"><![CDATA[<p>我为啥暂不看好ServiceMesh?</p><p><a href="https://blog.csdn.net/yang75108/article/details/87266458" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/87266458</a></p><blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>过去的2018年，ServiceMesh(服务网格)概念在社区里头非常火，有人提出2018年是ServiceMesh年，还有人提出ServiceMesh是下一代的微服务架构基础。作为架构师，如果你现在还不了解ServiceMesh的话，是否感觉有点落伍了？</p><p>那么到底什么是ServiceMesh？它诞生的背景是什么？它解决什么问题？企业是否适合引入ServiceMesh？根据近年在一线互联网企业的实践和思考，从个人视角出发，我为大家一一解答这些问题。</p><h2 id="微服务架构的核心技术问题"><a href="#微服务架构的核心技术问题" class="headerlink" title="微服务架构的核心技术问题"></a>微服务架构的核心技术问题</h2><p>在业务规模化和研发效能提升等因素的驱动下，从单块应用向微服务架构的转型(如下图所示)，已经成为很多企业(尤其是互联网企业)数字化转型的趋势。</p><p><img src="https://img-blog.csdnimg.cn/20200211204043954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在微服务模式下，企业内部服务少则几个到几十个，多则上百个，每个服务一般都以集群方式部署，这时自然产生两个问题(如下图所示)：</p><p><img src="https://img-blog.csdnimg.cn/20200211204322439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p><strong>一、服务发现</strong>：服务的消费方(Consumer)如何发现服务的提供方(Provider)？</p><p><strong>二、负载均衡</strong>：服务的消费方如何以某种负载均衡策略访问集群中的服务提供方实例？</p><p>作为架构师，如果你理解了这两个问题，可以说就理解了微服务架构在技术上的最核心问题。</p><h2 id="三种服务发现模式"><a href="#三种服务发现模式" class="headerlink" title="三种服务发现模式"></a>三种服务发现模式</h2><p>服务发现和负载均衡并不是新问题，业界其实已经探索和总结出一些常用的模式，这些模式的核心其实是代理(Proxy，如下图所以)，以及代理在架构中所处的位置，</p><p><img src="https://img-blog.csdnimg.cn/20200211204059487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在服务消费方和服务提供方之间增加一层代理，由代理负责服务发现和负载均衡功能，消费方通过代理间接访问目标服务。根据代理在架构上所处的位置不同，当前业界主要有三种不同的服务发现模式：</p><h3 id="模式一：传统集中式代理"><a href="#模式一：传统集中式代理" class="headerlink" title="模式一：传统集中式代理"></a>模式一：传统集中式代理</h3><p><img src="https://img-blog.csdnimg.cn/20200211204117995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，F5(4层负载)+Nginx(7层负载)这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。</p><p>这种方式通常在DNS域名服务器的配合下实现服务发现，服务注册(建立服务域名和IP地址之间的映射关系)一般由运维人员在代理上手工配置，服务消费方仅依赖服务域名，这个域名指向代理，由代理解析目标地址并做负载均衡和调用。</p><p>国外知名电商网站eBay，虽然体量巨大，但其内部的服务发现机制仍然是基于这种传统的集中代理模式，国内公司如携程，也是采用这种模式。</p><h3 id="模式二：客户端嵌入式代理"><a href="#模式二：客户端嵌入式代理" class="headerlink" title="模式二：客户端嵌入式代理"></a>模式二：客户端嵌入式代理</h3><p><img src="https://img-blog.csdnimg.cn/2020021120413546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。</p><p>Netflix开源的Eureka(注册中心)和Ribbon(客户端代理)是这种模式的典型案例，国内阿里开源的Dubbo也是采用这种模式。</p><h3 id="模式三：主机独立进程代理"><a href="#模式三：主机独立进程代理" class="headerlink" title="模式三：主机独立进程代理"></a>模式三：主机独立进程代理</h3><p>这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同模式二。</p><p><img src="https://img-blog.csdnimg.cn/20200211204151399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>Airbnb的SmartStack是这种模式早期实践产品，国内公司唯品会对这种模式也有探索和实践。</p><h2 id="三种服务发现模式的比较"><a href="#三种服务发现模式的比较" class="headerlink" title="三种服务发现模式的比较"></a>三种服务发现模式的比较</h2><p>上面介绍的三种服务发现模式各有优劣，没有绝对的好坏，可以认为是三种不同的架构风格，在不同的公司都有成功实践。下表总结三种服务发现模式的优劣比较，业界案例和适用场景建议，供架构师选型参考：</p><p><img src="https://img-blog.csdnimg.cn/20200211204205282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><h2 id="服务网格ServiceMesh"><a href="#服务网格ServiceMesh" class="headerlink" title="服务网格ServiceMesh"></a>服务网格ServiceMesh</h2><p>所谓的ServiceMesh，其实本质上就是上面提到的模式三~主机独立进程模式，这个模式其实并不新鲜，业界(国外的Airbnb和国内的唯品会等)早有实践，那么为什么现在这个概念又流行起来了呢？我认为主要原因如下：</p><ol><li>上述模式一和二有一些固有缺陷，模式一相对比较重，有单点问题和性能问题；模式二则有客户端复杂，支持多语言困难，无法集中治理的问题。模式三是模式一和二的折中，弥补了两者的不足，它是纯分布式的，没有单点问题，性能也OK，应用语言栈无关，可以集中治理。</li><li>微服务化、多语言和容器化发展的趋势，企业迫切需要一种轻量级的服务发现机制，ServiceMesh正是迎合这种趋势诞生，当然这还和一些大厂(如Google/IBM等)的背后推动有关。</li></ol><p>模式三(ServiceMesh)也被形象称为边车(Sidecar)模式，如下图，早期有一些摩托车，除了主驾驶位，还带一个边车位，可以额外坐一个人。在模式三中，业务代码进程(相当于主驾驶)共享一个代理(相当于边车)，代理除了负责服务发现和负载均衡，还负责动态路由、容错限流、监控度量和安全日志等功能，这些功能是具体业务无关的，属于跨横切面关注点(Cross-Cutting Concerns)范畴。</p><p><img src="https://img-blog.csdnimg.cn/20200211204224981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在新一代的ServiceMesh架构中(下图上方)，服务的消费方和提供方主机(或者容器)两边都会部署代理SideCar。ServiceMesh比较正式的术语也叫数据平面(DataPlane)，与数据平面对应的还有一个独立部署的控制平面(ControlPlane)，用来集中配置和管理数据平面，也可以对接各种服务发现机制(如K8S服务发现)。术语数据平面和控制平面，估计是偏网络SDN背景的人提出来的。</p><p><img src="https://img-blog.csdnimg.cn/20200211204234911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>上图左下角，每个主机上同时居住了业务逻辑代码(绿色表示)和代理(蓝色表示)，服务之间通过代理发现和调用目标服务，形成服务之间的一种网络状依赖关系，控制平面则可以配置这种依赖调用关系，也可以调拨路由流量。如果我们把主机和业务逻辑剥离，就出现一种网格状架构(上图右下角)，服务网格由此得名。</p><p><img src="https://img-blog.csdnimg.cn/20200211204245278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>Istio是Google/IBM等大厂支持和推进的一个ServiceMesh标准化工作组，上图是Istio给出的ServiceMesh参考架构(注意这个是老版架构，新版有一些调整，但是大框架没变)。Istio专注在控制平面的架构、功能、以及控制平面和数据平面之间API的标准化，它的控制平面功能主要包括：</p><ul><li>Istio-Manager：负责服务发现，路由分流，熔断限流等配置数据的管理和下发</li><li>Mixer：负责收集代理上采集的度量数据，进行集中监控</li><li>Istio-Auth：负责安全控制数据的管理和下发</li></ul><p>Envoy是目前Istio主力支持的数据平面代理，其它主流代理如nginx/kong等也正在陆续加入这个阵营。kubernetes是目前Isito主力支持的容器云环境。</p><h2 id="我的建议"><a href="#我的建议" class="headerlink" title="我的建议"></a>我的建议</h2><p>目前我本人并不特别看好ServiceMesh，也不是特别建议企业在生产上试水ServiceMesh，主要原因如下：</p><ol><li>ServiceMesh其实并不是什么新东西，本质就是上面提到的服务发现模式三~主机独立进程模式，这个模式很早就有公司在探索和实践，但是一直没有普遍流行起来，说明这个模式也是存在落地挑战的。从表面上看，模式三是模式一和模式二的折中，同时解决了模式一和模式二存在的问题，但是在每个主机上独立部署一个代理进程，是有很大运维管理开销的，一方面是规模化部署的问题(考虑服务很多，机器也很多的场景)；另一方面是如何监控治理的问题，代理挂了怎么办？你的团队是否具备自动化运维和监控的能力？另外开发人员在服务调试的时候，会依赖于这个独立的代理，调试排错比较麻烦，这个问题怎么解决？</li><li>Istio的确做了一些标准化工作，但是没有什么特别的创新，可是说换汤不换药，就是把模式三规范化和包装了一下。透过现象看本质，Google/IBM等行业大厂在背后推Isito/ServiceMesh，背后有一些市场利益诉求考虑，例如Google要推进它的kubernates和公有云生态。</li><li>ServiceMesh在年初声音比较大，最近渐渐安静下来，我听到国内只有一些大厂(华为，新浪微博，蚂蚁金服等)在试水，实际生产级落地的案例聊聊无几。大多数企业对ServiceMesh只是观望，很多架构师对ServiceMesh实际落地都存在疑虑。</li></ol><p>所以我的个人建议，对于大部分企业(一般运维和研发能力不是特别强)，采用模式一~集中代理模式就足够了。这个模式比较传统不新鲜，但是在很多一线企业已经切实落地，我甚至认为，除了一些大厂，大部分中小企业的服务发现架构采用的就是集中代理。我本人经历过三家互联网公司，大的有eBay，中等有携程，小的有拍拍贷，都是采用集中式代理模式，而且玩得都很好。我的架构理念很简单，对于生产级应用，不追新，老实采用大部分企业落地过的方案。</p><p>模式一的最大好处是集中治理，应用不侵入，语言栈无关，另外因为模式一是集中部署的，不像模式三是分布式部署，所以模式一的运维开销也远小于模式三。对于模式一，大家最大的顾虑是性能和单点问题，其实性能还是OK的，如果架构和容量规划合理的话，实际生产中经过集中代理的性能开销一般可以控制在小于10个ms，eBay和携程等大流量企业的成功实践已经验证了这点。单点问题一般建议采用两层负载结构，例如硬件F5+软件nginx两层负载，F5以主从HA部署，nginx则以集群多实例部署，这种架构兼顾了高可用和配置的灵活性。</p><p>另外，模式一还可以和服务注册中心结合，从而降低手工配置的复杂性，实现DevOps研发自助部署，一种方案如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200211204300581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>服务启动时自动注册到服务注册中心并定期报心跳，Proxy则定期到服务注册中心同步实例。这种方式下，不需要为每个服务申请一个域名，只需一个泛域名即可，消费者访问服务时采用服务名+泛域名即可，整个服务上线流程可以做到DevOps研发自助。目前社区流行的一些开源代理如traefik和kong\等都支持和多种服务注册中心(Consul/Eureka/Etcd/Zookeeper等)进行集成。目前这种方案在拍拍贷有初步成功实践，采用kong和自研服务注册中心Radar同时和容器云调度平台配合，实现了研发全自助式发布上线。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>服务注册发现和负载均衡是微服务架构在技术上的根本问题，解决的办法是采用代理Proxy。根据代理在架构上的位置不同，服务发现代理一般有三种模式：</li></ol><ul><li>模式一：集中式代理</li><li>模式二：客户端嵌入式代理</li><li>模式三：主机独立进程代理<br>这三种模式没有绝对的好还之分，只是三种不同的架构风格，各有优劣和适用场景，在不同企业都有成功落地案例。</li></ul><ol start="2"><li>ServiceMesh本质上就是模式三~主机独立进程代理，它结合了模式一和模式二的优势，但是分布式部署运维管理开销大。Istio对ServiceMesh的架构、功能和API进行了标准化。</li></ol></blockquote><blockquote><ol start="3"><li>ServiceMesh还在演进中，生产落地仍有挑战，一般企业不建议生产级使用。集中式代理最成熟，对于一般中小企业，建议从集中式代理开始，等达到一定规模和具备一定的研发运维能力，再根据需要考虑其它服务发现模式.</li><li>架构师不要盲目追新，在理解微服务架构原理的基础上，可以学习和试点新技术，但是对于生产级应用，应该以成熟稳定，有大规模落地案例作为选型第一准则。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我为啥暂不看好ServiceMesh?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/87266458&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog
      
    
    </summary>
    
      <category term="ServiceMesh" scheme="http://zhangyu8.me/categories/ServiceMesh/"/>
    
    
      <category term="ServiceMesh" scheme="http://zhangyu8.me/tags/ServiceMesh/"/>
    
  </entry>
  
  <entry>
    <title>微服务为什么要配置中心</title>
    <link href="http://zhangyu8.me/2020/06/28/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    <id>http://zhangyu8.me/2020/06/28/微服务为什么要配置中心/</id>
    <published>2020-06-28T03:00:00.000Z</published>
    <updated>2020-06-28T05:45:36.794Z</updated>
    
    <content type="html"><![CDATA[<p>微服务为什么要配置中心?</p><p>架构师波波的专栏</p><p><a href="https://blog.csdn.net/yang75108/article/details/86987941" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/86987941</a></p><blockquote><h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>在系统架构中，和安全、日志、监控等非功能需求一样，配置管理也是一种非功能需求。配置中心是整个微服务基础架构体系中的一个组件，如下图，它的功能看上去并不起眼，无非就是简单配置的管理和存取，但它是整个微服务架构中不可或缺的一环。另外，配置中心如果真得用好了，它还能推动技术组织持续交付和DevOps文化转型。<br><img src="https://img-blog.csdnimg.cn/20200212132045204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" width="100%" height="100%"></p><p>本文介绍在分布式微服务环境下，应用配置管理背后的业务需求，配置的各种分类和一些高级应用场景。</p><h2 id="二、配置定义和形态"><a href="#二、配置定义和形态" class="headerlink" title="二、配置定义和形态"></a>二、配置定义和形态</h2><p><strong>配置其实是独立于程序的可配变量</strong>，同一份程序在不同配置下会有不同的行为，常见的配置有连接字符串，应用配置和业务配置等。</p><p>配置有多种形态，下面是一些常见的：</p><ul><li><strong>程序内部hardcode</strong>，这种做法是反模式，一般我们<strong>不建议！</strong></li><li><strong>配置文件</strong>，比如Spring应用程序的配置一般放在<code>application.properties</code>文件中。</li><li><strong>环境变量</strong>，配置可以预置在操作系统的环境变量里头，程序运行时读取，这是很多PaaS平台，比如Heroku推荐的做法，参考12 factor app[附录9.1]。</li><li><strong>启动参数</strong>，可以在程序启动时一次性提供参数，例如java程序启动时可以通过<code>java -D</code>方式配启动参数。</li><li><strong>基于数据库</strong>，有经验的开发人员会把易变配置放在数据库中，这样可以在运行期灵活调整配置，这个做法和配置中心的思路已经有点接近了。</li></ul><p><img src="https://img-blog.csdnimg.cn/20200212132102689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9ECzTdDT-1581484808516)(http://jskillcloud.com/img/post/2018060701/config_format.png#pic_center)]"></p><h2 id="三、传统应用配置的痛点"><a href="#三、传统应用配置的痛点" class="headerlink" title="三、传统应用配置的痛点"></a>三、传统应用配置的痛点</h2><p>在没有引入配置中心之前，一般企业研发都会面临如下痛点：</p><h3 id="1-配置散乱格式不标准"><a href="#1-配置散乱格式不标准" class="headerlink" title="1. 配置散乱格式不标准"></a><strong>1. 配置散乱格式不标准</strong></h3><p>有的用properties格式，有的用xml格式，还有的存DB，团队倾向自造轮子，做法五花八门。</p><h3 id="2-主要采用本地静态配置，配置修改麻烦"><a href="#2-主要采用本地静态配置，配置修改麻烦" class="headerlink" title="2. 主要采用本地静态配置，配置修改麻烦"></a><strong>2. 主要采用本地静态配置，配置修改麻烦</strong></h3><p>配置修改一般需要经过一个较长的测试发布周期。在分布式微服务环境下，当服务实例很多时，修改配置费时费力。</p><h3 id="3-易引发生产事故"><a href="#3-易引发生产事故" class="headerlink" title="3. 易引发生产事故"></a><strong>3. 易引发生产事故</strong></h3><p>这个是我亲身经历，之前在一家互联网公司，有团队在发布的时候将测试环境的配置带到生产上，引发百万级资损事故。</p><h3 id="4-配置缺乏安全审计和版本控制功能"><a href="#4-配置缺乏安全审计和版本控制功能" class="headerlink" title="4. 配置缺乏安全审计和版本控制功能"></a><strong>4. 配置缺乏安全审计和版本控制功能</strong></h3><p>谁改的配置？改了什么？什么时候改的？无从追溯，出了问题也无法及时回滚。</p><h2 id="四、现代应用配置核心需求"><a href="#四、现代应用配置核心需求" class="headerlink" title="四、现代应用配置核心需求"></a>四、现代应用配置核心需求</h2><p>近年，持续交付和DevOps理念开始逐步被一线企业接受，微服务架构和容器云也逐渐在一线企业落地，这些都对应用配置管理提出了更高的要求：</p><p><img src="https://img-blog.csdnimg.cn/20200212132118751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-q00lcjAM-1581484808517)(http://jskillcloud.com/img/post/2018060701/core_requirements.png#pic_center)]"></p><h3 id="1-交付件和配置分离"><a href="#1-交付件和配置分离" class="headerlink" title="1. 交付件和配置分离"></a><strong>1. 交付件和配置分离</strong></h3><p>传统做法应用在打包部署时，会为不同环境打出不同配置的包，例如为开发/测试/UAT/生产环境分别制作发布包，每个包里头包含环境特定配置。</p><p>现代微服务提倡云原生(Cloud Native)和不可变基础设施（Immutable Infrastructure）的理念，推荐采用如容器镜像这种方式打包和交付微服务，应用镜像一般只打一份，可以部署到不同环境。这就要求交付件（比如容器镜像）和配置进行分离，交付件只制作一份，并且是不可变的，可以部署到任意环境，而配置由配置中心集中管理，所有环境的配置都可以在配置中心集中配，运行期应用根据自身环境到配置中心动态拉取相应的配置。</p><h3 id="2-抽象标准化"><a href="#2-抽象标准化" class="headerlink" title="2. 抽象标准化"></a><strong>2. 抽象标准化</strong></h3><p>企业应该由框架或者中间件团队提供标准化的配置中心服务(Configuration as a Service)，封装屏蔽配置管理的细节和配置的不同格式，方便用户进行自助式的配置管理。一般用户只需要关注两个抽象和标准化的接口：</p><ol><li>配置管理界面UI，方便应用开发人员管理和发布配置，</li><li>封装好的客户端API，方便应用集成和获取配置。</li></ol><h3 id="3-多环境多集群"><a href="#3-多环境多集群" class="headerlink" title="3. 多环境多集群"></a><strong>3. 多环境多集群</strong></h3><p>现代微服务应用大都采用多环境部署，一般标准化的环境有开发/测试/UAT/生产等，有些应用还需要多集群部署，例如支持跨机房或者多版本部署。配置中心需要支持对多环境和多集群应用配置的集中式管理。</p><h3 id="4-高可用"><a href="#4-高可用" class="headerlink" title="4. 高可用"></a><strong>4. 高可用</strong></h3><p>配置中心必须保证高可用，不能随便挂，否则可能大面积影响微服务。在极端的情况下，如果配置中心不可用，客户端也需要有降级策略，保证应用可以不受影响。</p><h3 id="5-实时性"><a href="#5-实时性" class="headerlink" title="5. 实时性"></a><strong>5. 实时性</strong></h3><p>配置更新需要尽快通知到客户端，这个周期不能太长，理想应该是实时的。有些配置的实时性要求很高，比方说主备切换配置或者蓝绿部署配置，需要秒级切换配置的能力。</p><h3 id="6-治理"><a href="#6-治理" class="headerlink" title="6. 治理"></a><strong>6. 治理</strong></h3><p>配置需要治理，具体包括：</p><ul><li>配置审计，谁、在什么时间、修改了什么配置，需要详细的审计，方便出现问题时能够追溯。</li><li>配置版本控制，每次变更需要版本化，出现问题时候能够及时回滚到上一版本。</li><li>配置权限控制，配置变更发布需要认证授权，不是所有人都能修改和发布配置。</li><li>灰度发布，高级的配置治理支持灰度发布，配置发布时可以先让少数实例生效，确保没有问题再逐步放量。</li></ul><h2 id="五、配置分类"><a href="#五、配置分类" class="headerlink" title="五、配置分类"></a>五、配置分类</h2><p>配置目前还没有特别标准的分类方法，我简单把配置分为静态和动态两大类，每一类再分为若干子类，如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200212132133488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-57ZCzoGH-1581484808518)(http://jskillcloud.com/img/post/2018060701/config_category.png#pic_center)]"></p><h3 id="1-静态配置"><a href="#1-静态配置" class="headerlink" title="1. 静态配置"></a><strong>1. 静态配置</strong></h3><p>所谓静态配置，就是在程序启动前一次性配好，启动时一次性生效，在程序运行期一般不会变化的配置。具体包括：</p><h4 id="1-1-环境相关配置"><a href="#1-1-环境相关配置" class="headerlink" title="1.1 环境相关配置"></a>1.1 环境相关配置</h4><p>有些配置是和环境相关的，每个环境的配置不一样，例如数据库、中间件和其它服务的连接字符串配置。这些配置一次性配好，运行期一般不变。</p><h4 id="1-2-安全配置"><a href="#1-2-安全配置" class="headerlink" title="1.2 安全配置"></a>1.2 安全配置</h4><p>有些配置和安全相关，例如用户名，密码，访问令牌，许可证书等，这些配置也是一次性配好，运行期一般不变。因为涉及安全，相关信息一般需要加密存储，对配置访问需要权限控制。</p><h3 id="2-动态配置"><a href="#2-动态配置" class="headerlink" title="2. 动态配置"></a><strong>2. 动态配置</strong></h3><p>所谓动态配置，就是在程序的运行期可以根据需要动态调整的配置。动态配置让应用行为和功能的调整变得更加灵活，是持续交付和DevOps的最佳实践。具体包括：</p><h4 id="2-1-应用配置"><a href="#2-1-应用配置" class="headerlink" title="2.1 应用配置"></a>2.1 应用配置</h4><p>和应用相关的配置，例如服务请求超时，线程池和队列的大小，缓存过期时间，数据库连接池的容量，日志输出级别，限流熔断阀值，服务安全黑白名单等。一般开发或者运维会根据应用的实际运行情况调整这些配置。</p><h4 id="2-2-业务配置"><a href="#2-2-业务配置" class="headerlink" title="2.2 业务配置"></a>2.2 业务配置</h4><p>和业务相关的一些配置，例如促销规则，贷款额度，利率等业务参数，A/B测试参数等。一般产品运营或开发人员会根据实际的业务需求，动态调整这些参数。</p><h4 id="2-3-功能开关"><a href="#2-3-功能开关" class="headerlink" title="2.3 功能开关"></a>2.3 功能开关</h4><p>在英文中也称Feature Flag/Toggle/Switch，简单的只有真假两个值，复杂的可以是多值参数。功能开关是DevOps的一种最佳实践，在运维中有很多应用场景，比如蓝绿部署，灰度开关，降级开关，主备切换开关，数据库迁移开关等。功能开关在国外互联网公司用得比较多，国内还没有普及开，所以我在下一节会给出一些功能开关的高级应用场景。</p><h2 id="六、配置中心高级应用场景"><a href="#六、配置中心高级应用场景" class="headerlink" title="六、配置中心高级应用场景"></a>六、配置中心高级应用场景</h2><h3 id="场景一、蓝绿部署"><a href="#场景一、蓝绿部署" class="headerlink" title="场景一、蓝绿部署"></a>场景一、蓝绿部署</h3><p>蓝绿部署的传统做法是通过负载均衡器切流量来实现，如下图左边所示。这种做法一般研发人员无法自助操作，需要提交工单由运维介入操作，操作和反馈周期比较长，出了问题回退还需运维人员介入，所以回退也比较慢，总体风险比较高。</p><p><img src="https://img-blog.csdnimg.cn/20200212132148357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ZFkvyLnE-1581484808519)(http://jskillcloud.com/img/post/2018060701/blue_green_deployment.png#pic_center)]"></p><p>蓝绿部署也可以通过配置中心+功能开关的方式来实现，如上图右边所示。开发人员在上线新功能时先将新功能隐藏在动态开关后面，开关的值在配置中心里头配。刚上线时新功能暂不启用，走老功能逻辑，然后开发人员通过配置中心打开开关，这个时候新功能就启用了。一旦发现新功能有问题，可以随时把开关关掉切回老功能。这种做法开发人员可以全程自助实现蓝绿部署，不需要运维人员介入，反馈周期短效率高。</p><h3 id="场景二、限流降级"><a href="#场景二、限流降级" class="headerlink" title="场景二、限流降级"></a>场景二、限流降级</h3><p>当业务团队在搞促销，或者是系统受DDOS攻击的时候，如果没有好的限流降级机制，则系统很容易被洪峰流量冲垮，这个时候所有用户无法访问，体验糟糕，如下图左边所示。</p><p><img src="https://img-blog.csdnimg.cn/20200212132159343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ieWQQ5Mr-1581484808520)(http://jskillcloud.com/img/post/2018060701/rate_limiting_degrade.png#pic_center)]"></p><p>所以我们需要限流降级机制来应对流量洪峰。常见做法，我们一般会在应用的过滤器层或者是网关代理层添加限流降级逻辑，并且和配置中心配合，实现限流降级开关和参数的动态调整。如果促销出现流量洪峰，我们可以通过配置中心启动限流降级策略，比如对于普通用户，我们可以先给出“网络不给力，请稍后再试”的友好提示，对于高级VIP用户，我们仍然保证他们的正常访问。</p><p>国内电商巨头阿里，它内部的系统大量采用<font color="red">限流降级机制，实现方式基于其内部的diamond+sentinel配置管理系统。</font>如果没有限流降级机制的保护，则阿里的系统也无法抵御双十一带来的洪峰流量冲击。</p><h3 id="场景三、数据库迁移"><a href="#场景三、数据库迁移" class="headerlink" title="场景三、数据库迁移"></a>场景三、数据库迁移</h3><p>LaunchDarkly是一家提供配置既服务(Configuration as a Service)的SAAS服务公司，它在其博客上给出了一片关于使用功能开关实现数据库迁移的案例文章，该案例基于其内部一次成功的数据库迁移实践，从MongdoDB迁移到DynamoDB[参考附录9.2]，下图是展示了一个简化的迁移流程：</p><p><img src="https://img-blog.csdnimg.cn/20200212132212489.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-N4Q8XNdr-1581484808520)(http://jskillcloud.com/img/post/2018060701/ff_database_migration-768x1024.jpg#pic_center)]"></p><p>简化迁移腾挪流程如下：</p><ol><li>开发人员先在应用端的DAO层埋好数据双写双读、以及数据比对逻辑。双写双读逻辑由开关控制，开关的值可在配置中心配。</li><li>先保证应用100%读写mongoDB，然后先放开10%的DynamoDB双写，也称金丝雀写(Canary Write)，确保金丝雀写没有功能和性能问题。</li><li>逐步放量DyanamoDB写到100%，确保全量双写没有功能和性能问题。</li><li>放开10%的DynamoDB双读，也称金丝雀读(Canary Read)，通过比对逻辑确保金丝雀读没有逻辑和性能问题。</li><li>逐步放量DynamoDB读到100%，通过比对逻辑确保全量双读没有逻辑和性能问题。</li><li>关闭对mongoDB的读写，迁移完成。</li></ol><p>整个迁移流程受配置中心的开关控制，可以灵活调整开关和参数，有问题可以随时回滚，大大降低迁移风险。</p><h3 id="场景四、A-B测试"><a href="#场景四、A-B测试" class="headerlink" title="场景四、A/B测试"></a>场景四、A/B测试</h3><p>如果我们需要对电商平台的结账(checkout)功能进行改版，考虑到结账功能业务影响面大，一下子上线风险大，为了减低风险，我们可以在配置中心配合下，对结账功能进行A/B测试，简化逻辑如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200212132223128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-xlzC9otk-1581484808521)(http://jskillcloud.com/img/post/2018060701/ab_test.png#pic_center)]"></p><p>我们在配置中心中增加一个<code>ab_test_flag</code>开关，控制A/B测试逻辑：</p><ol><li>如果A/B测试开关是关闭的(<code>ab_test_flag==false</code>)，那么就走老的结账逻辑。</li><li>如果A/B测试开关是打开的(<code>ab_test_flag==true</code>，并且是普通用户(<code>user==regular</code>，可以检查数据库中用户类型)，那么就走老的结账逻辑。</li><li>如果A/B测试开关是打开的(<code>ab_test_flag==true</code>)，并且是beta用户（<code>user==beta</code>），那么就走改版后的新结账逻辑。</li></ol><p>通过配置中心，我们可以灵活调整开关，先对新功能进行充分的beta试验，再考虑全量上线，大大降低关键业务新功能的上线风险。</p><h2 id="七、公司案例和产品"><a href="#七、公司案例和产品" class="headerlink" title="七、公司案例和产品"></a>七、公司案例和产品</h2><p>在一线前沿的互联网公司，配置中心都是其技术体系中的关键基础服务，下图给出一些公司案例产品：</p><p><img src="https://img-blog.csdnimg.cn/20200212132236228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-MA6Ky9GG-1581484808521)(http://jskillcloud.com/img/post/2018060701/config_center_products.png#pic_center)]"></p><ol><li>阿里巴巴中间件部门很早就自研了配置中心Diamond，并且是开源的。Diamond对阿里系统的灵活稳定性发挥了至关重要的作用。开源版本的Diamond由于研发时间比较早，使用的技术比较老，功能也不够完善，目前社区不热已经不维护了。</li><li>Facebook内部也有一整套完善的配置管理体系[可参考其论文，附录9.3]，其中一个产品叫Gatekeeper，目前没有开源。</li><li>Netflix内部有大量的微服务，它的服务的稳定灵活性也重度依赖于配置中心。Netflix开源了它的配置中心的客户端，叫变色龙Archaius[参考附录9.4]，比较可惜的是，Netflix没有开源它的配置中心的服务器端。</li><li>Apollo[参考附录9.5]是携程框架部研发并开源的一款配置中心产品，企业级治理功能完善，目前社区比较火，在github上有超过5k星，在国内众多互联网公司有落地案例。<strong>如果企业打算引入开源的配置中心，那么Apollo是我推荐的首选</strong>。</li><li>百度之前也开源过一个叫Disconf[参考附录9.6]的配置中心产品，作者是前百度资深工程师廖绮绮。在Apollo没有出来之前，Disconf在社区是比较火的，但是自从廖琦琦离开百度之后，他好像没有足够精力投入维护这个项目，目前社区活跃度已经大不如前。</li></ol><h2 id="八、结论"><a href="#八、结论" class="headerlink" title="八、结论"></a>八、结论</h2><ol><li>配置中心是微服务基础架构中不可或缺的核心组件，现代微服务架构和云原生环境，对应用配置管理提出了更高的要求。</li><li>配置中心有众多的应用场景，<strong>配置中心+功能开关是DevOps最佳实践</strong>。用好配置中心，它能帮助技术组织实现持续交付和DevOps文化转型。</li><li>携程开源的Apollo配置中心，企业级功能完善，经过大规模生产验证，社区活跃度高，是开源配置中心产品的首选。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;微服务为什么要配置中心?&lt;/p&gt;
&lt;p&gt;架构师波波的专栏&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/86987941&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;htt
      
    
    </summary>
    
      <category term="配置中心" scheme="http://zhangyu8.me/categories/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
    
      <category term="配置中心" scheme="http://zhangyu8.me/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>小米Redis的K8s容器化部署实践</title>
    <link href="http://zhangyu8.me/2020/06/22/%E5%B0%8F%E7%B1%B3Redis%E7%9A%84Kubernetes%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zhangyu8.me/2020/06/22/小米Redis的Kubernetes容器化部署实践/</id>
    <published>2020-06-22T03:00:00.000Z</published>
    <updated>2020-06-28T08:09:43.707Z</updated>
    
    <content type="html"><![CDATA[<p>小米Redis的K8s容器化部署实践</p><p>原创 崔凯峰 小米云技术 </p><p><a href="https://mp.weixin.qq.com/s/WrUU3C-C8TBgJfGuOv3qGQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/WrUU3C-C8TBgJfGuOv3qGQ</a></p><blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>小米的Redis使用规模很大，现在有数万个实例，并且每天有百万亿次的访问频率，支撑了几乎所有的产品线和生态链公司。之前所有的Redis都部署在物理机上，也没有做资源隔离，给管理治理带来了很大的困难。我们的运维人员工作压力很大，机器宕机网络抖动导致的Redis节点下线都经常需要人工介入处理。由于没有做CPU的资源隔离，slave节点打RDB或者由于流量突增导致节点QPS升高造成的节点CPU使用率升高，都可能对本集群或其他集群的节点造成影响，导致无法预测的时延增加。</p><p>Redis分片方式采用社区的Redis Cluster协议，集群自主分片。Redis Cluster带来了一定的易用性的同时，也提高了应用开发的门槛，应用开发人员需要一定程度上了解Redis Cluster，同时需要使用智能客户端访问Redis Cluster。这些智能客户端配置参数繁多，应用开发人员并无法完全掌握并设置这些参数，踩了很多坑。同时，由于智能客户端需要做分片计算，给应用端的机器也带来了一定的负载。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcVtxn9dUon7LbJnGJOOQkEZziaRaedAMJIkY8TaKru5STJXBtKibNxt7Ow/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Why-K8S"><a href="#Why-K8S" class="headerlink" title="Why K8S"></a>Why K8S</h2><h2 id="资源隔离"><a href="#资源隔离" class="headerlink" title="资源隔离"></a><strong>资源隔离</strong></h2><p>当前的Redis Cluster部署在物理机集群上，为了提高资源利用率节约成本，多业务线的Redis集群都是混布的。由于没有做CPU的资源隔离，经常出现某Redis节点CPU使用率过高导致其他Redis集群的节点争抢不到CPU资源引起时延抖动。因为不同的集群混布，这类问题很难快速定位，影响运维效率。K8s容器化部署可以指定 CPU request 和 CPU limit ，在提高资源利用率的同时避免了资源争抢。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="自动化部署"><a href="#自动化部署" class="headerlink" title="自动化部署"></a><strong>自动化部署</strong></h2><p>自动化部署。当前Redis Cluster在物理机上的部署过程十分繁琐，需要通过查看元信息数据库查找有空余资源的机器，手动修改很多配置文件再逐个部署节点，最后使用redis_trib工具创建集群，新集群的初始化工作经常需要一两个小时。</p><p>K8s通过StatefulSet部署Redis集群，使用configmap管理配置文件，新集群部署时间只需要几分钟，大大提高了运维效率。</p><h2 id="How-K8S"><a href="#How-K8S" class="headerlink" title="How K8S"></a>How K8S</h2><p>客户端通过LVS的VIP统一接入，通过Redis Proxy转发服务请求到Redis Cluster集群。这里我们引入了Redis Proxy来转发请求。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcV9yKs0YScPnxtplmyDUUxl1miaEib7Ekn9L73dKS2Tia7fA6XCOd3CDTNA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Redis-Cluster部署方式"><a href="#Redis-Cluster部署方式" class="headerlink" title="Redis Cluster部署方式"></a><strong>Redis Cluster部署方式</strong></h2><p>Redis部署为StatefulSet，作为有状态的服务，选择StatefulSet最为合理，可以将节点的RDB/AOF持久化到分布式存储中。当节点重启漂移到其他机器上时，可通过挂载的PVC(PersistentVolumeClaim)拿到原来的RDB/AOF来同步数据。我们选择的持久化存储PV(PersistentVolume)是Ceph Block Service。Ceph的读写性能低于本地磁盘，会带来100~200ms的读写时延。但由于Redis的RDB/AOF的写出都是异步的，分布式存储带来的读写延迟对服务并没有影响。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcVPzdQYX28SbZqwYl5tlFBkqfpxM3xOC3IQa3qEq2hnEYISoMWmrXFqA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="Proxy选型"><a href="#Proxy选型" class="headerlink" title="Proxy选型"></a><strong>Proxy选型</strong></h2><p>## </p><p>开源的Redis Proxy有很多，常见的开源Redis Proxy如下:</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRrJAgO3tqmgH2KF0EcfGqqW2pJcs1rgdmyqibPpJbRM4PiabXStSXeyObE7y8wqvZqeLZ7N4RVNfIog/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>我们希望能够继续使用Redis Cluster来管理Redis集群，所以Codis和Twemproxy不再考虑。redis-cluster-proxy是Redis官方在6.0版本推出的支持Redis Cluster协议的Proxy，但是目前还没有稳定版，暂时也无法大规模应用。</p><p>备选就只有Cerberus和Predixy两种。我们在K8s环境上对Cerberus和Predixy进行了性能测试，结果如下:</p><h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a><strong>测试环境</strong></h3><p>测试工具: redis-benchmark</p><p>Proxy CPU: 2 core</p><p>Client CPU: 2 core</p><p>Redis Cluster: 3 master nodes, 1 CPU per node</p><h3 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a><strong>测试结果</strong></h3><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRrJAgO3tqmgH2KF0EcfGqqWdUeLMTycZ2acica48JEiahLtSuFbULukibvVAFqbDOC4fcQRqKhx8RqgQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcVNwHM46icecJg2NzJJQMZqP9CicfnyK7LeTAt3kPoGe3ldvpRgZX5erwA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>## </p><p>在相同workload和配置下，Predixy的最高QPS要优于Cerberus，时延也比较接近。综合来看，Predixy比Cerberus的性能要高33%~60%，并且数据的key/value越大，Predixy优势越明显，所以最后我们选择了Predixy。</p><p>为了适应业务和K8s环境，在上线前我们对Predixy做了大量的改动，增加了很多新的功能，比如动态切换后端Redis Cluster、黑白名单、异常操作审计等。</p><h2 id="Proxy部署方式"><a href="#Proxy部署方式" class="headerlink" title="Proxy部署方式"></a><strong>Proxy部署方式</strong></h2><p>Proxy作为deployment部署，无状态轻量化，通过LB对外提供服务，很容易做到动态扩缩容。同时，我们为Proxy开发了动态切换后端Redis Cluster的功能，可实现在线添加和切换Redis Cluster。</p><h2 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h2><h2 id="Proxy自动扩缩容方式"><a href="#Proxy自动扩缩容方式" class="headerlink" title="Proxy自动扩缩容方式"></a><strong>Proxy自动扩缩容方式</strong></h2><p>我们使用K8s原生的HPA(Horizontal Pod Autoscaler)来实现Proxy的动态扩缩容。当Proxy所有pod的平均CPU使用率超过一定阈值时，会自动触发扩容，HPA会将Proxy的replica数加1，之后LVS就会探测到新的Proxy pod并将一部分流量切过去。如果扩容后CPU使用率仍然超过规定的阈值，会继续触发扩容逻辑。但是在扩容成功5分钟内，不论CPU使用率降到多低，都不会触发缩容逻辑，这样就避免了频繁的扩缩容给集群稳定性带来的影响。</p><p>HPA可配置集群的最少(MINPODS)和最多(MAXPODS)pod数量，集群负载再低也不会缩容到MINPODS以下数量的pods。建议客户可以根据自己的实际业务情况来决定MINPODS和MAXPODS的值。</p><h2 id="Why-Proxy"><a href="#Why-Proxy" class="headerlink" title="Why Proxy"></a>Why Proxy</h2><h2 id="Redis-pod重启可导致IP变化"><a href="#Redis-pod重启可导致IP变化" class="headerlink" title="Redis pod重启可导致IP变化"></a><strong>Redis pod重启可导致IP变化</strong></h2><p>使用Redis Cluster的Redis客户端，都需要配置集群的部分IP和Port，用于客户端重启时查找Redis Cluster的入口。对于物理机集群部署的Redis节点，即便遇到实例重启或者机器重启，IP和Port都可以保持不变，客户端依然能够找到Redis Cluster的拓扑。但是部署在K8s上的Redis Cluster，pod重启是不保证IP不变的(即便是重启在原来的K8s node上)，这样客户端重启时，就可能会找不到Redis Cluster的入口。</p><p>通过在客户端和Redis Cluster之间加上Proxy，就对客户端屏蔽了Redis Cluster的信息，Proxy可以动态感知Redis Cluster的拓扑变化，客户端只需要将LVS的IP:Port作为入口，请求转发到Proxy上，即可以像使用单机版Redis一样使用Redis Cluster集群，而不需要Redis智能客户端。</p><h2 id="Redis处理连接负载高"><a href="#Redis处理连接负载高" class="headerlink" title="Redis处理连接负载高"></a><strong>Redis处理连接负载高</strong></h2><p>在6.0版本之前，Redis都是单线程处理大部分任务的。当Redis节点的连接较高时，Redis需要消耗大量的CPU资源处理这些连接，导致时延升高。有了Proxy之后，大量连接都在Proxy上，而Proxy跟Redis实例之间只保持很少的连接，这样降低了Redis的负担，避免了因为连接增加而导致的Redis时延升高。</p><h2 id="集群迁移切换需要应用重启"><a href="#集群迁移切换需要应用重启" class="headerlink" title="集群迁移切换需要应用重启"></a><strong>集群迁移切换需要应用重启</strong></h2><p>在使用过程中，随着业务的增长，Redis集群的数据量会持续增加，当每个节点的数据量过高时，BGSAVE的时间会大大延长，降低集群的可用度。同时QPS的增加也会导致每个节点的CPU使用率增高。这都需要增加扩容集群来解决。目前Redis Cluster的横向扩展能力不是很好，原生的slots搬移方案效率很低。新增节点后，有些客户端比如Lettuce，会因为安全机制无法识别新节点。另外迁移时间也完全无法预估，迁移过程中遇到问题也无法回退。</p><p>当前物理机集群的扩容方案是：</p><ol><li><p>按需创建新集群</p></li><li><p>使用同步工具将数据从老集群同步到新集群</p></li><li><p>确认数据无误后，跟业务沟通，重启服务切换到新集群</p></li></ol><p>整个过程繁琐而且风险较大，还需要业务重启服务。</p><p>有了Proxy层，可以将后端的创建、同步和切换集群对客户端屏蔽掉。新老集群同步完成之后，向Proxy发送命令就可以将连接换到新集群，可以实现对客户端完全无感知的集群扩缩容。</p><h2 id="数据安全风险"><a href="#数据安全风险" class="headerlink" title="数据安全风险"></a><strong>数据安全风险</strong></h2><p>Redis是通过AUTH来实现鉴权操作，客户端直连Redis，密码还是需要在客户端保存。而使用Proxy，客户端只需要通过Proxy的密码来访问Proxy，不需要知道Redis的密码。Proxy还限制了FLUSHDB、CONFIG SET等操作，避免了客户误操作清空数据或修改Redis配置，大大提高了系统的安全性。</p><p>同时，Redis并没有提供审计功能。我们在Proxy上增加了高危操作的日志保存功能，可以在不影响整体性能的前提下提供审计能力。</p><h2 id="Proxy-带来的问题"><a href="#Proxy-带来的问题" class="headerlink" title="Proxy 带来的问题"></a>Proxy 带来的问题</h2><h2 id="多一跳带来的时延"><a href="#多一跳带来的时延" class="headerlink" title="多一跳带来的时延"></a><strong>多一跳带来的时延</strong></h2><p>Proxy在客户端和Redis实例之间，客户端访问Redis数据需要先访问Proxy再访问Redis节点，多了一跳，会导致时延增加。经测试，多一跳会增加0.2~0.3ms的时延，不过通常这对业务来说是可以接受的。</p><h2 id="Pod漂移造成IP变化"><a href="#Pod漂移造成IP变化" class="headerlink" title="Pod漂移造成IP变化"></a><strong>Pod漂移造成IP变化</strong></h2><p>Proxy在K8s上是通过deployment部署的，一样会有节点重启导致IP变化的问题。我们K8s的LB方案可以感知到Proxy的IP变化，动态的将LVS的流量切到重启后的Proxy上。</p><h2 id="LVS带来的时延"><a href="#LVS带来的时延" class="headerlink" title="LVS带来的时延"></a><strong>LVS带来的时延</strong></h2><p>LVS也会带来时延，如下表中的测试，不同的数据长度get/set操作，LVS引入的时延小于0.1ms。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRrJAgO3tqmgH2KF0EcfGqqWNBgfVsEkDqxONt4P9Wicn7rg44gSnFS6GgK2bt97NbeWGQmiaFc6PQiaA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="K8S-带来的好处"><a href="#K8S-带来的好处" class="headerlink" title="K8S 带来的好处"></a>K8S 带来的好处</h2><h2 id="部署方便"><a href="#部署方便" class="headerlink" title="部署方便"></a><strong>部署方便</strong></h2><p>通过运维平台调用K8s API部署集群，大大提高了运维效率。</p><h2 id="解决端口管理问题"><a href="#解决端口管理问题" class="headerlink" title="解决端口管理问题"></a><strong>解决端口管理问题</strong></h2><p>目前小米在物理机上部署Redis实例是通过端口来区分的，并且下线的端口不能复用，也就是说整个公司每个Redis实例都有唯一的端口号。目前65535个端口已经用到了40000多，按现在的业务发展速度，将在两年内耗尽端口资源。而通过K8s部署，每一个Redis实例对应的K8s pod都有独立的IP，不存在端口耗尽问题和复杂的管理问题。</p><h2 id="降低客户使用门槛"><a href="#降低客户使用门槛" class="headerlink" title="降低客户使用门槛"></a><strong>降低客户使用门槛</strong></h2><p>对应用来说，只需要使用单机版的非智能客户端连接VIP，降低了使用门槛，避免了繁琐复杂的参数设置。同时由于VIP和端口是固定不变的，应用程序不再需要自己管理Redis Cluster的拓扑。</p><h2 id="提高客户端性能"><a href="#提高客户端性能" class="headerlink" title="提高客户端性能"></a><strong>提高客户端性能</strong></h2><p>使用非智能客户端还可以降低客户端的负载，因为智能客户端需要在客户端对key进行hash以确定将请求发送到哪个Redis节点，在QPS比较高的情况下会消耗客户端机器的CPU资源。当然，为了降低客户端应用迁移的难度，我们让Proxy也支持了智能客户端协议。</p><h2 id="动态升级和扩缩容"><a href="#动态升级和扩缩容" class="headerlink" title="动态升级和扩缩容"></a><strong>动态升级和扩缩容</strong></h2><p>Proxy支持动态添加切换Redis Cluster的功能，这样Redis Cluster的集群升级和扩容切换过程可以做到对业务端完全无感知。例如，业务方使用30个节点的Redis Cluster集群，由于业务量的增加，数据量和QPS都增长的很快，需要将集群规模扩容两倍。如果在原有的物理机上扩容，需要以下过程:</p><ol><li><p>协调资源，部署60个节点的新集群</p></li><li><p>手动配置迁移工具，将当前集群的数据迁移到新集群</p></li><li><p>验证数据无误后，通知业务方修改Redis Cluster连接池拓扑，重启服务</p></li></ol><p>虽然Redis Cluster支持在线扩容，但是扩容过程中slots搬移会对线上业务造成影响，同时迁移时间不可控，所以现阶段很少采用这种方式，只有在资源严重不足时才会偶尔使用。</p><p>在新的K8s架构下，迁移过程如下：  </p><ol><li><p>通过API接口一键创建60个节点的新集群</p></li><li><p>同样通过API接口一键创建集群同步工具，将数据迁移到新集群</p></li><li><p>验证数据无误后，向Proxy发送命令添加新集群信息并完成切换</p></li></ol><p>整个过程对业务端完全无感知。</p><p>集群升级也很方便：如果业务方能接受一定的延迟毛刺，可以在低峰时通过StatefulSet滚动升级的方式来实现；如果业务对延迟有要求，可以通过创建新集群迁移数据的方式来实现。</p><h2 id="提高服务稳定性和资源利用率"><a href="#提高服务稳定性和资源利用率" class="headerlink" title="提高服务稳定性和资源利用率"></a><strong>提高服务稳定性和资源利用率</strong></h2><p>通过K8s自带的资源隔离能力，实现和其他不同类型应用混部，在提高资源利用率的同时，也能保证服务稳定性。</p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h2 id="Pod重启导致数据丢失"><a href="#Pod重启导致数据丢失" class="headerlink" title="Pod重启导致数据丢失"></a><strong>Pod重启导致数据丢失</strong></h2><p>K8s的pod碰到问题重启时，由于重启速度过快，会在Redis Cluster集群发现并切主前将pod重启。如果pod上的Redis是slave，不会造成什么影响。但如果Redis是master，并且没有AOF，重启后原先内存的数据都被清空，Redis会reload之前存储的RDB文件，但是RDB文件并不是实时的数据。之后slave也会跟着把自己的数据同步成之前的RDB文件中的数据镜像，会造成部分数据丢失。</p><p>StatefulSet是有状态服务，部署的pod名是固定格式(StatefulSet名+编号)。我们在初始化Redis Cluster时，将相邻编号的pod设置为主从关系。在重启pod时，通过pod名确定它的slave，在重启pod前向从节点发送cluster failover命令，强制将活着的从节点切主。这样在重启后，该节点会自动以从节点方式加入集群。</p><p>LVS映射时延</p><p>Proxy的pod是通过LVS实现负载均衡的，LVS对后端IP:Port的映射生效有一定的时延，Proxy节点突然下线会导致部分连接丢失。为减少Proxy运维对业务造成影响，我们在Proxy的deployment模板中增加了如下选项：</p><pre><code>lifecycle:  preStop:    exec:      command:      - sleep      - &quot;171&quot;</code></pre><p>对于正常的Proxy pod下线，例如集群缩容、滚动更新Proxy版本以及其它K8s可控的pod下线，在pod下线前会发消息给LVS并等待171秒，这段时间足够LVS将这个pod的流量逐渐切到其他pod上，对业务无感知。</p><h2 id="K8s-StatefulSet无法满足Redis-Cluster部署要求"><a href="#K8s-StatefulSet无法满足Redis-Cluster部署要求" class="headerlink" title="K8s StatefulSet无法满足Redis Cluster部署要求"></a><strong>K8s StatefulSet无法满足Redis Cluster部署要求</strong></h2><p>K8s原生的StatefulSet不能完全满足Redis Cluster部署的要求：</p><ol><li><p>Redis Cluster不允许同为主备关系的节点部署在同一台机器上。这个很好理解，如果该机器宕机，会导致这个数据分片不可用。</p></li><li><p>Redis Cluster不允许集群超过一半的主节点失效，因为如果超过一半主节点失效，就无法有足够的节点投票来满足gossip协议的要求。因为Redis Cluster的主备是可能随时切换的，我们无法避免同一个机器上的所有节点都是主节点这种情况，所以在部署时不能允许集群中超过1/4的节点部署在同一台机器上。</p></li></ol><p>为了满足上面的要求，原生StatefulSet可以通过 anti-affinity 功能来保证相同集群在同一台机器上只部署一个节点，但是这样机器利用率很低。</p><p>因此我们开发了基于StatefulSet的CRD：RedisStatefulSet，会采用多种策略部署Redis节点。同时，还在RedisStatefulSet中加入了一些Redis管理功能。这些我们将会在其他文章中来继续详细探讨。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前集团内部已经有多个业务的数十个Redis集群部署到了K8s上并运行了半年多。得益于K8s的快速部署和故障迁移能力，这些集群的运维工作量比物理机上的Redis集群低很多，稳定性也得到了充分的验证。</p><p>在运维过程中我们也遇到了不少问题，文章中提到的很多功能都是根据实际需求提炼出来的。目前还是有很多问题需要在后续逐步解决，以进一步提高资源利用率和服务质量。</p><h2 id="混布-Vs-独立部署"><a href="#混布-Vs-独立部署" class="headerlink" title="混布 Vs. 独立部署"></a><strong>混布 Vs. 独立部署</strong></h2><p>物理机的Redis实例是独立部署的，单台物理机上部署的都是Redis实例，这样有利于管理，但是资源利用率并不高。Redis实例使用了CPU、内存和网络IO，但存储空间基本都是浪费的。在K8s上部署Redis实例，其所在的机器上可能也会部署其他任意类型的服务，这样虽然可以提高机器的利用率，但是对于Redis这样的可用性和时延要求都很高的服务来说，如果因为机器内存不足而被驱逐，是不能接受的。这就需要运维人员监控所有部署了Redis实例的机器内存，一旦内存不足，就切主和迁移节点，但这样又增加运维的工作量。</p><p>同时，如果混部的其他服务是网络吞吐很高的应用，也可能对Redis服务造成影响。虽然K8s的 anti-affinity 功能可以将Redis实例有选择地部署到没有这类应用的机器上，但是在机器资源紧张时，还是无法避免这种情况。</p><h2 id="Redis-Cluster管理"><a href="#Redis-Cluster管理" class="headerlink" title="Redis Cluster管理"></a><strong>Redis Cluster管理</strong></h2><p>Redis Cluster是一个P2P无中心节点的集群架构，依靠gossip协议传播协同自动化修复集群的状态，节点上下线和网络问题都可能导致Redis Cluster的部分节点状态出现问题，例如会在集群拓扑中出现failed或者handshake状态的节点，甚至脑裂。对这种异常状态，我们可以在Redis CRD上增加更多的功能来逐步解决，进一步提高运维效率。</p><h2 id="审计与安全"><a href="#审计与安全" class="headerlink" title="审计与安全"></a><strong>审计与安全</strong></h2><p>Redis本身只提供了Auth密码认证保护功能，没有权限管理，安全性较差。通过Proxy，我们可以通过密码区分客户端类型，管理员和普通用户使用不同的密码登录，可执行的操作权限也不同，这样就可以实现权限管理和操作审计等功能。</p><h2 id="支持多Redis-Cluster"><a href="#支持多Redis-Cluster" class="headerlink" title="支持多Redis Cluster"></a><strong>支持多Redis Cluster</strong></h2><p>单个Redis Cluster由于gossip协议的限制，横向扩展能力有限，集群规模在300个节点时，节点选主这类拓扑变更的效率就明显降低。同时，由于单个Redis实例的容量不宜过高，单个Redis Cluster也很难支持TB以上的数据规模。通过Proxy，我们可以对key做逻辑分片，这样单个Proxy就可以接入多个Redis Cluster，从客户端的视角来看，就相当于接入了一个能够支持更大数据规模的Redis集群。</p><p>最后，像Redis这种有状态服务的容器化部署在国内大厂都还没有非常成熟的经验，小米云平台也是在摸索中逐步完善。目前我们新增集群已经大部分部署在K8s上，更计划在一到两年内将集团内大部分的物理机Redis集群都迁移到K8s上。这样就可以有效地降低运维人员的负担，在不显著增加运维人员的同时维护更多的Redis集群。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;小米Redis的K8s容器化部署实践&lt;/p&gt;
&lt;p&gt;原创 崔凯峰 小米云技术 &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/WrUU3C-C8TBgJfGuOv3qGQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;
      
    
    </summary>
    
      <category term="redis" scheme="http://zhangyu8.me/categories/redis/"/>
    
    
      <category term="redis" scheme="http://zhangyu8.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>k8s桌面客户端Lens</title>
    <link href="http://zhangyu8.me/2020/06/18/K8s%E6%A1%8C%E9%9D%A2%E5%AE%A2%E6%88%B7%E7%AB%AFLens/"/>
    <id>http://zhangyu8.me/2020/06/18/K8s桌面客户端Lens/</id>
    <published>2020-06-18T03:00:00.000Z</published>
    <updated>2020-06-18T14:00:33.541Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 的桌面客户端有那么几个，  Kubernetic  (<a href="https://kubernetic.com/)应该是最好用的，但是收费。" target="_blank" rel="noopener">https://kubernetic.com/)应该是最好用的，但是收费。</a></p><p>最近有个叫 Lens 的 APP 改变了这个格局，功能比 Kubernetic 多，使用体验更好，适合广大系统重启工程师装逼。<br><a href="https://github.com/lensapp/lens" target="_blank" rel="noopener">https://github.com/lensapp/lens</a></p><p><a href="https://k8slens.dev/" target="_blank" rel="noopener">https://k8slens.dev/</a></p><p>Lens 就是一个强大的 IDE，可以实时查看集群状态，实时查看日志流，方便排查故障。<br>日志流界面可以选择显示或隐藏时间戳，也可以指定显示的行数</p><p>Lens 可以管理多集群，它使用内置的 kubectl 通过 kubeconfig  来访问集群，支持本地集群和外部集群（如EKS、AKS、GKE、Pharos、UCP、Rancher 等）</p><p>Lens 内置了资源利用率的仪表板，支持多种对接 Prometheus 的方式：</p><p>Lens 内置了 kubectl，它的内置终端会确保集群的 API Server 版本与 kubectl 版本兼容，所以你不需要在本地安装 kubectl。 </p><p>Lens 内置了 helm 模板商店，可直接点击安装：</p><p>下载安装<br><a href="https://github.com/lensapp/lens/releases/download/v3.5.0/Lens-Setup-3.5.0.exe" target="_blank" rel="noopener">https://github.com/lensapp/lens/releases/download/v3.5.0/Lens-Setup-3.5.0.exe</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Kubernetes 的桌面客户端有那么几个，  Kubernetic  (&lt;a href=&quot;https://kubernetic.com/)应该是最好用的，但是收费。&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://kubernetic.c
      
    
    </summary>
    
      <category term="kubernetes" scheme="http://zhangyu8.me/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="http://zhangyu8.me/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>java线程池里面到底该设置多少个线程</title>
    <link href="http://zhangyu8.me/2020/06/16/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%87%8C%E9%9D%A2%E5%88%B0%E5%BA%95%E8%AF%A5%E8%AE%BE%E7%BD%AE%E5%A4%9A%E5%B0%91%E4%B8%AA%E7%BA%BF%E7%A8%8B/"/>
    <id>http://zhangyu8.me/2020/06/16/Java线程池里面到底该设置多少个线程/</id>
    <published>2020-06-16T03:00:00.000Z</published>
    <updated>2020-06-18T14:07:24.226Z</updated>
    
    <content type="html"><![CDATA[<p>根据CPU核心数确定线程池并发线程数</p><p><a href="https://www.cnblogs.com/dennyzhangdd/p/6909771.html" target="_blank" rel="noopener">https://www.cnblogs.com/dennyzhangdd/p/6909771.html</a></p><blockquote><h2 id="一、抛出问题"><a href="#一、抛出问题" class="headerlink" title="一、抛出问题"></a>一、抛出问题</h2><p>关于如何计算并发线程数，一般分两派，来自两本书，且都是好书，到底哪个是对的？问题追踪后，整理如下：</p><p>第一派：《Java Concurrency in Practice》即《java并发编程实践》，如下图：</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526162253247-2075463115.png" alt></p><p>如上图，在《Java Concurrency in Practice》一书中，给出了估算线程池大小的公式：</p><p>Nthreads=Ncpu*Ucpu*(1+w/c)，其中</p><p>Ncpu=CPU核心数</p><p>Ucpu=cpu使用率，0~1</p><p>W/C=等待时间与计算时间的比率</p><p>第二派：《Programming Concurrency on the JVM Mastering》即《Java 虚拟机并发编程》</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526170508450-925520860.png" alt></p><p>线程数=Ncpu/（1-阻塞系数）</p></blockquote><blockquote><h2 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h2><p>对于派系一，假设cpu100%运转，即撇开CPU使用率这个因素，线程数\=Ncpu*(1+w/c)。</p><p>现在假设将派系二的公式等于派系一公式，即Ncpu/（1-阻塞系数）=Ncpu*(1+w/c),===》阻塞系数=w/(w+c)，即阻塞系数=阻塞时间/（阻塞时间+计算时间），这个结论在派系二后续中得到应征，如下图：</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526171225919-888895376.png" alt></p><p>由此可见，派系一和派系二其实是一个公式……这样我就放心了……</p></blockquote><blockquote><h2 id="三、实际应用"><a href="#三、实际应用" class="headerlink" title="三、实际应用"></a>三、实际应用</h2><p>那么实际使用中并发线程数如何设置呢？分析如下（我们以派系一公式为例）：</p><p>Nthreads=Ncpu*(1+w/c)</p><p>IO密集型：一般情况下，如果存在IO，那么肯定w/c&gt;1（阻塞耗时一般都是计算耗时的很多倍）,但是需要考虑系统内存有限（每开启一个线程都需要内存空间），这里需要上服务器测试具体多少个线程数适合（CPU占比、线程数、总耗时、内存消耗）。如果不想去测试，保守点取1即，Nthreads=Ncpu*(1+1)=2Ncpu。这样设置一般都OK。</p><p>计算密集型：假设没有等待w=0，则W/C=0. Nthreads=Ncpu。</p><p><strong>至此结论就是：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; </span><br><span class="line">&gt; ```计算密集型\=Ncpu（常出现于线程中：复杂算法）</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>java中：Ncpu=<code>Runtime.getRuntime().availableProcessors()</code></strong></p><p>=========================此处可略过=============================================</p><p>当然派系一种《Java Concurrency in Practice》还有一种说法，</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526173437372-976370152.png" alt></p><p>即对于计算密集型的任务，在拥有N个处理器的系统上，当线程池的大小为N+1时，通常能实现最优的效率。(即使当计算密集型的线程偶尔由于缺失故障或者其他原因而暂停时，这个额外的线程也能确保CPU的时钟周期不会被浪费。)</p><p><code>即，计算密集型\=Ncpu+1，但是这种做法导致的多一个cpu上下文切换是否值得</code>，这里不考虑。读者可自己考量。  </p><p>======================================================================</p></blockquote><blockquote><h2 id="四、总结："><a href="#四、总结：" class="headerlink" title="四、总结："></a><strong>四、总结</strong>：</h2><p>选择线程池并发线程数的因素很多：任务类型、内存等线程中使用到所有资源都需要考虑</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;根据CPU核心数确定线程池并发线程数&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/dennyzhangdd/p/6909771.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblog
      
    
    </summary>
    
      <category term="java" scheme="http://zhangyu8.me/categories/java/"/>
    
    
      <category term="java" scheme="http://zhangyu8.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>做容灾，双活、多活、同城、异地、多云，到底应该怎么选</title>
    <link href="http://zhangyu8.me/2020/05/06/%E5%81%9A%E5%AE%B9%E7%81%BE%EF%BC%8C%E5%8F%8C%E6%B4%BB%E3%80%81%E5%A4%9A%E6%B4%BB%E3%80%81%E5%90%8C%E5%9F%8E%E3%80%81%E5%BC%82%E5%9C%B0%E3%80%81%E5%A4%9A%E4%BA%91%EF%BC%8C%E5%88%B0%E5%BA%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89/"/>
    <id>http://zhangyu8.me/2020/05/06/做容灾，双活、多活、同城、异地、多云，到底应该怎么选/</id>
    <published>2020-05-06T06:00:00.000Z</published>
    <updated>2020-05-06T06:05:38.644Z</updated>
    
    <content type="html"><![CDATA[<p> 做容灾，双活、多活、同城、异地、多云，到底应该怎么选？<br><a href="https://mp.weixin.qq.com/s/_NGqq-xvDtFrvKwbdGIhwQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/_NGqq-xvDtFrvKwbdGIhwQ</a> </p><p> 原创 Cheng哥 成哥的世界 2019-03-11 </p><blockquote><p>去年写过一篇 &lt;&lt;做容灾，冷备是不是个好方案？&gt;&gt;，当时提出来，冷备或者主备，其实并不是一个理想的方案，而且绝大多数情况下，只能是一个心理安慰，真正发生故障的情况下，这样的容灾模式根本起不到作用。  </p><p>原因我就不重复了，大家如果有兴趣可以直接看那篇文章。</p><p>最近，公有云又出了些大故障，各大群和朋友圈又开始沸沸扬扬，但是整体看下来，声音无非两种：</p><ul><li><p>单站点不靠谱，要有容灾，出现这种情况就得马上切，所以回去赶紧建设容灾站点；</p></li><li><p>鸡蛋不能放在一个篮子里，单云不靠谱，要多云。所以，多云就要选我们家的xx云，或者我们提供xx多云服务。</p></li></ul><p>我在我的一个讨论群里就提出来，第一种声音是有意识的建设，有这个意识很好，但是把这个事情想得太简单了。第二种声音，基本就是不动脑子的瞎BB，原因我下面讲。</p><p>转回正题来，既然上篇提到主备模式不靠谱，那到底怎么选？而且整天见各类技术文章，不是双活，就是多活，不是同城，就是异地，现在又出来个多云，好复杂。</p><p>下面我就谈谈我的理解：</p><p>首先，这么多名词是什么含义，要搞清楚，然后再看适不适合。</p><p>先讲相对简单的双活（简不简单，看后面就明白了），其实就是两个站点，同时承载业务流量，可以根据用户ID、地域或者其他业务属性也决定怎么分担流量，当一个站点故障时，可以快速（分钟级）切换到另一个站点，理想情况下，对业务基本是无损或者非常小的。</p><p>这里就跟前面讲的主备不同了，主备的另一个站点完全是不承载任何流量的。</p><p>这里再往深里看一眼，同时承载流量，也要看承载到那一层，也就是流量在统一站点内闭环，所有调用都是本机房内完成，还是只有应用层这样的无状态组件双活，但是数据访问、异步消息这些有状态的部件还是回到主站点调用，这两种模式又是不一样的。</p><p>其实第二种，就比前面讲的主备模式要好一些，因为这样至少可以保证应用层随时可用，不过真出故障的时候，还是少不了数据层的切换，这个其实是非常耗时的。跟主备模式一样，基本无法演练，因为代价太高，数据会有损。（如果数据层没有这么复杂，只有几个数据库，那是没问题问题的，但是分布式的场景下，上百个，几百个实例切换，这个代价和成本还是很大的。）</p><p>所以，再往下推导，如果想要做到有效果的双活，就必须保证每个站点，都是独立运行，所有的调用都是本机房调用且闭环，底层做好数据同步即可。</p><p>只有做到这个程度，当一个站点发生故障不可用时，就可以从接入层把故障站点的流量切换到另一个站点，双活的效果也就有了。</p><p>不过，做到这个程度，就不是说我们想要做就能做到的，如果您做个类似的架构设计，你会知道这里有三个关键的技术点：</p><p><strong>第一个，本机房调用</strong></p><p>也就是一个分布式请求不能跨机房调来调去，这个是不行的，必须要保证本机房调用闭环。所以从分布式服务的路由策略上，以及服务化框架上，必须得支持这也中调用模式，同理，数据访问层，以及消息组件也要支持这种特性。</p><p><strong>第二个，数据分片和一致性</strong></p><p>为什么要做这个事情？我们知道一个系统中数据准确性、完整性和一致性是非常关键的，放到双活这个场景下，最关键的就是数据一致性，我们不能允许有同一个记录两边同时在变更，还要双向同步，比如用户交易和支付类的数据，同时变更的情况下，我们无法确认哪边是准确的。</p><p>前面提到，两个站点是同时承载不同的流量的，这就要根据一些业务属性来分配，比如用户ID、所属地域等等策略，这里为的就是能够在数据层面也要做好隔离，一个站点内只提供固定部分的用户访问。</p><p>这样就保证了单站点内同一分片的数据，不会在另外一个站点被变更，后续的同步也可以做到单向。</p><p>所以，这里的关键，就是数据要做分片，就要用到分布式的数据中间件，要做数据访问的路由设计，数据要同机房读写，还要做数据拆分这样的工作，技术门槛和工作量也不低。</p><p><strong>这两点如果能够做到，其实就是我们经常说的“单元化”架构达成了，理论上，我们可以选择任何一个机房和地域，把系统搭建起来，就可以提供业务访问了。</strong></p><p>但现实是更为复杂的，因为用户业务系统产生的数据，有可能会被其它系统用到，比如商品库存这样的系统，这就要涉及异步消息和数据的同步问题，而<strong>数据同步不仅仅是一个技术问题，而是个物理问题</strong>，我们接下来讲。</p><p><strong>第三个，数据同步。</strong></p><p>其实单从同步角度而言，目前很多的同步工具和开源产品已经比较完善，所以这里最大的问题，其实不在技术层面，而是在物理层面。</p><p>准确点，就是物理距离上的时延问题，这个无论是双活、多活，还是同城、异地，都绕不开的痛苦问题。</p><p>既然要双活，必然会选择另一个跟当前机房有一定距离的机房（同城或异地），而且距离必须得拉开才有意义，如果都在一个园区里面，就没有任何容灾意义了。</p><p>距离一旦拉开，物理距离就出来了，即使是专线相连，中间也要经过很多网络设备，如果是云化的网络架构下，经过的软硬设备就更多，还有可能涉及协议转换，如果中途跨运营商，就更难保障，这样一来时延肯定是几倍、十几倍，甚至是上百倍的上涨，直接从0.x毫秒，上涨到秒级别。</p><p>对于同城来说，这个问题还好，但是一旦跨省就完全不可控，特别是机房如果不是自己的，根本无法控制。所以，想大公司自建机房，一定会在这个层面做大量的优化，尽最大可能降低时延。</p><p>就以淘宝、天猫为例，按照之前了解的情况，基本也是杭州和上海这两个城市为主做双活，再远时延这个问题就绕不开了。</p><p>数据同步及时性为什么这么重要，一个是业务体验，不能说库存都没了，其他用户看到的还是有货，这个是不会被接受的。</p><p>再就是故障时，如果同步不及时，极有可能造成几秒钟内的交易数据丢失，或者不一致，像淘宝这样每秒4位数订单量的系统，丢几秒钟数据，造成的损失也是巨大的。所以，这里就必须要建设有一整套的数据完整性和一致性保障措施，尽最大程度降低业务损失。  </p><p>所以，数<strong>据同步所依赖的时延问题，其实就已经超出了绝大部分公司所能掌控的范畴，也不是单纯靠自身技术能解决的问题，要看天时和地利。</strong></p><p>讲到这里，我想多活就不用讲了，时延这个问题解决不了，多活就是扯淡，至于同城和异地，我想看明白的读者，也知道怎么选择了，其实一样，还是取决于时延。</p><p><strong>我们可以得出的几个结论：</strong></p><ul><li><p>不管怎么选择容灾方案，我们自己的业务系统，从<strong>自身架构上，一定要支持单元化，一定要支持数据同步才行</strong>，如果这都不支持，讲双活和多活，就是特么的扯淡。所以，打算搞双活，先从这里下手，当然牵出来就要涉及到分布式，还有很多大量细节技术问题。</p></li><li><p><strong>一个合理的建设节奏应该是，同城双活—异地双活—两地三中心（同城双活+异地多活）</strong>，因为你要解决的问题的复杂度和难度也是在逐步上升的，不可能一蹴而就。</p></li><li><p><strong>题目里这些个名词，不是孤立的</strong>，而是从不同维度看到的结论，但是如果你偏离自己的业务场景去看，孤立的去看，就一定会被带到沟里去，而且不知道该如何下手，所以，一定别偏离你的业务场景，然后把它们联系起来。</p></li><li><p><strong>一切都是ROI</strong>，为了保证高可用，就一定会有成本，高可用程度越高，成本就一定越高，所以成本投入得到的收益到底划不划算，这个只能自家公司自家评判。</p></li><li><p>现实情况，比我写的要复杂的多的多，推荐大家看两个成功案例，<strong>一个是毕玄的异地多活数据中心，一个是饿了么异地多活</strong>，几个关键字google一下就有了，里面涉及到的场景化的细节对大家理解这件事情的复杂度会有更帮助。</p></li></ul><p>写的有点多了，关于多云先不写了，就当问题吧，大家觉得是不是需要多云建设？你怎么看？</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 做容灾，双活、多活、同城、异地、多云，到底应该怎么选？&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/_NGqq-xvDtFrvKwbdGIhwQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.wei
      
    
    </summary>
    
      <category term="容灾" scheme="http://zhangyu8.me/categories/%E5%AE%B9%E7%81%BE/"/>
    
    
      <category term="容灾" scheme="http://zhangyu8.me/tags/%E5%AE%B9%E7%81%BE/"/>
    
  </entry>
  
  <entry>
    <title>做容灾，冷备是不是个好方案</title>
    <link href="http://zhangyu8.me/2020/05/06/%E5%81%9A%E5%AE%B9%E7%81%BE%EF%BC%8C%E5%86%B7%E5%A4%87%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%AA%E5%A5%BD%E6%96%B9%E6%A1%88/"/>
    <id>http://zhangyu8.me/2020/05/06/做容灾，冷备是不是个好方案/</id>
    <published>2020-05-06T06:00:00.000Z</published>
    <updated>2020-05-06T06:05:22.279Z</updated>
    
    <content type="html"><![CDATA[<p> 做容灾，冷备是不是个好方案？ </p><p> 原创 Cheng哥 成哥的世界 2018-09-30 </p><blockquote><p>主备、冷备、热备、双活、多活、同城、异地、多云，等等等等，这些保证业务高可用和容灾名词，我们经常会听到，不绝于耳。  </p><p>但是，真的当我们自己要去建设，选择方案时，就发现不知道该怎么选择和搭配了。</p><p>结合近期我们的一些讨论，准备用几篇文章简单分享下我们的理解，今天先聊冷备。</p><p><strong>冷备是不是个好方案？</strong></p><p>这里的<strong>冷备我们可以理解为，是主站系统核心链路的镜像站点</strong>，应用、各类分布式服务以及底层基础设施都是独立，且启动的。</p><p><strong>它跟主站唯一的差别就是，正常情况下，不承载任何线上流量。</strong></p><p>理论上，只要有状态的数据（也就是各类分布式服务，如数据库、缓存、消息等组件）同步好，接入层流量能够灵活调度，当出现问题的时候，切入口流量，就可以顺畅的切过去。</p><p>看上去很美好，但是<strong>实际操作起来，基本不可行</strong>。</p><p>这里有<strong>一个关键点，就是业务应用</strong>，应用的代码和配置是<strong>随时在变化的。</strong></p><p>原则上，我们可以通过持续交付和运维自动化等等手段，确保每次变更都能够同步到备站点，并通过流程约束不允许有外部操作。</p><p>所以，手段上，我们可以做到非常完备，流程上，我们可以设计的非常严密。</p><p>但是，我们始终绕不开的一个命题，<strong>只要不承载真实的线上业务流量，我们就无法证明这个系统是可用的。</strong></p><p>何况，有可能是好几个月我们都不会发生真实的切换动作，所以，一个几个月没有经过线上流量检验的系统，在真正需要切换时，不会有任何人敢决策直接切换的。</p><p>当然，以上是我们的直接推断，确实行不通。但是我们仍然要经过一些详细的论证，从其它角度看是否有解。</p><p><strong>从另外一个角度的论证过程</strong></p><p>当时我们讨论在冷备的前提下，应该怎么保证系统的可用性，没想到，论证的过程，<strong>反而进一步证实了冷备只是一个美好的愿望</strong>。</p><p><strong>1、通过模拟压测的方式。</strong></p><p>但是我们知道，压测的模型是根据线上业务模型来定制的，但是业务场景和逻辑每天都在发生变化，压测模型的同步有时是跟不上业务模型变化的。</p><p>况且这个日常工作量要靠人，无法做到自动生成，所以基本不可持续。</p><p>再就是，<strong>压测的结果检验是通过技术指标衡量**</strong>，而非业务指标，**也就是是否200ok，或者出现5xx之类的错误。</p><p>业务逻辑上是否正确，并没有办法确保。这种情况就极易造成数据污染。数据故障的影响范围远远超过服务不可用的影响。</p><p>所以，<strong>压测可以最大程度评估系统容量，但是无法保证系统业务正确性。</strong></p><p><strong>2、切换后，接入线上流量前，QA介入验证。</strong></p><p>理由同上，工作量大，也无法覆盖到所有场景，时间不可控，完全起不到冷备节点的快速承载业务效果。</p><p><strong>3、定期模拟演练，确保系统周期范围内可用</strong></p><p>但是这里就有一个前提，冷备站点的建设目标，并不是全量建设，而是在极端状况下，确保核心业务临时可用，当主站点恢复后，仍然要切回去。</p><p>这里暗含的一个意思就是，一旦需要做这个动作，业务必然有损，而且涉及范围非常大，这就意味着，每一次演练都要付出极大的业务代价。</p><p>从这个角度，产品运营及决策者们是不会允许你经常干这种事情的。</p><p>到这里，你会发现，连日常演练的条件都不具备了。</p><p><strong>4、一个绕不开的限制条件</strong></p><p>数据同步必然是单向的，为了保证数据一致，通常要确保备用站点是禁写的，以防止各类误操作引起的数据污染。</p><p>所以，即使上面几个方案可行，基础条件上又不满足，因为根本无法写入数据，关键的业务逻辑根本不具备验证条件。</p><p><strong>最后，结论</strong></p><p><strong>冷备只能是冷备</strong>，关键时刻并不能起到快速承载业务的效果，<strong>在业务容灾建设时，这个思路其实是不可行的。</strong></p><p>但是对于部分组件，比如数据库、大数据、文件，这些存储类的部件，做冷备是有重大意义的。</p><p>也就是，后面我们在提到冷备时，应该叫做数据冷备、文件冷备、源代码冷备才有意义，或许会更准确些。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 做容灾，冷备是不是个好方案？ &lt;/p&gt;
&lt;p&gt; 原创 Cheng哥 成哥的世界 2018-09-30 &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;主备、冷备、热备、双活、多活、同城、异地、多云，等等等等，这些保证业务高可用和容灾名词，我们经常会听到，不绝于耳。  &lt;/p&gt;

      
    
    </summary>
    
      <category term="容灾" scheme="http://zhangyu8.me/categories/%E5%AE%B9%E7%81%BE/"/>
    
    
      <category term="容灾" scheme="http://zhangyu8.me/tags/%E5%AE%B9%E7%81%BE/"/>
    
  </entry>
  
  <entry>
    <title>MySQL如何优化cpu消耗</title>
    <link href="http://zhangyu8.me/2020/04/20/MySQL%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96cpu%E6%B6%88%E8%80%97/"/>
    <id>http://zhangyu8.me/2020/04/20/MySQL如何优化cpu消耗/</id>
    <published>2020-04-20T03:00:00.000Z</published>
    <updated>2020-04-20T03:14:15.521Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="谁在消耗cpu"><a href="#谁在消耗cpu" class="headerlink" title="谁在消耗cpu?"></a>谁在消耗cpu?</h2><p><em><strong>用户+系统+IO等待+软硬中断+空闲</strong></em><br><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220055993-65442699.png" alt></p><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220154673-599033146.png" alt></p><h2 id="祸首是谁？"><a href="#祸首是谁？" class="headerlink" title="祸首是谁？"></a>祸首是谁？</h2><h3 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h3><p><em><strong>用户空间CPU消耗，各种逻辑运算</strong></em></p><blockquote><p>正在进行大量tps<br>函数/排序/类型转化/逻辑IO访问…</p></blockquote><p><em><strong>用户空间消耗大量cpu，产生的系统调用是什么？那些函数使用了cpu周期？</strong></em><br>参考<a href="https://www.cnblogs.com/YangJiaXin/p/10928160.html" target="_blank" rel="noopener"><br>Linux 性能优化解析</a><br><a href="https://www.cnblogs.com/YangJiaXin/p/10853560.html" target="_blank" rel="noopener">MySQL 几种调式分析利器</a></p><h3 id="IO等待"><a href="#IO等待" class="headerlink" title="IO等待"></a>IO等待</h3><p><em><strong>等待IO请求的完成</strong></em></p><blockquote><p>此时CPU实际上空闲</p></blockquote><p><em><strong>如vmstat中的wa 很高。但IO等待增加，wa也不一定会上升（请求I/O后等待响应，但进程从核上移开了）</strong></em><br><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220232193-1150896123.png" alt></p><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220252309-1933130433.png" alt></p><h3 id="产生影响"><a href="#产生影响" class="headerlink" title="产生影响"></a>产生影响</h3><blockquote><p>用户和IO等待消耗了大部分cpu</p></blockquote><p><em><strong>吞吐量下降（tps）</strong></em><br><em><strong>查询响应时间增加</strong></em><br><em><strong>慢查询数增加</strong></em><br><em><strong>对mysql的并发陡增，也会产生上述影响</strong></em></p><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220350730-466848858.png" alt></p><h2 id="如何减少CPU消耗？"><a href="#如何减少CPU消耗？" class="headerlink" title="如何减少CPU消耗？"></a>如何减少CPU消耗？</h2><h3 id="减少等待"><a href="#减少等待" class="headerlink" title="减少等待"></a>减少等待</h3><p><em><strong>减少IO量</strong></em></p><blockquote><p>SQL/index，使用合适的索引减少扫描的行数（需平衡索引的正收益和维护开销，空间换时间）</p></blockquote><p><em><strong>提升IO处理能力</strong></em></p><blockquote><p>加cache/加磁盘/SSD</p></blockquote><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220402693-1264789500.png" alt></p><h3 id="减少计算"><a href="#减少计算" class="headerlink" title="减少计算"></a>减少计算</h3><h4 id="减少逻辑运算量"><a href="#减少逻辑运算量" class="headerlink" title="减少逻辑运算量"></a>减少逻辑运算量</h4><blockquote><ul><li><em><strong>避免使用函数</strong></em>，将运算转移至易扩展的应用服务器中<br>如substr等字符运算，dateadd/datesub等日期运算，abs等数学函数</li><li><em><strong>减少排序</strong></em>，利用索引取得有序数据或避免不必要排序<br>如union all代替 union，order by 索引字段等</li><li><em><strong>禁止类型转换</strong></em>，使用合适类型并保证传入参数类型与数据库字段类型绝对一致<br>如数字用tiny/int/bigint等，必需转换的在传入数据库之前在应用中转好</li><li><em><strong>简单类型</strong></em>，尽量避免复杂类型，降低由于复杂类型带来的附加运算。更小的数据类型占用更少的磁盘、内存、cpu缓存和cpu周期</li><li>….</li></ul></blockquote><h4 id="减少逻辑IO量"><a href="#减少逻辑IO量" class="headerlink" title="减少逻辑IO量"></a>减少逻辑IO量</h4><blockquote><ul><li><p><em><strong>index</strong></em>，优化索引，减少不必要的表扫描<br>如增加索引，调整组合索引字段顺序，去除选择性很差的索引字段等等</p></li><li><p><em><strong>table</strong></em>，合理拆分，适度冗余<br>如将很少使用的大字段拆分到独立表，非常频繁的小字段冗余到“引用表”</p></li><li><p><em><strong>SQL</strong></em>，调整SQL写法，充分利用现有索引，避免不必要的扫描，排序及其他操作<br>如减少复杂join，减少order by，尽量union all，避免子查询等</p></li><li><p><em><strong>数据类型</strong></em>，够用就好，减少不必要使用大字段<br>如tinyint够用就别总是int，int够用也别老bigint，date够用也别总是timestamp</p></li><li><p><em><strong>….</strong></em></p></li></ul></blockquote><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220428944-1005198827.png" alt></p><h4 id="减少query请求量（非数据库本身）"><a href="#减少query请求量（非数据库本身）" class="headerlink" title="减少query请求量（非数据库本身）"></a>减少query请求量（非数据库本身）</h4><blockquote><ul><li><em><strong>适当缓存</strong></em>，降低缓存数据粒度，对静态并被频繁请求的数据进行适当的缓存<br>如用户信息，商品信息等</li><li><em><strong>优化实现</strong></em>，尽量去除不必要的重复请求<br>如禁止同一页面多次重复请求相同数据的问题，通过跨页面参数传递减少访问等</li><li><em><strong>合理需求</strong></em>，评估需求产出比，对产出比极端底下的需求合理去除</li><li><em><strong>….</strong></em></li></ul></blockquote><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220447492-64921986.png" alt></p><h3 id="升级cpu"><a href="#升级cpu" class="headerlink" title="升级cpu"></a>升级cpu</h3><ul><li><em><strong>若经过减少计算和减少等待后还不能满足需求，cpu利用率还高</strong></em></li><li>___是时候拿出最后的杀手锏了，<em>升级cpu</em>，是选择更快的cpu还是更多的cpu了？_**</li></ul><blockquote><ul><li><em>低延迟（快速响应）</em>，需要更快的cpu（每个查询只能使用一个cpu）</li><li><em>高吞吐</em>，同时运行很多查询语句，能从多个cpu处理查询中收益</li></ul></blockquote><p><em><strong>参考</strong></em><br><em><strong>《高性能MySQL》</strong></em><br><em><strong>《图解性能优化》</strong></em><br><em><strong>大部分整理自《MySQL Tuning For CPU Bottleneck》</strong></em></p><p>作者：jiaxin</p><p>出处：<a href="http://www.cnblogs.com/YangJiaXin/" target="_blank" rel="noopener">http://www.cnblogs.com/YangJiaXin/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;谁在消耗cpu&quot;&gt;&lt;a href=&quot;#谁在消耗cpu&quot; class=&quot;headerlink&quot; title=&quot;谁在消耗cpu?&quot;&gt;&lt;/a&gt;谁在消耗cpu?&lt;/h2&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;用户+系统+IO等待+软硬中断+空闲&lt;/str
      
    
    </summary>
    
      <category term="mysql" scheme="http://zhangyu8.me/categories/mysql/"/>
    
    
      <category term="mysql" scheme="http://zhangyu8.me/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>MySQL性能优化实践</title>
    <link href="http://zhangyu8.me/2020/04/09/MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zhangyu8.me/2020/04/09/MySQL性能优化实践/</id>
    <published>2020-04-09T06:00:00.000Z</published>
    <updated>2020-04-09T06:38:29.344Z</updated>
    
    <content type="html"><![CDATA[<p>##超全的珍藏版MySQL性能优化实践！</p><p> 来源：<a href="https://www.xttblog.com/?p=4951" target="_blank" rel="noopener">https://www.xttblog.com/?p=4951</a></p><p>##一 题记</p><p>最近公司项目添加新功能，上线后发现有些功能的列表查询时间很久。原因是新功能用到旧功能的接口，而这些旧接口的 SQL 查询语句关联5,6张表且编写不够规范，导致 MySQL 在执行 SQL 语句时索引失效，进行全表扫描。原本负责优化的同事有事请假回家，因此优化查询数据的问题落在笔者手中。笔者在查阅网上 SQL 优化的资料后成功解决了问题，在此从全局角度，记录和总结 MySQL 查询优化相关技巧。</p><p>##二、优化思路</p><p><strong>数据查询慢，不代表 SQL 语句写法有问题。</strong> 首先，我们需要找到问题的源头才能“对症下药”。笔者用一张流程图展示 MySQL 优化的思路：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_jpg/JfTPiahTHJhqYoYCBMDfKtOOacVmxWyCS1gUnf8AB6Ona3ibLjOV9Ygy02BkQVqr1uLbiarnsNwq5Ph0xXRWynibEQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>无需更多言语，从图中可以清楚地看出，导致数据查询慢的原因有多种，如：缓存失效，在此一段时间内由于高并发访问导致 MySQL 服务器崩溃；SQL 语句编写问题；MySQL 服务器参数问题；硬件配置限制 MySQL 服务性能问题等。</p><p>##三、查看 MySQL 服务器运行的状态值</p><p><strong>如果系统的并发请求数不高，且查询速度慢，可以忽略该步骤直接进行 SQL 语句调优步骤。</strong></p><p>执行命令：</p><pre><code>show status</code></pre><p>由于返回结果太多，此处不贴出结果。其中，再返回的结果中，我们主要关注 “Queries”、“Threadsconnected” 和 “Threadsrunning” 的值，即查询次数、线程连接数和线程运行数。</p><p>我们可以通过执行如下脚本监控 MySQL 服务器运行的状态值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">while true</span><br><span class="line">do</span><br><span class="line">mysqladmin -uroot -p&quot;密码&quot; ext | awk &apos;/Queries/&#123;q=$4&#125;/Threads_connected/&#123;c=$4&#125;/Threads_running/&#123;r=$4&#125;END&#123;printf(&quot;%d %d %d\n&quot;,q,c,r)&#125;&apos; &gt;&gt; status.txt</span><br><span class="line">sleep 1</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行该脚本 24 小时，获取 status.txt 里的内容，再次通过 awk 计算==每秒请求 MySQL 服务的次数==</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">awk &apos;&#123;q=$1-last;last=$1&#125;&#123;printf(&quot;%d %d %d\n&quot;,q,$2,$3)&#125;&apos; status.txt</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">复制计算好的内容到 Excel 中生成图表观察数据周期性。</span><br><span class="line"> </span><br><span class="line">如果观察的数据有周期性的变化，如上图的解释，需要修改缓存失效策略。</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line"></span><br><span class="line">通过随机数在[3,6,9] 区间获取其中一个值作为缓存失效时间，这样分散了缓存失效时间，从而节省了一部分内存的消耗。</span><br><span class="line"></span><br><span class="line">当访问高峰期时，一部分请求分流到未失效的缓存，另一部分则访问 MySQL 数据库，这样减少了 MySQL 服务器的压力。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##四、获取需要优化的 SQL 语句</span><br><span class="line"> </span><br><span class="line">### 4.1 方式一：查看运行的线程</span><br><span class="line"></span><br><span class="line">执行命令：</span><br><span class="line"></span><br><span class="line">    show processlist</span><br><span class="line"></span><br><span class="line">返回结果：</span><br></pre></td></tr></table></figure><p>mysql&gt; show processlist;<br>+—-+——+———–+——+———+——+———-+——————+<br>| Id | User | Host      | db   | Command | Time | State    | Info             |<br>+—-+——+———–+——+———+——+———-+——————+<br>|  9 | root | localhost | test | Query   |    0 | starting | show processlist |<br>+—-+——+———–+——+———+——+———-+——————+<br>1 row in set (0.00 sec)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">从返回结果中我们可以了解该线程执行了什么命令/SQL 语句以及执行的时间。实际应用中，查询的返回结果会有 N 条记录。</span><br><span class="line"> </span><br><span class="line">其中, **返回的 State 的值是我们判断性能好坏的关键**，其值出现如下内容，则该行记录的 SQL 语句需要优化：</span><br></pre></td></tr></table></figure><p>Converting HEAP to MyISAM # 查询结果太大时，把结果放到磁盘，严重<br>Create tmp table #创建临时表，严重<br>Copying to tmp table on disk  #把内存临时表复制到磁盘，严重<br>locked #被其他查询锁住，严重<br>loggin slow query #记录慢查询<br>Sorting result #排序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">## 4.2 方式二：开启慢查询日志</span><br><span class="line"> </span><br><span class="line">在配置文件 my.cnf 中的 \[mysqld\] 一行下边添加两个参数：</span><br></pre></td></tr></table></figure></p><p>slow_query_log = 1<br>slow_query_log_file=/var/lib/mysql/slow-query.log<br>long_query_time = 2<br>log_queries_not_using_indexes = 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">其中，slowquerylog = 1 表示开启慢查询；</span><br><span class="line"></span><br><span class="line">slowquerylogfile 表示慢查询日志存放的位置；</span><br><span class="line"></span><br><span class="line">longquerytime = 2 表示查询 =2 秒才记录日志；</span><br><span class="line"></span><br><span class="line">logqueriesnotusing_indexes = 1 记录没有使用索引的 SQL 语句。</span><br><span class="line"> </span><br><span class="line">**注意：slowquerylog_file 的路径不能随便写，否则 MySQL 服务器可能没有权限将日志文件写到指定的目录中。建议直接复制上文的路径。**</span><br><span class="line"> </span><br><span class="line">修改保存文件后，重启 MySQL 服务。在 /var/lib/mysql/ 目录下会创建 slow-query.log 日志文件。连接 MySQL 服务端执行如下命令可以查看配置情况。</span><br></pre></td></tr></table></figure></p><p>show variables like ‘slow_query%’;</p><p>show variables like ‘long_query_time’;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"> </span><br><span class="line">测试慢查询日志：</span><br></pre></td></tr></table></figure></p><p>mysql&gt; select sleep(2);<br>+———-+<br>| sleep(2) |<br>+———-+<br>|        0 |<br>+———-+<br>1 row in set (2.00 sec)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">打开慢查询日志文件</span><br></pre></td></tr></table></figure></p><p>[root@localhost mysql]# vim /var/lib/mysql/slow-query.log<br>/usr/sbin/mysqld, Version: 5.7.19-log (MySQL Community Server (GPL)). started with:<br>Tcp port: 0  Unix socket: /var/lib/mysql/mysql.sock<br>Time                 Id Command    Argument</p><h1 id="Time-2017-10-05T04-39-11-408964Z"><a href="#Time-2017-10-05T04-39-11-408964Z" class="headerlink" title="Time: 2017-10-05T04:39:11.408964Z"></a>Time: 2017-10-05T04:39:11.408964Z</h1><h1 id="User-Host-root-root-localhost-Id-3"><a href="#User-Host-root-root-localhost-Id-3" class="headerlink" title="User@Host: root[root] @ localhost []  Id:     3"></a>User@Host: root[root] @ localhost []  Id:     3</h1><h1 id="Query-time-2-001395-Lock-time-0-000000-Rows-sent-1-Rows-examined-0"><a href="#Query-time-2-001395-Lock-time-0-000000-Rows-sent-1-Rows-examined-0" class="headerlink" title="Query_time: 2.001395  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0"></a>Query_time: 2.001395  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0</h1><p>use test;<br>SET timestamp=1507178351;<br>select sleep(2);<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">我们可以看到刚才执行了 2 秒的 SQL 语句被记录下来了。</span><br><span class="line"> </span><br><span class="line">虽然在慢查询日志中记录查询慢的 SQL 信息，但是日志记录的内容密集且不易查阅。因此，我们需要通过工具将 SQL 筛选出来。</span><br><span class="line"> </span><br><span class="line">MySQL 提供 mysqldumpslow 工具对日志进行分析。我们可以使用 mysqldumpslow --help 查看命令相关用法。</span><br><span class="line">  </span><br><span class="line">常用参数如下：</span><br></pre></td></tr></table></figure></p><pre><code>-s：排序方式，后边接着如下参数    c：访问次数    l：锁定时间    r：返回记录    t：查询时间al：平均锁定时间ar：平均返回记录书at：平均查询时间-t：返回前面多少条的数据-g：翻遍搭配一个正则表达式，大小写不敏感</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"> </span><br><span class="line">案例：</span><br></pre></td></tr></table></figure><p>获取返回记录集最多的10个sql<br>mysqldumpslow -s r -t 10 /var/lib/mysql/slow-query.log</p><p>获取访问次数最多的10个sql<br>mysqldumpslow -s c -t 10 /var/lib/mysql/slow-query.log</p><p>获取按照时间排序的前10条里面含有左连接的查询语句<br>mysqldumpslow -s t -t 10 -g “left join” /var/lib/mysql/slow-query.log<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">   </span><br><span class="line"> </span><br><span class="line">##五、分析 SQL 语句</span><br><span class="line"> </span><br><span class="line">### 5.1 方式一：explain</span><br><span class="line"> </span><br><span class="line">筛选出有问题的 SQL，我们可以使用 MySQL 提供的 explain 查看 SQL 执行计划情况（关联表，表查询顺序、索引使用情况等）。</span><br><span class="line"> </span><br><span class="line">用法：</span><br><span class="line"> </span><br><span class="line">     explain select * from category;</span><br><span class="line"> 返回结果：</span><br></pre></td></tr></table></figure></p><p>mysql&gt; explain select * from category;<br>+—-+————-+———-+————+——+—————+——+———+——+——+———-+——-+<br>| id | select_type | table    | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra |<br>+—-+————-+———-+————+——+—————+——+———+——+——+———-+——-+<br>|  1 | SIMPLE      | category | NULL       | ALL  | NULL          | NULL | NULL    | NULL |    1 |   100.00 | NULL  |<br>+—-+————-+———-+————+——+—————+——+———+——+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">字段解释：1) id：select 查询序列号。id相同，执行顺序由上至下；id不同，id值越大优先级越高，越先被执行</span><br><span class="line"></span><br><span class="line">2) select_type：查询数据的操作类型，其值如下：</span><br><span class="line"></span><br><span class="line">-   simple：简单查询，不包含子查询或 union</span><br><span class="line">    </span><br><span class="line">-   primary:包含复杂的子查询，最外层查询标记为该值</span><br><span class="line">    </span><br><span class="line">-   subquery：在 select 或 where 包含子查询，被标记为该值</span><br><span class="line">    </span><br><span class="line">-   derived：在 from 列表中包含的子查询被标记为该值，MySQL 会递归执行这些子查询，把结果放在临时表</span><br><span class="line">    </span><br><span class="line">-   union：若第二个 select 出现在 union 之后，则被标记为该值。若 union 包含在 from 的子查询中，外层 select 被标记为 derived</span><br><span class="line">    </span><br><span class="line">-   union result：从 union 表获取结果的 select</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">3) table：显示该行数据是关于哪张表</span><br><span class="line"></span><br><span class="line">4) partitions：匹配的分区</span><br><span class="line"></span><br><span class="line">5) type：表的连接类型，其值，性能由高到底排列如下：</span><br><span class="line"></span><br><span class="line">-   system：表只有一行记录，相当于系统表</span><br><span class="line">    </span><br><span class="line">-   const：通过索引一次就找到，只匹配一行数据</span><br><span class="line">    </span><br><span class="line">-   eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常用于主键或唯一索引扫描</span><br><span class="line">    </span><br><span class="line">-   ref：非唯一性索引扫描，返回匹配某个单独值的所有行。用于=、&lt; 或  操作符带索引的列</span><br><span class="line">    </span><br><span class="line">-   range：只检索给定范围的行，使用一个索引来选择行。一般使用between、、&lt;情况</span><br><span class="line">    </span><br><span class="line">-   index：只遍历索引树</span><br><span class="line">    </span><br><span class="line">-   ALL：全表扫描，性能最差</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">**注：前5种情况都是理想情况的索引使用情况。通常优化至少到range级别，最好能优化到 ref**</span><br><span class="line"></span><br><span class="line">6) possible_keys：指出 MySQL 使用哪个索引在该表找到行记录。如果该值为 NULL，说明没有使用索引，可以建立索引提高性能</span><br><span class="line"></span><br><span class="line">7) key：显示 MySQL 实际使用的索引。如果为 NULL，则没有使用索引查询</span><br><span class="line"></span><br><span class="line">8) key_len：表示索引中使用的字节数，通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好 显示的是索引字段的最大长度，并非实际使用长度</span><br><span class="line"></span><br><span class="line">9) ref：显示该表的索引字段关联了哪张表的哪个字段</span><br><span class="line"></span><br><span class="line">10) rows：根据表统计信息及选用情况，大致估算出找到所需的记录或所需读取的行数，数值越小越好</span><br><span class="line"></span><br><span class="line">11) filtered：返回结果的行数占读取行数的百分比，值越大越好</span><br><span class="line"></span><br><span class="line">12) extra：包含不合适在其他列中显示但十分重要的额外信息，常见的值如下：</span><br><span class="line"></span><br><span class="line">-   using filesort：说明 MySQL 会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。出现该值，应该优化 SQL</span><br><span class="line">    </span><br><span class="line">-   using temporary：使用了临时表保存中间结果，MySQL 在对查询结果排序时使用临时表。常见于排序 order by 和分组查询 group by。出现该值，应该优化 SQL</span><br><span class="line">    </span><br><span class="line">-   using index：表示相应的 select 操作使用了覆盖索引，避免了访问表的数据行，效率不错</span><br><span class="line">    </span><br><span class="line">-   using where：where 子句用于限制哪一行</span><br><span class="line">    </span><br><span class="line">-   using join buffer：使用连接缓存</span><br><span class="line">    </span><br><span class="line">-   distinct：发现第一个匹配后，停止为当前的行组合搜索更多的行</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">**注意：出现前 2 个值，SQL 语句必须要优化。**</span><br><span class="line"></span><br><span class="line">### 5.2 方式二：profiling</span><br><span class="line"></span><br><span class="line">使用 profiling 命令可以了解 SQL 语句消耗资源的详细信息（每个执行步骤的开销）。</span><br><span class="line"></span><br><span class="line">#### 5.2.1 查看 profile 开启情况</span><br><span class="line"></span><br><span class="line">select @@profiling;</span><br><span class="line"></span><br><span class="line">返回结果：</span><br></pre></td></tr></table></figure></p><p>mysql&gt; select @@profiling;<br>+————-+<br>| @@profiling |<br>+————-+<br>|           0 |<br>+————-+<br>1 row in set, 1 warning (0.00 sec)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"> 0 表示关闭状态,1 表示开启</span><br><span class="line"> </span><br><span class="line">#### 5.2.2 启用 profile</span><br><span class="line"> </span><br><span class="line">set profiling = 1;  </span><br><span class="line"> </span><br><span class="line"> 返回结果：</span><br></pre></td></tr></table></figure></p><p>mysql&gt; set profiling = 1;<br>Query OK, 0 rows affected, 1 warning (0.00 sec)</p><p>mysql&gt; select @@profiling;<br>+————-+<br>| @@profiling |<br>+————-+<br>|           1 |<br>+————-+<br>1 row in set, 1 warning (0.00 sec)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"> 在连接关闭后，profiling 状态自动设置为关闭状态。</span><br><span class="line"> </span><br><span class="line">#### 5.2.3 查看执行的 SQL 列表</span><br><span class="line"> </span><br><span class="line">     show profiles;</span><br><span class="line"> </span><br><span class="line"> 返回结果：</span><br></pre></td></tr></table></figure></p><p>mysql&gt; show profiles;<br>+———-+————+——————————+<br>| Query_ID | Duration   | Query                        |<br>+———-+————+——————————+<br>|        1 | 0.00062925 | select @@profiling           |<br>|        2 | 0.00094150 | show tables                  |<br>|        3 | 0.00119125 | show databases               |<br>|        4 | 0.00029750 | SELECT DATABASE()            |<br>|        5 | 0.00025975 | show databases               |<br>|        6 | 0.00023050 | show tables                  |<br>|        7 | 0.00042000 | show tables                  |<br>|        8 | 0.00260675 | desc role                    |<br>|        9 | 0.00074900 | select name,is_key from role |<br>+———-+————+——————————+<br>9 rows in set, 1 warning (0.00 sec)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"> 该命令执行之前，需要执行其他 SQL 语句才有记录。</span><br><span class="line"> </span><br><span class="line">#### 5.2.4 查询指定 ID 的执行详细信息</span><br><span class="line"> </span><br><span class="line">     show profile for query Query_ID;</span><br><span class="line"> </span><br><span class="line"> 返回结果：</span><br></pre></td></tr></table></figure></p><p>mysql&gt; show profile for query 9;<br>+———————-+———-+<br>| Status               | Duration |<br>+———————-+———-+<br>| starting             | 0.000207 |<br>| checking permissions | 0.000010 |<br>| Opening tables       | 0.000042 |<br>| init                 | 0.000050 |<br>| System lock          | 0.000012 |<br>| optimizing           | 0.000003 |<br>| statistics           | 0.000011 |<br>| preparing            | 0.000011 |<br>| executing            | 0.000002 |<br>| Sending data         | 0.000362 |<br>| end                  | 0.000006 |<br>| query end            | 0.000006 |<br>| closing tables       | 0.000006 |<br>| freeing items        | 0.000011 |<br>| cleaning up          | 0.000013 |<br>+———————-+———-+<br>15 rows in set, 1 warning (0.00 sec)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"> 每行都是状态变化的过程以及它们持续的时间。Status 这一列和 show processlist 的 State 是一致的。因此，需要优化的注意点与上文描述的一样。</span><br><span class="line"> </span><br><span class="line">#### 5.2.5 获取 CPU、 Block IO 等信息</span><br></pre></td></tr></table></figure></p><p>show profile block io,cpu for query Query_ID;</p><p>show profile cpu,block io,memory,swaps,context switches,source for query Query_ID;</p><p>show profile all for query Query_ID;<br><code>`</code> </p><p> 六、优化手段</p><p> 主要以查询优化、索引使用和表结构设计方面进行讲解。</p><h3 id="6-1-查询优化"><a href="#6-1-查询优化" class="headerlink" title="6.1 查询优化"></a>6.1 查询优化</h3><p> 1) 避免 SELECT *，需要什么数据，就查询对应的字段。</p><p> 2) 小表驱动大表，即小的数据集驱动大的数据集。如：以 A，B 两表为例，两表通过 id 字段进行关联。</p><p> 当 B 表的数据集小于 A 表时，用 in 优化 exist；使用 in ，两表执行顺序是先查 B 表，再查 A 表</p><pre><code>select * from A where id in (select id from B)</code></pre><p> 当 A 表的数据集小于 B 表时，用 exist 优化 in；使用 exists，两表执行顺序是先查 A 表，再查 B 表</p><pre><code>select * from A where exists (select 1 from B where B.id = A.id)</code></pre><p> 3) 一些情况下，可以使用连接代替子查询，因为使用 join，MySQL 不会在内存中创建临时表。</p><p> 4) 适当添加冗余字段，减少表关联。</p><p> 5) 合理使用索引（下文介绍）。如：为排序、分组字段建立索引，避免 filesort 的出现。</p><h3 id="6-2-索引使用"><a href="#6-2-索引使用" class="headerlink" title="6.2 索引使用"></a>6.2 索引使用</h3><h4 id="6-2-1-适合使用索引的场景"><a href="#6-2-1-适合使用索引的场景" class="headerlink" title="6.2.1 适合使用索引的场景"></a>6.2.1 适合使用索引的场景</h4><p> 1) 主键自动创建唯一索引</p><p> 2) 频繁作为查询条件的字段</p><p> 3) 查询中与其他表关联的字段</p><p> 4) 查询中排序的字段</p><p> 5) 查询中统计或分组字段</p><h4 id="6-2-2-不适合使用索引的场景"><a href="#6-2-2-不适合使用索引的场景" class="headerlink" title="6.2.2 不适合使用索引的场景"></a>6.2.2 不适合使用索引的场景</h4><p> 1) 频繁更新的字段</p><p> 2) where 条件中用不到的字段</p><p> 3) 表记录太少</p><p> 4) 经常增删改的表</p><p> 5) 字段的值的差异性不大或重复性高</p><h4 id="6-2-3-索引创建和使用原则"><a href="#6-2-3-索引创建和使用原则" class="headerlink" title="6.2.3 索引创建和使用原则"></a>6.2.3 索引创建和使用原则</h4><p> 1) 单表查询：哪个列作查询条件，就在该列创建索引</p><p> 2) 多表查询：left join 时，索引添加到右表关联字段；right join 时，索引添加到左表关联字段</p><p> 3) 不要对索引列进行任何操作（计算、函数、类型转换）</p><p> 4) 索引列中不要使用 !=，&lt; 非等于</p><p> 5) 索引列不要为空，且不要使用 is null 或 is not null 判断</p><p> 6) 索引字段是字符串类型，查询条件的值要加’’单引号,避免底层类型自动转换</p><p> <strong>违背上述原则可能会导致索引失效，具体情况需要使用 explain 命令进行查看</strong></p><h4 id="6-2-4-索引失效情况"><a href="#6-2-4-索引失效情况" class="headerlink" title="6.2.4 索引失效情况"></a>6.2.4 索引失效情况</h4><p> 除了违背索引创建和使用原则外，如下情况也会导致索引失效：</p><p> 1) 模糊查询时，以 % 开头</p><p> 2) 使用 or 时，如：字段1（非索引）or 字段2（索引）会导致索引失效。</p><p> 3) 使用复合索引时，不使用第一个索引列。</p><p> index(a,b,c) ，以字段 a,b,c 作为复合索引为例：</p><p> <img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/JfTPiahTHJhqYoYCBMDfKtOOacVmxWyCS9frQ0UoHy2VYKkrR7o2ibmNNj4dRVJdeQbVtvLSbicUJ6Qqxscvwb8IA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h3 id="6-3-1-选择合适的数据类型6-3-数据库表结构设计"><a href="#6-3-1-选择合适的数据类型6-3-数据库表结构设计" class="headerlink" title="6.3.1 选择合适的数据类型6.3 数据库表结构设计"></a>6.3.1 选择合适的数据类型6.3 数据库表结构设计</h3><p> 1) 使用可以存下数据最小的数据类型</p><p> 2) 使用简单的数据类型。int 要比 varchar 类型在mysql处理简单</p><p> 3) 尽量使用 tinyint、smallint、mediumint 作为整数类型而非 int</p><p> 4) 尽可能使用 not null 定义字段，因为 null 占用4字节空间</p><p> 5) 尽量少用 text 类型,非用不可时最好考虑分表</p><p> 6) 尽量使用 timestamp 而非 datetime</p><p> 7) 单表不要有太多字段，建议在 20 以内</p><h4 id="6-3-2-表的拆分"><a href="#6-3-2-表的拆分" class="headerlink" title="6.3.2 表的拆分"></a>6.3.2 表的拆分</h4><p> 当数据库中的数据非常大时，查询优化方案也不能解决查询速度慢的问题时，我们可以考虑拆分表，让每张表的数据量变小，从而提高查询效率。</p><p> 1) 垂直拆分：将表中多个列分开放到不同的表中。例如用户表中一些字段经常被访问，将这些字段放在一张表中，另外一些不常用的字段放在另一张表中。插入数据时，使用事务确保两张表的数据一致性。</p><p> 2) 水平拆分：按照行进行拆分。例如用户表中，使用用户ID，对用户ID取10的余数，将用户数据均匀的分配到0~9的10个用户表中。查找时也按照这个规则查询数据。</p><h4 id="6-3-3-读写分离"><a href="#6-3-3-读写分离" class="headerlink" title="6.3.3 读写分离"></a>6.3.3 读写分离</h4><p> 一般情况下对数据库而言都是“读多写少”。换言之，数据库的压力多数是因为大量的读取数据的操作造成的。我们可以采用数据库集群的方案，使用一个库作为主库，负责写入数据；其他库为从库，负责读取数据。这样可以缓解对数据库的访问压力。</p><p>##七、服务器参数调优</p><h3 id="7-1-内存相关"><a href="#7-1-内存相关" class="headerlink" title="7.1 内存相关"></a>7.1 内存相关</h3><p> sortbuffersize 排序缓冲区内存大小</p><p> joinbuffersize 使用连接缓冲区大小</p><p> readbuffersize 全表扫描时分配的缓冲区大小</p><h3 id="7-2-IO-相关"><a href="#7-2-IO-相关" class="headerlink" title="7.2 IO 相关"></a>7.2 IO 相关</h3><p> Innodblogfile_size 事务日志大小</p><p> Innodblogfilesingroup 事务日志个数</p><p> Innodblogbuffer_size 事务日志缓冲区大小</p><p> Innodbflushlogattrx_commit 事务日志刷新策略 ，其值如下：</p><p> 0：每秒进行一次 log 写入 cache，并 flush log 到磁盘</p><p> 1：在每次事务提交执行 log 写入 cache，并 flush log 到磁盘</p><p> 2：每次事务提交，执行 log 数据写到 cache，每秒执行一次 flush log 到磁盘</p><h3 id="7-3-安全相关"><a href="#7-3-安全相关" class="headerlink" title="7.3 安全相关"></a>7.3 安全相关</h3><p> expirelogsdays 指定自动清理 binlog 的天数</p><p> maxallowedpacket 控制 MySQL 可以接收的包的大小</p><p> skipnameresolve 禁用 DNS 查找</p><p> read_only 禁止非 super 权限用户写权限</p><p> skipslavestart 级你用 slave 自动恢复</p><h3 id="7-4-其他"><a href="#7-4-其他" class="headerlink" title="7.4 其他"></a>7.4 其他</h3><p> max_connections 控制允许的最大连接数</p><p> tmptablesize 临时表大小</p><p> maxheaptable_size 最大内存表大小</p><p> <strong>笔者并没有使用这些参数对 MySQL 服务器进行调优，具体详情介绍和性能效果请参考文章末尾的资料或另行百度。</strong></p><p>##八、硬件选购和参数优化</p><p> 硬件的性能直接决定 MySQL 数据库的性能。硬件的性能瓶颈，直接决定 MySQL 数据库的运行数据和效率。</p><p> <strong>作为软件开发程序员，我们主要关注软件方面的优化内容，以下硬件方面的优化作为了解即可</strong></p><h3 id="8-1-内存相关"><a href="#8-1-内存相关" class="headerlink" title="8.1 内存相关"></a>8.1 内存相关</h3><p> 内存的 IO 比硬盘的速度快很多，可以增加系统的缓冲区容量，使数据在内存停留的时间更长，以减少磁盘的 IO</p><h3 id="8-2-磁盘-I-O-相关"><a href="#8-2-磁盘-I-O-相关" class="headerlink" title="8.2 磁盘 I/O 相关"></a>8.2 磁盘 I/O 相关</h3><p> 1) 使用 SSD 或 PCle SSD 设备，至少获得数百倍甚至万倍的 IOPS 提升</p><p> 2) 购置阵列卡同时配备 CACHE 及 BBU 模块，可以明显提升 IOPS</p><p> 3) 尽可能选用 RAID-10，而非 RAID-5</p><h3 id="8-3-配置-CPU-相关"><a href="#8-3-配置-CPU-相关" class="headerlink" title="8.3 配置 CPU 相关"></a>8.3 配置 CPU 相关</h3><p> 在服务器的 BIOS 设置中，调整如下配置：</p><p> 1) 选择 Performance Per Watt Optimized（DAPC）模式，发挥 CPU 最大性能</p><p> 2) 关闭 C1E 和 C States 等选项，提升 CPU 效率</p><p> 3) Memory Frequency（内存频率）选择 Maximum Performance</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##超全的珍藏版MySQL性能优化实践！&lt;/p&gt;
&lt;p&gt; 来源：&lt;a href=&quot;https://www.xttblog.com/?p=4951&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.xttblog.com/?p=4951&lt;
      
    
    </summary>
    
      <category term="mysql" scheme="http://zhangyu8.me/categories/mysql/"/>
    
    
      <category term="mysql" scheme="http://zhangyu8.me/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Redis持久化讲解</title>
    <link href="http://zhangyu8.me/2020/03/17/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E8%AE%B2%E8%A7%A3/"/>
    <id>http://zhangyu8.me/2020/03/17/Redis持久化讲解/</id>
    <published>2020-03-17T09:00:00.000Z</published>
    <updated>2020-03-17T09:33:37.148Z</updated>
    
    <content type="html"><![CDATA[<p>好文转载</p><p><a href="http://www.wmyskxz.com/2020/03/13/redis-7-chi-jiu-hua-yi-wen-liao-jie/" target="_blank" rel="noopener">http://www.wmyskxz.com/2020/03/13/redis-7-chi-jiu-hua-yi-wen-liao-jie/</a></p><blockquote><h1 id="一、持久化简介"><a href="#一、持久化简介" class="headerlink" title="一、持久化简介"></a>一、持久化简介</h1><p><strong>Redis</strong> 的数据 <strong>全部存储</strong> 在 <strong>内存</strong> 中，如果 <strong>突然宕机</strong>，数据就会全部丢失，因此必须有一套机制来保证 Redis 的数据不会因为故障而丢失，这种机制就是 Redis 的 <strong>持久化机制</strong>，它会将内存中的数据库状态 <strong>保存到磁盘</strong> 中。</p><h2 id="持久化发生了什么-从内存到磁盘"><a href="#持久化发生了什么-从内存到磁盘" class="headerlink" title="持久化发生了什么 | 从内存到磁盘"></a>持久化发生了什么 | 从内存到磁盘</h2><p>我们来稍微考虑一下 <strong>Redis</strong> 作为一个 <strong>“内存数据库”</strong> 要做的关于持久化的事情。通常来说，从客户端发起请求开始，到服务器真实地写入磁盘，需要发生如下几件事情：</p><p><img src="http://www.wmyskxz.com/images/pasted-17.png" alt></p><p><strong>详细版</strong> 的文字描述大概就是下面这样：</p><ol><li><p>客户端向数据库 <strong>发送写命令</strong> <em>(数据在客户端的内存中)</em></p></li><li><p>数据库 <strong>接收</strong> 到客户端的 <strong>写请求</strong> <em>(数据在服务器的内存中)</em></p></li><li><p>数据库 <strong>调用系统 API</strong> 将数据写入磁盘 <em>(数据在内核缓冲区中)</em></p></li><li><p>操作系统将 <strong>写缓冲区</strong> 传输到 <strong>磁盘控控制器</strong> <em>(数据在磁盘缓存中)</em></p></li><li><p>操作系统的磁盘控制器将数据 <strong>写入实际的物理媒介</strong> 中 <em>(数据在磁盘中)</em></p></li></ol><p><strong>注意:</strong> 上面的过程其实是 <strong>极度精简</strong> 的，在实际的操作系统中，<strong>缓存</strong> 和 <strong>缓冲区</strong> 会比这 <strong>多得多</strong>…</p><h2 id="如何尽可能保证持久化的安全"><a href="#如何尽可能保证持久化的安全" class="headerlink" title="如何尽可能保证持久化的安全"></a>如何尽可能保证持久化的安全</h2><p>如果我们故障仅仅涉及到 <strong>软件层面</strong> <em>(该进程被管理员终止或程序崩溃)</em> 并且没有接触到内核，那么在 <em>上述步骤 3</em> 成功返回之后，我们就认为成功了。即使进程崩溃，操作系统仍然会帮助我们把数据正确地写入磁盘。</p><p>如果我们考虑 <strong>停电/ 火灾</strong> 等 <strong>更具灾难性</strong> 的事情，那么只有在完成了第 <strong>5</strong> 步之后，才是安全的。</p><p><img src="http://www.wmyskxz.com/images/pasted-18.png" alt></p><p>机房”火了“</p><p>所以我们可以总结得出数据安全最重要的阶段是：<strong>步骤三、四、五</strong>，即：</p><ul><li><p>数据库软件调用写操作将用户空间的缓冲区转移到内核缓冲区的频率是多少？</p></li><li><p>内核多久从缓冲区取数据刷新到磁盘控制器？</p></li><li><p>磁盘控制器多久把数据写入物理媒介一次？</p></li><li><p><strong>注意：</strong> 如果真的发生灾难性的事件，我们可以从上图的过程中看到，任何一步都可能被意外打断丢失，所以只能 <strong>尽可能地保证</strong> 数据的安全，这对于所有数据库来说都是一样的。</p></li></ul><p>我们从 <strong>第三步</strong> 开始。Linux 系统提供了清晰、易用的用于操作文件的 <code>POSIX file API</code>，<code>20</code> 多年过去，仍然还有很多人对于这一套 <code>API</code> 的设计津津乐道，我想其中一个原因就是因为你光从 <code>API</code> 的命名就能够很清晰地知道这一套 API 的用途：</p><pre><code>int open(const char *path, int oflag, .../*,mode_t mode */);int close (int filedes);int remove( const char *fname );ssize_t write(int fildes, const void *buf, size_t nbyte);ssize_t read(int fildes, void *buf, size_t nbyte);</code></pre><ul><li>参考自：API 设计最佳实践的思考 - <a href="https://www.cnblogs.com/yuanjiangw/p/10846560.html" target="_blank" rel="noopener">https://www.cnblogs.com/yuanjiangw/p/10846560.html</a></li></ul><p>所以，我们有很好的可用的 <code>API</code> 来完成 <strong>第三步</strong>，但是对于成功返回之前，我们对系统调用花费的时间没有太多的控制权。</p><p>然后我们来说说 <strong>第四步</strong>。我们知道，除了早期对电脑特别了解那帮人 <em>(操作系统就这帮人搞的)</em>，实际的物理硬件都不是我们能够 <strong>直接操作</strong> 的，都是通过 <strong>操作系统调用</strong> 来达到目的的。为了防止过慢的 I/O 操作拖慢整个系统的运行，操作系统层面做了很多的努力，譬如说 <strong>上述第四步</strong> 提到的 <strong>写缓冲区</strong>，并不是所有的写操作都会被立即写入磁盘，而是要先经过一个缓冲区，默认情况下，Linux 将在 <strong>30 秒</strong> 后实际提交写入。</p><p><img src="http://www.wmyskxz.com/images/pasted-19.png" alt></p><p>但是很明显，<strong>30 秒</strong> 并不是 Redis 能够承受的，这意味着，如果发生故障，那么最近 30 秒内写入的所有数据都可能会丢失。幸好 <code>PROSIX API</code> 提供了另一个解决方案：<code>fsync</code>，该命令会 <strong>强制</strong> 内核将 <strong>缓冲区</strong> 写入 <strong>磁盘</strong>，但这是一个非常消耗性能的操作，每次调用都会 <strong>阻塞等待</strong> 直到设备报告 IO 完成，所以一般在生产环境的服务器中，<strong>Redis</strong> 通常是每隔 1s 左右执行一次 <code>fsync</code> 操作。</p><p>到目前为止，我们了解到了如何控制 <code>第三步</code> 和 <code>第四步</code>，但是对于 <strong>第五步</strong>，我们 <strong>完全无法控制</strong>。也许一些内核实现将试图告诉驱动实际提交物理介质上的数据，或者控制器可能会为了提高速度而重新排序写操作，不会尽快将数据真正写到磁盘上，而是会等待几个多毫秒。这完全是我们无法控制的。</p><h1 id="二、Redis-中的两种持久化方式"><a href="#二、Redis-中的两种持久化方式" class="headerlink" title="二、Redis 中的两种持久化方式"></a>二、Redis 中的两种持久化方式</h1><h2 id="方式一：快照"><a href="#方式一：快照" class="headerlink" title="方式一：快照"></a>方式一：快照</h2><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_gif/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8eb76EtkgH2uiciaJhAgg0Pu8ibRCQ8BmjbqbO4MNwT3Hy6Gic1vHOVuN4Q/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt></p><p><strong>Redis 快照</strong> 是最简单的 Redis 持久性模式。当满足特定条件时，它将生成数据集的时间点快照，例如，如果先前的快照是在2分钟前创建的，并且现在已经至少有 <em>100</em> 次新写入，则将创建一个新的快照。此条件可以由用户配置 Redis 实例来控制，也可以在运行时修改而无需重新启动服务器。快照作为包含整个数据集的单个 <code>.rdb</code> 文件生成。</p><p>但我们知道，Redis 是一个 <strong>单线程</strong> 的程序，这意味着，我们不仅仅要响应用户的请求，还需要进行内存快照。而后者要求 Redis 必须进行 IO 操作，这会严重拖累服务器的性能。</p><p>还有一个重要的问题是，我们在 <strong>持久化的同时</strong>，<strong>内存数据结构</strong> 还可能在 <strong>变化</strong>，比如一个大型的 hash 字典正在持久化，结果一个请求过来把它删除了，可是这才刚持久化结束，咋办？</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_jpg/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8ibFtyVaHayfdG0hvmEDeGOgnVOdmYkxTmkad9JnVRicIq2m2dtVTDicNg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h3 id="使用系统多进程-COW-Copy-On-Write-机制-fork-函数"><a href="#使用系统多进程-COW-Copy-On-Write-机制-fork-函数" class="headerlink" title="使用系统多进程 COW(Copy On Write) 机制 | fork 函数"></a>使用系统多进程 COW(Copy On Write) 机制 | fork 函数</h3><p>操作系统多进程 <strong>COW(Copy On Write) 机制</strong> 拯救了我们。<strong>Redis</strong> 在持久化时会调用 <code>glibc</code> 的函数 <code>fork</code> 产生一个子进程，简单理解也就是基于当前进程 <strong>复制</strong> 了一个进程，主进程和子进程会共享内存里面的代码块和数据段：</p><p><img src="http://www.wmyskxz.com/images/pasted-21.png" alt></p><p>这里多说一点，<strong>为什么 fork 成功调用后会有两个返回值呢？</strong> 因为子进程在复制时复制了父进程的堆栈段，所以两个进程都停留在了 <code>fork</code> 函数中 <em>(都在同一个地方往下继续”同时”执行)</em>，等待返回，所以 <strong>一次在父进程中返回子进程的 pid，另一次在子进程中返回零，系统资源不够时返回负数</strong>。<em>(伪代码如下)</em></p><pre><code>pid = os.fork()if pid &gt; 0:  handle_client_request()  # 父进程继续处理客户端请求if pid == 0:  handle_snapshot_write()  # 子进程处理快照写磁盘if pid &lt; 0:  # fork error</code></pre><p>所以 <strong>快照持久化</strong> 可以完全交给 <strong>子进程</strong> 来处理，<strong>父进程</strong> 则继续 <strong>处理客户端请求</strong>。<strong>子进程</strong> 做数据持久化，它 <strong>不会修改现有的内存数据结构</strong>，它只是对数据结构进行遍历读取，然后序列化写到磁盘中。但是 <strong>父进程</strong> 不一样，它必须持续服务客户端请求，然后对 <strong>内存数据结构进行不间断的修改</strong>。</p><p>这个时候就会使用操作系统的 COW 机制来进行 <strong>数据段页面</strong> 的分离。数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复 制一份分离出来，然后 <strong>对这个复制的页面进行修改</strong>。这时 <strong>子进程</strong> 相应的页面是 <strong>没有变化的</strong>，还是进程产生时那一瞬间的数据。</p><p>子进程因为数据没有变化，它能看到的内存里的数据在进程产生的一瞬间就凝固了，再也不会改变，这也是为什么 <strong>Redis</strong> 的持久化 <strong>叫「快照」的原因</strong>。接下来子进程就可以非常安心的遍历数据了进行序列化写磁盘了。</p><h2 id="方式二：AOF"><a href="#方式二：AOF" class="headerlink" title="方式二：AOF"></a>方式二：AOF</h2><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_gif/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8x2OvibGxGibPh2tZPWATrYDrXtOVuHWzGLg0uMQ8Y4OCPFIRAVN9ibvDQ/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt></p><p><strong>快照不是很持久</strong>。如果运行 Redis 的计算机停止运行，电源线出现故障或者您 <code>kill -9</code> 的实例意外发生，则写入 Redis 的最新数据将丢失。尽管这对于某些应用程序可能不是什么大问题，但有些使用案例具有充分的耐用性，在这些情况下，快照并不是可行的选择。</p><p><strong>AOF(Append Only File - 仅追加文件)</strong> 它的工作方式非常简单：每次执行 <strong>修改内存</strong> 中数据集的写操作时，都会 <strong>记录</strong> 该操作。假设 AOF 日志记录了自 Redis 实例创建以来 <strong>所有的修改性指令序列</strong>，那么就可以通过对一个空的 Redis 实例 <strong>顺序执行所有的指令</strong>，也就是 <strong>「重放」</strong>，来恢复 Redis 当前实例的内存数据结构的状态。</p><p>为了展示 AOF 在实际中的工作方式，我们来做一个简单的实验：</p><pre><code>./redis-server --appendonly yes  # 设置一个新实例为 AOF 模式</code></pre><p>然后我们执行一些写操作：</p><pre><code>redis 127.0.0.1:6379&gt; set key1 HelloOKredis 127.0.0.1:6379&gt; append key1 &quot; World!&quot;(integer) 12redis 127.0.0.1:6379&gt; del key1(integer) 1redis 127.0.0.1:6379&gt; del non_existing_key(integer) 0</code></pre><p>前三个操作实际上修改了数据集，第四个操作没有修改，因为没有指定名称的键。这是 AOF 日志保存的文本：</p><pre><code>$ cat appendonly.aof*2$6SELECT$10*3$3set$4key1$5Hello*3$6append$4key1$7 World!*2$3del$4key1</code></pre><p>如您所见，最后的那一条 <code>DEL</code> 指令不见了，因为它没有对数据集进行任何修改。</p><p>就是这么简单。当 Redis 收到客户端修改指令后，会先进行参数校验、逻辑处理，如果没问题，就 <strong>立即</strong> 将该指令文本 <strong>存储</strong> 到 AOF 日志中，也就是说，<strong>先执行指令再将日志存盘</strong>。这一点不同于 <code>MySQL</code>、<code>LevelDB</code>、<code>HBase</code> 等存储引擎，如果我们先存储日志再做逻辑处理，这样就可以保证即使宕机了，我们仍然可以通过之前保存的日志恢复到之前的数据状态，但是 <strong>Redis 为什么没有这么做呢？</strong></p><blockquote><p>Emmm… 没找到特别满意的答案，引用一条来自知乎上的回答吧：</p><ul><li><p><strong>@缘于专注</strong> - 我甚至觉得没有什么特别的原因。仅仅是因为，由于AOF文件会比较大，为了避免写入无效指令（错误指令），必须先做指令检查？如何检查，只能先执行了。因为语法级别检查并不能保证指令的有效性，比如删除一个不存在的key。而MySQL这种是因为它本身就维护了所有的表的信息，所以可以语法检查后过滤掉大部分无效指令直接记录日志，然后再执行。</p></li><li><p>更多讨论参见：为什么Redis先执行指令，再记录AOF日志，而不是像其它存储引擎一样反过来呢？- <a href="https://www.zhihu.com/question/342427472" target="_blank" rel="noopener">https://www.zhihu.com/question/342427472</a></p></li></ul></blockquote><h3 id="AOF-重写"><a href="#AOF-重写" class="headerlink" title="AOF 重写"></a>AOF 重写</h3><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_jpg/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8LRC7fRaWoa3Y0yVhWBtibhtFXugAQLzMGTdbEIRfvIDvvTHs1mWaGzA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>Redis</strong> 在长期运行的过程中，AOF 的日志会越变越长。如果实例宕机重启，重放整个 AOF 日志会非常耗时，导致长时间 Redis 无法对外提供服务。所以需要对 <strong>AOF 日志 “瘦身”</strong>。</p><p><strong>Redis</strong> 提供了 <code>bgrewriteaof</code> 指令用于对 AOF 日志进行瘦身。其 <strong>原理</strong> 就是 <strong>开辟一个子进程</strong> 对内存进行 <strong>遍历</strong> 转换成一系列 Redis 的操作指令，<strong>序列化到一个新的 AOF 日志文件</strong> 中。序列化完毕后再将操作期间发生的 <strong>增量 AOF 日志</strong> 追加到这个新的 AOF 日志文件中，追加完毕后就立即替代旧的 AOF 日志文件了，瘦身工作就完成了。</p><h3 id="fsync"><a href="#fsync" class="headerlink" title="fsync"></a>fsync</h3><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_gif/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8OUwLyk4qWaVC5060sJUttyhaL9I9Q4wZPacnWCNwrOUXicRatz5xfmA/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt></p><p>AOF 日志是以文件的形式存在的，当程序对 AOF 日志文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个内存缓存中，然后内核会异步将脏数据刷回到磁盘的。</p><p>就像我们 <em>上方第四步</em> 描述的那样，我们需要借助 <code>glibc</code> 提供的 <code>fsync(int fd)</code> 函数来讲指定的文件内容 <strong>强制从内核缓存刷到磁盘</strong>。但 <strong>“强制开车”</strong> 仍然是一个很消耗资源的一个过程，需要 <strong>“节制”</strong>！通常来说，生产环境的服务器，Redis 每隔 1s 左右执行一次 <code>fsync</code> 操作就可以了。</p><p>Redis 同样也提供了另外两种策略，一个是 <strong>永不 <code>fsync</code></strong>，来让操作系统来决定合适同步磁盘，很不安全，另一个是 <strong>来一个指令就 <code>fsync</code> 一次</strong>，非常慢。但是在生产环境基本不会使用，了解一下即可。</p><h2 id="Redis-4-0-混合持久化"><a href="#Redis-4-0-混合持久化" class="headerlink" title="Redis 4.0 混合持久化"></a>Redis 4.0 混合持久化</h2><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_gif/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV80Hibf7Vu8Lu4eG1IcZRvIMSmperb6OsZD01Oicf9DJG3Oty8jEsCJ13w/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt></p><p>重启 Redis 时，我们很少使用 <code>rdb</code> 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 <code>rdb</code> 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。</p><p><strong>Redis 4.0</strong> 为了解决这个问题，带来了一个新的持久化选项——<strong>混合持久化</strong>。将 <code>rdb</code> 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，而是 <strong>自持久化开始到持久化结束</strong> 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H7PMcYtBZdH78LrPP2OrMV8sicEbY3SLIW8PerMMmjy3A7abvbYXPnRxuWsVLecdjkB36To07BoC5g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>于是在 Redis 重启的时候，可以先加载 <code>rdb</code> 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。</p><h1 id="相关阅读"><a href="#相关阅读" class="headerlink" title="相关阅读"></a>相关阅读</h1><ol><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyMTg0NDA2Ng==&amp;mid=2247483990&amp;idx=1&amp;sn=513e18c063d32a33a4c819a15fe79afe&amp;chksm=f9d5a65bcea22f4d92563d8ac84cb88d7c2e14507b398a5639d569b575b64c368175f00756ab&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Redis(1)——5种基本数据结构</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyMTg0NDA2Ng==&amp;mid=2247484000&amp;idx=1&amp;sn=a7e02adebea31535c3870cc514719493&amp;chksm=f9d5a66dcea22f7b7cdf210993cbe6c057c0456967ac106d76c89fdd76ed5cb271c879a83f3e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Redis(2)——跳跃表</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyMTg0NDA2Ng==&amp;mid=2247484005&amp;idx=1&amp;sn=0dcb0eb3f79649e0233a79d40094019a&amp;chksm=f9d5a668cea22f7e9a2b9b3a1a5659286bdb8ee9ac19192d74675c6c124be73c6c032b094e60&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Redis(3)——分布式锁深入探究</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyMTg0NDA2Ng==&amp;mid=2247484012&amp;idx=1&amp;sn=3624989d388d17331e1f7ad78fc7a257&amp;chksm=f9d5a661cea22f7781ed997d05afee8f28da52a0013691961915a2831780e481ae94b6e9a1ae&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Reids(4)——神奇的HyperLoglog解决统计问题</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyMTg0NDA2Ng==&amp;mid=2247484022&amp;idx=1&amp;sn=a98c479b4cac96c6af45f219a7c0bde4&amp;chksm=f9d5a67bcea22f6d03b30ce8660f3f3ded294390fd394e138a40a439d0f96d56c8dda2082203&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Redis(5)——亿级数据过滤和布隆过滤器</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzUyMTg0NDA2Ng==&amp;mid=2247484027&amp;idx=1&amp;sn=6237212c01a009be9a5ca88e0e50a46d&amp;chksm=f9d5a676cea22f603e06d1e61d6293985c8ddb063621b1cdf2696da2eaeb1681e307ef2a3f20&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Redis(6)——GeoHash查找附近的人</a></p></li></ol><h1 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h1><ol><li><p>Redis 数据备份与恢复 | 菜鸟教程 - <a href="https://www.runoob.com/redis/redis-backup.html" target="_blank" rel="noopener">https://www.runoob.com/redis/redis-backup.html</a></p></li><li><p>Java Fork/Join 框架 - <a href="https://www.cnblogs.com/cjsblog/p/9078341.html" target="_blank" rel="noopener">https://www.cnblogs.com/cjsblog/p/9078341.html</a></p></li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><p>Redis persistence demystified | antirez weblog (作者博客) - <a href="http://oldblog.antirez.com/post/redis-persistence-demystified.html" target="_blank" rel="noopener">http://oldblog.antirez.com/post/redis-persistence-demystified.html</a></p></li><li><p>操作系统 — fork()函数的使用与底层原理 - <a href="https://blog.csdn.net/Dawn_sf/article/details/78709839" target="_blank" rel="noopener">https://blog.csdn.net/Dawn_sf/article/details/78709839</a></p></li><li><p>磁盘和内存读写简单原理 - <a href="https://blog.csdn.net/zhanghongzheng3213/article/details/54141202" target="_blank" rel="noopener">https://blog.csdn.net/zhanghongzheng3213/article/details/54141202</a></p></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;好文转载&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.wmyskxz.com/2020/03/13/redis-7-chi-jiu-hua-yi-wen-liao-jie/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.wm
      
    
    </summary>
    
      <category term="redis" scheme="http://zhangyu8.me/categories/redis/"/>
    
    
      <category term="redis" scheme="http://zhangyu8.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>被忽视的time命令</title>
    <link href="http://zhangyu8.me/2020/01/10/%E8%A2%AB%E5%BF%BD%E8%A7%86%E7%9A%84time%E5%91%BD%E4%BB%A4/"/>
    <id>http://zhangyu8.me/2020/01/10/被忽视的time命令/</id>
    <published>2020-01-10T09:13:47.000Z</published>
    <updated>2020-01-10T09:12:21.608Z</updated>
    
    <content type="html"><![CDATA[<p>火丁笔记<br> <a href="https://blog.huoding.com/2019/12/08/788" target="_blank" rel="noopener">https://blog.huoding.com/2019/12/08/788</a> </p><p> 如果要选 Linux 下最容易被忽视的命令，time 应该算一个。简单来说，它是一个用来计算命令运行时间的工具，之所以说它容易被忽视，一方面很多人根本不知道 time 的存在，而是习惯在命令启动前后记录两个时间戳，然后手动计算命令运行时间；另一方面很多人虽然知道 time 的存在，但是却并没有真正理解它的含义。</p><p> 下面让我们通过若干例子来理解 time 的真正含义：</p><p> #time ls</p><p> real    0m0.003s<br> user    0m0.001s<br> sys    0m0.002s</p><p> 大概意思是 ls 命令运行花了 0.003 秒，其中用户态花了 0.001 秒，内核态花了 0.002 秒，看上去似乎「real = user + sys」？此等式是否成立，在回答这个问题之前我们不妨看看 real、user、sys 的确切含义，如下定义源自 <a href="https://stackoverflow.com/questions/556405/what-do-real-user-and-sys-mean-in-the-output-of-time1" target="_blank" rel="noopener">Stackoverflow</a>：</p><ul><li><p>Real is wall clock time – time from start to finish of the call. This is all elapsed time including time slices used by other processes and time the process spends blocked (for example if it is waiting for I/O to complete).</p></li><li><p>User is the amount of CPU time spent in user-mode code (outside the kernel) within the process. This is only actual CPU time used in executing the process. Other processes and time the process spends blocked do not count towards this figure.</p></li><li><p>Sys is the amount of CPU time spent in the kernel within the process. This means executing CPU time spent in system calls within the kernel, as opposed to library code, which is still running in user-space. Like ‘user’, this is only CPU time used by the process.</p><p>总的来说，real 是我们直观感受到的消耗的时间，如果命令运行时被堵塞了，那么堵塞时间也是被统计在内的， user 统计在用户态态模式下消耗的 CPU 时间，如果命令运行时被堵塞了，那么堵塞时间并不被统计在内，sys 统计在内核态模式下消耗的 CPU 时间，如果命令运行时被堵塞了，那么堵塞时间并不被统计在内。</p><p>看上去是否统计堵塞时间是区分 real 和 user、sys 的关键，看看下面这个 sleep 例子：</p><p>#time sleep 1</p><p>real    0m1.002s<br>user    0m0.001s<br>sys    0m0.001s</p><p>那么除了堵塞时间，还有别的关键点么，让我们再看看下面两个例子：</p><p>#time find /etc -type f | xargs -n1 -I{} cat {}  /dev/null</p><p>real    0m2.050s<br>user    0m0.626s<br>sys    0m1.533s</p><p>#time find /etc -type f | xargs -n1 -I{} -P2 cat {}  /dev/null</p><p>real    0m1.079s<br>user    0m0.681s<br>sys    0m1.486s</p><p>前后两个例子的区别在于后者在使用 xargs 的时候通过「-P」选项激活了多进程，换句话说，后者可以同时用到多个 CPU。</p><p>了解了相关知识之后，我们通过 real、user、sys 的大小就可以判断程序的行为：</p></li><li><p>如果 real 远远大于 user + sys，那么说明程序可能有严重的堵塞问题。</p></li><li>如果 real 基本等于 user + sys，那么说明程序可能没有用到多 CPU 能力，</li><li><p>如果 real 远远小于 user + sys，那么说明程序可能用到了多 CPU 能力。</p><p>怎么样？看似简单的 time 命令，是不是远比你想的要复杂得多！</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;火丁笔记&lt;br&gt; &lt;a href=&quot;https://blog.huoding.com/2019/12/08/788&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.huoding.com/2019/12/08/788&lt;/a&gt; &lt;/p
      
    
    </summary>
    
      <category term="好文转载" scheme="http://zhangyu8.me/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="好文转载" scheme="http://zhangyu8.me/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>使用简单的逻辑方法进行独立思考</title>
    <link href="http://zhangyu8.me/2020/01/10/%E4%BD%BF%E7%94%A8%E7%AE%80%E5%8D%95%E7%9A%84%E9%80%BB%E8%BE%91%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E7%8B%AC%E7%AB%8B%E6%80%9D%E8%80%83/"/>
    <id>http://zhangyu8.me/2020/01/10/使用简单的逻辑方法进行独立思考/</id>
    <published>2020-01-10T09:04:47.000Z</published>
    <updated>2020-01-10T09:07:14.140Z</updated>
    
    <content type="html"><![CDATA[<p>使用简单的逻辑方法进行独立思考<br> 酷 壳 - CoolShell<br><a href="https://coolshell.cn/articles/20533.html" target="_blank" rel="noopener">https://coolshell.cn/articles/20533.html</a></p><p> 这是一个非常复杂的世界，这个世界上有很多各式各样的观点和思维方式，作为一个程序员的我，也会有程序员的思维方式，程序员的思维方式更接近数学的思维方式，数学的思维方式让可以很容易地理清楚这个混乱的世界，其实，并不需要太复杂的数学逻辑，只需要使用一些简单的数学方法，就可以大幅提升自己的认识能力，所以，在这里，记录一篇我自己的思维方式，一方面给大家做个参考，另一方面也供更高阶的人给我进行指正。算是“开源我的思维方式”，开放不仅仅是为了输出，更是为了看看有没有更好的方式。</p><p> 我的思维方式中，使用数学逻辑的方式进行思考，通常来说，我会使用五步思考的方式：</p><p> <strong>第一步：信息数据可考证</strong>。如果一个观点或是一个见解的数据是错误的，那么就会造成后面的观点全是错的，所以，首要的是要进行数据的查证或考证。一般来说，如果一篇文章的作者足够严谨的话，他的需要给他的数据建立相关的引用或是可以考证的方法方式。如果一篇文章中出现的是，“有关专家表明”、“美国科学家证明”、“经济学家指出”，但是没有任出处，也没有点明这个专家或是科学家的名字，或是，也没有说明或引用让读者可以自己去验证的方法。那么，其引用的话或是数据是无法考证的，如果是无法考证的，那么，这篇文章的水份就非常大了。一般来说，当我读到一篇文章中的东西没有可考证的来源或是方法时，通常来说，我就不会再读了，因为这篇文章的价值已经不大了，如果我关心这篇文章中的东西，我会改为自己去查找的方式，虽然变“重”了，但是很安全。（所以，像Wikipedia这样的网站是我经常去获得信息的地方，因为信息可以被考证是其基本价值观）</p><p> <strong>第二步：处理集合和其包含关系</strong>。这是一个非常简单的人人都会的数学逻辑。比如：哲学家是人，柏拉图是哲学家，所以，柏拉图是人。就是一个在包含关系下的推理。你不要小看这个简单的逻辑，其实很多人并不会很好的应用，相反，当感情支配了他们以后，他们会以点代面，以特例代替普遍性。比如，地图炮就是一种，他们看到了多个案例，他们就开始把这个案例上升上更大的范围，比如：河南人新疆人都是小偷，上海人都是小市民。日本人都是变态和反人类……等等。除了这些地图炮外，还有否定整个人的，比如一个人犯了个错或是性格上有缺陷，就会把整个人全盘否定掉，员工抢个月饼就上升到其价值观有问题……。在数学的逻辑包含中，超集的定义可以适用于子集，通过子集的特征可以对超集进行探索，但是没法定义超集。另外，集合的大小也是一个很重要的事，<a href="https://zh.wikipedia.org/wiki/%E5%80%96%E5%AD%98%E8%80%85%E5%81%8F%E5%B7%AE" target="_blank" rel="noopener">幸存者偏差</a>会是一个很容易让人掉下去的陷阱，因为可能会有很大的样本集可能在你的视线盲区。</p><p> <strong>第三步：处理逻辑因果关系</strong>。所谓因果关系，其实就是分辨充分条件、必要条件和充分必要条件，然后处理其中的逻辑是否有关联性，而且有非常强的因果关系。没有能力分辨充分必要条件处理因果关系是很多人的硬伤。就像我在《<a href="https://coolshell.cn/articles/19271.html" target="_blank" rel="noopener">努力就会成功</a>》中说的一样，“努力” 和 “成功”是否有因果关系？各种逻辑混淆、概念偷换、模糊因果、似是而非全是在这里。比如：掩耳盗铃、刻舟求剑就是因果关系混乱的表现。人们会经常地混淆两个看来一起发生，但是并没有关联在一起的事。因果关系是最容易被模糊和偷换的，比如：很多人都容易混淆“加班”就会有“产出”，混淆了“行动”就会有“结果”，混淆了“抵制”就会赢得“尊重”，混淆了“批评”等于“反对”……等等。除了这些以外，微信公众号里的很多时评文章，他们的文章中的结论和其论据是没有因果关系的，好多文章就是混淆、模糊、偷换……<strong>因果关系出问题的文章读多了是对大脑有损伤的，要尽量远离</strong>。</p><p> <strong>第四步：找到靠谱的基准线</strong>。就像我们写代码一样，我们都是会去找一些最佳实践或是业内标准，原因是因为，这样的东西都是经过长时间被这个世界上很多人Review过的，是值得依赖和靠谱的，他们会考虑到很多你没有考虑过的问题。所以，你也会看到很多时评都会找欧美发达国家的作参考的做法，因为毕竟人家的文化是相对比较文明、科学、开放和先进的。找到世界或是国际的通行标准，会更容易让人进步。比如：以开放包容加强沟通的心态，就会比封闭抵制敌对的心态要好得多得多，智者建桥，愚者建墙。当然，我们也开始发现，有一些事上，有利于自己的就对标，不利于自己的就不对标，而且，除了好的事，不好的事也在找欧美作对标，于是开始“多基准线”和“乱基准线”，这种方式需要我们小心分辨。</p><p> <strong>第五步：更为深入和高维的思考</strong>。如果一件事情只在表面上进行思考其实只是一种浅度思考，在Amazon，线上系统出现故障的时候，需要写一个Correction of Errors的报告，其中需要Ask 5 Whys（参看 Wikipedia 的 <a href="https://en.wikipedia.org/wiki/Five_whys" target="_blank" rel="noopener">Five Whys 词条</a>），这种思考方式可以让你不断追问到深层次的本质问题，会让你自己做大量的调查和研究，让你不会成为一个只会在表面上进行思考的简单动物。比如：当你看到有出乎你意料的事件发生时（比如负面的暴力事件），你需要问一下，为什么会发生，原因是什么？然后罗列尽可能全的原因，再不断地追问并拷证下去（这跟写程序一样，需要从正向案例和负向案例进行考虑分析，才可能写出健壮性很强的代码），我们才会得出一个比较健壮的答案或结构。</p><p> 需要注意的是，在上述的这五种思维方式下，你的思考是不可能快得起来的，这是一个“慢思考”（注：如果读过《<a href="https://book.douban.com/subject/10785583//" target="_blank" rel="noopener">思考，快与慢</a>》这本书的人就知道我在说什么），独立思考是需要使用大脑中的“慢系统”，慢系统是反人性的，所以，能真正做到独立思考的人很少。更多的人的“独立思考”其实只不过是毫无章法的乱思考罢了。</p><p> 通过上述的这五点，我相信你是很容易识别或是分辨出哪些信息是靠谱的，哪些信息是很扯的，甚至会改善你自己的言论和思考。但是，<strong>请注意，这些方法并不能让你获得真理或是真相</strong>。但是这也够了，一个人如果拥有了能够分辨是非的能力，也是很不错的了。虽然不知道事实是什么，但是你也不会盲从和偏信，从而不会被人煽动，而成为幕后黑手的的一只“肉鸡”。</p><p> 多说两句，下面是一些我个人的一些实践：</p><ul><li>当新闻报道报道的不是客观事实，而是加入了很多观点，那么这篇新闻报道是不可信的。</li><li>对于评论性的文章，没有充足权威可信的论据时，不能完全相信。</li><li>不是当事人，不是见证人，还要装作自己是知情的……不知道这种人的自信怎么来的？</li><li>信息不公开的，并有意屏蔽信息的，不能作为可信的信息源。</li><li><p>当出现大是或是大非的事时，一定要非常小心，这个世界不存在完全的美和完全的丑，这样的观点通常来说都是危险的，此时要多看看不同角度的报道和评论，要多收集一些信息，还要多问问为什么。</p><p>欢迎你告诉我一些你的实践和思维方式。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用简单的逻辑方法进行独立思考&lt;br&gt; 酷 壳 - CoolShell&lt;br&gt;&lt;a href=&quot;https://coolshell.cn/articles/20533.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://coolshel
      
    
    </summary>
    
      <category term="好文转载" scheme="http://zhangyu8.me/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="好文转载" scheme="http://zhangyu8.me/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>神器pt-kill</title>
    <link href="http://zhangyu8.me/2019/12/18/%E7%A5%9E%E5%99%A8pt-kill/"/>
    <id>http://zhangyu8.me/2019/12/18/神器pt-kill/</id>
    <published>2019-12-18T03:13:41.000Z</published>
    <updated>2020-01-10T06:51:32.836Z</updated>
    
    <content type="html"><![CDATA[<p>pt-kill是percona-toolkit中的一个工具，用来杀掉指定匹配条件的慢查询会话</p><p><a href="https://www.percona.com/doc/percona-toolkit/3.0/pt-kill.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/3.0/pt-kill.html</a></p><p>##一、Percona Toolkit安装：<br> wget <a href="https://www.percona.com/downloads/percona-toolkit/3.1.0/binary/redhat/7/x86_64/percona-toolkit-3.1.0-2.el7.x86_64.rpm" target="_blank" rel="noopener">https://www.percona.com/downloads/percona-toolkit/3.1.0/binary/redhat/7/x86_64/percona-toolkit-3.1.0-2.el7.x86_64.rpm</a></p><p>yum install -y perl-CPAN perl-Time-HiRes<br>yum install -y percona-toolkit-3.1.0-2.el7.x86_64.rpm</p><p>##注意 pt-kill 运行会报错“Wide character in printf ”</p><p>在第二行前插入use open “:std”, “:encoding(UTF-8)”;<br> 执行<br>sed -i ‘2 i\use open “:std”, “:encoding(UTF-8)”;’   /usr/bin/pt-kill </p><p>##二、下面将pt-kill操作的日志记录在数据库中<br>在目标数据库里执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE ptkill DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;</span><br><span class="line"></span><br><span class="line">use ptkill;</span><br><span class="line"></span><br><span class="line">也可以使用--create-log-table  自动创建表</span><br><span class="line"></span><br><span class="line">CREATE TABLE kill_log (</span><br><span class="line">   kill_id     int(10) unsigned NOT NULL AUTO_INCREMENT,</span><br><span class="line">   server_id   bigint(4) NOT NULL DEFAULT &apos;0&apos;,</span><br><span class="line">   timestamp   DATETIME,</span><br><span class="line">   reason      TEXT,</span><br><span class="line">   kill_error  TEXT,</span><br><span class="line">   Id          bigint(4) NOT NULL DEFAULT &apos;0&apos;,</span><br><span class="line">   User        varchar(16) NOT NULL DEFAULT &apos;&apos;,</span><br><span class="line">   Host        varchar(64) NOT NULL DEFAULT &apos;&apos;,</span><br><span class="line">   db          varchar(64) DEFAULT NULL,</span><br><span class="line">   Command     varchar(16) NOT NULL DEFAULT &apos;&apos;,</span><br><span class="line">   Time        int(7) NOT NULL DEFAULT &apos;0&apos;,</span><br><span class="line">   State       varchar(64) DEFAULT NULL,</span><br><span class="line">   Info        longtext,</span><br><span class="line">   Time_ms     bigint(21) DEFAULT &apos;0&apos;, # NOTE, TODO: currently not used</span><br><span class="line">   PRIMARY KEY (kill_id)</span><br><span class="line">) DEFAULT  CHARSET=utf8mb4 COLLATE utf8mb4_unicode_ci</span><br></pre></td></tr></table></figure></p><p>##执行<br>pt-kill  –host 192.168.1.1 –port 3306 –user ‘root’ –password ‘123456’ –charset utf8 –log-dsn D=ptkill,t=kill_log –match-info “SELECT|select” –busy-time 5  –victims all –interval 5 –print –kill-query –daemonize </p><p>说明</p><p>–log-dsn D=ptkill,t=kill_log   将pt-kill操作的日志记录在表中</p><p>–busy-time=5 执行时间超过5秒的<br>–print –kill-query   动作是 进行print和 kill query，<br>–match-info ‘SELECT|select’  只匹配SELECT 语句</p><p>#################其他</p><p>py-kill 属于简易版的 pt-kill 工具<br><a href="https://github.com/dongwenpeng/py-kill" target="_blank" rel="noopener">https://github.com/dongwenpeng/py-kill</a></p><p>支持后台或前台运行<br>支持邮件报警<br>支持多线程监控<br>pt-kill常规选项也都支持</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;pt-kill是percona-toolkit中的一个工具，用来杀掉指定匹配条件的慢查询会话&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.percona.com/doc/percona-toolkit/3.0/pt-kill.html&quot; target=&quot;_bla
      
    
    </summary>
    
      <category term="tools" scheme="http://zhangyu8.me/categories/tools/"/>
    
    
      <category term="tools" scheme="http://zhangyu8.me/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>邓宁-克鲁格效应</title>
    <link href="http://zhangyu8.me/2019/10/12/%E9%82%93%E5%AE%81-%E5%85%8B%E9%B2%81%E6%A0%BC%E6%95%88%E5%BA%94/"/>
    <id>http://zhangyu8.me/2019/10/12/邓宁-克鲁格效应/</id>
    <published>2019-10-12T04:13:47.000Z</published>
    <updated>2020-01-10T06:52:58.638Z</updated>
    
    <content type="html"><![CDATA[<p>好文转载</p><p><a href="https://www.jianshu.com/p/cf56ffebf9f2" target="_blank" rel="noopener">https://www.jianshu.com/p/cf56ffebf9f2</a></p><p><a href="https://www.jianshu.com/p/d3c5f798134a" target="_blank" rel="noopener">https://www.jianshu.com/p/d3c5f798134a</a></p><p><a href="https://www.jianshu.com/p/cf56ffebf9f2" target="_blank" rel="noopener">邓宁-克鲁格效应 - 简书</a></p><p> <img src="https://upload-images.jianshu.io/upload_images/6198628-cb79177413c0cdcf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1078" alt></p><p> <img src="https://upload-images.jianshu.io/upload_images/6198628-4290120f0e93f00d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/559" alt></p><p> <img src="https://upload-images.jianshu.io/upload_images/6198628-959438c9e29ddd2d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" alt></p><h1 id="愚昧颠覆："><a href="#愚昧颠覆：" class="headerlink" title="愚昧颠覆："></a>愚昧颠覆：</h1><p> 在图中，把高考完作为一个愚昧巅峰， 如果从进入社会的角度来说的话，确实 这是一个节点。 用通俗的话来说，这个点是不知道自己知道，或者说 以为世界是这样的，然后其实不是。 从思维角度来说， 这是一个思维被打破的时候， 认知发生了崩溃。</p><h1 id="绝望之谷："><a href="#绝望之谷：" class="headerlink" title="绝望之谷："></a>绝望之谷：</h1><p> 认知崩溃的过程非常的快， 就像摩天大厦的地基被打破了， 大厦将倾确实非常快。直到某一天 认知完全崩溃，开始建立新的认知。</p><h1 id="开悟之坡："><a href="#开悟之坡：" class="headerlink" title="开悟之坡："></a>开悟之坡：</h1><p> 这是一个慢慢重新构建认知的过程， 也是对思维模式进行重构。 而且是一点点，通过知识和经验慢慢重构。<br> 而大师就这事在不断重构的过程中慢慢完善自己的新的认知。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p> 总的来说，这个是一个升级的过程。 按照 <strong>原则</strong> 上面来说 这也是一个 <code>痛苦+反思=进步</code> 的过程。<br> 这个过程需要 极度透明的自我审查和谦虚的心态。</p><p>##########<br>DK效应图（图一）是在国外加工版（图二）的翻译基础上的夹带私货版，我猜测出自于那些目前流行的禅旗培训机构。</p><p> 图二在图三的基础上进行了加工，但这种篡改别人的理论的做法挺流氓的。</p><p> 图三是原版图。</p><p> 下面转述一下维基百科的内容：</p><p> 达克效应（英语：D-K effect），全称为邓宁-克鲁格效应（英语：Dunning–Kruger effect），是一种认知偏差，能力欠缺的人有一种虚幻的自我优越感，错误地认为自己比真实情况更加优秀。简言之即：庸人容易因欠缺自知之明而自我膨胀。  </p><p> Kruger和Dunning将其归咎于元认知上的缺陷，能力欠缺的人无法认识到自身的无能，不能准确评估自身的能力。他们的研究还表明，反之，非常能干的人会低估自己的能力，错误地假定他们自己能够很容易完成的任务，别人也能够很容易地完成。[1]康奈尔大学的David Dunning和Justin Kruger于1999年首次在实验中观测到此认识偏差。</p><p> Kruger和Dunning通过对人们阅读、驾驶、下棋或打网球等各种技能的研究发现：</p><p> 1. 能力差的人通常会高估自己的技能水准；</p><p> 2. 能力差的人不能正确认识到其他真正有此技能的人的水准；</p><p> 3. 能力差的人无法认知且正视自身的不足，及其不足之极端程度；</p><p> 4. 如果能力差的人能够经过恰当训练大幅度提高能力水准，他们最终会认知到且能承认他们之前的无能程度。</p><p> Dunning和Kruger认为这种效应是由于能力欠缺者的内在错觉和能干者对外界的错误认知：“无能者的错误标度源自于对自我的错误认知，而极有才能者的错误标度源自于对他人的错误认知。”[1]</p><p> 虽然达克效应早在1999年就被Dunningt和Kruger两人提出来了，但是虚幻优越性的认知偏差是众所周知的，历史上很多知识分子都说过关于这一方面的话，比如：</p><p> * 孔子（知之为知之，不知为不知，是知也）[2]</p><p> * 威廉莎士比亚 在皆大欢喜《As you like it》（”傻瓜认为自己是明智的，而聪明的人认为自己是个傻瓜”）[3]</p><p> * 查尔斯·达尔文（“无知比知识更容易招致自信”）[4]</p><p> * 伯特兰·罗素（“我们这个时代让人困扰的事之一是: 那些对事确信无疑的人其实很蠢，而那些富有想象力和理解力的人却总是怀疑和优柔寡断”）也列为发现这个现象的人。</p><p> Dunning和Kruger因为他们的论文《论无法正确认识能力不足如何导致过高自我评价》，被授予2000年的搞笑诺贝尔奖心理学奖。[5]</p><p> 1^ Kruger, Justin; David Dunning. Unskilled and Unaware of It: How Difficulties in Recognizing One’s Own Incompetence Lead to Inflated Self-Assessments. Journal of Personality and Social Psychology. 1999, 77 (6): 1121–34. PMID 10626367. doi:10.1037/0022-3514.77.6.1121. CiteSeerX: 10.1.1.64.2655.  </p><p> 2^ Dunning, David; Johnson, Kerri; Ehrlinger, Joyce; Kruger, Justin. Why people fail to recognize their own incompetence. Current Directions in Psychological Science. 2003, 12 (3): 83–87 [4 January 2016]. doi:10.1111/1467-8721.01235. （原始内容 (abstract)存档于2016年1月14日）.</p><p> 3^ Fuller, Geraint. Ignorant of ignorance?. Practical Neurology. 2011-12-01, 11 (6): 365–365. ISSN 1474-7758. PMID 22100949. doi:10.1136/practneurol-2011-000117 （英语）.</p><p> 4^ volume I, “Introduction”, page 3. [2018-11-23].</p><p> 5^ Ig Nobel Past Winners. [2011-03-07].</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;好文转载&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/cf56ffebf9f2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/cf56ffebf9f2&lt;/a&gt;&lt;/p
      
    
    </summary>
    
      <category term="好文转载" scheme="http://zhangyu8.me/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="好文转载" scheme="http://zhangyu8.me/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>IT从运维到运营</title>
    <link href="http://zhangyu8.me/2019/10/11/IT%E4%BB%8E%E8%BF%90%E7%BB%B4%E5%88%B0%E8%BF%90%E8%90%A5/"/>
    <id>http://zhangyu8.me/2019/10/11/IT从运维到运营/</id>
    <published>2019-10-11T02:13:47.000Z</published>
    <updated>2020-01-10T06:52:27.101Z</updated>
    
    <content type="html"><![CDATA[<p>IT：从运维到运营</p><p>作者为优锘科技CEO陈傲寒</p><h2 id="IT运维？IT运营？"><a href="#IT运维？IT运营？" class="headerlink" title="IT运维？IT运营？"></a>IT运维？IT运营？</h2><p> 都是 IT Operations，有什么区别？</p><p> IT运维管理？IT运营管理？</p><p> 都是 ITOM，有什么区别？</p><p> 一字之差，只是翻译不同，还是另有玄机？</p><p> 其实，中文真的是一门更精确的语言 :-)</p><ul><li>IT运维是“活着”，IT运营是“活得好”；</li><li>IT运维更多是被动式“维持”，IT运营更多是主动式“经营”；</li><li>IT运维更多是面向基础设施面向软硬件，IT运营更多是面向业务面向服务面向人；</li><li>IT运维的关键词是“稳定”、“安全”、“可靠”；IT运营的关键词是“体验”、“效率”、“效益”；</li><li><p>IT运维管理工具更多是关注故障防范和修复的“监管控”，IT运营管理工具开始更多应用性能、用户感知、快速交付、数据分析和可视化。。。</p><p>企业IT正站在这样一个拐点上，要么从运维走向运营，要么从运维走向被代维</p><p><strong>正文之前的说明：IT运维和 IT运营都非常重要，运维是运营的基础，任何一个组织，首先是要活着，之后才要追求活得好，是 IT Operations的不同发展阶段，今天的 IT运维部门的工作内容其实包括本文所说的 IT运营。</strong></p><p>大多数ITOM领域的从业者，一直以来都约定俗成地把ITOM（IT Operation Management）翻译成IT运维管理，相应的也把IT Operations叫做IT运维。近两年来，开始有越来越多的人使用“IT运营管理”和“IT运营”这样的说法，对应的英文是一样的，但这里“运维”和“运营”是同样的意思吗？两者之间有什么异同？</p><p>关于这个问题，仁者见仁智者见智。有人认为其实运维就是运营，用个新名词只是哗众取宠的噱头而已；有人认为运维是面向IT设施的，运营是面向业务服务的；有人认为运维是关注IT指标，运营是关注业务指标的；甚至有人说，运维是“眼前的苟且”，运营是“诗和远方”:-)</p><p>总体来看，大多数人认为两者含义并不完全一样，很多人都认为IT运营比IT运维的层次更高，有些成熟度较高的大型IT组织已经提出并在执行“从IT运维到IT运营”的发展规划。但即使在提出这类理念和计划的组织内部，对于究竟什么是IT运维管理，什么是IT运营管理，也还没有非常清晰的分析和定义，更多的是将传统IT运维管理领域之外的一些新内容笼统的归到IT运营管理的部分里去。我在和某个正在执行此规划的IT组织中的某位高管交流时，他就提到：“From Operations to Operations？连定义都没搞清楚，怎么能成为指导方向和发展目标？”</p><p>他的问题让我这个ITOM的老兵也开始思考“IT运营”这个新“翻译”的真正含义，以及近几年来它日益流行的真实原因，在和许多同业交流之后，笔者在此分享一下我关于这个问题的一些想法和心得，作引玉之砖，希望能带来更多同业的讨论和指教。</p><p>首先，IT运维和IT运营，英文都是IT Operations，在老外来看，并无区别，是指关于IT运行的所有事情。而中文之所以有两种不同的翻译，是因为IT Operations包括的内容很多，IT运维和IT运营两种中文译法分别侧重其中某一部分的内容，假如归纳成一句话的话，可以说IT运维管理关注的是“活着”，而IT运营管理则有更高层次的需求，不仅要“活着”，还要“活得好”。</p><p>先看个实例，某大型数据中心IT服务能力的愿景是“以业务为中心，交付稳定、安全、高效的IT运营服务，构建业界领先的IT运营能力，支撑企业的持续发展和战略成功。”这个愿景中，“稳定、安全”就是解决活着的问题，属于传统IT运维管理的范畴，“以业务为中心”、“高效”、“业界领先”则属于如何“活得好”的范畴，更多的是IT运营管理的范畴。</p><p>能力建设是有循序渐进的过程的，任何一个组织，首先都要解决“活着”的问题，然后才有可能追求“活得好”，因此，过去三十年，在大多数IT组织面临IT设施规模快速扩张，IT应用数量不断增多，IT运行压力越来越大的挑战时，首先要确保IT系统“活着”，也就是能够持续“运行”，稳定“运转”，通过日常“维护”工作让系统少出故障，出了故障能快速“维修”，“维持”系统的正常“运转”。这个阶段把IT Operations翻译成IT运维，把ITOM翻译成IT运维管理，无可厚非。</p><p>IT运维管理阶段的关键词是“稳定”、“安全”、“可靠”，关注可用性指标（MTTR、MTTF、MTBF等）、可靠性指标（RTO、RPO）和安全合规。相应地，在技术、工具和流程上，都以稳定、安全、可靠作为最优先考虑的要素：</p></li><li><p><strong>技术上</strong>，倾向选择稳定成熟的技术架构和产品，愿意为提升可靠性支付大量溢价，上得起小型机的就上小型机，买得起大机那就大机，能备份的地方就备份，尽量采用全冗余架构；</p></li><li><strong>流程上</strong>，首先从事件管理和变更管理做起，主要目标是能确保故障事件得到追踪和及时解决，以及管控变更避免人为故障多发，关注重点还是在提升可用性；</li><li><p><strong>工具上</strong>，采用“监-管-控”架构，其中监控更关注设备级监控，重点发现故障节点，“管”就是配合实现变更和事件流程，至于“控”，此时上配置自动化工具，更关心的是实现配置的标准化和合规检查，重点还是在增强可靠性减少故障，而非减少运维人员工作量。</p><p>在以“活着”为主要目标，以“稳”为主要形态的IT运维和IT运维管理发展多年后，越来越多的IT组织开始走出这个解决基本生存需求的阶段，从“被动维持”走向“主动经营”，追求如何“活得好”，近十年来，APM、BSM、云计算、运维大数据等新的理念、技术和工具的出现、发展和变迁，都和IT正逐步开始从运维走向运营有密切关系，时至今日，从全局角度来看，可以说企业IT已经站在了从运维到运营的一个重要拐点上。</p><p>IT运营是建立在良好的IT运维的基础上的，没有“活着”，“活得好”就无从谈起。 但怎样才叫活得好呢？ 换言之，IT运营追求的目标究竟是什么？比IT运维多了哪些东西呢？</p><p>与IT运维更多地是面向基础设施不同，IT运营更多的是面向业务、面向服务，本质上是面向人。我们说某个人活得好不好，如何判断呢？大多数人认同的马斯洛需求层次理论说，在解决了基本的生存问题和安全感之后，一个人要感觉自己活得好，是需要有社会认同和自我实现的。对于CIO来说，他所管理的IT组织假如能让三类人满意，我们就可以说这个IT组织已经从基本的IT运维阶段走到IT运营阶段，已经处在活得好的状态了。</p><p>哪三类人呢？</p><p><strong>用户、老板和IT人。</strong>假如IT组织是一个独立公司的话，这三类人基本对应着客户、股东和员工，CIO如果是公司老板，就会知道其实这三类人是哪个都得罪不起的：客户不满意会流失，企业就没有生存之本；股东不满意会换人，说明企业没有竞争力；员工不满意会换地儿，企业就缺乏持久发展的能力。尽管行业特点和企业文化不同会带来优先级和侧重点的不同，但本质上，一个有长远发展前景的卓越公司，往往是做到了让客户、股东和员工都满意的公司。</p><p>IT运维阶段，IT组织更多地还是在解决三类人的基本需求，让用户能用，让老板批钱，让员工干活，当然也希望大家更满意，但受限于阶段性能力和各方面因素，先能保证这些基本需求就已经很不容易了，而做到这些，在相当长时间内也已经足够，主要因为几个原因：</p></li><li><p>各企业信息化之初，能够利用IT实现对业务和管理流程的优化、固化和自动化，就已经达到目标；</p></li><li>初期系统以内部员工为主要用户，且没有同类系统做对比，用户对系统效率和体验的容忍度高；</li><li><p>IT部门在企业内部的IT能力供给上基本是垄断的，用户没有其它选择。</p><p>因此，过去虽然IT部门提供的即使只是满足基本需求的服务，大多数情况下也并没有多大问题。但短短十年间，互联网和移动互联网大潮席卷世界的每个角落，每天用着微信滴滴淘宝携程的用户们的胃口已经越来越高了，过去能够忍受的一些小问题也已经变得忍无可忍了：</p></li><li><p>人家网站那么快，咱们的系统怎么都是老和尚，点一下鼠标要等一炷香才动一下？</p></li><li>人家网站第一次用没人教我就全部自己搞定，咱们系统怎么培训几回我都搞不清怎么用？</li><li>人家网站一看就是赏心悦目高大上，咱们系统怎么就总是Low逼的不行？</li><li><p>人家网站免费邮箱都无限容量，咱们怎么花那么多钱还每人限收发10M内邮件？</p><p>不知从哪天起，过去和企业IT八竿子打不着的“人家”一下子蹦出来，成了IT部门的变相竞争对手了，没抢走用户，但把用户满意度抢走了。更要命的是，随着云计算各种aaS的风起云涌，这些“人家”未来没准儿真的要来抢走用户了。假如IT部门不能与时俱进，还是停留在满足基本需求的运维上，而不主动向追求卓越的运营迈进，提供更有竞争力的优质IT服务，那就很可能会在几年后会碰到更大的挑战。</p><p>而在IT运营阶段，与IT运维阶段的关键词“稳定”、“安全”、“可靠”不同，关注的关键词变成了“体验”、“效率”、“效益”。回顾前面我们提到某大型数据中心的愿景中“以业务为中心”、“高效”两个运营关键词，其实“以业务为中心”就对应着“以用户为中心”，业务就是以用户为中心的吗，而用户关心的就是体验（稳定可靠也是体验的一部分）。“高效”则包含着高效率和高效益两个含义，一个关注敏捷性，交付速度、响应速度，一个关注成本收益，关注服务获取效率。</p><p>（假如说IT运维以“稳”为主，那么IT运营则以”敏“为主，在技术架构选择和IT管理流程和系统的建设上面，IT运营阶段都和传统IT运维阶段的关注重点有所转变，从而带来了新旧架构、新旧工具、新旧方法并存甚至交汇的复杂情况，Gartner在提的Bimodal，联想所说的双态IT，也都在反映这种状态。）</p><p>让我们围绕三类人的需求简单看看IT运营比之IT运维阶段要面临的新挑战，以及应对挑战在出现的一些新的理念、工具和技术：</p><h3 id="让用户满意"><a href="#让用户满意" class="headerlink" title="让用户满意"></a><strong>让用户满意</strong></h3><p>用户大致有两类，个人用户和业务部门：</p><p><strong>个人用户</strong>，不论是内部用户还是外部用户，更关心的是体验，体验主要是易用性、容错性和响应速度；要提升体验，对于IT运营管理领域就带来了新的要求，要在传统的设备和组件监控的基础上，增加端到端的用户体验感知能力、应用性能的深入探测和分析能力、应用及系统性能瓶颈的发现和优化能力。</p><p>越来越多IT组织开始关注用户体验，从而纷纷部署包括外部模拟仿真探测、流量数据分析、日志数据分析、嵌码采集探测等各种针对应用性能管理的手段工具 ，造就了近年来APM市场热度飙升。</p><p>这些采用不同手段的APM工具虽然有功能重叠的部分，但各有其侧重点，多种工具的部署能带来数据和功能的丰富性和多样性，对于准确测量和提升客户体验是有必要的，事实上在那些特别重视用户体验的IT组织里，已经或者正在进行全方位的工具部署，并在尝试在各种专业分析工具之间架设运营大数据工具，集成多样化数据，提供数据的统一可视化和整合分析等能力，提升故障和优化点的定位分析能力，深度改善用户体验。</p><p><strong>业务部门</strong>，除了关心最终用户的体验，更关心交付效率，与之相应的，IT部门开始在各个环节上采用新架构、新技术和新工具，从各个环节上提升效率，加快业务服务的交付速度。</p></li><li><p><strong>提高采购流程和硬件上架的效率</strong>：IaaS云和资源池模式改变了传统的按需采购模式，通过资源整合，将资源规划和资源准备的工作批量前移，极致地提高了预算、采购和硬件上架的效率；</p></li><li><p><strong>提高系统部署和应用发布更新的效率</strong>：采用各种云管理工具、云管理平台及DevOps工具，通过自动化部署、配置管理等功能组件的组合，或从横向的系统层次上，或从纵向的应用发布运行链条上，或者协同配合，不同程度地提高了应用组件甚至是整个业务系统的交付和发布效率，实现对业务部门交付需求的及时甚至实时响应，达到“敏捷”的程度。</p><h3 id="让老板满意"><a href="#让老板满意" class="headerlink" title="让老板满意"></a><strong>让老板满意</strong></h3><p>让用户满意是让老板满意的基础，假如业务部门天天在老板那儿告状，老板怎么都满意不了。但是即便业务部门都说你好话了，老板就会满意了吗？要是你真的这么认为，说明你太不了解老板这种动物了。</p><p>老板要的不只是结果，也一定会追求高效率和高效益，同样的成果，能否用更低的成本达成？我们现在的成本收益水平，对应业界同行，是人傻钱多还是精明高效？说要追求“业界领先”，怎么就是领先了？不能说技术更新应用更多就是领先吧？总要有个从效益角度的衡量方法吧？假如IT部门是一个独立运营的实体，作为给钱的股东，也是要问这些问题的。</p><p>效益本质上是投资回报率，成本越低，效益越好，做的事情越有用，效益越高。要追求高效益，首先面临的难题是要有一套成本收益的衡量体系，没有量化方法，既搞不清楚IT部门当前在同业中所处的水平，更无法通过指标考核的方式推动IT部门不断提高效益水平。在没有这套衡量体系的时候，往往只能采用一些非常粗线条甚至感性的衡量方式，比如看每年的IT采购金额、IT员工数量、工业标准产品的采购单价等，导致很多IT部门在采购时往往要求厂商保证提供同行业最低价，可当大家都这么要求的时候，显然很难真正起到效果。更为重要的是，由于每个企业在业务和IT服务方面存在的差异性，这些粗线条指标并不能反映IT部门的效率和效益水平。</p><p>ITIL体系中早就提出了IT服务财务管理的概念，许多IT组织在过去十年尝试了一些BSM（业务服务管理）和ITFM（IT财务管理）的项目，一个重要动因就是试图建立IT效益的衡量体系，可在内部IT部门中成功者寥寥，主要原因是全部精力投入到基础运维工作中还忙不过来，另一方面也和缺乏特别成功的最佳实践有关。</p><p>不过随着大家的不断尝试，伴随近年来IT架构的演进和公有云的兴起，一些走在前面的IT部门已经看到了建立IT效益衡量体系的可能性，并开始在某些架构层级上开始尝试性的探索：他们采用服务分层、成本归集、各自对标的方式，对DC层、IaaS层、PaaS层的资源单位成本、资源利用效率、能源单位成本、能源利用效率和人员运营效率进行分别统计和分析，并分别和IDC、IaaS云、PaaS云的外部供应商市场价位水平做对照，来衡量自己的效率和效益水平。</p><p>IT效益衡量体系的建立，也让IT自己可以从效益角度分解目标，推动IT内各个部门能够逐年不断提升效率和效益水平，让IT部门的思考方式从成本中心转变到利润中心。近年来绿色数据中心概念和PUE指标被关注，都反映了这一变化趋势。</p><p>要注意的是，即使建立了效益衡量体系，要让它真正发挥作用，离不开大量的数据统计和数据分析，以及关键效益指标的可视化和透明化，很多IT组织开始尝试建立IT运维/运营大数据平台，引入可视化和BVD概念，也都和追求IT效益可衡量有密切关系。而这些也会带来额外的投入，IT组织可以根据自身的规模和目标优先级，在有必要的情况下，选择合适和成熟的切入点，分步尝试，逐渐建立效益衡量体系。</p><h3 id="让员工满意"><a href="#让员工满意" class="headerlink" title="让员工满意"></a><strong>让员工满意</strong></h3><p>互联网企业的火热和各行业互联网+的热闹，都带来了IT人才的争夺，如何吸引和保留高素质的IT员工，已经成为许多IT部门不得不面对的新问题。要让IT员工满意，前面的两个满意（用户满意和老板满意）也是个重要基础，否则IT部门自己地位都不高，员工也没有成就感，士气低迷，满意度很难高起来。</p><p>但即使做到了前面两个满意，假如让IT员工每天都疲于奔命，员工满意度同样会差，也不是长久之计。要解决员工满意度的问题，有几个方面是要考虑到的：</p></li><li><p><strong>提高自动化水平</strong>：与运维阶段自动化更关注的是让标准化落地以减少故障不同，运营阶段更关注通过自动化减少员工的重复性劳动，更多地将精力放在能带来更大价值的标准制定和技术优化上面，让IT员工从技术工人变成真正的工程师；（自动化也会带来效益的提升，随着分布式、虚拟化和云计算的普及，自动化已经成为不可或缺的手段，在一些大型互联网公司，人均管理服务器数量早已超过了业界1:200的良好水平）</p></li><li><p><strong>增加人性化因素</strong>：传统运维阶段为了稳定安全不但在软硬件上投入巨大，而且往往在某种程度上不惜增加员工工作的繁琐程度，在人性化方面考虑较少。不少IT组织已经开始从几个方面进行改善：优化流程并引入新工具以减少员工的繁琐文案工作；提供场景化运维能力改善工具的易用性，让IT人员在运维和排障工作中更得心应手，提高IT系统稳定性的同时形成以工作场景为中心的运维方式；与时俱进引入新技术，在保持安全和风控水平的同时改善IT人员的操作复杂度（比如打破僵硬的网络隔离机制、实现移动化运维等）；</p></li><li><p><strong>尝试和引入先进技术</strong>：为追求稳定安全，传统IT运维在技术选择和使用上偏向保守，这固然有其道理，但优秀的IT人往往是对新技术有追求的，在技术演进日新月异、新技术传播和应用速度如飞的今天，假如工作中接触不到新技术新思路，IT人的技术追求被压抑，并往往会伴生强烈的技术危机感，会导致对IT人才吸引力和保持力不够。IT部门应在技术规划中重视这一因素，在保证关键业务稳定运行的前提下，有意识有计划地不断尝试和引进新技术，确保技术的先进性，抛开其它收益不谈，但就提高员工满意度和优秀人才吸引力而言，已经是非常值得的。</p><p>以上从三个满意的角度简单聊了聊从IT运维到IT运营的一些内容，有趣的是，这些满意是递进和包含的关系，让员工满意包括让老板满意，让老板满意包括让用户满意，让业务部门满意包括让个人用户满意，但每个满意之间又都有各自的个性化内容。</p><p>要做到三个满意，让IT从“活着”到“活得好”，从重点“维”稳走向经营业务价值，意味着IT管理要更加精细化、自动化、智能化，也必须建立多样化的数据采集、多维度的数据分析/挖掘和全方位的可视化的能力，IT运营管理的架构也将在传统监管控的IT运维管理架构上有所发展和变化，以适应IT运营在体验、效率和效益方面的更多要求。</p><p>需要注意的是，IT涉及到规划、设计、开发和运营多个环节，我们更多的是从运营的角度来谈的，事实上要从IT运维走向IT运营，不仅需要运营部门（不再只是运维部门啦）的努力，也需要规划、管理和开发部门的协同配合和齐头并进。</p><p>从IT运维到IT运营，其实标志着IT组织成熟度的提升，假如借用Gartner的I&amp;O成熟度模型来看的话，IT运维更多是在前几个阶段，而更多开始关注IT运营，则标志着IT组织走到了后两个阶段：Service Aligned和Business Partnership，开始把IT本身当做业务来运营，以客户为中心，关注客户体验，运营效率和成本收益。</p><p>以上是关于IT运维到IT运营的一些不成熟的思考，抛砖引玉，希望能得到大家的批评和指教。</p><p>从IT运维到IT运营，许多IT组织已经在路上，同样也有许多IT产品和IT服务的提供商已经洞悉到这一发展趋势，配合IT运营的要求，开发和提供了许多新的运营工具和运营服务，我们希望能够与各位有志于ITOM领域的同仁们一起，齐心协力，精益求精，共同提供优秀的ITOM产品和服务，为IT从运维到运营做一点事情，让IT不仅活着，而且要活得好，活得精彩。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;IT：从运维到运营&lt;/p&gt;
&lt;p&gt;作者为优锘科技CEO陈傲寒&lt;/p&gt;
&lt;h2 id=&quot;IT运维？IT运营？&quot;&gt;&lt;a href=&quot;#IT运维？IT运营？&quot; class=&quot;headerlink&quot; title=&quot;IT运维？IT运营？&quot;&gt;&lt;/a&gt;IT运维？IT运营？&lt;/h2&gt;&lt;p&gt;
      
    
    </summary>
    
      <category term="好文转载" scheme="http://zhangyu8.me/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="好文转载" scheme="http://zhangyu8.me/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>世界上没有技术驱动型公司</title>
    <link href="http://zhangyu8.me/2019/10/10/%E4%B8%96%E7%95%8C%E4%B8%8A%E6%B2%A1%E6%9C%89%E6%8A%80%E6%9C%AF%E9%A9%B1%E5%8A%A8%E5%9E%8B%E5%85%AC%E5%8F%B8/"/>
    <id>http://zhangyu8.me/2019/10/10/世界上没有技术驱动型公司/</id>
    <published>2019-10-10T07:13:47.000Z</published>
    <updated>2020-01-10T06:50:42.182Z</updated>
    
    <content type="html"><![CDATA[<p> 别傻了，你还真相信世界上有技术驱动型公司？ </p><p> 世界上没有技术驱动型公司，不论Google、Facebook，还是腾讯、阿里，都不是技术驱动型公司。<strong>因为技术不是源头，需求才是。</strong></p><p> 因此一切技术问题，都要服从产品交付和市场反馈。所以，任何公司都不可能以技术去驱动自身。人可以以技术驱动自己进步，但公司不行。</p><p> 一家公司可以以技术切入某个市场，但如果它想生存下去，就一定不能以技术为导向，坚持以技术为导向的公司的生命力为零，其下场有两个：破产或者在破产之前被收购。</p><p> 如果你真的很痴迷钻研技术，请读研读博最后留校，或者进研究院让国家用纳税人的钱养你。</p><p> <strong>0x01</strong></p><p> <strong>每个人都得加班</strong></p><p> 资本富集的地方，人都得加班，加班的本质，是人跟着机器跑、人跟着钱跑。</p><p> 更为本质地说，资本富集的地方，人作为劳动力，也是资本的一种。<strong>即人是资本而不是人本身。</strong></p><p> 资本的运转是不能停的，因为停一下损失的钱太多了，中国和外国都一样。</p><p> 知道发达国家为什么产业工人不加班吗？因为制造业已经不是这些国家主要创造财富的领域了。</p><p> 发达国家资本富集的地方是金融行业，所以西方国家的金融狗一样加班。</p><p> 劳动法？加班费？都不存在的。劳动法和加班费只有在资本离开这个市场后才能给你保证。</p><p> 一般公司的策略是：付给你高于其他行业的薪水、换取你“自愿”加班。不想加班的同学们，你们可以去考公务员或者去欧洲做IT，我保证你不加班、不但不用加班，你甚至会很闲。</p><p> <strong>0x02</strong></p><p> <strong>先想后写</strong></p><p> IT是工科，不是理科，和IT行业相似度最高的行业是盖楼房。真的，相似度相当惊人。</p><p> IT领域最重要的是经验，而不是你有多聪明，不聪明的人，或者更准确地说，不适合做这个行业的人，大学毕业后就改行了。</p><p> 记住：你做得好不好，不取决于你是否聪明，而取决于你是否愿意不断读书、不断学习和不断积累。因此，如果你打算投身这个行业，还在学校的话就请抓紧一切时间多读书。</p><p> 公司是你创造财富的地方，公司不是学校。你可以在工作中学习，但你不能放下工作然后去学习，除非你的工作已经做完了。</p><p> <strong>能大规模商用的技术，都不需要智商，否则这种技术就不可能规模化</strong>。某些程序员们，请停止你们的蜜汁自信。</p><p> 技术栈，一旦确立了，就很难改了。一个技术人员是如此，一家公司也是如此。根本原因是：每一个栈的size都太深了，就像是ulimit -s unlimited过一样。</p><p> 一个程序员，应该花80%的时间做代码设计、画UML图、画时序图，20%的时间写Code和Debug，菜鸟程序员的这个比例恰好是反的。</p><p> 一句话，不论这个需求有多紧急，你都一定要“想好再动手”。“想好”的标志就是设计文档写好了，文档一旦写好，写代码就是纯粹的无脑工作。</p><p> 写文档的目的是让你在Code的时候，不需要停下来思考，更不需要推倒重来。如果没有文档也可以做到这一点，你当然可以不写文档，同时思考下自己水平这么高是不是可以要求升职加薪了。</p><p> 或者，你是不是在做无聊的if else编码工作？</p><p> <strong>0x03</strong></p><p> <strong>关注软技能</strong></p><p> 英语，很重要。能否使用英语查阅资料，是区分技术人员水平的重要指示之一。寄希望于“有人迟早会翻译成中文”的人是愚蠢的、是会被淘汰的。</p><p> <strong>要有分享精神，不要担心你知道的东西告诉别人后你就没价值了。你最大的价值在于你知道那些东西的过程，而不是那些东西本身。</strong></p><p> 你愿意和别人分享，别人自然也会愿意和你分享，最终达到1+1大于2的效果。</p><p> 不分享，就像一个失去了互联网的程序员，试问他还能创造多少价值？恐怕他连日常工作都无法展开了。</p><p> 持有“我把别人知道的都学会，把自己知道的都藏起来，别让别人学去”想法的人，其实是默认全世界只有你聪明别人都是傻瓜，这样的人，在信息传输成本高的时代，可以活下去，但是在今天这个时代，他们的路会越走越窄，最后会自己走入死胡同。</p><p> 当然，如果你真的知道了了不得的黑科技，那就请你保护好自己的知识产权，然后自己开公司玩吧。</p><p> <strong>0x04</strong></p><p> <strong>工作要有热情</strong></p><p> 智商决定你的起点，情商决定你能走多远爬多高。混职场，靠的是情商。</p><p> 情商高就是：别人愿意和你一起工作、你有问题的时候别人愿意帮你。智商有时候可以稍微弥补一下情商，但不起决定性的作用。</p><p> 现代管理学的精髓，就是让每个人（包括老板本人）都变得可替代。如果你觉得自己不可替代，要么是你的错觉，要么是你在一家管理非常原始的、摇摇欲坠马上要完蛋的公司。</p><p> <strong>0x05</strong></p><p> <strong>写好文档</strong></p><p> 怎样让程序员变得可替代？三个字：写文档。</p><p> 不愿意写文档的程序员，应该立刻马上毫不犹豫地开掉。程序员工作创造的价值，至少一半是通过文档体现出来才对。</p><p> “一个项目换一个人就要让项目大地震一下”，“解决Bug换一个人就不行，因为只有老人知道要改哪一行的哪个关键字”，这不说明这个项目所涉及的技术有多复杂、不说明这个老人是什么技术大牛，而只说明这个项目的项目经理很蠢，这个项目已经失控了。</p><p> 文档不是事无巨细的流水账，写文档以及组织文档是需要智商的、是需要架构师去设计的。</p><p> 美国的航天飞机那么复杂，但是在Pilot手里的手册也就那么多，而这个手册可以在航天飞机出问题的时候协助Pilot快速定位绝大多数问题。</p><p> <strong>不可替代的打工者只有一种：以中高层领导的身份跟完了一个项目，而且这个项目正处于大红大紫的阶段</strong>，公司为了防止你跳槽到竞争对手那里，愿意付给你薪水，养着你天天在办公室喝茶。只要项目一直红着，公司就愿意一直养着你。</p><p> <strong>0x06</strong></p><p> <strong>开发人员的文档的作用</strong></p><p> 给正在Code的自己看、给几个月后已经忘记这个模块当初是怎么开发的自己看、给要接手自己工作的新人看、给周边有关联开发任务的同事看、给领导等有关人员看，这是产品出bug的时候用来和别人怼的武器。</p><p> 如果没有文档，这些工作量都会成倍增长。</p><p> 代码再精简再直观，也不可能有人类语言直观，谁觉得自己厉害到读代码和读人类语言写的文档速度一样快，那我给你一个我上大学时候写的小程序，麻烦你读一下代码，看看你多长时间可以看明白。</p><p> 参考链接：<a href="https://github.com/YvesZHI/FallingCode" target="_blank" rel="noopener">https://github.com/YvesZHI/FallingCode</a></p><p> 这段代码本身并不复杂，应该说非常简单，但是没有文档……读读看吧。</p><p> 简而言之，文档，就像盖楼房的设计图，没有图纸，你是不能开始搬砖的。</p><p> 领导有没有给你看需求分析文档？有没有拿着需求分析文档给你宣讲你要做什么？没有？不干活。</p><p> 测试的同事有没有给你看测试用例文档？有没有给你宣讲？没有？不干活。</p><p> 你自己明白领导的意图了吗？明白测试同事的意图了吗？想明白后，开始想自己要开发的模块里的各个功能模块之间的关系，可以画时序图。</p><p> 时序图画完了，看看是否有（可能）频繁变化的模块/需求，如果有，请务必使用一些设计模式，如果要用设计模式，请务必画UML类图，如果没有频繁变化的模块/需求，请一定不要用设计模式。</p><p> 最后，看看在一个功能模块中，有没有逻辑比较复杂的地方，如果有，请画流程图。</p><p> 模块和模块之间有没有需要明确的协议？如果有，请把协议写出来。</p><p> 上面这一段话，就是你要写的文档，这个文档的读者主要是你，在你的模块出问题之前，别人通常不会读这个文档（不排除你的领导会要求看你这个文档）。</p><p> 如果你既不需要时序图又不需要类图又没什么协议需要明确，那么，你就可以不写这个文档。另外，如果这个文档写得好，你的代码是不需要任何注释的。</p><p> <strong>0x07</strong></p><p> <strong>技术驱动</strong></p><p> 如果一家公司打着“我们是技术驱动型公司”的名号在招人，我劝你一定要想好考察好，再决定是否去这家公司。</p><p> 为什么呢？因为我知道他的那句“技术驱动”很吸引你，你<strong>想学东西，但是对小公司来说，它最大的任务是活下去，然后才是其他。</strong></p><p> 我不是说小公司学不到东西，我只是说小公司很难很难做到真正的技术驱动。</p><p> 有人坚持认为微软这种公司是技术驱动，但微软从没大张旗鼓地说自己是“技术驱动”公司，并以此忽悠新人。</p><p> 以华为为例。华为成功的内在原因，早就敲锣打鼓地告诉全世界了：以客户为中心，以奋斗者为本，长期艰苦奋斗，坚持自我批判。</p><p> 这四句话，没一句是直接和技术相关的。</p><p> 这里我先特别声明一下，我不是说，技术人员在华为就不会搞技术、不会提升自己的技术水平、华为的技术水平差。我绝不是这个意思。</p><p> 华为的技术，不需要我多说，全世界的人都是有目共睹的，华为公司的技术专利数就摆在那里，那是谁也抹杀不了的，华为公司里的技术大牛多了去了。</p><p> 但在这里，我要说的还是第一段的意思：一个人可以以技术驱动，但一家公司不行。</p><p> 华为公司的核心理念，本质就是“成就客户”，你把客户成就了，你就把自己成就了，华为不是先成就自己再去成就客户的公司。</p><p> 你去华为工作，你可以以技术驱动自己，但华为不能这样做。</p><p> 这一点和微软与IBM的合作极其相似：IBM说，你们微软现在搞的东西我愿意用，但是我需要你们给我搞个操作系统，这样我们才能继续合作。</p><p> 然后微软怎么做的呢？它马上购买了另外一家公司搞的DOS操作系统，然后直接授权给IBM使用。</p><p> 这里面有四个问题值得思考：</p><ol><li><p>为什么那家开发DOS的公司没能直接和IBM合作？</p></li><li><p>微软购买DOS系统的钱哪里来的？</p></li><li><p>微软为什么不自己开发操作系统？</p></li><li><p>技术在前三个问题中的角色和作用是什么？</p></li></ol><p> 至于有人说Intel是技术驱动公司，我建议大家可以去了解一下Intel为什么放弃了手机市场：重点关注Intel决定放弃手机市场的原因，你就会发现，这个原因的本质，就是一种技术情节的产物。</p><p> Intel放弃手机市场与华为决定进军手机市场是截然不同的。华为本来是做基站、路由器和交换机的，这是它的主营业务。</p><p> 那么华为为什么决定进入手机市场？是什么原因驱使华为在没有任何技术积累的前提下进入手机市场？以至于最初华为的手机被华为员工戏称为“暖手宝”，倒贴钱都没人愿意用，而现在却如此成功？</p><p> <strong>所以，我还是那个观点：世界上没有技术驱动型公司。</strong></p><p> 我本人就是程序员，我一直都以技术在驱动自己，努力提升自己的技术水平。但是我还是要说：世界上没有技术驱动型公司。</p><p> 一个新的team要开发一款软件，它首先要解决的问题，是在产品1.0开发出来并且赚到钱之前这个team的经费。</p><p> 其次，它要提前找好产品的客户群和可能存在的销售渠道，并且做完相应的工作。</p><p> 再次，它要做产品规划，如什么时候出1.0版本的产品、哪个模块开发大概要多久、什么类型的问题可以暂时搁置、什么类型的问题不能搁置、要组织公关组公关等（全是项目管理相关内容，和技术没有直接关系）。</p><p> 最后，进入产品开发阶段。一旦进入产品开发，就像工厂的流水线一样，是不可能出现什么导致产品开发进行不下去的技术难点的（否则技术leader就是白痴，这种产品在头脑风暴阶段就应该被拍死才对）。</p><p> <strong>所以，“期望出现决定产品生死的技术难点，然后自己nb闪闪地搞定”这种事情，是不可能发生的。</strong></p><p> 同时，在开发过程中，难免出现各种意料之外的bug，比如，你负责的模块出现了三个bug，其中一个是必现问题，且直接影响功能实现，那这是一定要搞定的，如果你搞不定，team会找其他老手和你一起攻关。</p><p> 攻关结果有两种，一种是bug解决了，但是不知道为什么；另一种是bug解决了也知道了是为什么。</p><p> 对于第一种情况，team是不会为了找到原因而让你潜心研究几个月的，为什么？</p><p> 因为你还有后续工作要完成，而这个bug已经解决了，不影响用户使用了。</p><p> 什么时候才有可能让你继续跟进这个问题呢？1.0版本的产品市场反馈符合预期，且公司决定要继续投入2.0版本 ——只有这个条件满足，你才有可能继续跟进这个问题，为什么是有可能呢？</p><p> 因为这个bug已经不影响客户使用了，没必要投入人力去研究了，你如果花几个月的时间去找这个bug的原因，那么请问：2.0版本的工作谁做？</p><p> 在很多项目中，类似这种“问题解决了但是不知道原因”的bug，是比较常见的，很多时候，直到这个产品生命周期结束，这些bug的原因都没有找到。</p><p> 因此，“期望碰到神秘bug，然后自己潜心研究几个月，终于把原因找到”这种事情，很多时候是不存在的。</p><p> 接着上面的“三个bug”继续：另外两个bug，是概率发生且发生概率很低。</p><p> 这个时候如果工期比较赶，公司会想办法绕过这两个bug，比如定时重启服务器、定时清理缓存等（这些方法通常可以绕开低概率bug），不会给你“潜心研究三个月然后把bug解决”的机会的。</p><p> 什么时候才有可能让你继续研究这两个bug呢？和第一个bug一样，只有后续继续开发，才有可能让你继续跟进。</p><p> <strong>现在，请各位再重新品味一下“技术驱动”这个词。到底什么是技术驱动？</strong></p><p> 其实这个词真正的含义就是：我们公司效益很好，能养活nb的技术团队，所以产品能不断迭代演进开发，随着产品的不断迭代，技术人员有可能会遇到一些其他公司遇不到的问题。</p><p> 所以，如果一家新成立的小公司说自己是技术驱动的……连1.0版本的产品都没有，就敢说自己是技术驱动？你信吗？不管你信不信，反正我不信。</p><p> <strong>简而言之，“技术驱动”的同义词就是“我们公司很有钱”+“我们公司不是炒股炒房而是做产品的公司”。</strong></p><p> 至于为什么不直接这么说呢？这是因为这种说法不容易被十年寒窗苦读、潜心研究技术的同学接受……</p><p> 被“技术驱动”迷惑的同学，其实就是读书读傻了，什么叫“读书读傻了”？就是把社会和学校等同成同样的东西……</p><p> “很有钱的做IT产品的公司”，这个世界上当然是有的，但是这样的公司，根本不会用“技术驱动”这种词来忽悠新人。</p><p> 最后，隔行如隔山，但隔行不隔理。如果你读完上面的东西，对自己所处的行业有了进一步的认识，我以为，是很正常的。</p><p> 来源：<a href="https://www.zhihu.com/question/312019918/answer/608965942" target="_blank" rel="noopener">https://www.zhihu.com/question/312019918/answer/608965942</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 别傻了，你还真相信世界上有技术驱动型公司？ &lt;/p&gt;
&lt;p&gt; 世界上没有技术驱动型公司，不论Google、Facebook，还是腾讯、阿里，都不是技术驱动型公司。&lt;strong&gt;因为技术不是源头，需求才是。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; 因此一切技术问题，都要服从产品
      
    
    </summary>
    
      <category term="好文转载" scheme="http://zhangyu8.me/categories/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="好文转载" scheme="http://zhangyu8.me/tags/%E5%A5%BD%E6%96%87%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>HTTP的前世今生</title>
    <link href="http://zhangyu8.me/2019/10/09/HTTP%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
    <id>http://zhangyu8.me/2019/10/09/HTTP的前世今生/</id>
    <published>2019-10-08T16:00:00.000Z</published>
    <updated>2020-06-28T08:11:24.746Z</updated>
    
    <content type="html"><![CDATA[<p>HTTP的前世今生</p><p>( Chrome、Firefox 和 Cloudflare 均已支持 HTTP/3 )]</p><p> 来源：_酷壳_</p><p> 原文：<em><a href="https://url.cn/56Z548W" target="_blank" rel="noopener">https://url.cn/56Z548W</a></em></p><p> HTTP (Hypertext transfer protocol) 翻译成中文是超文本传输协议，是互联网上重要的一个协议。由欧洲核子研究委员会 CERN 的英国工程师 Tim Berners-Lee v 发明的，同时他也是 WWW 的发明人，最初的主要是用于传递通过 HTML 封装过的数据。</p><p> 在 1991 年发布了 HTTP 0.9 版，在 1996 年发布 1.0 版。1997 年是 1.1 版，1.1 版也是到今天为止传输最广泛的版本（初始 RFC 2068 在 1997 年发布， 然后在 1999 年被 RFC 2616 取代，再在 2014 年被 RFC 7230/7231/7232/7233/7234/7235 取代）。</p><p> 2015 年发布了 2.0 版，其极大的优化了 HTTP/1.1 的性能和安全性，而 2018 年发布的 3.0 版，继续优化 HTTP/2，激进地使用 UDP 取代 TCP 协议。</p><p> 目前，HTTP/3 在 2019 年 9 月 26 日 被 Chrome、Firefox、和 Cloudflare 支持。所以我想写下这篇文章，简单地说一下 HTTP 的前世今生，让大家学到一些知识，并希望可以在推动一下 HTTP 标准协议的发展。</p><p> <strong>_1_</strong></p><p> <strong>HTTP 0.9 / 1.0</strong></p><p> 0.9 和 1.0 这两个版本，就是最传统的 Request – Response 的模式了。HTTP 0.9 版本的协议简单到极点，请求时不支持请求头，只支持 GET 方法，没了。HTTP 1.0 扩展了 0.9 版，其中主要增加了几个变化：</p><ul><li><p>在请求中加入了 HTTP 版本号，如：GET /coolshell/index.html HTTP/1.0</p></li><li><p>HTTP 开始有 Header了，不管是 Request 还是 Response 都有 Header 了。</p></li><li><p>增加了 HTTP Status Code 标识相关的状态码。</p></li><li><p>还有 Content-Type 可以传输其它的文件了。</p></li></ul><p> 我们可以看到，HTTP 1.0 开始让这个协议变得很文明了，一种工程文明。因为：</p><ul><li><p>一个协议有没有版本管理，是一个工程化的象征。</p></li><li><p>Header 可以说是把元数据和业务数据解耦，也可以说是控制逻辑和业务逻辑的分离。</p></li><li><p>Status Code 的出现可以让请求双方以及第三方的监控或管理程序有了统一的认识。最关键是还是控制错误和业务错误的分离。</p></li></ul><p> 注：国内很多公司 HTTP 无论对错只返回 200，这种把 HTTP Status Code 全部抹掉完全是一种工程界的倒退。</p><p> 但是，HTTP 1.0 性能上有一个很大的问题，那就是每请求一个资源都要新建一个 TCP 链接。而且是串行请求，所以就算网络变快了，打开网页的速度也还是很慢。所以，HTTP 1.0 应该是一个必须要淘汰的协议了。</p><p> <strong>_2_</strong></p><p> <strong>HTTP/1.1</strong></p><p> HTTP/1.1 主要解决了 HTTP 1.0 的网络性能的问题，以及增加了一些新的东西：</p><ul><li><p>可以设置 Keepalive 来让 HTTP 重用 TCP 链接，重用 TCP 链接可以省了每次请求都要在广域网上进行的 TCP 的三次握手的巨大开销。这是所谓的 “HTTP 长链接” 或是 “请求响应式的 HTTP 持久链接”。英文叫 HTTP Persistent Connection.</p></li><li><p>然后支持 Pipeline 网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。（注：非幂等的 POST 方法或是有依赖的请求是不能被 Pipeline 化的）</p></li><li><p>支持 Chunked Responses，也就是说，在 Response 的时候，不必说明 Content-Length 这样，客户端就不能断连接，直到收到服务端的 EOF 标识。这种技术又叫 “服务端 Push 模型”，或是 “服务端 Push 式的 HTTP 持久链接”</p></li><li><p>还增加了 Cache Control 机制。</p></li><li><p>协议头增加了 Language、Encoding、Type 等等头，让客户端可以跟服务器端进行更多的协商。</p></li><li><p>还正式加入了一个很重要的头 —— HOST 这样的话，服务器就知道你要请求哪个网站了。因为可以有多个域名解析到同一个 IP 上，要区分用户是请求的哪个域名，就需要在 HTTP 的协议中加入域名的信息，而不是被 DNS 转换过的 IP 信息。</p></li><li><p>正式加入了 OPTIONS 方法，其主要用于 CORS – Cross Origin Resource Sharing 应用。</p></li></ul><p> HTTP/1.1 应该分成两个时代，一个是 2014 年前，一个是 2014 年后。因为 2014 年 HTTP/1.1 有了一组 RFC（7230 /7231/7232/7233/7234/7235），这组 RFC 又叫 “HTTP/2 预览版”。其中影响 HTTP 发展的是两个大的需求：</p><ul><li><p>一个需要是加大了 HTTP 的安全性，这样就可以让 HTTP 应用得广泛。比如，使用 TLS 协议。</p></li><li><p>另一个是让 HTTP 可以支持更多的应用，在 HTTP/1.1 下，HTTP 已经支持四种网络协议：</p></li></ul><ul><li><p>传统的短链接。</p></li><li><p>可重用 TCP 的的长链接模型。</p></li><li><p>服务端 Push 的模型。</p></li><li><p>WebSocket 模型。</p></li></ul><p> 自从 2005 年以来，整个世界的应用 API 越来多，这些都造就了整个世界在推动 HTTP 的前进。我们可以看到，自 2014 的 HTTP/1.1 以来，这个世界基本的应用协议的标准基本上都是向 HTTP 看齐了。也许 2014 年前，还有一些专用的 RPC 协议。但是 2014 年以后，HTTP 协议的增强，让我们实在找不出什么理由不向标准靠拢，还要重新发明轮子了。</p><p> <strong>_3_</strong></p><p> <strong>HTTP/2</strong></p><p> 虽然 HTTP/1.1 已经开始变成应用层通讯协议的一等公民了，但是还是有性能问题，虽然 HTTP/1.1 可以重用 TCP 链接，但是请求还是一个一个串行发的，需要保证其顺序。然而，大量的网页请求中都是些资源类的东西，这些东西占了整个 HTTP 请求中最多的传输数据量。所以，理论上来说，如果能够并行这些请求，那就会增加更大的网络吞吐和性能。</p><p> 另外，HTTP/1.1 传输数据时，是以文本的方式。借助耗 CPU 的 Zip 压缩的方式减少网络带宽，但是耗了前端和后端的 CPU。这也是为什么很多 RPC 协议诟病 HTTP 的一个原因，就是数据传输的成本比较大。</p><p> 其实，在 2010 年时，Google 就在搞一个实验型的协议，这个协议叫 SPDY。这个协议成为了 HTTP/2 的基础（也可以说成 HTTP/2 就是 SPDY 的复刻）。HTTP/2 基本上解决了之前的这些性能问题，其和 HTTP/1.1 最主要的不同是：</p><ul><li><p>HTTP/2 是一个二进制协议，增加了数据传输的效率。</p></li><li><p>HTTP/2 是可以在一个 TCP 链接中并发请求多个 HTTP 请求，移除了 HTTP/1.1 中的串行请求。</p></li><li><p>HTTP/2 会压缩头，如果你同时发出多个请求，他们的头是一样的或是相似的。那么，协议会帮你消除重复的部分。这就是所谓的 HPACK 算法（参看 RFC 7541 附录 A）</p></li><li><p>HTTP/2 允许服务端在客户端放 Cache，又叫服务端 Push，也就是说，你没有请求的东西，我服务端可以先送给你放在你的本地缓存中。比如，你请求 X，我服务端知道 X 依赖于 Y，虽然你没有的请求 Y，但我把 Y 跟着 X 的请求一起返回客户端。</p></li></ul><p> 对于这些性能上的改善，在 Medium 上有篇文章 “ HTTP/2: the difference between HTTP/1.1, benefits and how to use it (<a href="https://url.cn/5Ij0hXz" target="_blank" rel="noopener">https://url.cn/5Ij0hXz</a>) ” 你可看一下相关的细节说明和测试。</p><p> 当然，还需要注意到的是 HTTP/2 的协议复杂度比之前所有的 HTTP 协议的复杂度都上升了许多许多。其内部还有很多看不见的东西，比如其需要维护一个 “优先级树” 来用于来做一些资源和请求的调度和控制。如此复杂的协议，自然会产生一些不同的声音，或是降低协议的可维护和可扩展性。所以也有一些争议。尽管如此，HTTP/2 还是很快地被世界所采用。</p><p> HTTP/2 是 2015 年推出的。其发布后，Google 宣布移除对 SPDY 的支持，拥抱标准的 HTTP/2。过了一年后，就有 8.7% 的网站开启了 HTTP/2，根据这份报告 (<a href="https://url.cn/5YOuflM" target="_blank" rel="noopener">https://url.cn/5YOuflM</a>) ，截止至本文发布时（2019 年 10 月 1 日）， 在全世界范围内已经有 41% 的网站开启了 HTTP/2。</p><p> HTTP/2 的官方组织在 Github 上维护了一份各种语言对 HTTP/2 的实现列表，大家可以去看看。</p><p> 我们可以看到，HTTP/2 在性能上对 HTTP 有质的提高。所以，HTTP/2 被采用的也很快。如果你在你的公司内负责架构的话，HTTP/2 是你一个非常重要的需要推动的一个事。除了因为性能上的问题，推动标准落地也是架构师的主要职责。因为，你企业内部的架构越标准，你可以使用到开源软件，或是开发方式就会越有效率。跟随着工业界的标准的发展，你的企业会非常自然的享受到标准所带来的红利。</p><p> <strong>_4_</strong></p><p> <strong>HTTP/3</strong></p><p> 然而，这个世界没有完美的解决方案。HTTP/2 也不例外，其主要的问题是：若干个 HTTP 的请求在复用一个 TCP 的连接，底层的 TCP 协议是不知道上层有多少个 HTTP 的请求的。所以，一旦发生丢包，造成的问题就是所有的 HTTP 请求都必须等待这个丢了的包被重传回来，哪怕丢的那个包不是我这个 HTTP 请求的。因为 TCP 底层是没有这个知识了。</p><p> 这个问题又叫 Head-of-Line Blocking 问题，这也是一个比较经典的流量调度的问题。这个问题最早主要的发生的交换机上。下图来自 Wikipedia。</p><p> <img src="https://coolshell.cn/wp-content/uploads/2019/10/HOL_blocking.png" alt></p><p> 图中，左边的是输入队列。其中的 1、2、3、4 表示四个队列，四个队列中的 1、2、3、4 要去右边的 Output 的端口号。此时，第一个队列和第三个队列都要写右边的第四个端口。然后，一个时刻只能处理一个包。所以，一个队列只能在那等另一个队列写完。其此时的 3 号或 1 号端口是空闲的，而队列中的要去 1 和 3 号端口号的数据，被第四号端口给 Block 住了。这就是所谓的 HOL Blocking 问题。</p><p> HTTP/1.1 中的 Pipeline 中如果有一个请求 Block 了，那么队列后请求也统统被 Block 住了；HTTP/2 多请求复用一个 TCP 连接，一旦发生丢包就会 Block 住所有的 HTTP 请求。这样的问题很讨厌。好像基本无解了。</p><p> 是的 TCP 是无解了，但是 UDP 是有解的 ！于是 HTTP/3 破天荒地把 HTTP 底层的 TCP 协议改成了 UDP！</p><p> 然后又是 Google 家的协议进入了标准 – QUIC （Quick UDP Internet Connections）。接下来是 QUIC 协议的几个重要的特性，为了讲清楚这些特性，我需要带着问题来讲（注：下面的网络知识，如果你看不懂的话，你需要学习一下 《TCP/IP 详解》 一书（ 在我写 Blog 的这 15 年里，这本书推荐了无数次了），或是看一下本站的 《 TCP 的那些事》。）：</p><ul><li><p>首先是上面的 Head-of-Line Blocking 问题，在 UDP 的世界中，这个就没了。这个应该比较好理解，因为 UDP 不管顺序，不管丢包（当然，QUIC 的一个任务是要像 TCP 的一个稳定，所以 QUIC 有自己的丢包重传的机制）</p></li><li><p>TCP 是一个无私的协议，也就是说，如果网络上出现拥塞，大家都会丢包，于是大家都会进入拥塞控制的算法中。这个算法会让所有人都 “冷静” 下来，然后进入一个 “慢启动” 的过程，包括在 TCP 连接建立时，这个慢启动也在，所以导致 TCP 性能迸发地比较慢。QUIC 基于 UDP，使用更为激进的方式。同时，QUIC 有一套自己的丢包重传和拥塞控制的协议，一开始 QUIC 是重新实现 TCP 的 CUBIC 算法。但是随着 BBR 算法的成熟（BBR 也在借鉴 CUBIC 算法的数学模型），QUIC 也可以使用 BBR 算法。这里，多说几句，从模型来说，以前的 TCP 的拥塞控制算法玩的是数学模型，而新型的 TCP 拥塞控制算法是以 BBR 为代表的测量模型。理论上来说，后者会更好，但 QUIC 的团队在一开始觉得 BBR 不如 CUBIC 的算法好，所以没有用。现在的 BBR 2.x 借鉴了 CUBIC 数学模型让拥塞控制更公平。这里有文章大家可以一读 “TCP BBR : Magic dust for network performance. ”</p></li><li><p>接下来，现在要建立一个 HTTPS 的连接。先是 TCP 的三次握手，然后是 TLS 的三次握手，要整出六次网络交互，一个连接才建好。虽说 HTTP/1.1 和 HTTP/2 的连接复用解决这个问题，但是基于 UDP 后，UDP 也得要实现这个事。于是 QUIC 直接把 TCP 的和 TLS 的合并成了三次握手（对此，在 HTTP/2 的时候，是否默认开启 TLS 业内是有争议的。反对派说，TLS 在一些情况下是不需要的，比如企业内网的时候。而支持派则说，TLS 的那些开销，什么也不算了）。</p></li></ul><p> <img src="https://coolshell.cn/wp-content/uploads/2019/10/http-request-over-tcp-tls@2x-292x300.png" alt></p><p> 所以，QUIC 是一个在 UDP 之上的伪 TCP + TLS + HTTP/2 的多路复用的协议。</p><p> 但是对于 UDP 还是有一些挑战的，这个挑战主要来自互联网上的各种网络设备。这些设备根本不知道是什么 QUIC，他们看 QUIC 就只能看到的就是 UDP，所以，在一些情况下，UDP 就是有问题的。</p><ul><li><p>比如在 NAT 的环境下，如果是 TCP 话，NAT 路由或是代理服务器，可以通过记录 TCP 的四元组（源地址、源端口、目标地址、目标端口）来做连接映射的。然而，在 UDP 的情况下不行了。于是，QUIC 引入了个叫 Connection ID 的不透明的 ID 来标识一个链接，用这种业务 ID 很爽的一个事是如果你从你的 3G/4G 的网络切到 WiFi网络（或是反过来），你的链接不会断，因为我们用的是 Connection ID，而不是四元组。</p></li><li><p>然而就算引用了 Connection ID，也还是会有问题，比如一些不够 “聪明” 的等价路由交换机。这些交换机会通过四元组来做 Hash 把你的请求的 IP 转到后端的实际的服务器上。然而，他们不懂 Connection ID，只懂四元组。这么导致属于同一个 Connection ID 但是四元组不同的网络包就转到了不同的服务器上，这就是导致数据不能传到同一台服务器上，数据不完整，链接只能断了。所以，你需要更聪明的算法（可以参看 Facebook 的 Katran 开源项目 ）</p></li></ul><p> 好了，就算搞定上面的东西，还有一些业务层的事没解。这个事就是 HTTP/2 的头压缩算法 HPACK，HPACK 需要维护一个动态的字典表来分析请求的头中哪些是重复的，HPACK 的这个数据结构需要在 Encoder 和 Decoder 端同步这个东西。在 TCP 上，这种同步是透明的，然而在 UDP 上这个事不好干了。所以，这个事也必须要重新设计了，基于 QUIC 的 QPACK 就出来了，利用两个附加的 QUIC Steam，一个用来发送这个字典表的更新给对方，另一个用来 Ack 对方发过来的 Update。</p><p> 目前看下来，HTTP/3 目前看上去没有太多的协议业务逻辑上的东西，更多是 HTTP/2 + QUIC 协议。但 HTTP/3 因为动到了底层协议，所以，在普及方面上可能会比 HTTP/2 要慢的多的多。但是，可以看到 QUIC 协议的强大。细思及恐，QUIC 这个协议真对 TCP 是个威胁，如果 QUIC成熟了，TCP 是不是会有可能成为历史呢？</p><p> 未来十年，让我们看看 UDP 是否能够逆袭 TCP……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;HTTP的前世今生&lt;/p&gt;
&lt;p&gt;( Chrome、Firefox 和 Cloudflare 均已支持 HTTP/3 )]&lt;/p&gt;
&lt;p&gt; 来源：_酷壳_&lt;/p&gt;
&lt;p&gt; 原文：&lt;em&gt;&lt;a href=&quot;https://url.cn/56Z548W&quot; target=&quot;_bl
      
    
    </summary>
    
      <category term="web" scheme="http://zhangyu8.me/categories/web/"/>
    
    
      <category term="web" scheme="http://zhangyu8.me/tags/web/"/>
    
  </entry>
  
</feed>
