<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大雨哥</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhang-yu.me/"/>
  <updated>2020-11-03T15:00:59.627Z</updated>
  <id>http://zhang-yu.me/</id>
  
  <author>
    <name>大雨哥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CPU飙高如何排查</title>
    <link href="http://zhang-yu.me/2020/10/29/CPU%E9%A3%99%E9%AB%98%E5%A6%82%E4%BD%95%E6%8E%92%E6%9F%A5/"/>
    <id>http://zhang-yu.me/2020/10/29/CPU飙高如何排查/</id>
    <published>2020-10-29T03:00:00.000Z</published>
    <updated>2020-11-03T15:00:59.627Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;groupCode=alitech" target="_blank" rel="noopener">https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;groupCode=alitech</a></p><p><a href="https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;groupCode=alitech" target="_blank" rel="noopener">CPU飙高，系统性能问题如何排查？-阿里云开发者社区</a></p><blockquote><p><img src="https://ucc.alicdn.com/pic/developer-ecology/d0cf7464f1af4e23b3543cf5cfb4540e.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h3 id="一-背景知识"><a href="#一-背景知识" class="headerlink" title="一 背景知识"></a>一 背景知识</h3><h4 id="LINUX进程状态"><a href="#LINUX进程状态" class="headerlink" title="LINUX进程状态"></a>LINUX进程状态</h4><p>LINUX 2.6以后的内核中，进程一般存在7种基础状态：D-不可中断睡眠、R-可执行、S-可中断睡眠、T-暂停态、t-跟踪态、X-死亡态、Z-僵尸态，这几种状态在PS命令中有对应解释。</p><p><img src="https://ucc.alicdn.com/pic/developer-ecology/51ff89c423b24966bbd88490cc8ff158.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><ul><li><p>D (TASK_UNINTERRUPTIBLE)，不可中断睡眠态。顾名思义，位于这种状态的进程处于睡眠中，并且不允许被其他进程或中断(异步信号)打断。因此这种状态的进程，是无法使用kill -9杀死的(kill也是一种信号)，除非重启系统(没错，就是这么头硬)。不过这种状态一般由I/O等待(比如磁盘I/O、网络I/O、外设I/O等)引起，出现时间非常短暂，大多很难被PS或者TOP命令捕获(除非I/O HANG死)。SLEEP态进程不会占用任何CPU资源。</p></li><li><p>R (TASK_RUNNING)，可执行态。这种状态的进程都位于CPU的可执行队列中，正在运行或者正在等待运行，即不是在上班就是在上班的路上。</p></li><li><p>S (TASK_INTERRUPTIBLE)，可中断睡眠态。不同于D，这种状态的进程虽然也处于睡眠中，但是是允许被中断的。这种进程一般在等待某事件的发生（比如socket连接、信号量等），而被挂起。一旦这些时间完成，进程将被唤醒转为R态。如果不在高负载时期，系统中大部分进程都处于S态。SLEEP态进程不会占用任何CPU资源。</p></li><li><p>T&amp;t (__TASK_STOPPED &amp; __TASK_TRACED)，暂停or跟踪态。这种两种状态的进程都处于运行停止的状态。不同之处是暂停态一般由于收到SIGSTOP、SIGTSTP、SIGTTIN、SIGTTOUT四种信号被停止，而跟踪态是由于进程被另一个进程跟踪引起(比如gdb断点）。暂停态进程会释放所有占用资源。</p></li><li><p>Z (EXIT_ZOMBIE), 僵尸态。这种状态的进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID等）。僵尸态进程会释放除进程入口之外的所有资源。</p></li><li><p>X (EXIT_DEAD), 死亡态。进程的真正结束态，这种状态一般在正常系统中捕获不到。</p></li></ul><h4 id="Load-Average-amp-CPU使用率"><a href="#Load-Average-amp-CPU使用率" class="headerlink" title="Load Average &amp; CPU使用率"></a>Load Average &amp; CPU使用率</h4><p>谈到系统性能，Load和CPU使用率是最直观的两个指标，那么这两个指标是怎么被计算出来的呢？是否能互相等价呢？</p><p><strong>Load Average</strong></p><p>不少人都认为，Load代表正在CPU上运行&amp;等待运行的进程数，即<br><img src="https://ucc.alicdn.com/pic/developer-ecology/96cb14429ca744a4b69c4b49801225dd.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>但Linux系统中，这种描述并不完全准确。</p><p>以下为Linux内核源码中Load Average计算方法，可以看出来，因此除了可执行态进程，不可中断睡眠态进程也会被一起纳入计算，即：<br><img src="https://ucc.alicdn.com/pic/developer-ecology/6c209bcb66f41508572a829a3081367.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><pre><code>602staticunsignedlongcount_active_tasks(void)603 {604structtask_struct*p;605unsignedlongnr=0;606607read_lock(&amp;tasklist_lock);608for_each_task(p) {609if ((p-&gt;state==TASK_RUNNING610 (p-&gt;state&amp;TASK_UNINTERRUPTIBLE)))611nr+=FIXED_1;612 }613read_unlock(&amp;tasklist_lock);614returnnr;615 }......625staticinlinevoidcalc_load(unsignedlongticks)626 {627unsignedlongactive_tasks; /* fixed-point */628staticintcount=LOAD_FREQ;629630count-=ticks;631if (count&lt;0) {632count+=LOAD_FREQ;633active_tasks=count_active_tasks();634CALC_LOAD(avenrun[0], EXP_1, active_tasks);635CALC_LOAD(avenrun[1], EXP_5, active_tasks);636CALC_LOAD(avenrun[2], EXP_15, active_tasks);637 }638 }</code></pre><p>在前文 Linux进程状态 中有提到过，不可中断睡眠态的进程(TASK_UNINTERRUTED)一般都在进行I/O等待，比如磁盘、网络或者其他外设等待。由此我们可以看出，Load Average在Linux中体现的是整体系统负载，即CPU负载 + Disk负载 + 网络负载 + 其余外设负载，并不能完全等同于CPU使用率(这种情况只出现在Linux中，其余系统比如Unix，Load还是只代表CPU负载)。</p><p><strong>CPU使用率</strong></p><p>CPU的时间分片一般可分为4大类：用户进程运行时间 - User Time, 系统内核运行时间 - System Time, 空闲时间 - Idle Time, 被抢占时间 - Steal Time。除了Idle Time外，其余时间CPU都处于工作运行状态。<br><img src="https://ucc.alicdn.com/pic/developer-ecology/db4155049419b40cd9755adf475011979.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>通常而言，我们泛指的整体CPU使用率为User Time 和 Systime占比之和(例如tsar中CPU util)，即：<br><img src="https://ucc.alicdn.com/pic/developer-ecology/309084761b534400a9cfd95619bad787.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>为了便于定位问题，大多数性能统计工具都将这4类时间片进一步细化成了8类，如下为TOP对CPU时间片的分类。<br><img src="https://ucc.alicdn.com/pic/developer-ecology/228ef627fdeb4c59aaa2769c22ded5d1.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p><ul><li>us：用户进程空间中未改变过优先级的进程占用CPU百分比</li><li>sy：内核空间占用CPU百分比</li><li>ni：用户进程空间内改变过优先级的进程占用CPU百分比</li><li>id：空闲时间百分比</li><li>wa：空闲&amp;等待I/O的时间百分比</li><li>hi：硬中断时间百分比</li><li>si：软中断时间百分比</li><li>st：虚拟化时被其余VM窃取时间百分比</li></ul><p>这8类分片中，除wa和id外，其余分片CPU都处于工作态。</p><h3 id="二-资源-amp-瓶颈分析"><a href="#二-资源-amp-瓶颈分析" class="headerlink" title="二 资源&amp;瓶颈分析"></a>二 资源&amp;瓶颈分析</h3><p>从上文我们了解到，Load Average和CPU使用率可被细分为不同的子域指标，指向不同的资源瓶颈。总体来说，指标与资源瓶颈的对应关系基本如下图所示。<br><img src="https://ucc.alicdn.com/pic/developer-ecology/7e703e3393c453b95848736cdf7ce65.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h4 id="Load高-amp-CPU高"><a href="#Load高-amp-CPU高" class="headerlink" title="Load高 &amp; CPU高"></a>Load高 &amp; CPU高</h4><p>这是我们最常遇到的一类情况，即load上涨是CPU负载上升导致。根据CPU具体资源分配表现，可分为以下几类：</p><p><strong>CPU sys高</strong></p><p>这种情况CPU主要开销在于系统内核，可进一步查看上下文切换情况。</p><ul><li><p>如果非自愿上下文切换较多，说明CPU抢占较为激烈，大量进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。</p></li><li><p>如果自愿上下文切换较多，说明可能存在I/O、内存等系统资源瓶颈，大量进程无法获取所需资源，导致的上下文切换。</p></li></ul><p><strong>CPU si高</strong></p><p>这种情况CPU大量消耗在软中断，可进一步查看软中断类型。一般而言，网络I/O或者线程调度引起软中断最为常见：</p><ul><li><p>NET_TX &amp; NET_RX。NET_TX是发送网络数据包的软中断，NET_RX是接收网络数据包的软中断，这两种类型的软中断较高时，系统存在网络I/O瓶颈可能性较大。</p></li><li><p>SCHED。SCHED为进程调度以及负载均衡引起的中断，这种中断出现较多时，系统存在较多进程切换，一般与非自愿上下文切换高同时出现，可能存在CPU瓶颈。</p></li></ul><p><strong>CPU us高</strong></p><p>这种情况说明资源主要消耗在应用进程，可能引发的原因有以下几类：</p><ul><li><p>死循环或代码中存在CPU密集计算。这种情况多核CPU us会同时上涨。</p></li><li><p>内存问题，导致大量FULLGC，阻塞线程。这种情况一般只有一核CPU us上涨。</p></li><li><p>资源等待造成线程池满，连带引发CPU上涨。这种情况下，线程池满等异常会同时出现。</p></li></ul><p><strong>Load高 &amp; CPU低</strong></p><p>这种情况出现的根本原因在于不可中断睡眠态(TASK_UNINTERRUPTIBLE)进程数较多，即CPU负载不高，但I/O负载较高。可进一步定位是磁盘I/O还是网络I/O导致。</p><h3 id="三-排查策略"><a href="#三-排查策略" class="headerlink" title="三 排查策略"></a>三 排查策略</h3><p>利用现有常用的工具，我们常用的排查策略基本如下图所示：<br><img src="https://ucc.alicdn.com/pic/developer-ecology/335db4d413bb4b1fafceb9d0cb5cdcd9.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>从问题发现到最终定位，基本可分为四个阶段：</p><h4 id="资源瓶颈定位"><a href="#资源瓶颈定位" class="headerlink" title="资源瓶颈定位"></a>资源瓶颈定位</h4><p>这一阶段通过全局性能检测工具，初步定位资源消耗异常位点。</p><p>常用的工具有：</p><ul><li>top、vmstat、tsar(历史)</li><li><ul><li>中断：/proc/softirqs、/proc/interrupts</li></ul></li><li><ul><li>I/O：iostat、dstat</li></ul></li></ul><h4 id="热点进程定位"><a href="#热点进程定位" class="headerlink" title="热点进程定位"></a>热点进程定位</h4><p>定位到资源瓶颈后，可进一步分析具体进程资源消耗情况，找到热点进程。</p><p>常用工具有：</p><ul><li>上下文切换：pidstat -w</li><li>CPU：pidstat -u</li><li>I/O：iotop、pidstat -d</li><li>僵尸进程：ps</li></ul><h4 id="线程-amp-进程内部资源定位"><a href="#线程-amp-进程内部资源定位" class="headerlink" title="线程&amp;进程内部资源定位"></a>线程&amp;进程内部资源定位</h4><p>找到具体进程后，可细化分析进程内部资源开销情况。</p><p>常用工具有：</p><ul><li>上下文切换：pidstat -w -p [pid]</li><li>CPU：pidstat -u -p [pid]</li><li>I/O: lsof</li></ul><h4 id="热点事件-amp-方法分析"><a href="#热点事件-amp-方法分析" class="headerlink" title="热点事件&amp;方法分析"></a>热点事件&amp;方法分析</h4><p>获取到热点线程后，我们可用trace或者dump工具，将线程反向关联，将问题范围定位到具体方法&amp;堆栈。</p><p>常用的工具有：</p><ul><li><p>perf：Linux自带性能分析工具，功能类似hotmethod，基于事件采样原理，以性能事件为基础，支持针对处理器相关性能指标与操作系统相关性能指标的性能剖析。</p></li><li><p>jstack</p></li><li><ul><li>结合ps -Lp或者pidstat -p一起使用，可初步定位热点线程。</li></ul></li><li><ul><li>结合zprofile-threaddump一起使用，可统计线程分布、等锁情况，常用与线程数增加分析。</li></ul></li><li><p>strace：跟踪进程执行时的系统调用和所接收的信号。</p></li><li><p>tcpdump：抓包分析，常用于网络I/O瓶颈定位。</p></li></ul><blockquote><p>相关阅读</p></blockquote><p>[1]Linux Load Averages: Solving the Mystery<br><a href="http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html" target="_blank" rel="noopener">http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html</a><br>[2]What exactly is a load average?<br><a href="http://linuxtechsupport.blogspot.com/2008/10/what-exactly-is-load-average.html" target="_blank" rel="noopener">http://linuxtechsupport.blogspot.com/2008/10/what-exactly-is-load-average.html</a>   </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;amp;groupCode=alitech&quot; target=&quot;_blank&quot; rel=&quot;no
      
    
    </summary>
    
      <category term="linux" scheme="http://zhang-yu.me/categories/linux/"/>
    
    
      <category term="linux" scheme="http://zhang-yu.me/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第一篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E4%B8%80%E7%AF%87/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第一篇/</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:00:21.573Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p>互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第一篇  </p><blockquote><h1 id="背景引入"><a href="#背景引入" class="headerlink" title="背景引入"></a>背景引入</h1><p>这篇文章，我们来聊聊在线上生产环境使用消息中间件技术的时候，从前到后的全链路到底如何保证数据不能丢失。</p><p>这个问题，在互联网公司面试的时候高频出现，而且也是非常现实的生产环境问题。</p><p>如果你的简历中写了自己熟悉MQ技术（RabbitMQ、RocketMQ、Kafka），而且在项目里有使用的经验，那么非常实际的一个生产环境问题就是：投递消息到MQ，然后从MQ消费消息来处理的这个过程，数据到底会不会丢失。</p><p>面试官此时会问：如果数据会丢失的话，你们项目生产部署的时候，是通过什么手段保证基于MQ传输的数据100%不会丢失的？麻烦结合你们线上使用的消息中间件来具体说说你们的技术方案。</p><p>这个其实就是非常区分面试候选人技术水平的一个问题。</p><p>实际上相当大比例的普通工程师，哪怕是在一些中小型互联网公司里工作过的，也就是基于公司部署的MQ集群简单的使用一下罢了，可能代码层面就是基本的发送消息和消费消息，基本没考虑太多的技术方案。</p><p>但是实际上，对于MQ、缓存、分库分表、NoSQL等各式各类的技术以及中间件在使用的时候，都会有对应技术相关的一堆生产环境问题。</p><p>那么针对这些问题，就必须要有相对应的一整套技术方案来保证系统的健壮性、稳定性以及高可用性。</p><p>所以其实中大型互联网公司的面试官在面试候选人的时候，如果考察对MQ相关技术的经验和掌握程度，十有八九都会抛出这个使用MQ时一定会涉及的数据丢失问题。因为这个问题，能够非常好的区分候选人的技术水平。</p><p>所以这篇文章，我们就来具体聊聊基于RabbitMQ这种消息中间件的背景下，从投递消息到MQ，到从MQ消费消息出来，这个过程中有哪些数据丢失的风险和可能。</p><p>然后我们再一起来看看，应该如何结合MQ自身提供的一些技术特性来保证数据不丢失？</p><h1 id="前情回顾"><a href="#前情回顾" class="headerlink" title="前情回顾"></a>前情回顾</h1><p>首先给大伙一点提醒，有些新同学可能还对MQ相关技术不太了解，建议看一下之前的MQ系列文章，看看MQ的基本使用和原理：</p><ul><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484149&amp;idx=1&amp;sn=98186297335e13ec7222b3fd43cfae5a&amp;chksm=fba6eaf6ccd163e0c2c3086daa725de224a97814d31e7b3f62dd3ec763b4abbb0689cc7565b0&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《哥们，你们的系统架构中为什么要引入消息中间件？》</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484157&amp;idx=1&amp;sn=f4644be2db6b1c230846cb4d62ae5be9&amp;chksm=fba6eafeccd163e817b420d57478829d92251a6a5fd446f81805f0983a0d95cb6853a6735c4b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《哥们，那你说说系统架构引入消息中间件有什么缺点？》</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484191&amp;idx=1&amp;sn=fac0c513cf9ad480fc39c5b51f6c4fde&amp;chksm=fba6eb1cccd1620aa0b48c72d1c6b51706400f24268db6c774bac6ec02a0f31885db9b7d6cad&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《哥们，消息中间件在你们项目里是如何落地的？》</a></p></li></ul><p>另外，其实之前我们有过2篇文章是讨论消息中间件的数据不丢失问题的。</p><p>我们分别从消费者突然宕机可能导致数据丢失，以及集群突然崩溃可能导致的数据丢失两个角度讨论了一下数据如何不丢失。</p><p>只不过仅仅那两个方案还无法保证全链路数据不丢失，但是大家如果没看过的建议也先回过头看看：</p><ul><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484204&amp;idx=1&amp;sn=6fc43b0620857b653dbef20693d1c6c6&amp;chksm=fba6eb2fccd16239056e4b52dc0895585292b830bfd2652dea81b7360556fe36aceac0951761&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《扎心！线上服务宕机时，如何保证数据100%不丢失？》</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484257&amp;idx=1&amp;sn=e7704f92a1008ab7a292e2826bd079aa&amp;chksm=fba6eb62ccd1627451d439bbc21e46e6fc1d7bfbe2a431fd887cf974a7bd0d9d482697f0e4fd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《消息中间件集群崩溃，如何保证百万生产数据不丢失？》</a></p></li></ul><p>总之，希望对MQ不太熟悉的同学，先把前面那些系列文章熟悉一下，然后再来一起系统性的研究一下MQ数据如何做到100%不丢失。</p><h1 id="目前已有的技术方案"><a href="#目前已有的技术方案" class="headerlink" title="目前已有的技术方案"></a>目前已有的技术方案</h1><p>经过之前几篇文章的讨论，目前我们已经初步知道，第一个会导致数据丢失的地方，就是消费者获取到消息之后，没有来得及处理完毕，自己直接宕机了。</p><p>此时RabbitMQ的自动ack机制会通知MQ集群这条消息已经处理好了，MQ集群就会删除这条消息。</p><p>那么这条消息不就丢失了么？不会有任何一个消费者处理到这条消息了。</p><p>所以之前我们详细讨论过，通过在消费者服务中调整为手动ack机制，来确保消息一定是已经成功处理完了，才会发送ack通知给MQ集群。</p><p>否则没发送ack之前消费者服务宕机，此时MQ集群会自动感知到，然后重发消息给其他的消费者服务实例。</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484204&amp;idx=1&amp;sn=6fc43b0620857b653dbef20693d1c6c6&amp;chksm=fba6eb2fccd16239056e4b52dc0895585292b830bfd2652dea81b7360556fe36aceac0951761&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《扎心！线上服务宕机时，如何保证数据100%不丢失？》</a>这篇文章，详细讨论了这个问题，手动ack机制之下的架构图如下所示：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111212900106-165253661.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>当时除了这个数据丢失问题之外，还有另外一个问题，就是MQ集群自身如果突然宕机，是不是会导致数据丢失？</p><p>默认情况下是肯定会的，因为queue和message都没采用持久化的方式来投递，所以MQ集群重启会导致部分数据丢失。</p><p>所以<a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484257&amp;idx=1&amp;sn=e7704f92a1008ab7a292e2826bd079aa&amp;chksm=fba6eb62ccd1627451d439bbc21e46e6fc1d7bfbe2a431fd887cf974a7bd0d9d482697f0e4fd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《消息中间件集群崩溃，如何保证百万生产数据不丢失？》</a>这篇文章，我们分析了如何采用持久化的方式来创建queue，同时采用持久化的方式来投递消息到MQ集群，这样MQ集群会将消息持久化到磁盘上去。</p><p>此时如果消息还没来得及投递给消费者服务，然后MQ集群突然宕机了，数据是不会丢失的，因为MQ集群重启之后会自动从磁盘文件里加载出来没投递出去的消息，然后继续投递给消费者服务。</p><p>同样，该方案沉淀下来的系统架构图，如下所示：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111212925797-142776737.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="数据100-不丢失了吗？"><a href="#数据100-不丢失了吗？" class="headerlink" title="数据100%不丢失了吗？"></a>数据100%不丢失了吗？</h1><p>大家想一想，到目前为止，咱们的架构一定可以保证数据不丢失了吗？</p><p>其实，现在的架构，还是有一个数据可能会丢失的问题。</p><p>那就是上面作为生产者的订单服务把消息投递到MQ集群之后，暂时还驻留在MQ的内存里，还没来得及持久化到磁盘上，同时也还没来得及投递到作为消费者的仓储服务。</p><p>此时要是MQ集群自身突然宕机，咋办呢？</p><p>尴尬了吧，驻留在内存里的数据是一定会丢失的，我们来看看下面的图示。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213004493-756910885.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="按需制定技术方案"><a href="#按需制定技术方案" class="headerlink" title="按需制定技术方案"></a>按需制定技术方案</h1><p>现在，我们需要考虑的技术方案是：订单服务如何保证消息一定已经持久化到磁盘？</p><p>实际上，作为生产者的订单服务把消息投递到MQ集群的过程是很容易丢数据的。</p><p>比如说网络出了点什么故障，数据压根儿没传输过去，或者就是上面说的消息刚刚被MQ接收但是还驻留在内存里，没落地到磁盘上，此时MQ集群宕机就会丢数据。</p><p>所以首先，我们得考虑一下作为生产者的订单服务要如何利用RabbitMQ提供的相关功能来实现一个技术方案。</p><p>这个技术方案需要保证：<strong>只要订单服务发送出去的消息确认成功了，此时MQ集群就一定已经将消息持久化到磁盘了</strong>。</p><p>我们必须实现这样的一个效果，才能保证投递到MQ集群的数据是不会丢失的。</p><h1 id="需要研究的技术细节"><a href="#需要研究的技术细节" class="headerlink" title="需要研究的技术细节"></a>需要研究的技术细节</h1><p>这里我们需要研究的技术细节是：仓储服务手动ack保证数据不丢失的实现原理。</p><p>之前，笔者就收到很多同学提问：</p><ul><li>仓储服务那块到底是如何基于手动ack就可以实现数据不丢失的？</li><li>RabbitMQ底层实现的细节和原理到底是什么？</li><li>为什么仓储服务没发送ack就宕机了，RabbitMQ可以自动感知到他宕机了，然后自动重发消息给其他的仓储服务实例呢？</li></ul><p>这些东西背后的实现原理和底层细节，到底是什么？</p><p>大伙儿稍安勿躁，接下来，咱们会通过一系列文章，仔细探究一下这背后的原理。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt;互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第一篇  &lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;背景引入&quot;&gt;&lt;a href=&quot;#背景引入&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第三篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E4%B8%89%E7%AF%87/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第三篇/</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:01:18.909Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p>互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第三篇 </p><blockquote><h1 id="前情提示"><a href="#前情提示" class="headerlink" title="前情提示"></a>前情提示</h1><p>上一篇文章：<a href="https://www.cnblogs.com/jajian/p/10293093.html" target="_blank" rel="noopener">&lt;&lt;互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第二篇&gt;&gt;</a>，我们分析了 ack 机制的底层实现原理（<code>delivery tag</code>机制），还有消除处理失败时的nack机制如何触发消息重发。</p><p>通过这个，已经让大家进一步对消费端保证数据不丢失的方案的理解更进一层了。</p><p>这篇文章，我们将会对 ack 底层的<code>delivery tag</code>机制进行更加深入的分析，让大家理解的更加透彻一些。</p><p>面试时，如果被问到消息中间件数据不丢失问题的时候，可以更深入到底层，给面试官进行分析。</p><h1 id="unack消息的积压问题"><a href="#unack消息的积压问题" class="headerlink" title="unack消息的积压问题"></a>unack消息的积压问题</h1><p>首先，我们要给大家介绍一下RabbitMQ的<code>prefetch count</code>这个概念。</p><p>大家看过上篇文章之后应该都知道了，对每个 <code>channel</code>（其实对应了一个消费者服务实例，你大体可以这么来认为），RabbitMQ 投递消息的时候，都是会带上本次消息投递的一个<code>delivery tag</code>的，唯一标识一次消息投递。</p><p>然后，我们进行 ack 时，也会带上这个 <code>delivery tag</code>，基于同一个 <code>channel</code> 进行 ack，ack 消息里会带上 <code>delivery tag</code> 让RabbitMQ知道是对哪一次消息投递进行了 ack，此时就可以对那条消息进行删除了。</p><p>大家先来看一张图，帮助大家回忆一下这个<code>delivery tag</code>的概念。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119201956874-1938458409.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>所以大家可以考虑一下，对于每个<code>channel</code>而言（你就认为是针对每个消费者服务实例吧，比如一个仓储服务实例），其实都有一些处于<code>unack</code>状态的消息。</p><p>比如RabbitMQ正在投递一条消息到<code>channel</code>，此时消息肯定是<code>unack</code>状态吧？</p><p>然后仓储服务接收到一条消息以后，要处理这条消息需要耗费时间，此时消息肯定是<code>unack</code>状态吧？</p><p>同时，即使你执行了<code>ack</code>之后，你要知道这个<code>ack</code>他默认是异步执行的，尤其如果你开启了批量<code>ack</code>的话，更是有一个延迟时间才会<code>ack</code>的，此时消息也是<code>unack</code>吧？</p><p>那么大家考虑一下，RabbitMQ 他能够无限制的不停给你的消费者服务实例推送消息吗？</p><p>明显是不能的，如果 RabbitMQ 给你的消费者服务实例推送的消息过多过快，比如都有几千条消息积压在某个消费者服务实例的内存中。</p><p>那么此时这几千条消息都是<code>unack</code>的状态，一直积压着，是不是有可能会导致消费者服务实例的内存溢出？内存消耗过大？甚至内存泄露之类的问题产生？</p><p>所以说，RabbitMQ 是必须要考虑一下消费者服务的处理能力的。</p><p>大家看看下面的图，感受一下如果消费者服务实例的内存中积压消息过多，都是<code>unack</code>的状态，此时会怎么样。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202028721-10842096.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="如何解决unack消息的积压问题"><a href="#如何解决unack消息的积压问题" class="headerlink" title="如何解决unack消息的积压问题"></a>如何解决unack消息的积压问题</h1><p>正是因为这个原因，RabbitMQ基于一个<code>prefetch count</code>来控制这个<code>unack message</code>的数量。</p><p>你可以通过<code>“channel.basicQos(10)”</code>这个方法来设置当前<code>channel</code>的<code>prefetch count</code>。</p><p>举个例子，比如你要是设置为10的话，那么意味着当前这个<code>channel</code>里，<code>unack message</code>的数量不能超过10个，以此来避免消费者服务实例积压unack message过多。</p><p>这样的话，就意味着RabbitMQ正在投递到channel过程中的<code>unack message</code>，以及消费者服务在处理中的<code>unack message</code>，以及异步ack之后还没完成 ack 的<code>unack message</code>，所有这些message 加起来，一个 channel 也不能超过10个。</p><p>如果你要简单粗浅的理解的话，也大致可以理解为这个<code>prefetch count</code>就代表了一个消费者服务同时最多可以获取多少个 message 来处理。所以这里也点出了 prefetch 这个单词的意思。</p><p>prefetch 就是预抓取的意思，就意味着你的消费者服务实例预抓取多少条 message 过来处理，但是最多只能同时处理这么多消息。</p><p>如果一个 channel 里的<code>unack message</code>超过了<code>prefetch count</code>指定的数量，此时RabbitMQ就会停止给这个 channel 投递消息了，必须要等待已经投递过去的消息被 ack 了，此时才能继续投递下一个消息。</p><p>老规矩，给大家上一张图，我们一起来看看这个东西是啥意思。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202058498-1935372139.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="高并发场景下的内存溢出问题"><a href="#高并发场景下的内存溢出问题" class="headerlink" title="高并发场景下的内存溢出问题"></a>高并发场景下的内存溢出问题</h1><p>好！现在大家对 ack 机制底层的另外一个核心机制：prefetch 机制也有了一个深刻的理解了。</p><p>此时，咱们就应该来考虑一个问题了。就是如何来设置这个<code>prefetch count</code>呢？这个东西设置的过大或者过小有什么影响呢？</p><p>其实大家理解了上面的图就很好理解这个问题了。</p><p>假如说我们把 <code>prefetch count</code> 设置的很大，比如说3000，5000，甚至100000，就这样特别大的值，那么此时会如何呢？</p><p>这个时候，在高并发大流量的场景下，可能就会导致消费者服务的内存被快速的消耗掉。</p><p>因为假如说现在MQ接收到的流量特别的大，每秒都上千条消息，而且此时你的消费者服务的<code>prefetch count</code>还设置的特别大，就会导致可能一瞬间你的消费者服务接收到了达到<code>prefetch count</code>指定数量的消息。</p><p>打个比方，比如一下子你的消费者服务内存里积压了10万条消息，都是unack的状态，反正你的prefetch count设置的是10万。</p><p>那么对一个channel，RabbitMQ就会最多容忍10万个unack状态的消息，在高并发下也就最多可能积压10万条消息在消费者服务的内存里。</p><p>那么此时导致的结果，就是消费者服务直接被击垮了，内存溢出，OOM，服务宕机，然后大量unack的消息会被重新投递给其他的消费者服务，此时其他消费者服务一样的情况，直接宕机，最后造成雪崩效应。</p><p>所有的消费者服务因为扛不住这么大的数据量，全部宕机。</p><p>大家来看看下面的图，自己感受一下现场的氛围。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202121988-833235887.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="低吞吐量问题"><a href="#低吞吐量问题" class="headerlink" title="低吞吐量问题"></a>低吞吐量问题</h1><p>那么如果反过来呢，我们要是把prefetch count设置的很小会如何呢？</p><p>比如说我们把 prefetch count 设置为1？此时就必然会导致消费者服务的吞吐量极低。因为你即使处理完一条消息，执行ack了也是异步的。</p><p>给你举个例子，假如说你的 <code>prefetch count = 1</code>，RabbitMQ最多投递给你1条消息处于 unack 状态。</p><p>此时比如你刚处理完这条消息，然后执行了 ack 的那行代码，结果不幸的是，ack需要异步执行，也就是需要100ms之后才会让RabbitMQ感知到。</p><p>那么100ms之后RabbitMQ感知到消息被ack了，此时才会投递给你下一条消息！</p><p>这就尴尬了，在这100ms期间，你的消费者服务是不是啥都没干啊？</p><p>这不就直接导致了你的消费者服务处理消息的吞吐量可能下降10倍，甚至百倍，千倍，都有这种可能！</p><p>大家看看下面的图，感受一下低吞吐量的现场。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202156488-260456800.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="合理的设置prefetch-count"><a href="#合理的设置prefetch-count" class="headerlink" title="合理的设置prefetch count"></a>合理的设置prefetch count</h1><p>所以鉴于上面两种极端情况，RabbitMQ官方给出的建议是prefetch count一般设置在100~300之间。</p><p>也就是一个消费者服务最多接收到100~300个message来处理，允许处于unack状态。</p><p>这个状态下可以兼顾吞吐量也很高，同时也不容易造成内存溢出的问题。</p><p>但是其实在我们的实践中，这个prefetch count大家完全是可以自己去压测一下的。</p><p>比如说慢慢调节这个值，不断加大，观察高并发大流量之下，吞吐量是否越来越大，而且观察消费者服务的内存消耗，会不会OOM、频繁FullGC等问题。</p><h1 id="阶段性总结"><a href="#阶段性总结" class="headerlink" title="阶段性总结"></a>阶段性总结</h1><p>其实通过最近几篇文章，基本上已经把消息中间件的消费端如何保证数据不丢失这个问题剖析的较为深入和透彻了。</p><p>如果你是基于RabbitMQ来做消息中间件的话，消费端的代码里，必须考虑三个问题：手动ack、处理失败的nack、prefetch count的合理设置</p><p>这三个问题背后涉及到了各种机制：</p><ul><li>自动ack机制</li><li>delivery tag机制</li><li>ack批量与异步提交机制</li><li>消息重发机制</li><li>手动nack触发消息重发机制</li><li>prefetch count过大导致内存溢出问题</li><li>prefetch count过小导致吞吐量过低</li></ul><p>这些底层机制和问题，咱们都一步步分析清楚了。</p><p>所以到现在，单论消费端这块的数据不丢失技术方案，相信大家在面试的时候就可以有一整套自己的理解和方案可以阐述了。</p><p>接下来下篇文章开始，我们就来具体聊一聊：消息中间件的生产端如何保证数据不丢失。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt;互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第三篇 &lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;前情提示&quot;&gt;&lt;a href=&quot;#前情提示&quot; class=&quot;headerlink&quot; title=&quot;前
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第二篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E4%BA%8C%E7%AF%87%20/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第二篇 /</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:00:55.862Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p> 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第二篇  </p><blockquote><p>上一篇文章<a href="https://www.cnblogs.com/jajian/p/10257555.html" target="_blank" rel="noopener">《互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第一篇》</a>,我们初步介绍了之前制定的那些消息中间件数据不丢失的技术方案遗留的问题。</p><p>一个最大的问题，就是生产者投递出去的消息，可能会丢失。</p><p>丢失的原因有很多，比如消息在网络传输到一半的时候因为网络故障就丢了，或者是消息投递到MQ的内存时，MQ突发故障宕机导致消息就丢失了。</p><p>针对这种生产者投递数据丢失的问题，RabbitMQ实际上是提供了一些机制的。</p><p>比如，有一种重量级的机制，就是<strong>事务消息机制</strong>。采用类事务的机制把消息投递到MQ，可以保证消息不丢失，但是性能极差，经过测试性能会呈现几百倍的下降。</p><p>所以说现在一般是不会用这种过于重量级的机制，而是会用<strong>轻量级的confirm机制</strong>。</p><p>但是我们这篇文章还不能直接讲解生产者保证消息不丢失的confirm机制，因为这种confirm机制实际上是采用了类似消费者的ack机制来实现的。</p><p>所以，要深入理解confirm机制，我们得先从这篇文章开始，深入的分析一下消费者手动ack机制保证消息不丢失的底层原理。</p><h1 id="ack机制回顾"><a href="#ack机制回顾" class="headerlink" title="ack机制回顾"></a>ack机制回顾</h1><p>其实手动ack机制非常的简单，必须要消费者确保自己处理完毕了一个消息，才能手动发送ack给MQ，MQ收到ack之后才会删除这个消息。</p><p>如果消费者还没发送ack，自己就宕机了，此时MQ感知到他的宕机，就会重新投递这条消息给其他的消费者实例。</p><p>通过这种机制保证消费者实例宕机的时候，数据是不会丢失的。</p><p>再次提醒一下大家，如果还对手动ack机制不太熟悉的同学，可以回头看一下之前的一篇文章：<a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484204&amp;idx=1&amp;sn=6fc43b0620857b653dbef20693d1c6c6&amp;chksm=fba6eb2fccd16239056e4b52dc0895585292b830bfd2652dea81b7360556fe36aceac0951761&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《扎心！线上服务宕机时，如何保证数据100%不丢失？》</a>。然后这篇文章，我们将继续深入探讨一下ack机制的实现原理。</p><h1 id="ack机制实现原理：delivery-tag"><a href="#ack机制实现原理：delivery-tag" class="headerlink" title="ack机制实现原理：delivery tag"></a>ack机制实现原理：delivery tag</h1><p>如果你写好了一个消费者服务的代码，让他开始从RabbitMQ消费数据，这时这个消费者服务实例就会自己注册到RabbitMQ。</p><p>所以，RabbitMQ其实是知道有哪些消费者服务实例存在的。</p><p>大家看看下面的图，直观的感受一下：<br> <img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213354238-905111691.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>接着，RabbitMQ就会通过自己内部的一个“basic.delivery”方法来投递消息到仓储服务里去，让他消费消息。</p><p>投递的时候，会给这次消息的投递带上一个重要的东西，就是“delivery tag”，你可以认为是本次消息投递的一个唯一标识。</p><p>这个所谓的唯一标识，有点类似于一个ID，比如说消息本次投递到一个仓储服务实例的唯一ID。通过这个唯一ID，我们就可以定位一次消息投递。</p><p>所以这个delivery tag机制不要看很简单，实际上他是后面要说的很多机制的核心基础。</p><p>而且这里要给大家强调另外一个概念，就是每个消费者从RabbitMQ获取消息的时候，都是通过一个channel的概念来进行的。</p><p>大家回看一下下面的消费者代码片段，我们必须是先对指定机器上部署的RabbitMQ建立连接，然后通过这个连接获取一个channel。</p><p>ConnectionFactory factory = new ConnectionFactory(); </p></blockquote><blockquote><p>factory.setHost(“localhost”); </p></blockquote><blockquote><p>Connection connection = factory.newConnection();</p></blockquote><blockquote><p>Channel channel = connection.createChannel();</p><p>而且如果大家还有点印象的话，我们在仓储服务里对消息的消费、ack等操作，全部都是基于这个channel来进行的，channel又有点类似于是我们跟RabbitMQ进行通信的这么一个句柄，比如看看下面的代码：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111214453240-695115936.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><blockquote><p>另外这里提一句：之前写那篇文章讲解手动ack保证数据不丢失的时候，有很多人提出疑问：为什么上面代码里直接是try finally，如果代码有异常，那还是会直接执行finally里的手动ack？其实很简单，自己加上catch就可以了。</p></blockquote><p>好的，咱们继续。你大概可以认为这个channel就是进行数据传输的一个管道吧。对于每个channel而言，一个“delivery tag”就可以唯一的标识一次消息投递，这个delivery tag大致而言就是一个不断增长的数字。</p><p>大家来看看下面的图，相信会很好理解的：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213504848-417638307.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>如果采用手动ack机制，实际上仓储服务每次消费了一条消息，处理完毕完成调度发货之后，就会发送一个ack消息给RabbitMQ服务器，这个ack消息是会带上自己本次消息的delivery tag的。</p><p>咱们看看下面的ack代码，是不是带上了一个delivery tag？</p><pre><code>channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);</code></pre><p>然后，RabbitMQ根据哪个channel的哪个delivery tag，不就可以唯一定位一次消息投递了？</p><p>接下来就可以对那条消息删除，标识为已经处理完毕。</p><p>这里大家必须注意的一点，就是delivery tag仅仅在一个channel内部是唯一标识消息投递的。</p><p>所以说，你ack一条消息的时候，必须是通过接受这条消息的同一个channel来进行。</p><p>大家看看下面的图，直观的感受一下。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213612518-291570713.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>其实这里还有一个很重要的点，就是我们可以设置一个参数，然后就批量的发送ack消息给RabbitMQ，这样可以提升整体的性能和吞吐量。</p><p>比如下面那行代码，把第二个参数设置为true就可以了。</p><pre><code>channel.basicAck(delivery.getEnvelope().getDeliveryTag(), true);</code></pre><p>看到这里，大家应该对这个ack机制的底层原理有了稍微进一步的认识了。起码是知道delivery tag是啥东西了，他是实现ack的一个底层机制。</p><p>然后，我们再来简单回顾一下自动ack、手动ack的区别。</p><p>实际上默认用自动ack，是非常简单的。RabbitMQ只要投递一个消息出去给仓储服务，那么他立马就把这个消息给标记为删除，因为他是不管仓储服务到底接收到没有，处理完没有的。</p><p>所以这种情况下，性能很好，但是数据容易丢失。</p><p>如果手动ack，那么就是必须等仓储服务完成商品调度发货以后，才会手动发送ack给RabbitMQ，此时RabbitMQ才会认为消息处理完毕，然后才会标记消息为删除。</p><p>这样在发送ack之前，仓储服务宕机，RabbitMQ会重发消息给另外一个仓储服务实例，保证数据不丢。</p><h1 id="RabbitMQ如何感知到仓储服务实例宕机"><a href="#RabbitMQ如何感知到仓储服务实例宕机" class="headerlink" title="RabbitMQ如何感知到仓储服务实例宕机"></a>RabbitMQ如何感知到仓储服务实例宕机</h1><p>之前就有同学提出过这个问题，但是其实要搞清楚这个问题，其实不需要深入的探索底层，只要自己大致的思考和推测一下就可以了。</p><p>如果你的仓储服务实例接收到了消息，但是没有来得及调度发货，没有发送ack，此时他宕机了。</p><p>我们想一想就知道，RabbitMQ之前既然收到了仓储服务实例的注册，因此他们之间必然是建立有某种联系的。</p><p>一旦某个仓储服务实例宕机，那么RabbitMQ就必然会感知到他的宕机，而且对发送给他的还没ack的消息，都发送给其他仓储服务实例。</p><p>所以这个问题以后有机会我们可以深入聊一聊，在这里，大家其实先建立起来这种认识即可。</p><p>我们再回头看看下面的架构图：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213730961-1499375762.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="仓储服务处理失败时的消息重发"><a href="#仓储服务处理失败时的消息重发" class="headerlink" title="仓储服务处理失败时的消息重发"></a>仓储服务处理失败时的消息重发</h1><p>首先，我们来看看下面一段代码：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111214524017-261713897.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>假如说某个仓储服务实例处理某个消息失败了，此时会进入catch代码块，那么此时我们怎么办呢？难道还是直接ack消息吗？</p><p>当然不是了，你要是还是ack，那会导致消息被删除，但是实际没有完成调度发货。</p><p>这样的话，数据不是还是丢失了吗？因此，合理的方式是使用nack操作。</p><p>就是通知RabbitMQ自己没处理成功消息，然后让RabbitMQ将这个消息再次投递给其他的仓储服务实例尝试去完成调度发货的任务。</p><p>我们只要在catch代码块里加入下面的代码即可：</p><pre><code>channel.basicNack(delivery.getEnvelope().getDeliveryTag(),  true);</code></pre><p>注意上面第二个参数是true，意思就是让RabbitMQ把这条消息重新投递给其他的仓储服务实例，因为自己没处理成功。</p><p>你要是设置为false的话，就会导致RabbitMQ知道你处理失败，但是还是删除这条消息，这是不对的。</p><p>同样，我们还是来一张图，大家一起来感受一下：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213911750-654802068.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="阶段总结"><a href="#阶段总结" class="headerlink" title="阶段总结"></a>阶段总结</h1><p>这篇文章对之前的ack机制做了进一步的分析，包括底层的delivery tag机制，以及消息处理失败时的消息重发。</p><p>通过ack机制、消息重发等这套机制的落地实现，就可以保证一个消费者服务自身突然宕机、消息处理失败等场景下，都不会丢失数据。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt; 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第二篇  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上一篇文章&lt;a href=&quot;https://www.cnblogs.com/jajian/p/10257
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第四篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E5%9B%9B%E7%AF%87/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第四篇/</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:01:41.064Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p> 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第四篇 </p><blockquote><h1 id="前情提示"><a href="#前情提示" class="headerlink" title="前情提示"></a>前情提示</h1><p>上篇文章：<a href="https://www.cnblogs.com/jajian/p/10293391.html" target="_blank" rel="noopener">《互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第三篇》</a>，我们分析了 RabbitMQ 开启手动ack机制保证消费端数据不丢失的时候，prefetch 机制对消费者的吞吐量以及内存消耗的影响。</p><p>通过分析，我们知道了 prefetch 过大容易导致内存溢出，prefetch 过小又会导致消费吞吐量过低，所以在实际项目中需要慎重测试和设置。</p><p>这篇文章，我们转移到消息中间件的生产端，一起来看看如何保证投递到 MQ 的数据不丢失。</p><p>如果投递出去的消息在网络传输过程中丢失，或者在 RabbitMQ 的内存中还没写入磁盘的时候宕机，都会导致生产端投递到MQ的数据丢失。</p><p>而且丢失之后，生产端自己还感知不到，同时还没办法来补救。</p><p>下面的图就展示了这个问题。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214653541-577481334.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>所以本文呢，我们就来逐步分析一下。</p><h1 id="保证投递消息不丢失的-confirm-机制"><a href="#保证投递消息不丢失的-confirm-机制" class="headerlink" title="保证投递消息不丢失的 confirm 机制"></a>保证投递消息不丢失的 confirm 机制</h1><p>其实要解决这个问题，相信大家看过之前的消费端 ack 机制之后，也都猜到了。</p><p>很简单，就是生产端（比如上图的订单服务）首先需要开启一个 confirm 模式，接着投递到 MQ 的消息，如果 MQ 一旦将消息持久化到磁盘之后，必须也要回传一个 confirm 消息给生产端。</p><p>这样的话，如果生产端的服务接收到了这个 confirm 消息，就知道是已经持久化到磁盘了。</p><p>否则如果没有接收到confirm消息，那么就说明这条消息半路可能丢失了，此时你就可以重新投递消息到 MQ 去，确保消息不要丢失。</p><p>而且一旦你开启了confirm模式之后，每次消息投递也同样是有一个 <code>delivery tag</code> 的，也是起到唯一标识一次消息投递的作用。</p><p>这样，MQ回传ack给生产端的时候，会带上这个 <code>delivery tag</code>。你就知道具体对应着哪一次消息投递了，可以删除这条消息。</p><p>此外，如果 RabbitMQ 接收到一条消息之后，结果内部出错发现无法处理这条消息，那么他会回传一个 nack 消息给生产端。此时你就会感知到这条消息可能处理有问题，你可以选择重新再次投递这条消息到MQ去。</p><p>或者另一种情况，如果某条消息很长时间都没给你回传 ack/nack，那可能是极端意外情况发生了，数据也丢了，你也可以自己重新投递消息到 MQ 去。</p><p>通过这套 confirm 机制，就可以实现生产端投递消息不会丢失的效果。大家来看看下面的图，一起来感受一下。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214724244-1755660067.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p><h1 id="confirm机制的代码实现"><a href="#confirm机制的代码实现" class="headerlink" title="confirm机制的代码实现"></a>confirm机制的代码实现</h1><p>下面，我们再来看看confirm机制的代码实现：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214748749-1779146961.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="confirm机制投递消息的高延迟性"><a href="#confirm机制投递消息的高延迟性" class="headerlink" title="confirm机制投递消息的高延迟性"></a>confirm机制投递消息的高延迟性</h1><p>这里有一个很关键的点，就是一旦启用了 confirm 机制投递消息到 MQ 之后，MQ 是不保证什么时候会给你一个ack或者nack的。</p><p>因为 RabbitMQ 自己内部将消息持久化到磁盘，本身就是通过异步批量的方式来进行的。</p><p>正常情况下，你投递到 RabbitMQ 的消息都会先驻留在内存里，然后过了几百毫秒的延迟时间之后，再一次性批量把多条消息持久化到磁盘里去。</p><p>这样做，是为了兼顾高并发写入的吞吐量和性能的，因为要是你来一条消息就写一次磁盘，那么性能会很差，每次写磁盘都是一次 fsync 强制刷入磁盘的操作，是很耗时的。</p><p>所以正是因为这个原因，你打开了 confirm 模式之后，很可能你投递出去一条消息，要间隔几百毫秒之后，MQ 才会把消息写入磁盘，接着你才会收到 MQ 回传过来的 ack 消息，这个就是所谓confirm机制投递消息的高延迟性。</p><p>大家看看下面的图，一起来感受一下。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214823183-1953400051.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="高并发下如何投递消息才能不丢失"><a href="#高并发下如何投递消息才能不丢失" class="headerlink" title="高并发下如何投递消息才能不丢失"></a>高并发下如何投递消息才能不丢失</h1><p>大家可以考虑一下，在生产端高并发写入 MQ 的场景下，你会面临两个问题：</p><ul><li>1、你每次写一条消息到 MQ，为了等待这条消息的ack，必须把消息保存到一个存储里。</li></ul><p>并且这个存储不建议是内存，因为高并发下消息是很多的，每秒可能都几千甚至上万的消息投递出去，消息的 ack 要等几百毫秒的话，放内存可能有内存溢出的风险。</p><ul><li>2、绝对不能以同步写消息 + 等待 ack 的方式来投递，那样会导致每次投递一个消息都同步阻塞等待几百毫秒，会导致投递性能和吞吐量大幅度下降。</li></ul><p>针对这两个问题，相对应的方案其实也呼之欲出了。</p><p>首先，用来临时存放未 ack 消息的存储需要承载高并发写入，而且我们不需要什么复杂的运算操作，这种存储首选绝对不是 MySQL 之类的数据库，而建议采用 kv 存储。kv 存储承载高并发能力极强，而且 kv 操作性能很高。</p><p>其次，投递消息之后等待 ack 的过程必须是异步的，也就是类似上面那样的代码，已经给出了一个初步的异步回调的方式。</p><p>消息投递出去之后，这个投递的线程其实就可以返回了，至于每个消息的异步回调，是通过在channel注册一个confirm监听器实现的。</p><p>收到一个消息 ack 之后，就从kv存储中删除这条临时消息；收到一个消息 nack 之后，就从 kv 存储提取这条消息然后重新投递一次即可；也可以自己对 kv 存储里的消息做监控，如果超过一定时长没收到 ack，就主动重发消息。</p><p>大家看看下面的图，一起来体会一下：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214848776-1757813065.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="消息中间件全链路100-数据不丢失能做到吗"><a href="#消息中间件全链路100-数据不丢失能做到吗" class="headerlink" title="消息中间件全链路100%数据不丢失能做到吗"></a>消息中间件全链路100%数据不丢失能做到吗</h1><p>到此为止，我们已经把生产端和消费端如何保证消息不丢失的相关技术方案结合RabbitMQ这种中间件都给大家分析过了。</p><p>其实，架构思想是通用的， 无论你用的是哪一种MQ中间件，他们提供的功能是不太一样的，但是你都需要考虑如下几点：</p><ol><li><p>生产端如何保证投递出去的消息不丢失：消息在半路丢失，或者在 MQ 内存中宕机导致丢失，此时你如何基于 MQ 的功能保证消息不要丢失？</p></li><li><p>MQ 自身如何保证消息不丢失：起码需要让 MQ 对消息是有持久化到磁盘这个机制。</p></li><li><p>消费端如何保证消费到的消息不丢失：如果你处理到一半消费端宕机，导致消息丢失，此时怎么办？</p></li></ol><p>目前来说，我们初步的借着 RabbitMQ 举例，已经把从前到后一整套技术方案的原理、设计和实现都给大家分析了一遍了。</p><p>但是此时真的能做到100%数据不丢失吗？恐怕未必，大家再考虑一下个特殊的场景。</p><p>生产端投递了消息到 MQ，而且持久化到磁盘并且回传ack给生产端了。</p><p>但是此时 MQ 还没投递消息给消费端，结果 MQ 部署的机器突然宕机，而且因为未知的原因磁盘损坏了，直接在物理层面导致 MQ 持久化到磁盘的数据找不回来了。</p><p>这个大家千万别以为是开玩笑的，大家如果留意留意行业新闻，这种磁盘损坏导致数据丢失的是真的有的。</p><p>那么此时即使你把 MQ 重启了，磁盘上的数据也丢失了，数据是不是还是丢失了？</p><p>你说，我可以用 MQ 的集群机制啊，给一个数据做多个副本，比如后面我们就会给大家分析 RabbitMQ 的镜像集群机制，确实可以做到数据多副本。</p><p>但是即使数据多副本，一定可以做到100%数据不丢失？</p><p>比如说你的机房突然遇到地震，结果机房里的机器全部没了，数据是不是还是全丢了？</p><p>说这个，并不是说要抬杠。而是告诉大家，技术这个东西，100%都是理论上的期望。</p><p>应该说，我们凡事都朝着100%去做，但是理论上是不可能完全做到100%保证的，可能就是做到99.9999%的可能性数据不丢失，但是还是有千万分之一的概率会丢失。</p><p>当然，从实际的情况来说，能做到这种地步，其实基本上已经基本数据不会丢失了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt; 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第四篇 &lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;前情提示&quot;&gt;&lt;a href=&quot;#前情提示&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch性能优化</title>
    <link href="http://zhang-yu.me/2020/09/28/Elasticsearch%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://zhang-yu.me/2020/09/28/Elasticsearch性能优化/</id>
    <published>2020-09-28T03:00:00.000Z</published>
    <updated>2020-11-03T14:03:32.192Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/jajian/category/1280015.html" target="_blank" rel="noopener">https://www.cnblogs.com/jajian/category/1280015.html</a><br><a href="https://www.cnblogs.com/jajian/p/10465519.html" target="_blank" rel="noopener">https://www.cnblogs.com/jajian/p/10465519.html</a></p><blockquote><h1 id="硬件选择"><a href="#硬件选择" class="headerlink" title="硬件选择"></a>硬件选择</h1><p>Elasticsearch（后文简称 ES）的基础是 Lucene，所有的索引和文档数据是存储在本地的磁盘中，具体的路径可在 ES 的配置文件<code>../config/elasticsearch.yml</code>中配置，如下：</p></blockquote><p><a href="https://www.cnblogs.com/jajian/p/10465519.html" target="_blank" rel="noopener">Elasticsearch 技术分析（七）： Elasticsearch 的性能优化 - JaJian - 博客园</a></p><pre><code># ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /path/to/data## Path to log files:#path.logs: /path/to/logs</code></pre><blockquote><p>磁盘在现代服务器上通常都是瓶颈。Elasticsearch 重度使用磁盘，你的磁盘能处理的吞吐量越大，你的节点就越稳定。这里有一些优化磁盘 I/O 的技巧：</p><ul><li>使用 SSD。就像其他地方提过的， 他们比机械磁盘优秀多了。</li><li>使用 RAID 0。条带化 RAID 会提高磁盘 I/O，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验 RAID 因为副本已经提供了这个功能。</li><li>另外，使用多块硬盘，并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面。</li><li>不要使用远程挂载的存储，比如 NFS 或者 SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的。</li><li>如果你用的是 EC2，当心 EBS。即便是基于 SSD 的 EBS，通常也比本地实例的存储要慢。</li></ul><h1 id="内部压缩"><a href="#内部压缩" class="headerlink" title="内部压缩"></a>内部压缩</h1><p>硬件资源比较昂贵，一般不会花大成本去购置这些，可控的解决方案还是需要从软件方面来实现性能优化提升。</p><p>其实，对于一个分布式、可扩展、支持PB级别数据、实时的搜索与数据分析引擎，ES 本身对于索引数据和文档数据的存储方面内部做了很多优化，具体体现在对数据的压缩，那么是如何压缩的呢？介绍前先要说明下 <strong>Postings lists</strong> 的概念。</p><h2 id="倒排列表-postings-list"><a href="#倒排列表-postings-list" class="headerlink" title="倒排列表 - postings list"></a>倒排列表 - postings list</h2><p>搜索引擎一项很重要的工作就是高效的压缩和快速的解压缩一系列有序的整数列表。我们都知道，Elasticsearch 基于 Lucene，一个 Lucene 索引 我们在 Elasticsearch 称作 _分片_ ， 并且引入了 <strong>按段搜索</strong> 的概念。</p><p>新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段。在每个 segment 内文档都会有一个 0 到文档个数之间的标识符（最高值 2^31 -1），称之为 <strong>doc ID</strong>。这在概念上类似于数组中的索引：它本身不做存储，但足以识别每个item 数据。</p><p>Segments 按顺序存储有关文档的数据，在一个Segments 中 doc ID 是 文档的索引。因此，segment 中的第一个文档的 doc ID 为0，第二个为1，等等。直到最后一个文档，其 doc ID 等于 segment 中文档的总数减1。</p><p>那么这些 doc ID 有什么用呢？倒排索引需要将 terms 映射到包含该单词 （term） 的文档列表，这样的映射列表我们称之为：<strong>倒排列表（postings list）</strong>。具体某一条映射数据称之为：<strong>倒排索引项（Posting）</strong>。</p><p>举个例子，文档和词条之间的关系如下图所示，右边的关系表即为倒排列表：</p><p><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154646364-1594632349.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><blockquote><p><strong>倒排列表</strong> 用来记录有哪些文档包含了某个单词（Term）。一般在文档集合里会有很多文档包含某个单词，每个文档会记录文档编号（doc ID），单词在这个文档中出现的次数（TF）及单词在文档中哪些位置出现过等信息，这样与一个文档相关的信息被称做 <strong>倒排索引项（Posting）</strong>，包含这个单词的一系列倒排索引项形成了列表结构，这就是某个单词对应的 <strong>倒排列表</strong> 。</p></blockquote><h2 id="Frame-Of-Reference"><a href="#Frame-Of-Reference" class="headerlink" title="Frame Of Reference"></a>Frame Of Reference</h2><p>了解了分词（Term）和文档（Document）之间的映射关系后，为了高效的计算交集和并集，我们需要倒排列表（postings lists）是有序的，这样方便我们压缩和解压缩。</p><p>针对倒排列表，Lucene 采用一种增量编码的方式将一系列 ID 进行压缩存储，即称为<strong>Frame Of Reference的压缩方式（FOR）</strong>，自Lucene 4.1以来一直在使用。</p><p>在实际的搜索引擎系统中，并不存储倒排索引项中的实际文档编号（Doc ID），而是代之以文档编号差值（D-Gap）。文档编号差值是倒排列表中相邻的两个倒排索引项文档编号的差值，一般在索引构建过程中，可以保证倒排列表中后面出现的文档编号大于之前出现的文档编号，所以文档编号差值总是大于0的整数。</p><p>如下图所示的例子中，原始的 3个文档编号分别是187、196和199，通过编号差值计算，在实际存储的时候就转化成了：187、9、3。<br><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154656895-261135422.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>之所以要对文档编号进行差值计算，主要原因是为了更好地对数据进行压缩，原始文档编号一般都是大数值，通过差值计算，就有效地将大数值转换为了小数值，而这有助于增加数据的压缩率。</p><p>比如一个词对应的文档ID 列表 <code>[73, 300, 302, 332,343, 372]</code> ，ID列表首先要从小到大排好序；</p><ul><li><strong>第一步：</strong> 增量编码就是从第二个数开始每个数存储与前一个id的差值，即<code>300-73=227</code>，<code>302-300=2</code>，…，一直到最后一个数。</li><li><strong>第二步：</strong> 就是将这些差值放到不同的区块，Lucene使用256个区块，下面示例为了方便展示使用了3个区块，即每3个数一组。</li><li><strong>第三步：</strong> 位压缩，计算每组3个数中最大的那个数需要占用bit位数，比如30、11、29中最大数30最小需要5个bit位存储，这样11、29也用5个bit位存储，这样才占用15个bit，不到2个字节，压缩效果很好。</li></ul><p>如下面原理图所示，这是一个区块大小为3的示例（实际上是256）：<br><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154706304-740632821.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>考虑到频繁出现的term（所谓low cardinality的值），比如gender里的男或者女。如果有1百万个文档，那么性别为男的 posting list 里就会有50万个int值。用 <strong>Frame of Reference</strong> 编码进行压缩可以极大减少磁盘占用。这个优化对于减少索引尺寸有非常重要的意义。</p><p>因为这个 <strong>FOR</strong> 的编码是有解压缩成本的。利用skip list(跳表)，除了跳过了遍历的成本，也跳过了解压缩这些压缩过的block的过程，从而节省了cpu。</p><h2 id="Roaring-bitmaps-（RBM）"><a href="#Roaring-bitmaps-（RBM）" class="headerlink" title="Roaring bitmaps （RBM）"></a>Roaring bitmaps （RBM）</h2><p>在 elasticsearch 中使用filters 优化查询，filter查询只处理文档是否匹配与否，不涉及文档评分操作，查询的结果可以被缓存。具体的 Filter 和Query 的异同读者可以自行网上查阅资料。</p><p>对于filter 查询，elasticsearch 提供了Filter cache 这种特殊的缓存，filter cache 用来存储 filters 得到的结果集。缓存 filters 不需要太多的内存，它只保留一种信息，即哪些文档与filter相匹配。同时它可以由其它的查询复用，极大地提升了查询的性能。</p><p>Frame Of Reference 压缩算法对于倒排表来说效果很好，但对于需要存储在内存中的 Filter cache 等不太合适。</p><p>倒排表和Filter cache两者之间有很多不同之处：</p><ul><li>倒排表存储在磁盘，针对每个词都需要进行编码，而Filter等内存缓存只会存储那些经常使用的数据。</li><li>针对Filter数据的缓存就是为了加速处理效率，对压缩算法要求更高。</li></ul><p>这就产生了下面针对内存缓存数据可以进行高效压缩解压和逻辑运算的roaring bitmaps算法。</p><p>说到Roaring bitmaps，就必须先从bitmap说起。Bitmap是一种数据结构，假设有某个posting list：</p><pre><code>[3,1,4,7,8]</code></pre><p>对应的Bitmap就是：</p><pre><code>[0,1,0,1,1,0,0,1,1]</code></pre><p>非常直观，用0/1表示某个值是否存在，比如8这个值就对应第8位，对应的bit值是1，这样用一个字节就可以代表8个文档id（1B = 8bit），旧版本(5.0之前)的Lucene就是用这样的方式来压缩的。但这样的压缩方式仍然不够高效，Bitmap自身就有压缩的特点，其用一个byte就可以代表8个文档，所以100万个文档只需要12.5万个byte。但是考虑到文档可能有数十亿之多，在内存里保存Bitmap仍然是很奢侈的事情。而且对于个每一个filter都要消耗一个Bitmap，比如age=18缓存起来的话是一个Bitmap，18&lt;=age&lt;25是另外一个filter缓存起来也要一个Bitmap。</p><p>Bitmap的缺点是存储空间随着文档个数线性增长，所以秘诀就在于需要有一个数据结构打破这个魔咒，那么就一定要用到某些指数特性：</p><ul><li>可以很压缩地保存上亿个bit代表对应的文档是否匹配filter；</li><li>这个压缩的Bitmap仍然可以很快地进行AND和 OR的逻辑操作。</li></ul><p>Lucene使用的这个数据结构叫做 <strong>Roaring Bitmap</strong>，即位图压缩算法，简称<strong>BMP</strong>。<br><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154718895-359906247.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>其压缩的思路其实很简单。与其保存100个0，占用100个bit。还不如保存0一次，然后声明这个0重复了100遍。</p><p>这两种合并使用索引的方式都有其用途。Elasticsearch 对其性能有详细的对比，可阅读 <a href="https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps" target="_blank" rel="noopener">Frame of Reference and Roaring Bitmaps</a>。</p><h1 id="分片策略"><a href="#分片策略" class="headerlink" title="分片策略"></a>分片策略</h1><h2 id="合理设置分片数"><a href="#合理设置分片数" class="headerlink" title="合理设置分片数"></a>合理设置分片数</h2><p>创建索引的时候，我们需要预分配 ES 集群的分片数和副本数，即使是单机情况下。如果没有在 mapping 文件中指定，那么索引在默认情况下会被分配5个主分片和每个主分片的1个副本。</p><p>分片和副本的设计为 ES 提供了支持分布式和故障转移的特性，但并不意味着分片和副本是可以无限分配的。而且索引的分片完成分配后由于索引的路由机制，我们是不能重新修改分片数的。</p><p>例如某个创业公司初始用户的索引 _t_user_ 分片数为2，但是随着业务的发展用户的数据量迅速增长，这时我们是不能重新将索引 _t_user_ 的分片数增加为3或者更大的数。</p><p>可能有人会说，我不知道这个索引将来会变得多大，并且过后我也不能更改索引的大小，所以为了保险起见，还是给它设为 1000 个分片吧…</p><p>一个分片并不是没有代价的。需要了解：</p><ul><li>一个分片的底层即为一个 Lucene 索引，会消耗一定文件句柄、内存、以及 CPU 运转。</li><li>每一个搜索请求都需要命中索引中的每一个分片，如果每一个分片都处于不同的节点还好， 但如果多个分片都需要在同一个节点上竞争使用相同的资源就有些糟糕了。</li><li>用于计算相关度的词项统计信息是基于分片的。如果有许多分片，每一个都只有很少的数据会导致很低的相关度。</li></ul><blockquote><p>适当的预分配是好的。但上千个分片就有些糟糕。我们很难去定义分片是否过多了，这取决于它们的大小以及如何去使用它们。 一百个分片但很少使用还好，两个分片但非常频繁地使用有可能就有点多了。 监控你的节点保证它们留有足够的空闲资源来处理一些特殊情况。</p></blockquote><p>一个业务索引具体需要分配多少分片可能需要架构师和技术人员对业务的增长有个预先的判断，横向扩展应当分阶段进行。为下一阶段准备好足够的资源。 只有当你进入到下一个阶段，你才有时间思考需要作出哪些改变来达到这个阶段。</p><p>一般来说，我们遵循一些原则：</p><ol><li><p>控制每个分片占用的硬盘容量不超过ES的最大JVM的堆空间设置（一般设置不超过32G，参考下文的JVM设置原则），因此，如果索引的总容量在500G左右，那分片大小在16个左右即可；当然，最好同时考虑原则2。</p></li><li><p>考虑一下node数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了1个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以， 一般都设置分片数不超过节点数的3倍。</p></li><li><p>主分片，副本和节点最大数之间数量，我们分配的时候可以参考以下关系：</p></li></ol><ol><li>节点数&lt;=主分片数*（副本数+1）</li></ol><p>创建索引的时候需要控制分片分配行为，合理分配分片，如果后期索引所对应的数据越来越多，我们还可以通过<em>索引别名</em>等其他方式解决。</p><h2 id="调整分片分配器的类型"><a href="#调整分片分配器的类型" class="headerlink" title="调整分片分配器的类型"></a>调整分片分配器的类型</h2><p>以上是在创建每个索引的时候需要考虑的优化方法，然而在索引已创建好的前提下，是否就是没有办法从分片的角度提高了性能了呢？当然不是，首先能做的是调整分片分配器的类型，具体是在 <code>elasticsearch.yml</code> 中设置<code>cluster.routing.allocation.type</code> 属性，共有两种分片器<code>even_shard</code>，<code>balanced（默认）</code>。</p><p><strong>even_shard</strong> 是尽量保证每个节点都具有相同数量的分片，<strong>balanced</strong> 是基于可控制的权重进行分配，相对于前一个分配器，它更暴漏了一些参数而引入调整分配过程的能力。</p><p>每次ES的分片调整都是在ES上的数据分布发生了变化的时候进行的，最有代表性的就是有新的数据节点加入了集群的时候。当然调整分片的时机并不是由某个阈值触发的，ES内置十一个裁决者来决定是否触发分片调整，这里暂不赘述。另外，这些分配部署策略都是可以在运行时更新的，更多配置分片的属性也请大家自行查阅网上资料。</p><h2 id="推迟分片分配"><a href="#推迟分片分配" class="headerlink" title="推迟分片分配"></a>推迟分片分配</h2><p>对于节点瞬时中断的问题，默认情况，集群会等待一分钟来查看节点是否会重新加入，如果这个节点在此期间重新加入，重新加入的节点会保持其现有的分片数据，不会触发新的分片分配。这样就可以减少 ES 在自动再平衡可用分片时所带来的极大开销。</p><p>通过修改参数 <code>delayed_timeout</code> ，可以延长再均衡的时间，可以全局设置也可以在索引级别进行修改:</p><p>PUT /_all/_settings { “settings”: { “index.unassigned.node_left.delayed_timeout”: “5m” }</p><pre><code>}</code></pre><p>通过使用 _all 索引名，我们可以为集群里面的所有的索引使用这个参数，默认时间被延长成了 5 分钟。</p><p>这个配置是动态的，可以在运行时进行修改。如果你希望分片立即分配而不想等待，你可以设置参数： <code>delayed_timeout: 0</code>。</p><blockquote><p>延迟分配不会阻止副本被提拔为主分片。集群还是会进行必要的提拔来让集群回到 yellow 状态。缺失副本的重建是唯一被延迟的过程。</p></blockquote><h1 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h1><h2 id="Mapping建模"><a href="#Mapping建模" class="headerlink" title="Mapping建模"></a>Mapping建模</h2><ol><li><p>尽量避免使用nested或 parent/child，能不用就不用；</p><p>nested query慢， parent/child query 更慢，比nested query慢上百倍；因此能在mapping设计阶段搞定的（大宽表设计或采用比较smart的数据结构），就不要用父子关系的mapping。</p></li><li><p>如果一定要使用nested fields，保证nested fields字段不能过多，目前ES默认限制是50。参考：</p></li></ol><ul><li>index.mapping.nested_fields.limit ：50</li></ul><pre><code>因为针对1个document, 每一个nested field, 都会生成一个独立的document, 这将使Doc数量剧增，影响查询效率，尤其是Join的效率。</code></pre><ul><li><p>避免使用动态值作字段(key)，动态递增的mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。</p><p>控制索引的字段数量、mapping深度、索引字段的类型，对于ES的性能优化是重中之重。以下是ES关于字段数、mapping深度的一些默认设置：</p></li></ul><p>index.mapping.nested_objects.limit :10000 index.mapping.total_fields.limit:1000</p><ol><li>index.mapping.depth.limit: 20</li></ol><ol start="2"><li><p>不需要做模糊检索的字段使用 <code>keyword</code>类型代替 <code>text</code> 类型，这样可以避免在建立索引前对这些文本进行分词。</p></li><li><p>对于那些不需要聚合和排序的索引字段禁用Doc values。</p><p>Doc Values 默认对所有字段启用，除了 <code>analyzed strings</code>。也就是说所有的数字、地理坐标、日期、IP 和不分析（ <code>not_analyzed</code> ）字符类型都会默认开启。</p><p>因为 Doc Values 默认启用，也就是说ES对你数据集里面的大多数字段都可以进行聚合和排序操作。但是如果你知道你永远也不会对某些字段进行聚合、排序或是使用脚本操作， 尽管这并不常见，这时你可以通过禁用特定字段的 Doc Values 。这样不仅节省磁盘空间，也会提升索引的速度。</p><p>要禁用 Doc Values ，在字段的映射（mapping）设置 <code>doc_values: false</code> 即可。</p></li></ol><h2 id="索引设置"><a href="#索引设置" class="headerlink" title="索引设置"></a>索引设置</h2><ol><li><p>如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 <code>index.refresh_interval</code> 改到 30s或者更大。 如果你是在做大批量导入，设置 <code>refresh_interval</code> 为-1，同时设置 <code>number_of_replicas</code> 为0，通过关闭 refresh 间隔周期，同时不设置副本来提高写性能。</p><p>文档在复制的时候，整个文档内容都被发往副本节点，然后逐字的把索引过程重复一遍。这意味着每个副本也会执行分析、索引以及可能的合并过程。</p><p>相反，如果你的索引是零副本，然后在写入完成后再开启副本，恢复过程本质上只是一个字节到字节的网络传输。相比重复索引过程，这个算是相当高效的了。</p></li><li><p>修改 <code>index_buffer_size</code> 的设置，可以设置成百分数，也可设置成具体的大小，最多给512M，大于这个值会触发refresh。默认值是JVM的内存10%，但是是所有切片共享大小。可根据集群的规模做不同的设置测试。</p></li></ol><p>indices.memory.index_buffer_size：10%（默认） indices.memory.min_index_buffer_size： 48mb（默认）</p><ol><li>indices.memory.max_index_buffer_size</li></ol><ol start="2"><li>修改 translog 相关的设置：</li></ol><ul><li>a. 控制数据从内存到硬盘的操作频率，以减少硬盘IO。可将 <code>sync_interval</code> 的时间设置大一些。</li></ul><ul><li>index.translog.sync_interval：5s(默认)。</li></ul><ul><li>b. 控制 tranlog 数据块的大小，达到 threshold 大小时，才会 flush 到 lucene 索引文件。</li></ul><ul><li>index.translog.flush_threshold_size：512mb(默认)</li></ul><ol start="4"><li><p>_id字段的使用，应尽可能避免自定义_id, 以避免针对ID的版本管理；建议使用ES的默认ID生成策略或使用数字类型ID做为主键，包括零填充序列 ID、UUID-1 和纳秒；这些 ID 都是有一致的，压缩良好的序列模式。相反的，像 UUID-4 这样的 ID，本质上是随机的，压缩比很低，会明显拖慢 Lucene。</p></li><li><p><code>_all</code> 字段及 <code>_source</code> 字段的使用，应该注意场景和需要，_all字段包含了所有的索引字段，方便做全文检索，如果无此需求，可以禁用；_source存储了原始的document内容，如果没有获取原始文档数据的需求，可通过设置includes、excludes 属性来定义放入_source的字段。</p></li><li><p>合理的配置使用index属性，analyzed 和not_analyzed，根据业务需求来控制字段是否分词或不分词。只有 groupby需求的字段，配置时就设置成not_analyzed, 以提高查询或聚类的效率。</p></li></ol><h1 id="查询效率"><a href="#查询效率" class="headerlink" title="查询效率"></a>查询效率</h1><ol><li><p>使用批量请求，批量索引的效率肯定比单条索引的效率要高。</p></li><li><p><code>query_string</code> 或 <code>multi_match</code> 的查询字段越多， 查询越慢。可以在 mapping 阶段，利用 copy_to 属性将多字段的值索引到一个新字段，<code>multi_match</code>时，用新的字段查询。</p></li><li><p>日期字段的查询， 尤其是用now 的查询实际上是不存在缓存的，因此， 可以从业务的角度来考虑是否一定要用now, 毕竟利用 <code>query cache</code> 是能够大大提高查询效率的。</p></li><li><p>查询结果集的大小不能随意设置成大得离谱的值， 如<code>query.setSize</code>不能设置成 <code>Integer.MAX_VALUE</code>， 因为ES内部需要建立一个数据结构来放指定大小的结果集数据。</p></li><li><p>尽量避免使用 <code>script</code>，万不得已需要使用的话，选择<code>painless &amp; experssions</code> 引擎。一旦使用 script 查询，一定要注意控制返回，千万不要有死循环（如下错误的例子），因为ES没有脚本运行的超时控制，只要当前的脚本没执行完，该查询会一直阻塞。如：</p></li></ol><p>{ “script_fields”：{ “test1”：{ “lang”：“groovy”， “script”：“while（true）{print ‘don’t use script’}” } }-       }</p><ul><li><p>避免层级过深的聚合查询， 层级过深的group by , 会导致内存、CPU消耗，建议在服务层通过程序来组装业务，也可以通过pipeline 的方式来优化。</p></li><li><p>复用预索引数据方式来提高 AGG 性能：</p><p>如通过 <code>terms aggregations</code> 替代 <code>range aggregations</code>， 如要根据年龄来分组，分组目标是: 少年（14岁以下） 青年（14-28） 中年（29-50） 老年（51以上）， 可以在索引的时候设置一个age_group字段，预先将数据进行分类。从而不用按age来做range aggregations, 通过age_group字段就可以了。</p></li><li><p>Cache的设置及使用：</p><p>**a) QueryCache: **ES查询的时候，使用filter查询会使用query cache, 如果业务场景中的过滤查询比较多，建议将querycache设置大一些，以提高查询速度。</p></li></ul><ol><li>indices.queries.cache.size： 10%（默认），//可设置成百分比，也可设置成具体值，如256mb。</li></ol><pre><code>当然也可以禁用查询缓存（默认是开启）， 通过`index.queries.cache.enabled：false`设置。\*\*b) FieldDataCache: \*\*在聚类或排序时，field data cache会使用频繁，因此，设置字段数据缓存的大小，在聚类或排序场景较多的情形下很有必要，可通过indices.fielddata.cache.size：30% 或具体值10GB来设置。但是如果场景或数据变更比较频繁，设置cache并不是好的做法，因为缓存加载的开销也是特别大的。\*\*c) ShardRequestCache: \*\*查询请求发起后，每个分片会将结果返回给协调节点(Coordinating Node), 由协调节点将结果整合。如果有需求，可以设置开启; 通过设置`index.requests.cache.enable: true`来开启。不过，shard request cache 只缓存 hits.total, aggregations, suggestions 类型的数据，并不会缓存hits的内容。也可以通过设置`indices.requests.cache.size: 1%（默认）`来控制缓存空间大小。</code></pre><h1 id="ES的内存设置"><a href="#ES的内存设置" class="headerlink" title="ES的内存设置"></a>ES的内存设置</h1><p>由于ES构建基于lucene, 而lucene设计强大之处在于lucene能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能。lucene的索引文件segements是存储在单文件中的，并且不可变，对于OS来说，能够很友好地将索引文件保持在cache中，以便快速访问；因此，我们很有必要将一半的物理内存留给lucene ; 另一半的物理内存留给ES（JVM heap )。所以， 在ES内存设置方面，可以遵循以下原则：</p><ol><li><p>当机器内存小于64G时，遵循通用的原则，50%给ES，50%留给lucene。</p></li><li><p>当机器内存大于64G时，遵循以下原则：</p><ul><li>a. 如果主要的使用场景是全文检索, 那么建议给ES Heap分配 4~32G的内存即可；其它内存留给操作系统, 供lucene使用（segments cache), 以提供更快的查询性能。</li><li>b. 如果主要的使用场景是聚合或排序， 并且大多数是numerics, dates, geo_points 以及not_analyzed的字符类型， 建议分配给ES Heap分配 4~32G的内存即可，其它内存留给操作系统，供lucene使用(doc values cache)，提供快速的基于文档的聚类、排序性能。</li><li>c. 如果使用场景是聚合或排序，并且都是基于analyzed 字符数据，这时需要更多的 heap size, 建议机器上运行多ES实例，每个实例保持不超过50%的ES heap设置(但不超过32G，堆内存设置32G以下时，JVM使用对象指标压缩技巧节省空间)，50%以上留给lucene。</li></ul></li><li><p>禁止swap，一旦允许内存与磁盘的交换，会引起致命的性能问题。 通过： 在elasticsearch.yml 中 bootstrap.memory_lock: true， 以保持JVM锁定内存，保证ES的性能。</p></li><li><p>GC设置原则：</p><ul><li>a. 保持GC的现有设置，默认设置为：Concurrent-Mark and Sweep (CMS)，别换成G1GC，因为目前G1还有很多BUG。</li><li>b. 保持线程池的现有设置，目前ES的线程池较1.X有了较多优化设置，保持现状即可；默认线程池大小等于CPU核心数。如果一定要改，按公式（（CPU核心数* 3）/ 2）+ 1 设置；不能超过CPU核心数的2倍；但是不建议修改默认配置，否则会对CPU造成硬伤。</li></ul></li></ol><h1 id="调整JVM设置"><a href="#调整JVM设置" class="headerlink" title="调整JVM设置#"></a>调整JVM设置<a href="https://www.cnblogs.com/jajian/p/10465519.html#调整jvm设置" target="_blank" rel="noopener">#</a></h1><p>ES 是在 lucene 的基础上进行研发的，隐藏了 lucene 的复杂性，提供简单易用的 RESTful Api接口。ES 的分片相当于 lucene 的索引。由于 lucene 是 Java 语言开发的，是 Java 语言就涉及到 JVM，所以 ES 存在 JVM的调优问题。</p><ul><li>调整内存大小。当频繁出现full gc后考虑增加内存大小，但是堆内存和堆外内存不要超过32G。</li><li>调整写入的线程数和队列大小。不过线程数最大不能超过33个（es控制死）。</li><li>ES非常依赖文件系统缓存，以便快速搜索。一般来说，应该至少确保物理上有一半的可用内存分配到文件系统缓存。</li></ul><p>参考文档：</p><ol><li><a href="https://blog.csdn.net/ok0011/article/details/82185133" target="_blank" rel="noopener">elasticsearch倒排表压缩及缓存合并策略</a></li><li><a href="https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps" target="_blank" rel="noopener">Frame of Reference and Roaring Bitmaps</a></li><li><a href="https://zhuanlan.zhihu.com/p/33671444" target="_blank" rel="noopener">elasticsearch 倒排索引原理</a></li><li><a href="https://zhuanlan.zhihu.com/p/43437056" target="_blank" rel="noopener">Elasticsearch性能优化总结</a></li><li><a href="https://blog.51cto.com/13527416/2132270?source=dra" target="_blank" rel="noopener">亿级 Elasticsearch 性能优化</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/jajian/category/1280015.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/jajian/category/128
      
    
    </summary>
    
      <category term="elasticsearch" scheme="http://zhang-yu.me/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="http://zhang-yu.me/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>三次握手-四次挥手-你真的懂吗</title>
    <link href="http://zhang-yu.me/2020/09/28/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B-%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B-%E4%BD%A0%E7%9C%9F%E7%9A%84%E6%87%82%E5%90%97/"/>
    <id>http://zhang-yu.me/2020/09/28/三次握手-四次挥手-你真的懂吗/</id>
    <published>2020-09-28T03:00:00.000Z</published>
    <updated>2020-11-03T13:59:35.881Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/qcrao-2018/p/10182185.html#%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8Bredis%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90" target="_blank" rel="noopener">https://www.cnblogs.com/qcrao-2018/p/10182185.html#%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8Bredis%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90</a></p><p>“三次握手，四次挥手”你真的懂吗？ </p><blockquote><p>记得刚毕业找工作面试的时候，经常会被问到：你知道“3次握手，4次挥手”吗？这时候我会“胸有成竹”地“背诵”前期准备好的“答案”，第一次怎么怎么，第二次……答完就没有下文了，面试官貌似也没有深入下去的意思，深入下去我也不懂，皆大欢喜！</p><p>作为程序员，要有“刨根问底”的精神。知其然，更要知其所以然。这篇文章希望能抽丝剥茧，还原背后的原理。  </p><p>目录</p><ul><li>[什么是“3次握手，4次挥手”]<ul><li>[TCP服务模型] </li><li>[TCP头部]</li><li>[状态转换]</li></ul></li><li>[为什么要“三次握手，四次挥手”]<ul><li>[三次握手]</li><li>[四次挥手]</li></ul></li><li>[“三次握手，四次挥手”怎么完成？]<ul><li>[三次握手]</li><li>[四次挥手]</li><li>[为什么建立连接是三次握手，而关闭连接却是四次挥手呢？]</li></ul></li><li>[“三次握手，四次挥手”进阶]<ul><li>[ISN]</li><li>[序列号回绕]</li><li>[syn flood攻击]<ul><li>[无效连接的监视释放]</li><li>[延缓TCB分配方法]<ul><li>[Syn Cache技术]</li><li>[Syn Cookie技术]</li></ul></li><li>[使用SYN Proxy防火墙]</li></ul></li><li>[连接队列]<ul><li>[半连接队列满了]</li><li>[全连接队列满了]</li><li>[命令]</li><li>[小结]</li></ul></li></ul></li><li>[“三次握手，四次挥手”redis实例分析]</li><li>[总结]</li><li>[参考资料]</li></ul><h1 id="什么是“3次握手，4次挥手”"><a href="#什么是“3次握手，4次挥手”" class="headerlink" title="什么是“3次握手，4次挥手”"></a>什么是“3次握手，4次挥手”</h1><p>TCP是一种面向连接的单播协议，在发送数据前，通信双方必须在彼此间建立一条连接。所谓的“连接”，其实是客户端和服务器的内存里保存的一份关于对方的信息，如ip地址、端口号等。</p><p>TCP可以看成是一种字节流，它会处理IP层或以下的层的丢包、重复以及错误问题。在连接的建立过程中，双方需要交换一些连接的参数。这些参数可以放在TCP头部。</p><p>TCP提供了一种可靠、面向连接、字节流、传输层的服务，采用三次握手建立一个连接。采用4次挥手来关闭一个连接。</p><h2 id="TCP服务模型"><a href="#TCP服务模型" class="headerlink" title="TCP服务模型"></a>TCP服务模型</h2><p>在了解了建立连接、关闭连接的“三次握手和四次挥手”后，我们再来看下TCP相关的东西。</p><p>一个TCP连接由一个4元组构成，分别是两个IP地址和两个端口号。一个TCP连接通常分为三个阶段：启动、数据传输、退出（关闭）。</p><p>当TCP接收到另一端的数据时，它会发送一个确认，但这个确认不会立即发送，一般会延迟一会儿。ACK是累积的，一个确认字节号N的ACK表示所有直到N的字节（不包括N）已经成功被接收了。这样的好处是如果一个ACK丢失，很可能后续的ACK就足以确认前面的报文段了。</p><p>一个完整的TCP连接是双向和对称的，数据可以在两个方向上平等地流动。给上层应用程序提供一种<code>双工服务</code>。一旦建立了一个连接，这个连接的一个方向上的每个TCP报文段都包含了相反方向上的报文段的一个ACK。</p><p>序列号的作用是使得一个TCP接收端可丢弃重复的报文段，记录以杂乱次序到达的报文段。因为TCP使用IP来传输报文段，而IP不提供重复消除或者保证次序正确的功能。另一方面，TCP是一个字节流协议，绝不会以杂乱的次序给上层程序发送数据。因此TCP接收端会被迫先保持大序列号的数据不交给应用程序，直到缺失的小序列号的报文段被填满。</p><h2 id="TCP头部"><a href="#TCP头部" class="headerlink" title="TCP头部"></a>TCP头部</h2><p><img src="https://upload-images.jianshu.io/upload_images/12234098-40089b5b24b9d38b.png" referrerpolicy="no-referrer" width="50%" height="50%"><br>源端口和目的端口在TCP层确定双方进程，序列号表示的是报文段数据中的第一个字节号，ACK表示确认号，该确认号的发送方期待接收的下一个序列号，即最后被成功接收的数据字节序列号加1，这个字段只有在ACK位被启用的时候才有效。</p><p>当新建一个连接时，从客户端发送到服务端的第一个报文段的SYN位被启用，这称为SYN报文段，这时序列号字段包含了在本次连接的这个方向上要使用的第一个序列号，即初始序列号<code>ISN</code>，之后发送的数据是ISN加1，因此SYN位字段会<code>消耗</code>一个序列号，这意味着使用重传进行可靠传输。而不消耗序列号的ACK则不是。</p><p>头部长度（图中的数据偏移）以32位字为单位，也就是以4bytes为单位，它只有4位，最大为15，因此头部最大长度为60字节，而其最小为5，也就是头部最小为20字节（可变选项为空）。</p><p>ACK —— 确认，使得确认号有效。<br>RST —— 重置连接（经常看到的reset by peer）就是此字段搞的鬼。<br>SYN —— 用于初如化一个连接的序列号。<br>FIN —— 该报文段的发送方已经结束向对方发送数据。</p><p>当一个连接被建立或被终止时，交换的报文段只包含TCP头部，而没有数据。</p><h2 id="状态转换"><a href="#状态转换" class="headerlink" title="状态转换"></a>状态转换</h2><p>三次握手和四次挥手的状态转换如下图。<br><img src="https://upload-images.jianshu.io/upload_images/12234098-40f65020a755ca18" referrerpolicy="no-referrer" width="50%" height="50%"> </p><h1 id="为什么要“三次握手，四次挥手”"><a href="#为什么要“三次握手，四次挥手”" class="headerlink" title="为什么要“三次握手，四次挥手”"></a>为什么要“三次握手，四次挥手”</h1><h2 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h2><p>换个易于理解的视角来看为什么要3次握手。</p><p>客户端和服务端通信前要进行连接，“3次握手”的作用就是<code>双方都能明确自己和对方的收、发能力是正常的</code>。</p><p><code>第一次握手</code>：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。</p><p><code>第二次握手</code>：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。  </p></blockquote><blockquote><p>从客户端的视角来看，我接到了服务端发送过来的响应数据包，说明服务端接收到了我在第一次握手时发送的网络包，并且成功发送了响应数据包，这就说明，服务端的接收、发送能力正常。而另一方面，我收到了服务端的响应数据包，说明我第一次发送的网络包成功到达服务端，这样，我自己的发送和接收能力也是正常的。</p><p><code>第三次握手</code>：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力，服务端的发送、接收能力是正常的。  </p></blockquote><blockquote><p>第一、二次握手后，服务端并不知道客户端的接收能力以及自己的发送能力是否正常。而在第三次握手时，服务端收到了客户端对第二次握手作的回应。从服务端的角度，我在第二次握手时的响应数据发送出去了，客户端接收到了。所以，我的发送能力是正常的。而客户端的接收能力也是正常的。</p><p>经历了上面的三次握手过程，客户端和服务端都确认了自己的接收、发送能力是正常的。之后就可以正常通信了。</p><p>每次都是接收到数据包的一方可以得到一些结论，发送的一方其实没有任何头绪。我虽然有发包的动作，但是我怎么知道我有没有发出去，而对方有没有接收到呢？</p><p>而从上面的过程可以看到，最少是需要三次握手过程的。两次达不到让双方都得出自己、对方的接收、发送能力都正常的结论。其实每次收到网络包的一方至少是可以得到：对方的发送、我方的接收是正常的。而每一步都是有关联的，下一次的“响应”是由于第一次的“请求”触发，因此每次握手其实是可以得到额外的结论的。比如第三次握手时，服务端收到数据包，表明看服务端只能得到客户端的发送能力、服务端的接收能力是正常的，但是结合第二次，说明服务端在第二次发送的响应包，客户端接收到了，并且作出了响应，从而得到额外的结论：客户端的接收、服务端的发送是正常的。</p></blockquote><blockquote><h2 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h2><p>TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解<code>“上层的意志”</code>。</p><h1 id="“三次握手，四次挥手”怎么完成？"><a href="#“三次握手，四次挥手”怎么完成？" class="headerlink" title="“三次握手，四次挥手”怎么完成？"></a>“三次握手，四次挥手”怎么完成？</h1><p>其实3次握手的目的并不只是让通信双方都了解到一个连接正在建立，还在于利用数据包的选项来传输特殊的信息，交换初始序列号ISN。</p><p>3次握手是指发送了3个报文段，4次挥手是指发送了4个报文段。注意，SYN和FIN段都是会利用重传进行可靠传输的。<br><img src="https://upload-images.jianshu.io/upload_images/12234098-8604b533d42457b5.png0" referrerpolicy="no-referrer" width="100%" height="100%"> </p><h2 id="三次握手-1"><a href="#三次握手-1" class="headerlink" title="三次握手"></a>三次握手</h2><ol><li>客户端发送一个SYN段，并指明客户端的初始序列号，即ISN.</li><li>服务端发送自己的SYN段作为应答，同样指明自己的ISN+1作为ACK数值。这样，每发送一个SYN，序列号就会加1. 如果有丢失的情况，则会重传。</li><li>为了确认服务器端的SYN，客户端将ISN+1作为返回的ACK数值。</li></ol><h2 id="四次挥手-1"><a href="#四次挥手-1" class="headerlink" title="四次挥手"></a>四次挥手</h2><p><img src="https://upload-images.jianshu.io/upload_images/12234098-3754de754cbcf2af.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p></blockquote><blockquote><ol><li>客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。</li><li>服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。</li><li>服务端发起自己的FIN段，ACK=K+1, Seq=L</li><li>客户端确认。ACK=L+1</li></ol><h2 id="为什么建立连接是三次握手，而关闭连接却是四次挥手呢？"><a href="#为什么建立连接是三次握手，而关闭连接却是四次挥手呢？" class="headerlink" title="为什么建立连接是三次握手，而关闭连接却是四次挥手呢？"></a>为什么建立连接是三次握手，而关闭连接却是四次挥手呢？</h2><p>这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方是否现在关闭发送数据通道，需要上层应用来决定，因此，己方ACK和FIN一般都会分开发送。</p><h1 id="“三次握手，四次挥手”进阶"><a href="#“三次握手，四次挥手”进阶" class="headerlink" title="“三次握手，四次挥手”进阶"></a>“三次握手，四次挥手”进阶</h1><h2 id="ISN"><a href="#ISN" class="headerlink" title="ISN"></a>ISN</h2><p>三次握手的一个重要功能是客户端和服务端交换ISN, 以便让对方知道接下来接收数据的时候如何按序列号组装数据。</p><p>如果ISN是固定的，攻击者很容易猜出后续的确认号。</p><pre><code>ISN = M + F</code></pre><p>M是一个计时器，每隔4微秒加1。<br>F是一个Hash算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值。要保证hash算法不能被外部轻易推算得出。</p><h2 id="序列号回绕"><a href="#序列号回绕" class="headerlink" title="序列号回绕"></a>序列号回绕</h2><p>因为ISN是随机的，所以序列号容易就会超过2^31 -1. 而tcp对于丢包和乱序等问题的判断都是依赖于序列号大小比较的。此时就出现了所谓的tcp序列号回绕（sequence wraparound）问题。怎么解决？</p><pre><code>/** The next routines deal with comparing 32 bit unsigned ints* and worry about wraparound .*/static inline int before{    return  &lt; 0;}#define after</code></pre><p>上述代码是内核中的解决回绕问题代码。__s32是有符号整型的意思，而__u32则是无符号整型。序列号发生回绕后，序列号变小，相减之后，把结果变成有符号数了，因此结果成了负数。</p><pre><code>假设seq1=255， seq2=1（发生了回绕）。seq1 = 1111 1111 seq2 = 0000 0001我们希望比较结果是 seq1 - seq2= 1111 1111-0000 0001----------- 1111 1110由于我们将结果转化成了有符号数，由于最高位是1，因此结果是一个负数，负数的绝对值为 0000 0001 + 1 = 0000 0010 = 2因此seq1 - seq2 &lt; 0</code></pre><h2 id="syn-flood攻击"><a href="#syn-flood攻击" class="headerlink" title="syn flood攻击"></a>syn flood攻击</h2><p>最基本的DoS攻击就是利用合理的服务请求来占用过多的服务资源，从而使合法用户无法得到服务的响应。syn flood属于Dos攻击的一种。</p><p>如果恶意的向某个服务器端口发送大量的SYN包，则可以使服务器打开大量的半开连接，分配TCB（Transmission Control Block）, 从而消耗大量的服务器资源，同时也使得正常的连接请求无法被相应。当开放了一个TCP端口后，该端口就处于Listening状态，不停地监视发到该端口的Syn报文，一 旦接收到Client发来的Syn报文，就需要为该请求分配一个TCB，通常一个TCB至少需要280个字节，在某些操作系统中TCB甚至需要1300个字节，并返回一个SYN ACK命令，立即转为SYN-RECEIVED即半开连接状态。系统会为此耗尽资源。</p><p>常见的防攻击方法有：</p><h3 id="无效连接的监视释放"><a href="#无效连接的监视释放" class="headerlink" title="无效连接的监视释放"></a>无效连接的监视释放</h3><p>监视系统的半开连接和不活动连接，当达到一定阈值时拆除这些连接，从而释放系统资源。这种方法对于所有的连接一视同仁，而且由于SYN Flood造成的半开连接数量很大，正常连接请求也被淹没在其中被这种方式误释放掉，因此这种方法属于入门级的SYN Flood方法。</p><h3 id="延缓TCB分配方法"><a href="#延缓TCB分配方法" class="headerlink" title="延缓TCB分配方法"></a>延缓TCB分配方法</h3><p>消耗服务器资源主要是因为当SYN数据报文一到达，系统立即分配TCB，从而占用了资源。而SYN Flood由于很难建立起正常连接，因此，当正常连接建立起来后再分配TCB则可以有效地减轻服务器资源的消耗。常见的方法是使用Syn Cache和Syn Cookie技术。</p><h4 id="Syn-Cache技术"><a href="#Syn-Cache技术" class="headerlink" title="Syn Cache技术"></a>Syn Cache技术</h4><p>系统在收到一个SYN报文时，在一个专用HASH表中保存这种半连接信息，直到收到正确的回应ACK报文再分配TCB。这个开销远小于TCB的开销。当然还需要保存序列号。</p><h4 id="Syn-Cookie技术"><a href="#Syn-Cookie技术" class="headerlink" title="Syn Cookie技术"></a>Syn Cookie技术</h4><p>Syn Cookie技术则完全不使用任何存储资源，这种方法比较巧妙，它使用一种特殊的算法生成Sequence Number，这种算法考虑到了对方的IP、端口、己方IP、端口的固定信息，以及对方无法知道而己方比较固定的一些信息，如MSS、时间等，在收到对方 的ACK报文后，重新计算一遍，看其是否与对方回应报文中的（Sequence Number-1）相同，从而决定是否分配TCB资源。</p><h3 id="使用SYN-Proxy防火墙"><a href="#使用SYN-Proxy防火墙" class="headerlink" title="使用SYN Proxy防火墙"></a>使用SYN Proxy防火墙</h3><p>一种方式是防止墙dqywb连接的有效性后，防火墙才会向内部服务器发起SYN请求。防火墙代服务器发出的SYN ACK包使用的序列号为c, 而真正的服务器回应的序列号为c’, 这样，在每个数据报文经过防火墙的时候进行序列号的修改。另一种方式是防火墙确定了连接的安全后，会发出一个safe reset命令，client会进行重新连接，这时出现的syn报文会直接放行。这样不需要修改序列号了。但是，client需要发起两次握手过程，因此建立连接的时间将会延长。</p><h2 id="连接队列"><a href="#连接队列" class="headerlink" title="连接队列"></a>连接队列</h2><p>在外部请求到达时，被服务程序最终感知到前，连接可能处于SYN_RCVD状态或是ESTABLISHED状态，但还未被应用程序接受。<br>  <img src="https://upload-images.jianshu.io/upload_images/12234098-36b3c46688c685c7.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p><p>对应地，服务器端也会维护两种队列，处于SYN_RCVD状态的半连接队列，而处于ESTABLISHED状态但仍未被应用程序accept的为全连接队列。如果这两个队列满了之后，就会出现各种丢包的情形。</p><pre><code>查看是否有连接溢出netstat -s | grep LISTEN</code></pre><h3 id="半连接队列满了"><a href="#半连接队列满了" class="headerlink" title="半连接队列满了"></a>半连接队列满了</h3><p>在三次握手协议中，服务器维护一个半连接队列，该队列为每个客户端的SYN包开设一个条目，该条目表明服务器已收到SYN包，并向客户发出确认，正在等待客户的确认包。这些条目所标识的连接在服务器处于Syn_RECV状态，当服务器收到客户的确认包时，删除该条目，服务器进入ESTABLISHED状态。</p><blockquote><p>目前，Linux下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s, 总共31s, 称为<code>指数退避</code>，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s, TCP才会把断开这个连接。由于，<code>SYN超时需要63秒</code>，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server，用于耗尽Server的SYN队列。对于应对SYN 过多的问题，linux提供了几个TCP参数：tcp_syncookies、tcp_synack_retries、tcp_max_syn_backlog、tcp_abort_on_overflow 来调整应对。</p></blockquote></blockquote><blockquote><p>tcp_syncookies</p><p>SYNcookie将连接信息编码在ISN中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。</p><p>tcp_syncookies</p><p>内核放弃建立连接之前发送SYN包的数量。</p><p>tcp_synack_retries</p><p>内核放弃连接之前发送SYN+ACK包的数量</p><p>tcp_max_syn_backlog</p><p>默认为1000. 这表示半连接队列的长度，如果超过则放弃当前连接。</p><p>tcp_abort_on_overflow</p><p>如果设置了此项，则直接reset. 否则，不做任何操作，这样当服务器半连接队列有空了之后，会重新接受连接。<code>Linux坚持在能力许可范围内不忽略进入的连接</code>。客户端在这期间会重复发送sys包，当重试次数到达上限之后，会得到<code>connection time out</code>响应。</p><h3 id="全连接队列满了"><a href="#全连接队列满了" class="headerlink" title="全连接队列满了"></a>全连接队列满了</h3><p>当第三次握手时，当server接收到ACK包之后，会进入一个新的叫 accept 的队列。</p><p>当accept队列满了之后，即使client继续向server发送ACK的包，也会不被响应，此时ListenOverflows+1，同时server通过tcp_abort_on_overflow来决定如何返回，0表示直接丢弃该ACK，1表示发送RST通知client；相应的，client则会分别返回<code>read timeout</code> 或者 <code>connection reset by peer</code>。另外，tcp_abort_on_overflow是0的话，server过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短，就很容易异常了。而客户端收到多个 SYN ACK 包，则会认为之前的 ACK 丢包了。于是促使客户端再次发送 ACK ，在 accept队列有空闲的时候最终完成连接。若 accept队列始终满员，则最终客户端收到 RST 包（此时服务端发送syn+ack的次数超出了tcp_synack_retries）。</p><p>服务端仅仅只是创建一个定时器，以固定间隔重传syn和ack到服务端</p></blockquote><blockquote><p>tcp_abort_on_overflow</p><p>如果设置了此项，则直接reset. 否则，不做任何操作，这样当服务器半连接队列有空了之后，会重新接受连接。<code>Linux坚持在能力许可范围内不忽略进入的连接</code>。客户端在这期间会重复发送sys包，当重试次数到达上限之后，会得到<code>connection time out</code>响应。</p><p>min</p><p>全连接队列的长度。</p><h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><p>netstat -s命令</p><pre><code>[root@server ~]#  netstat -s | egrep &quot;listen|LISTEN&quot; 667399 times the listen queue of a socket overflowed667399 SYNs to LISTEN sockets ignored</code></pre><p>上面看到的 667399 times ，表示全连接队列溢出的次数，隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了。</p><pre><code>[root@server ~]#  netstat -s | grep TCPBacklogDrop</code></pre><p>查看 Accept queue 是否有溢出</p><p>ss命令</p><pre><code>[root@server ~]#  ss -lntState Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN     0      128 *:6379 *:*LISTEN     0      128 *:22 *:*</code></pre><p>如果State是listen状态，Send-Q 表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少。<br>非 LISTEN 状态中 Recv-Q 表示 receive queue 中的 bytes 数量；Send-Q 表示 send queue 中的 bytes 数值。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>当外部连接请求到来时，TCP模块会首先查看max_syn_backlog，如果处于SYN_RCVD状态的连接数目超过这一阈值，进入的连接会被拒绝。根据tcp_abort_on_overflow字段来决定是直接丢弃，还是直接reset.</p><p>从服务端来说，三次握手中，第一步server接受到client的syn后，把相关信息放到半连接队列中，同时回复syn+ack给client. 第三步当收到客户端的ack, 将连接加入到全连接队列。</p><p>一般，全连接队列比较小，会先满，此时半连接队列还没满。如果这时收到syn报文，则会进入半连接队列，没有问题。但是如果收到了三次握手中的第3步，则会根据tcp_abort_on_overflow字段来决定是直接丢弃，还是直接reset.此时，客户端发送了ACK, 那么客户端认为三次握手完成，它认为服务端已经准备好了接收数据的准备。但此时服务端可能因为全连接队列满了而无法将连接放入，会重新发送第2步的syn+ack, 如果这时有数据到来，服务器TCP模块会将数据存入队列中。一段时间后，client端没收到回复，超时，连接异常，client会主动关闭连接。</p><h1 id="“三次握手，四次挥手”redis实例分析"><a href="#“三次握手，四次挥手”redis实例分析" class="headerlink" title="“三次握手，四次挥手”redis实例分析"></a>“三次握手，四次挥手”redis实例分析</h1><ol><li>我在dev机器上部署redis服务，端口号为6379,</li><li><p>通过tcpdump工具获取数据包，使用如下命令</p><p>tcpdump -w /tmp/a.cap port 6379 -s0<br>-w把数据写入文件，-s0设置每个数据包的大小默认为68字节，如果用-S 0则会抓到完整数据包</p></li></ol><ol start="3"><li>在dev2机器上用redis-cli访问dev:6379, 发送一个ping, 得到回复pong</li><li><p>停止抓包，用tcpdump读取捕获到的数据包</p><p>tcpdump -r /tmp/a.cap -n -nn -A -x| vim -<br>（-x 以16进制形式展示，便于后面分析）</p></li></ol><p>共收到了7个包。</p><p>抓到的是IP数据包，IP数据包分为IP头部和IP数据部分，IP数据部分是TCP头部加TCP数据部分。</p><p>IP的数据格式为：<br>  <img src="https://upload-images.jianshu.io/upload_images/12234098-e6b04f3e9bebdac4.png" referrerpolicy="no-referrer" width="100%" height="100%"><br>它由固定长度20B+可变长度构成。</p><pre><code>10:55:45.662077 IP dev2.39070 &gt; dev.6379: Flags [S], seq 4133153791, win 29200, options [mss 1460,sackOK,TS val 2959270704 ecr 0,nop,wscale 7], length 0        0x0000:  4500 003c 08cf 4000 3606 14a5 0ab3 b561        0x0010:  0a60 5cd4 989e 18eb f65a ebff 0000 0000        0x0020:  a002 7210 872f 0000 0204 05b4 0402 080a        0x0030:  b062 e330 0000 0000 0103 0307</code></pre><p>对着IP头部格式，来拆解数据包的具体含义。</p><p>字节值</p><p>字节含义</p><p>0x4</p><p>IP版本为ipv4</p><p>0x5</p><p>首部长度为5 * 4字节=20B</p><p>0x00</p><p>服务类型，现在基本都置为0</p><p>0x003c</p><p>总长度为3*16+12=60字节，上面所有的长度就是60字节</p><p>0x08cf</p><p>标识。同一个数据报的唯一标识。当IP数据报被拆分时，会复制到每一个数据中。</p><p>0x4000</p><p><code>3bit 标志 + 13bit 片偏移</code>。3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8 )</p><p>0x36</p><p>生存时间TTL。IP报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。 此处为54.</p><p>0x06</p><p>协议类型。指出IP报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。</p><p>0x14a5</p><p>16bitIP首部校验和。</p><p>0x0ab3 b561</p><p>32bit源ip地址。</p><p>0x0a60 5cd4</p><p>32bit目的ip地址。</p><p>剩余的数据部分即为TCP协议相关的。TCP也是20B固定长度+可变长度部分。</p><p>字节值</p><p>字节含义</p><p>0x989e</p><p>16bit源端口。1_16_16_16+8_16_16+14_16+11=39070</p><p>0x18eb</p><p>16bit目的端口6379</p><p>0xf65a ebff</p><p>32bit序列号。4133153791</p><p>0x0000 0000</p><p>32bit确认号。</p><p>0xa</p><p>4bit首部长度，以4byte为单位。共10*4=40字节。因此TCP报文的可选长度为40-20=20</p><p>0b000000</p><p>6bit保留位。目前置为0.</p><p>0b000010</p><p>6bitTCP标志位。从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。</p><p>0x7210</p><p>滑动窗口大小，滑动窗口即tcp接收缓冲区的大小，用于tcp拥塞控制。29200</p><p>0x872f</p><p>16bit校验和。</p><p>0x0000</p><p>紧急指针。仅在 URG = 1时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。</p><p>可变长度部分，协议如下：</p><p>字节值</p><p>字节含义</p><p>0x0204 05b4</p><p>最大报文长度为，05b4=1460. 即可接收的最大包长度，通常为MTU减40字节，IP头和TCP头各20字节</p><p>0x0402</p><p>表示支持SACK</p><p>0x080a b062 e330 0000 0000</p><p>时间戳。Ts val=b062 e330=2959270704, ecr=0</p><p>0x01</p><p>无操作</p><p>0x03 0307</p><p>窗口扩大因子为7. 移位7, 乘以128</p><p>这样第一个包分析完了。dev2向dev发送SYN请求。<code>也就是三次握手中的第一次了。</code><br><code>SYN seq=4133153791</code></p><p>第二个包，dev响应连接，ack=4133153792. 表明dev下次准备接收这个序号的包，用于tcp字节注的顺序控制。dev（也就是server端）的初始序号为seq=4264776963, syn=1.<br><code>SYN ack=seq=4264776963</code></p><p>第三个包，client包确认，这里使用了相对值应答。seq=4133153792, 等于第二个包的ack. ack=4264776964.<br><code>ack=seq+1</code><br>至此，三次握手完成。接下来就是发送ping和pong的数据了。</p><p>接着第四个包。</p><pre><code>10:55:48.090073 IP dev2.39070 &gt; dev.6379: Flags [P.], seq 1:15, ack 1, win 229, options [nop,nop,TS val 2959273132 ecr 3132256230], length 14        0x0000:  4500 0042 08d1 4000 3606 149d 0ab3 b561        0x0010:  0a60 5cd4 989e 18eb f65a ec00 fe33 5504        0x0020:  8018 00e5 4b5f 0000 0101 080a b062 ecac        0x0030:  bab2 6fe6 2a31 0d0a 2434 0d0a 7069 6e67        0x0040:  0d0a</code></pre><p>tcp首部长度为32B, 可选长度为12B. IP报文的总长度为66B, 首部长度为20B, 因此TCP数据部分长度为14B. seq=0xf65a ec00=4133153792<br>ACK, PSH. 数据部分为2a31 0d0a 2434 0d0a 7069 6e67 0d0a</p><pre><code>0x2a31         -&gt; *10x0d0a         -&gt; \r\n0x2434         -&gt; $40x0d0a         -&gt; \r\n0x7069 0x6e67  -&gt; ping0x0d0a         -&gt; \r\n</code></pre><p>dev2向dev发送了ping数据，第四个包完毕。</p><p>第五个包，dev2向dev发送ack响应。<br>序列号为0xfe33 5504=4264776964, ack确认号为0xf65a ec0e=4133153806=.</p><p>第六个包，dev向dev2响应pong消息。序列号fe33 5504，确认号f65a ec0e, TCP头部可选长度为12B, IP数据报总长度为59B, 首部长度为20B, 因此TCP数据长度为7B.<br>数据部分2b50 4f4e 470d 0a, 翻译过来就是<code>+PONG\r\n</code>.</p><p>至此，Redis客户端和Server端的三次握手过程分析完毕。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>“三次握手，四次挥手”看似简单，但是深究进去，还是可以延伸出很多知识点的。比如半连接队列、全连接队列等等。以前关于TCP建立连接、关闭连接的过程很容易就会忘记，可能是因为只是死记硬背了几个过程，没有深入研究背后的原理。</p><p>所以，“三次握手，四次挥手”你真的懂了吗？欢迎一起交流~~</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>【redis】[<a href="https://segmentfault.com/a/1190000015044878]" target="_blank" rel="noopener">https://segmentfault.com/a/1190000015044878]</a><br>【tcp option】[<a href="https://blog.csdn.net/wdscq1234/article/details/52423272]" target="_blank" rel="noopener">https://blog.csdn.net/wdscq1234/article/details/52423272]</a><br>【滑动窗口】[<a href="https://www.zhihu.com/question/32255109]" target="_blank" rel="noopener">https://www.zhihu.com/question/32255109]</a><br>【全连接队列】[<a href="http://jm.taobao.org/2017/05/25/525-1/]" target="_blank" rel="noopener">http://jm.taobao.org/2017/05/25/525-1/]</a><br>【client fooling】 [<a href="https://github.com/torvalds/linux/commit/5ea8ea2cb7f1d0db15762c9b0bb9e7330425a071]" target="_blank" rel="noopener">https://github.com/torvalds/linux/commit/5ea8ea2cb7f1d0db15762c9b0bb9e7330425a071]</a><br>【backlog RECV_Q】[<a href="http://blog.51cto.com/59090939/1947443]" target="_blank" rel="noopener">http://blog.51cto.com/59090939/1947443]</a><br>【定时器】[<a href="https://www.cnblogs.com/menghuanbiao/p/5212131.html]" target="_blank" rel="noopener">https://www.cnblogs.com/menghuanbiao/p/5212131.html]</a><br>【队列图示】[<a href="https://www.itcodemonkey.com/article/5834.html]" target="_blank" rel="noopener">https://www.itcodemonkey.com/article/5834.html]</a><br>【tcp flood攻击】[<a href="https://www.cnblogs.com/hubavyn/p/4477883.html]" target="_blank" rel="noopener">https://www.cnblogs.com/hubavyn/p/4477883.html]</a><br>【MSS MTU】[<a href="https://blog.csdn.net/LoseInVain/article/details/53694265]" target="_blank" rel="noopener">https://blog.csdn.net/LoseInVain/article/details/53694265]</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/qcrao-2018/p/10182185.html#%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8Bredis
      
    
    </summary>
    
      <category term="it" scheme="http://zhang-yu.me/categories/it/"/>
    
    
      <category term="it" scheme="http://zhang-yu.me/tags/it/"/>
    
  </entry>
  
  <entry>
    <title>线程数究竟设多少合理</title>
    <link href="http://zhang-yu.me/2020/09/28/%E7%BA%BF%E7%A8%8B%E6%95%B0%E7%A9%B6%E7%AB%9F%E8%AE%BE%E5%A4%9A%E5%B0%91%E5%90%88%E7%90%86/"/>
    <id>http://zhang-yu.me/2020/09/28/线程数究竟设多少合理/</id>
    <published>2020-09-28T03:00:00.000Z</published>
    <updated>2020-11-03T14:02:21.012Z</updated>
    
    <content type="html"><![CDATA[<p>58沈剑 架构师之路 2016-03-29</p><blockquote><p><strong>一、需求缘起</strong></p><p>Web-Server通常有个配置，<strong>最大工作线程数</strong>，后端服务一般也有个配置，工作线程池的<strong>线程数量</strong>，这个线程数的配置不同的业务架构师有不同的经验值，有些业务设置为CPU核数的2倍，有些业务设置为CPU核数的8倍，有些业务设置为CPU核数的32倍。</p><p>“工作线程数”的设置依据是什么，到底设置为多少能够最大化CPU性能，是本文要讨论的问题。</p><h2 id="二、一些共性认知"><a href="#二、一些共性认知" class="headerlink" title="二、一些共性认知"></a><strong>二、一些共性认知</strong></h2><p>在进行进一步深入讨论之前，先以提问的方式就一些共性认知达成一致。</p><p><strong>提问：工作线程数是不是设置的越大越好？</strong></p><p>回答：肯定不是的</p><p>1）一来服务器CPU核数有限，同时并发的线程数是有限的，1核CPU设置10000个工作线程没有意义</p><p>2）线程切换是有开销的，如果线程切换过于频繁，反而会使性能降低</p><p><strong>提问：调用sleep()**</strong>函数的时候，线程是否一直占用CPU<strong>**？</strong></p><p>回答：不占用，等待时会把CPU让出来，给其他需要CPU资源的线程使用</p><p>不止调用sleep()函数，在进行一些阻塞调用，例如网络编程中的<strong>阻塞</strong>accept()【等待客户端连接】和<strong>阻塞</strong>recv()【等待下游回包】也不占用CPU资源</p><p><strong>提问：如果CPU**</strong>是单核，设置多线程有意义么，能提高并发性能么？**</p><p>回答：即使是单核，使用多线程也是有意义的</p><p>1）多线程编码可以让我们的服务/代码更加清晰，有些IO线程收发包，有些Worker线程进行任务处理，有些Timeout线程进行超时检测</p><p>2）如果有一个任务一直占用CPU资源在进行计算，那么此时增加线程并不能增加并发，例如这样的一个代码</p><p> while(1){ i++; }</p><p>该代码一直不停的占用CPU资源进行计算，会使CPU占用率达到100%</p><p>3）通常来说，Worker线程一般不会一直占用CPU进行计算，此时即使CPU是单核，增加Worker线程也能够提高并发，因为这个线程在休息的时候，其他的线程可以继续工作</p><h2 id="三、常见服务线程模型"><a href="#三、常见服务线程模型" class="headerlink" title="三、常见服务线程模型"></a><strong>三、常见服务线程模型</strong></h2><p>了解常见的服务线程模型，有助于理解服务并发的原理，一般来说互联网常见的服务线程模型有如下两种</p><p><strong>IO**</strong>线程与工作线程通过队列解耦类模型**<br><img src="https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190514151730305-485918824.png" referrerpolicy="no-referrer" width="100%" height="100%"><br>如上图，大部分Web-Server与服务框架都是使用这样的一种“IO线程与Worker线程通过队列解耦”类线程模型：</p><p>1）<strong>有少数几个IO线程</strong>监听上游发过来的请求，并进行收发包（生产者）</p><p>2）<strong>有一个或者多个任务队列</strong>，作为IO线程与Worker线程异步解耦的数据传输通道（临界资源）</p><p>3）<strong>有多个工作线程</strong>执行正真的任务（消费者）</p><p>这个线程模型应用很广，符合大部分场景，这个线程模型的特点是，<strong>工作线程内部是同步阻塞执行任务的</strong>（回想一下tomcat线程中是怎么执行Java程序的，dubbo工作线程中是怎么执行任务的），因此可以通过增加Worker线程数来增加并发能力，今天要讨论的重点是“该模型Worker线程数设置为多少能达到最大的并发”。</p><p><strong>纯异步线程模型</strong></p><p>任何地方都没有阻塞，这种线程模型只需要设置很少的线程数就能够做到很高的吞吐量，Lighttpd有一种单进程单线程模式，并发处理能力很强，就是使用的的这种模型。该模型的缺点是：</p><p>1）如果使用单线程模式，难以利用多CPU多核的优势</p><p>2）程序员更习惯写同步代码，callback的方式对代码的可读性有冲击，对程序员的要求也更高</p><p>3）框架更复杂，往往需要server端收发组件，server端队列，client端收发组件，client端队列，上下文管理组件，有限状态机组件，超时管理组件的支持</p><p>however，这个模型不是今天讨论的重点。</p><h2 id="四、工作线程的工作模式"><a href="#四、工作线程的工作模式" class="headerlink" title="四、工作线程的工作模式"></a><strong>四、工作线程的工作模式</strong></h2><p>了解工作线程的工作模式，对量化分析线程数的设置非常有帮助：<br> <img src="https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190514151718013-879900903.png" referrerpolicy="no-referrer" width="100%" height="100%"><br>上图是一个典型的工作线程的处理过程，从开始处理start到结束处理end，该任务的处理共有7个步骤：</p><p>1）从工作队列里拿出任务，进行一些<strong>本地</strong>初始化计算，例如http协议分析、参数解析、参数校验等</p><p>2）<strong>访问cache</strong>拿一些数据</p><p>3）拿到cache里的数据后，再进行一些<strong>本地</strong>计算，这些计算和业务逻辑相关</p><p>4）通过<strong>RPC调用下游service</strong>再拿一些数据，或者让下游service去处理一些相关的任务</p><p>5）RPC调用结束后，再进行一些<strong>本地</strong>计算，怎么计算和业务逻辑相关</p><p>6）<strong>访问DB</strong>进行一些数据操作</p><p>7）操作完数据库之后做一些收尾工作，同样这些收尾工作也是<strong>本地</strong>计算，和业务逻辑相关</p><p>分析整个处理的时间轴，会发现：</p><p>1）其中1，3，5，7步骤中【上图中粉色时间轴】，线程进行本地业务逻辑计算时<strong>需要占用**</strong>CPU**</p><p>2）而2，4，6步骤中【上图中橙色时间轴】，访问cache、service、DB过程中线程处于一个等待结果的状态，<strong>不需要占用**</strong>CPU**，进一步的分解，这个“等待结果”的时间共分为三部分：</p><p>2.1）请求在网络上传输到下游的cache、service、DB</p><p>2.2）下游cache、service、DB进行任务处理</p><p>2.3）cache、service、DB将报文在网络上传回工作线程</p><h2 id="五、量化分析并合理设置工作线程数"><a href="#五、量化分析并合理设置工作线程数" class="headerlink" title="五、量化分析并合理设置工作线程数"></a><strong>五、量化分析并合理设置工作线程数</strong></h2><p>最后一起来回答工作线程数设置为多少合理的问题。</p><p>通过上面的分析，Worker线程在执行的过程中，有一部计算时间需要占用CPU，另一部分等待时间不需要占用CPU，通过量化分析，例如打日志进行统计，可以统计出整个Worker线程执行过程中这两部分时间的比例，例如：</p><p>1）时间轴1，3，5，7【上图中粉色时间轴】的计算执行时间是100ms</p><p>2）时间轴2，4，6【上图中橙色时间轴】的等待时间也是100ms</p><p>得到的结果是，这个线程计算和等待的时间是1：1，即有50%的时间在计算（占用CPU），50%的时间在等待（不占用CPU）：</p><p>1）假设此时是<strong>单核</strong>，则设置为2个工作线程就可以把CPU充分利用起来，让CPU跑到100%</p><p>2）假设此时是<strong>N核</strong>，则设置为2N个工作现场就可以把CPU充分利用起来，让CPU跑到N*100%</p><p>结论：</p><p>N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。</p><p>经验：</p><p>一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的。</p><h2 id="六、结论"><a href="#六、结论" class="headerlink" title="六、结论"></a><strong>六、结论</strong></h2><p>N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;58沈剑 架构师之路 2016-03-29&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;一、需求缘起&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Web-Server通常有个配置，&lt;strong&gt;最大工作线程数&lt;/strong&gt;，后端服务一般也有个配置，工作线程池的&lt;str
      
    
    </summary>
    
      <category term="it" scheme="http://zhang-yu.me/categories/it/"/>
    
    
      <category term="it" scheme="http://zhang-yu.me/tags/it/"/>
    
  </entry>
  
  <entry>
    <title>如何在Kubernetes上部署MySQL数据库</title>
    <link href="http://zhang-yu.me/2020/08/12/%E5%9C%A8Kubernetes%E4%B8%8A%E9%83%A8%E7%BD%B2MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://zhang-yu.me/2020/08/12/在Kubernetes上部署MySQL数据库/</id>
    <published>2020-08-11T16:00:00.000Z</published>
    <updated>2020-11-03T14:01:57.070Z</updated>
    
    <content type="html"><![CDATA[<p>译者：王延飞</p><p>原文链接：<a href="https://www.magalix.com/blog/kubernetes-and-database" target="_blank" rel="noopener">https://www.magalix.com/blog/kubernetes-and-database</a></p><p><a href="https://mp.weixin.qq.com/s/j-Dbyp2xzk2zbYBZcA-lCQ" target="_blank" rel="noopener">如何在Kubernetes上部署MySQL数据库</a></p><blockquote><p>Kubernetes改变了开发的方式，数据库是应用程序的重要组成部分。在本文中，我们将展示如何在Kubernetes中部署数据库，以及可以使用哪些方法在Kubernetes中部署数据库。</p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p>数据库是一种用于在计算机系统上存储和处理数据的系统。数据库引擎可以在数据库上创建，读取，更新和删除。数据库由数据库管理系统（DBMS）控制。</p><p>在大多数数据库中，数据按行和列进行建模，称为关系型，这种类型的数据库在80年代占主导地位。在2000年代，非关系数据库开始流行，被称为No-SQL，它们使用不同的查询语言，并且这些类型的数据库可用于键值对。</p><h2 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h2><p>在本文中，我们将在Kubernetes中部署数据库，因此我们必须了解什么是StatefulSet。</p><p>StatefulSet是用于管理有状态应用程序的工作负载。它管理一组Pod的实现和扩展，并保证这些Pod的顺序和唯一性。</p><p>像Deployment一样，StatefulSet也管理具有相同容器规范的一组Pod。由StatefulSets维护的Pod具有唯一的，持久的身份和稳定的主机名，而不用管它们位于哪个节点上。如果我们想要一个跨存储的持久性，我们可以创建一个持久性卷并将StatefulSet用作解决方案的一部分。即使StatefulSet中的Pod容易发生故障，存储卷与新Pod进行匹配也很容易。</p><p>StatefulSet对于需要以下一项或多项功能的应用程序很有价值：</p><ul><li><p>稳定的唯一网络标识符。</p></li><li><p>稳定，持久的存储。</p></li><li><p>有序，顺畅的部署和扩展。</p></li><li><p>有序的自动滚动更新。</p></li></ul><p>在Kubernetes上部署数据库时，我们需要使用StatefulSet，但是使用StatefulSet有一些局限性：</p><ul><li><p>需要使用持久性存储卷为Pod提供存储。</p></li><li><p>删除副本或按比例缩小副本将不会删除附加到StatefulSet的存储卷。存储卷确保数据的安全性。</p></li><li><p>StatefulSet当前需要Headless Service 来负责Pod的网络标识。</p></li><li><p>与Deployment 不同，StatefulSet不保证删除StatefulSet资源时删除所有Pod，而Deployment在被删除时会删除与Deployment关联的所有Pod。在删除StatefulSet之前，你必须将pod副本数量缩小到0 。</p></li></ul><h2 id="Kubernetes上的数据库"><a href="#Kubernetes上的数据库" class="headerlink" title="Kubernetes上的数据库"></a>Kubernetes上的数据库</h2><p>我们可以将数据库作为有状态应用程序部署到Kubernetes。通常，当我们部署Pod时，它们具有自己的存储空间，但是该存储空间是短暂的-如果容器被杀死了，则其存储空间将随之消失。</p><p>因此，我们需要有一个Kubernetes资源对象来解决这种情况：当我们想要数据持久化时，我们就把Pod和持久化存储卷声明关联。通过这种方式，如果我们的容器被杀死了，我们的数据仍将位于集群中，新的pod也能够相应地访问数据。</p><p>Pod -&gt; PVC-&gt; PV</p><ul><li><p>PV =持久性存储</p></li><li><p>PVC =持久性存储声明</p></li></ul><h2 id="Operators将数据库部署到Kubernetes"><a href="#Operators将数据库部署到Kubernetes" class="headerlink" title="Operators将数据库部署到Kubernetes"></a>Operators将数据库部署到Kubernetes</h2><ul><li>我们可以使用由Oracle开发的Kubernetes Operators来部署MySQL数据库：</li></ul><pre><code>https://github.com/oracle/mysql-operator</code></pre><ul><li>使用Crunchydata开发的PostgreSQL Operators，、将PostgreSQL部署到Kubernetes：</li></ul><pre><code>https://github.com/CrunchyData/postgres-operator</code></pre><ul><li>使用MongoDB开发的Operators，可将MongoDB Enterprise部署到Kubernetes集群：</li></ul><pre><code>https://github.com/mongodb/mongodb-enterprise-kubernetes</code></pre><h2 id="在Kubernetes上部署数据库是否可行？"><a href="#在Kubernetes上部署数据库是否可行？" class="headerlink" title="在Kubernetes上部署数据库是否可行？"></a>在Kubernetes上部署数据库是否可行？</h2><p>在当今世界上，越来越多的公司致力于容器技术。在进行深入研究之前，让我们回顾一下用于运行数据库的选项。</p><h3 id="1-完全托管的数据库"><a href="#1-完全托管的数据库" class="headerlink" title="1.完全托管的数据库"></a>1.完全托管的数据库</h3><p>完全托管的数据库是那些不用自己来管理的数据库-这种管理可以由AWS Google，Azure或Digital Cloud等云提供商完成。托管数据库包括Amazon Web Services，Aurora DynamoDB或Google Spanner等。</p><p>使用这些完全托管的数据库的优势是操作少，云提供商可以处理许多维护任务，例如备份，扩展补丁等。你只需创建数据库即可构建应用程序，其他的由云提供商帮你处理。</p><h3 id="2-在VM或本地自行部署"><a href="#2-在VM或本地自行部署" class="headerlink" title="2.在VM或本地自行部署"></a>2.在VM或本地自行部署</h3><p>使用此选项，你可以将数据库部署到任何虚拟机（EC2或Compute Engine），并且将拥有完全控制权。你将能够部署任何版本的数据库，并且可以设置自己的安全性和备份计划。</p><p>另一方面，这意味着你将自行管理，修补，扩展或配置数据库。这将增加基础架构的成本，但具有灵活性的优势。</p><h3 id="3-在Kubernetes上运行"><a href="#3-在Kubernetes上运行" class="headerlink" title="3.在Kubernetes上运行"></a>3.在Kubernetes上运行</h3><p>在Kubernetes中部署数据库更接近full-ops选项，但是从Kubernetes提供的自动化方面来看，你将获得一些好处–能够保持数据库应用程序的正常运行。</p><p>要注意，pod是短暂的，因此数据库应用程序重新启动或失败的可能性更大。另外，你将负责更具体的数据库管理任务，例如备份，扩展等。</p><p>选择在Kubernetes上部署数据库时要考虑的一些重要点是：</p><ul><li><p>有一些自定义资源和 operators可用于在Kubernetes上管理数据库。</p></li><li><p>具有缓存层和瞬时态存储的数据库更适合Kubernetes。</p></li><li><p>你必须了解数据库中可用的复制模式。异步复制模式为数据丢失留有空间，因为事务可能会提交给主数据库，而不会提交给从数据库。</p><p><img src="https://www.magalix.com/hs-fs/hubfs/Google%20Drive%20Integration/kubernetes%20and%20database.png" referrerpolicy="no-referrer" width="50%" height="50%"><br>上面，我们用一个简单的图表来显示在Kubernetes上部署数据库时的决策。</p></li></ul><p>首先，我们需要尝试了解数据库是否具有与Kubernetes友好的功能，例如MySQL或PostgreSQL，然后我们查找kubernetes operators将数据库与其他功能打包在一起。</p><p>第二个问题是-考虑到在Kubernetes中部署数据库需要多少工作量，这是可以接受的？我们是否有一个运维团队，或者在托管数据库上部署数据库是否可行？</p><h2 id="在Kubernetes上部署有状态应用程序："><a href="#在Kubernetes上部署有状态应用程序：" class="headerlink" title="在Kubernetes上部署有状态应用程序："></a>在Kubernetes上部署有状态应用程序：</h2><h3 id="步骤1：部署MySQL服务"><a href="#步骤1：部署MySQL服务" class="headerlink" title="步骤1：部署MySQL服务"></a>步骤1：部署MySQL服务</h3><pre><code>apiVersion: v1</code></pre><p>首先，我们在端口3306上为MySQL数据库部署服务，所有Pod均具有标签键app: mysql。</p><p>接下来，创建以下资源：</p><pre><code>Kubectl create -f mysql_service.yaml</code></pre><h3 id="步骤2：部署MySQL-Deployment"><a href="#步骤2：部署MySQL-Deployment" class="headerlink" title="步骤2：部署MySQL Deployment"></a>步骤2：部署MySQL Deployment</h3><pre><code>apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2</code></pre><p>此Deployment在3306端口上创建带有MySQL5.6镜像和密码(使用secret)的Pod。我们还将附加一个持久卷mysql-pv-claim，将在接下来的步骤中进行显示。</p><p>创建资源：</p><pre><code>Kubectl create -f mysql_deployment.yaml</code></pre><h3 id="第3步：创建持久卷"><a href="#第3步：创建持久卷" class="headerlink" title="第3步：创建持久卷"></a>第3步：创建持久卷</h3><pre><code>apiVersion: v1</code></pre><p>这将创建一个持久卷，我们将使用它来附加到容器，以确保Pod重启时的数据安全。该持久卷具有ReadWriteOne访问模式，拥有20GB的存储空间，存放路径是/ mnt/data，我们所有的数据都将保存在该路径中。</p><p>创建以下资源：</p><pre><code>Kubectl create -f persistence_volume.yaml</code></pre><h3 id="第4步：创建持久卷声明"><a href="#第4步：创建持久卷声明" class="headerlink" title="第4步：创建持久卷声明"></a>第4步：创建持久卷声明</h3><pre><code>apiVersion: v1</code></pre><p>该声明从上面创建的“持久卷”中声明20GB，并具有与上面的“持久卷”相同的访问模式。</p><p>创建以下资源：</p><pre><code>Kubectl create -f pvClaim.yaml</code></pre><h3 id="步骤5：测试MySQL数据库"><a href="#步骤5：测试MySQL数据库" class="headerlink" title="步骤5：测试MySQL数据库"></a>步骤5：测试MySQL数据库</h3><pre><code>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword</code></pre><p>此命令在运行MySQL的集群中创建一个新的Pod，并连接到MySQL服务器。如果连接成功，则说明你的MySQL数据库已启动并正在运行。</p><pre><code>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false</code></pre><p>以上完整代码存放在这个位置：<a href="https://github.com/zarakM/mysql-k8.git" target="_blank" rel="noopener">https://github.com/zarakM/mysql-k8.git</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>有状态应用程序是存储用户会话状态的应用程序，保存的数据称为应用程序状态。</p></li><li><p>StatefulSet是一个Kubernetes资源对象，用于管理有状态应用程序，并提供有关Pod顺序和唯一性的保证。</p></li><li><p>通过删除StatefulSet，不会删除StatefulSet中的pod。相反如果删除，你必须将有状态应用程序副本数量缩小为0。</p></li><li><p>Kubernetes上的数据库部署有一个持久存储卷，只要你的集群正在运行，该存储卷就可以永久存储数据。这意味着它可以抵御pod的破坏，并且创建的任何新pod将能够再次使用该存储卷。</p></li><li><p>完全托管的数据库是由云提供商管理的数据库。我们不必管理数据库。这些数据库需要额外的费用，但是如果你想专注于应用程序，它们是最佳选择。</p></li><li><p>你可以通过VM部署数据库。但你将必须处理所有数据库操作，例如扩展，设置和修补。</p></li><li><p>最后，我们展示了如何在Kubernetes上部署数据库。</p></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;译者：王延飞&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://www.magalix.com/blog/kubernetes-and-database&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.magalix.com/
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>GitOps</title>
    <link href="http://zhang-yu.me/2020/07/13/GitOps/"/>
    <id>http://zhang-yu.me/2020/07/13/GitOps/</id>
    <published>2020-07-13T07:00:00.000Z</published>
    <updated>2020-07-13T09:14:41.549Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h1 id="GitOps-在-Kubernetes-中进行-DevOps-的方式"><a href="#GitOps-在-Kubernetes-中进行-DevOps-的方式" class="headerlink" title="GitOps - 在 Kubernetes 中进行 DevOps 的方式"></a>GitOps - 在 Kubernetes 中进行 DevOps 的方式</h1></blockquote><blockquote><h1 id="什么是-GitOps？"><a href="#什么是-GitOps？" class="headerlink" title="什么是 GitOps？"></a>什么是 GitOps？</h1><p>GitOps 是一个概念，将软件的端到端描述放置到 Git 中，然后尝试着让集群状态和 Git 仓库持续同步，其中有两个概念需要说明下。</p><ol><li><strong>软件的描述表示</strong>：Kubernetes、应用和底层基础架构之间的关系是一种声明式的，我们用声明式方式（YAML）来描述我们需要的基础架构。这些 YAML 的实现细节被底层的 Kubernetes 集群的 Controller、Schedulers、CoreDNS、Operator 等等抽象出来，这使得我们可以从传统的<strong>基础架构即代码</strong>转向<strong>基础架构即数据，</strong>我们也可以从 GitHub 上了解到更多相关的信息。这里的关键是，你需要的每一个应用声明的角色（应用开发者/应用运维/集群运维）都会被用到持续交互流水线的 YAML 中，最后被推送到 GitOps 仓库中去。</li></ol><ol start="2"><li><strong>持续同步</strong>：持续同步的意思是不断地检查 Git 仓库，将任何状态变化都反映到 Kubernetes 集群中。这种思路来自于 Flux 工具，Flux 使用 Kubernetes Operator 将自动化部署方式从 Kubernetes 集群外转移到集群内部来。</li></ol><p>GitOps 由以下4个主要的组件组成：</p><ol><li><strong>Git 仓库</strong>：用来存储我们的应用程序的声明式定义的 YAML 文件的源代码仓库。</li></ol><ol start="2"><li><strong>Kubernetes 集群</strong>：用于部署我们应用程序的底层集群。</li></ol><ol start="3"><li><strong>同步代理</strong>：Kubernetes Operator 扩展，它的工作是将 Git 仓库和应用状态持续同步到集群中。</li></ol><ol start="4"><li><strong>CD Pipeline</strong>：持续部署流水线，用来编排整个流程的持续部署流水线。</li></ol><p>关于这些组件如何协同工作来创建 GitOps 流程的架构图如下所示。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cezgWwU6NE50RPybSiauPxOMibG7ycygsHBhlesTN2Cw5JY3D38I5hlRg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>在上面的架构图中，YAML 文件的创建和修改分为应用开发、应用运维和集群运维三部分。根据我们的组织团队架构、集群多租户等需求，可以选择分一到两步进行。接下来我们来看看为什么需要使用 GitOps？</p><h1 id="为什么需要-GitOps？"><a href="#为什么需要-GitOps？" class="headerlink" title="为什么需要 GitOps？"></a>为什么需要 GitOps？</h1><p>GitOps 可以在很多方面都产生价值，下面我们来看看其中的一些关键的价值。</p><p><strong>应用交付速度</strong></p><p>持续的 GitOps 可以通过以下几个方面来提高产品交付速度。</p><ol><li>可以比较最终的 YAML 和集群状态的能力，这也可以作为批准发布的决策指南。</li></ol><ol start="2"><li>借助 Prometheus 的应用程序指标，通过自动化的蓝绿部署，非常容易进行部署。</li></ol><ol start="3"><li>根据策略自动更新容器镜像，例如，Istio sidecar 次要版本的发布是向后兼容的，可以自动更新。</li></ol><ol start="4"><li>GitOps 将以运维和开发为中心，提高效率。</li></ol><ol start="5"><li>应用团队可以接管一些运维工作，而运维团队则可以更加专注于平台建设。</li></ol><ol start="6"><li>GitOps 仓库可以绕过完整的持续部署流程进行紧急发布。</li></ol><p><strong>端到端的自动化</strong></p><p>在 GitOps 中，所有和应用开发、应用运维和集群运维相关的声明都通过 git 嵌入到 YAML 文件中，实现了端到端的自动化。</p><p><strong>安全、审计和合规性</strong></p><p>零手动更改到集群中应用的策略将大大增加集群的安全性，由于集群中的所有配置都在 git 中，我们将拥有一个完整的审计日志，记录集群中发生的事情。</p><p><strong>集群可观测性</strong></p><p>有了完整的审计日志，我们就可以很容易获得集群中发生的变化，来帮助调试一些问题。</p><p><strong>关注点分离和迁移</strong></p><p>GitOps 将应用开发者、应用运维和集群运维之间的关注点进行分离，这些团队中的依赖关系以声明式的方式注入到 git 中，这将大大缓解我们对底层 K8S 集群、治理策略等工具的迁移。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cEibKKfVx9n7ziaTLss4Ee8kpnAtaEg2iamLYNF7ecZOCtw0y4p67w5cHA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h1 id="如何采用-GitOps？"><a href="#如何采用-GitOps？" class="headerlink" title="如何采用 GitOps？"></a>如何采用 GitOps？</h1><p>我们将通过以下四个不同的方面进行阐述来帮助我们实现 GitOps 这一目标。</p><ul><li>GitOps 工作流的实现</li></ul><ul><li>管理声明式的 YAML</li></ul><ul><li>工具</li></ul><ul><li>从什么地方开始？</li></ul><h2 id="GitOps-工作流的实现"><a href="#GitOps-工作流的实现" class="headerlink" title="GitOps 工作流的实现"></a>GitOps 工作流的实现</h2><p>以下三个工作流程是我们在开始使用 GitOps 时要采用的比较流行的工作流程。</p><p><strong>工作流1</strong>：标准的 GitOps 流程</p><p>这是标准的 GitOps 工作流，我们将应用程序的 YAML 描述推送到 GitOps 仓库中，GitOps Agent 就会自动同步状态变化。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cCpLKt5qIvNNOM1x2c1iazNoYPV2WKme8Zmo2lZKrc9TMCxAHuQm2CcQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>工作流2</strong>：镜像自动更新</p><p>在这个工作流中，GitOps Agent 会根据指定的策略从容器镜像仓库中自动更新新版本的容器镜像，例如，我们可以设置这样的策略，如果镜像有一个小版本变化，我们就可以自动更新，因为它们是向后兼容的。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cq6k4LwFZwMibibjr5QsEOpw7dckGPTvbovB08OC57dwXwnSQP17NbdGw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>工作流3</strong>：自动化金丝雀部署</p><p>该工作流非常强大，我们可以在这里实现金丝雀自动化部署。有了这个，我们在用 Prometheus 测量 HTTP 请求成功率、请求平均持续时间和 Pods 健康状态等关键性能指标的同时，可以逐步将流量迁移到金丝雀实例上。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5crKDdUgNZDpCOyWicuISxVibCScHxLDXRDkMlPjibBHU8wUFDMGOSRSoBQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="管理声明式的-YAML"><a href="#管理声明式的-YAML" class="headerlink" title="管理声明式的 YAML"></a>管理声明式的 YAML</h2><p>假设我们有一个电子商务购物车应用，完整的应用程序定义如下所示。</p><ol><li>应用程序镜像。</li></ol><ol start="2"><li>Pod、Deployment、Service、Volume 和 ConfigMap 的 YAML 文件。</li></ol><ol start="3"><li>连接数据的一些 sealed secrets 对象。</li></ol><ol start="4"><li>标记将 Istio 作为默认的集群服务治理策略网格。</li></ol><ol start="5"><li>环境治理策略，比如 staging 环境1个副本，生产环境3个副本。</li></ol><ol start="6"><li>标记添加节点亲和性和容忍用于高可用的节点调度。</li></ol><ol start="7"><li>基于 Pod 标签的网络安全策略 YAML 声明。</li></ol><p>要构建一个最终的应用 YAML 描述文件，我们需要应用开发者、应用运维和集群运维人员的一些输入。</p><p>假设我们是一个比较小的团队并且管理了很多的 Pod，下面的流程就可以来表示如何构建一个持续部署的自动化流水线。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cb9oVNicve4A2ribtmX4CE3xC1DFUjqjqSNwibsp1qxBazPCxHiaIq6MyPw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>这种关注点分离的方式深受 OAM（开放应用模型）的影响，该模型试图为云原生应用开发提供一个完善的框架。</p><p>OAM(<a href="https://oam.dev/" target="_blank" rel="noopener">https://oam.dev/</a>) 描述了一种模式：</p><ul><li><strong>开发人员</strong>负责定义应用组件。</li></ul><ul><li><strong>应用运维人员</strong>负责创建这些组件的实例，并为其分配应用配置。</li></ul><ul><li><strong>基础设施运维人员</strong>负责声明、安装和维护平台上的可用的底层服务。</li></ul><p>通过使用 OAM 框架，会将 YAML 的贡献者责任进行分离，当然这可能会根据你的团队组织结构和使用的 Kubernetes 集群类型有所变化。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cjySamdPt5BtuuUstOzIm8C6uAOrkVcU0C67JQoyh3mppdlYnonPeEg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>如果你比较赞同 GitOps 的理念，那么下面就可以来选择一些需要用到的工具了。有很多工具可以支持我们去实现 GitOps 的不同功能。接下来我们简单介绍一些工具及其使用方法。</p><p><strong>Git</strong>：这是我们使用 GitOps 来存储 YAML 清单的基础。</p><p><strong>Helm &amp; Kustomize</strong>：这是一个强大的组合，可以帮助我们生成声明式的 YAML 资源清单文件，我们可以使用 Helm 打包应用程序和它的依赖关系。然后 Kustomize 会帮助我们自定义和修补 YAML 文件，而不需要去改变原来的 YAML 文件。单独使用 Helm 是不够的，特别是用于区分不同环境的资源清单的时候，我们还需要结合 Kustomize。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5c2mpOqEica4h1icq3G7crcqbDH0llawQMfOS7nuNXLCzDbPQvgsiaZYHRg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>Argo CD</strong>：这是一个 GitOps 持续交付工具，它可以作为一个 Agent，将 GitOps 仓库中的改动同步到 Kubernetes 集群中。</p><p><strong>Flux</strong>：这是另外一个 GitOps 持续交付的工具，功能和 Argo CD 类似。</p><p><strong>Flagger</strong>：这个工具和 Flux 配合使用，可以很好地实现金丝雀部署。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cpMDSbOoen7N0BhnC7OAarv386DMCagr4f285nibvpL7M7abQFRD6rZg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="如何入手？"><a href="#如何入手？" class="headerlink" title="如何入手？"></a>如何入手？</h2><p>如果你正准备开始一个新的项目，那么从一开始就采用 GitOps 是比较容易的，我们所要做的就是选择我们的 CI/CD、声明式 YAML 文件管理以及 GitOps Agent 等工具来启动即可。</p><p>如果你想在现有的项目中实施 GitOps，下面的几点可以帮助你实施：</p><ol><li>你可以一次只选择一个应用，用成功的案例来推动其他应用的改造。</li></ol><ol start="2"><li>当选择第一个应用的时候，可以选择变化比较频繁的应用，这将有助于我们为成功案例建立一些可靠的指标说明。</li></ol><ol start="3"><li>选择一个经常出问题的应用，使用了 GitOps 过后，这些应用的问题频率应该会下降不少，当它出现问题的时候，应该有更好的可观测性了。</li></ol><ol start="4"><li>优先选择业务应用，而不是像 Istio、RBAC 集成的等运维复杂的应用。</li></ol><ol start="5"><li>如果需要的话可以暂时引入人工审批的步骤。</li></ol><p>  转发来源  阳明 k8s技术圈 </p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect</a></p><blockquote><p>原文链接：<a href="https://itnext.io/continuous-gitops-the-way-to-do-devops-in-kubernetes-896b0ea1d0fb" target="_blank" rel="noopener">https://itnext.io/continuous-gitops-the-way-to-do-devops-in-kubernetes-896b0ea1d0fb</a></p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h1 id=&quot;GitOps-在-Kubernetes-中进行-DevOps-的方式&quot;&gt;&lt;a href=&quot;#GitOps-在-Kubernetes-中进行-DevOps-的方式&quot; class=&quot;headerlink&quot; title=&quot;GitOps - 在 
      
    
    </summary>
    
      <category term="devops" scheme="http://zhang-yu.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhang-yu.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>使用GitLab-CI与Argo-CD进行GitOps实践</title>
    <link href="http://zhang-yu.me/2020/07/13/%E4%BD%BF%E7%94%A8GitLab-CI%E4%B8%8EArgo-CD%E8%BF%9B%E8%A1%8CGitOps%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zhang-yu.me/2020/07/13/使用GitLab-CI与Argo-CD进行GitOps实践/</id>
    <published>2020-07-13T06:00:00.000Z</published>
    <updated>2020-07-13T09:14:52.452Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h1 id="使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践"><a href="#使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践" class="headerlink" title="使用 GitLab CI 与 Argo CD 进行 GitOps 实践"></a>使用 GitLab CI 与 Argo CD 进行 GitOps 实践</h1></blockquote><blockquote><p>在现在的云原生世界里面 GitOps 不断的被提及，这种持续交付的模式越来越受到了大家的青睐，<a href="http://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">我们前面也有文章详细讲解了 GitOps 的相关概念</a>，在网上也可以找到很多关于它的资源，但是关于 GitOps 相关的工作流实践的示例却并不多见，我们这里就将详细介绍一个使用示例，希望对大家实践 GitOps 有所帮助。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUk1FGGtWPmb7xyeiaOicS5Zwk9fyZJVgqVwL44SHpBOJw3u459iaYicAyfSg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitOps Workflow</p><p>上图是当前示例中的 GitOps 工作流程。GitLab 和 Argo CD 是两个主要的核心组件：</p><p><strong>Argo CD</strong> 是一个声明式、GitOps 持续交付的 Kubernetes 工具，它的配置和使用非常简单，并且自带一个简单易用的 Dashboard 页面，更重要的是 Argo CD 支持 kustomzie、helm、ksonnet 等多种工具。应用程序可以通过 Argo CD 提供的 CRD 资源对象进行配置，可以在指定的目标环境中自动部署所需的应用程序。关于 Argo CD 更多的信息可以查看官方文档了解更多。</p><p><strong>GitLab CI</strong> 是 GitLab 的持续集成和持续交付的工具，也是非常流行的 CI/CD 工具，相比 Jenkins 更加轻量级，更重要的是和 GitLab 天然集成在一起的，所以非常方便。</p><h2 id="Argo-CD-安装"><a href="#Argo-CD-安装" class="headerlink" title="Argo CD 安装"></a>Argo CD 安装</h2><p>当前前提条件是有一个可用的 Kubernetes 集群，通过 kubectl 可以正常访问集群，为了访问 Argo CD 的 Dashboard 页面，我们可以通过 Ingress 来暴露服务，为此需要在 Kubernetes 中安装一个 Ingress Controller，我这里已经提前安装了 <code>ingress-nginx</code>，接下来我们将 Helm3 来安装 Argo CD，关于 Helm 以及 ingress-nginx 的使用我们前面的文章中已经多次提到，这里就不再详细介绍他们的使用了。</p><p>首先创建一个 argocd 的命名空间：</p><pre><code>$ kubectl create ns argocd</code></pre><p>然后添加 argocd 的 chart 仓库地址：</p><pre><code>$ helm repo add argo https://argoproj.github.io/argo-helm</code></pre><p>接下来我们就可以使用 Helm 安装 Argo CD 了：</p><pre><code>$ helm install argocd -n argocd argo/argo-cd --values values.yaml</code></pre><p>其中 values.yaml 文件如下所示，用来定制安装的 Argo CD：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server:</span><br><span class="line">  ingress:</span><br><span class="line">    enabled: true</span><br><span class="line">    annotations:</span><br><span class="line">      kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</span><br><span class="line">    hosts:</span><br><span class="line">    - argocd.k8s.local</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>执行上面的安装命令后，Argo CD 就会被安装在 argocd 命名空间之下，可以在本地 <code>/etc/hosts</code> 中添加一个映射，将 argocd.k8s.local 映射到 ingress-nginx 所在的节点即可：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls -n argocd</span><br><span class="line">NAME    NAMESPACE       REVISION        UPDATED                                 STATUS    CHART            APP VERSION</span><br><span class="line">argocd  argocd          2               2020-07-10 15:26:38.259258 +0800 CST    deployed  argo-cd-2.5.0    1.6.1</span><br><span class="line">$ kubectl get pods -n argocd</span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">argocd-application-controller-85c4788ffc-p2m4c   1/1     Running   0          49m</span><br><span class="line">argocd-dex-server-cc65c7546-x78bj                1/1     Running   0          49m</span><br><span class="line">argocd-redis-5f45875bc7-mnx8b                    1/1     Running   0          49m</span><br><span class="line">argocd-repo-server-7bcf647588-h8gtq              1/1     Running   0          49m</span><br><span class="line">argocd-server-7877ff8889-zp7tq                   1/1     Running   0          49m</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>当所有 Pod 变成 Running 状态后，我们就可以通过浏览器访问 Argo CD 的 Dashboard 页面了：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkDyG5dBqSjrkqaHc0ic1ib5BdR9mEMM6iamffu3PxxlJ5mFxpib0bAAx2Tw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>默认的用户名为 admin，密码为 server Pod 的名称，可以通过如下所示的命令来获取：</p><p>$ kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d’/‘ -f 2</p><p>用上面的用户名和密码即可登录成功，接下来我们在 GitLab 中来创建示例项目。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkJNQ7dCTBtUALC3dBo76DHkQZCD0iceFNTVoWJiaClFiafhZZjdEEUibDVg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="GitLab-项目配置"><a href="#GitLab-项目配置" class="headerlink" title="GitLab 项目配置"></a>GitLab 项目配置</h2><p>我们这里使用的示例项目是一个 Golang 程序，在页面上显示一个文本信息和 Pod 名称，代码地址：<a href="https://github.com/cnych/gitops-webapp-demo。我们可以将该项目代码上传到我们自己的" target="_blank" rel="noopener">https://github.com/cnych/gitops-webapp-demo。我们可以将该项目代码上传到我们自己的</a> GitLab 上面去，我这里的 GitLab 安装在 Kubernetes 之上，通过配置域名 git.k8s.local 进行访问，调整过后我们本地的代码仓库地址为：<a href="http://git.k8s.local/course/gitops-webapp" target="_blank" rel="noopener">http://git.k8s.local/course/gitops-webapp</a> 。</p><p>接下来需要添加一些在 GitLab CI 流水线中用到的环境变量（Settings → CI/CD → Variables）：</p><ul><li>CI_REGISTRY - 镜像仓库地址，值为：<a href="https://index.docker.io/v1/" target="_blank" rel="noopener">https://index.docker.io/v1/</a></li></ul><ul><li>CI_REGISTRY_IMAGE - 镜像名称，值为：cnych/gitops-webapp</li></ul><ul><li>CI_REGISTRY_USER - Docker Hub 仓库用户名，值为 cnych</li></ul><ul><li>CI_REGISTRY_PASSWORD - Docker Hub 仓库密码</li></ul><ul><li>CI_PASSWORD - Git 仓库访问密码</li></ul><ul><li>CI_USERNAME - Git 仓库访问用户名</li></ul><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkYIDTCMRrglaBa0mHtYVSsMaZRj2GpXXxZ6cROTs1eAPjYqyzjvyk6A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Argo-CD-配置"><a href="#Argo-CD-配置" class="headerlink" title="Argo CD 配置"></a>Argo CD 配置</h2><p>现在我们可以开始使用 GitOps 来配置我们的 Kubernetes 中的应用了。Argo CD 自带了一套 CRD 对象，可以用来进行声明式配置，这当然也是推荐的方式，把我们的基础设施作为代码来进行托管，下面是我们为开发和生产两套环境配置的资源清单：</p> <figure class="highlight plain"><figcaption><span>gitops-demo-app.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: web-app-dev</span><br><span class="line">  namespace: argocd</span><br><span class="line">spec:</span><br><span class="line">  project: default</span><br><span class="line">  source:</span><br><span class="line">    repoURL: http://git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">    path: deployment/dev</span><br><span class="line">  destination:</span><br><span class="line">    server: https://kubernetes.default.svc</span><br><span class="line">    namespace: dev</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: true</span><br><span class="line">---</span><br><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: web-app-prod</span><br><span class="line">  namespace: argocd</span><br><span class="line">spec:</span><br><span class="line">  project: default</span><br><span class="line">  source:</span><br><span class="line">    repoURL: http://git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">    path: deployment/prod</span><br><span class="line">  destination:</span><br><span class="line">    server: https://kubernetes.default.svc</span><br><span class="line">    namespace: prod</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: true</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>上面定义的 Application 这个资源，就是 Argo CD 用于描述应用的 CRD 对象：</p><ul><li>name：Argo CD 应用程序的名称</li></ul><ul><li>project：应用程序将被配置的项目名称，这是在 Argo CD 中应用程序的一种组织方式</li></ul><ul><li>repoURL：源代码的仓库地址</li></ul><ul><li>targetRevision：想要使用的 git 分支</li></ul><ul><li>path：Kubernetes 资源清单在仓库中的路径</li></ul><ul><li>destination：Kubernetes 集群中的目标</li></ul><p>然后同样使用 kubectl 工具直接部署上面的资源对象即可，将会创建两个 Application 类型的对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f gitops-demo-app.yaml</span><br><span class="line">application.argoproj.io/web-app-dev created</span><br><span class="line">application.argoproj.io/web-app-prod created</span><br><span class="line">$ kubectl get application -n argocd</span><br><span class="line">NAME           AGE</span><br><span class="line">web-app-dev    25s</span><br><span class="line">web-app-prod   24s</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>此时我们再去 Argo CD 的 Dashboard 首页同样将会看到两个 Application 的信息：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkEnwJ6x3ic9sOvqtJ4a9omlxSKic74ADdIciaDBIriaKCmh3Nj3qwYOUaZQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>点击其中一个就可以看到关于应用的详细信息，我们可以在 gitops-webapp 代码仓库的 <code>deployment/&lt;env&gt;</code> 目录里面找到资源对象。我们可以看到，在每个文件夹下面都有一个 <code>kustomization.yaml</code> 文件，Argo CD 可以识别它，不需要任何其他的设置就可以使用。</p><p>由于我们这里的代码仓库是私有的 GitLab，所以我们还需要配置对应的仓库地址，在页面上 Settings → Repositories，点击 <code>Connect Repo using HTTPS</code> 按钮：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkwbI136LplOtAo1PFmJTt9ic46ZFrambxuy5rmhQkrpoq0DoaXD6ug5g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>添加我们的代码仓库认证信息：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkHxTR6pFhDytdTh8AkzV67x7In3HiaOQ7ibZyAaw6EjGLNlXrEkc8gw9w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>需要注意的是这里默认使用的是 HTTPS，所以我们需要勾选下方的 <code>Skip server verification</code>，然后点击上方的 <code>CONNECT</code> 按钮添加即可。然后重新同步上面的两个 Application，就可以看到正常的状态了。</p><h2 id="GitLab-CI-流水线"><a href="#GitLab-CI-流水线" class="headerlink" title="GitLab CI 流水线"></a>GitLab CI 流水线</h2><p>接下来我们需要为应用程序创建流水线，自动构建我们的应用程序，推送到镜像仓库，然后更新 Kubernetes 的资源清单文件。</p><p>下面的示例并不是一个多么完美的流水线，但是基本上可以展示整个 GitOps 的工作流。开发人员在自己的分支上开发代码，他们分支的每一次提交都会触发一个阶段性的构建，当他们将自己的修改和主分支合并时，完整的流水线就被触发。将构建应用程序，打包成 Docker 镜像，将镜推送到 Docker 仓库，并自动更新 Kubernetes 资源清单，此外，一般情况下将应用部署到生产环境需要手动操作。</p><p>GitLab CI 中的流水线默认定义在代码仓库根目录下的 <code>.gitlab-ci.yml</code> 文件中，在该文件的最上面定义了一些构建阶段和环境变量、镜像以及一些前置脚本：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stages:</span><br><span class="line">- build</span><br><span class="line">- publish</span><br><span class="line">- deploy-dev</span><br><span class="line">- deploy-prod</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>接下来是阶段的定义和所需的任务声明。我们这里的构建过程比较简单，只需要在一个 golang 镜像中执行一个构建命令即可，然后将编译好的二进制文件保存到下一个阶段处理，这一个阶段适合分支的任何变更：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">build:</span><br><span class="line">  stage: build</span><br><span class="line">  image:</span><br><span class="line">    name: golang:1.13.1</span><br><span class="line">  script:</span><br><span class="line">    - go build -o main main.go</span><br><span class="line">  artifacts:</span><br><span class="line">    paths:</span><br><span class="line">      - main</span><br><span class="line">  variables:</span><br><span class="line">    CGO_ENABLED: 0</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>然后就是构建镜像并推送到镜像仓库，这里我们使用 Kaniko，当然也可以使用 DinD 模式进行构建，只是安全性不高，这里我们可以使用 GIT 提交的 commit 哈希值作为镜像 tag，关于 Docker 镜像仓库的认证和镜像地址信息可以通过项目的参数来进行传递，不过这个阶段只在主分支发生变化时才会触发：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">publish:</span><br><span class="line">  stage: publish</span><br><span class="line">  image:</span><br><span class="line">    name: cnych/kaniko-executor:v0.22.0</span><br><span class="line">    entrypoint: [&quot;&quot;]</span><br><span class="line">  script:</span><br><span class="line">    - echo &quot;&#123;\&quot;auths\&quot;:&#123;\&quot;$CI_REGISTRY\&quot;:&#123;\&quot;username\&quot;:\&quot;$CI_REGISTRY_USER\&quot;,\&quot;password\&quot;:\&quot;$CI_REGISTRY_PASSWORD\&quot;&#125;&#125;&#125;&quot; &gt; /kaniko/.docker/config.json</span><br><span class="line">    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile ./Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA</span><br><span class="line">  dependencies:</span><br><span class="line">    - build</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>下一个阶段就是将应用程序部署到开发环境中，在 GitOps 中就意味着需要更新 Kubernetes 的资源清单，这样 Argo CD 就可以拉取更新的版本来部署应用。这里我们使用了为项目定义的环境变量，包括用户名和 TOKEN，此外在提交消息里面增加 <code>[skip ci]</code> 这样的关键字，这样流水线就不会被触发：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">deploy-dev:</span><br><span class="line">  stage: deploy-dev</span><br><span class="line">  image: cnych/kustomize:v1.0</span><br><span class="line">  before_script:</span><br><span class="line">    - git remote set-url origin http://$&#123;CI_USERNAME&#125;:$&#123;CI_PASSWORD&#125;@git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    - git config --global user.email &quot;gitlab@git.k8s.local&quot;</span><br><span class="line">    - git config --global user.name &quot;GitLab CI/CD&quot;</span><br><span class="line">  script:</span><br><span class="line">    - git checkout -B master</span><br><span class="line">    - cd deployment/dev</span><br><span class="line">    - kustomize edit set image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span><br><span class="line">    - cat kustomization.yaml</span><br><span class="line">    - git commit -am &apos;[skip ci] DEV image update&apos;</span><br><span class="line">    - git push origin master</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>最后添加一个部署到 prod 环境的阶段，和前面非常类似，只是添加了一个手动操作的流程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">deploy-prod:</span><br><span class="line">  stage: deploy-prod</span><br><span class="line">  image: cnych/kustomize:v1.0</span><br><span class="line">  before_script:</span><br><span class="line">    - git remote set-url origin http://$&#123;CI_USERNAME&#125;:$&#123;CI_PASSWORD&#125;@git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    - git config --global user.email &quot;gitlab@git.k8s.local&quot;</span><br><span class="line">    - git config --global user.name &quot;GitLab CI/CD&quot;</span><br><span class="line">  script:</span><br><span class="line">    - git checkout -B master</span><br><span class="line">    - git pull origin master</span><br><span class="line">    - cd deployment/prod</span><br><span class="line">    - kustomize edit set image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span><br><span class="line">    - cat kustomization.yaml</span><br><span class="line">    - git commit -am &apos;[skip ci] PROD image update&apos;</span><br><span class="line">    - git push origin master</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br><span class="line">  when: manual</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>这样我们就完成了整个流水线的定义。</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>接下来我们来看看它们是如何一起工作的。我们将开发和线上两个环境的应用分别部署在了 dev 和 prod 命名空间之下，通过 Ingress 暴露服务，同样需要将两个应用的域名 <a href="http://webapp.dev.k8s.local/" target="_blank" rel="noopener">http://webapp.dev.k8s.local/</a> 与 <a href="http://webapp.prod.k8s.local/" target="_blank" rel="noopener">http://webapp.prod.k8s.local/</a> 在本地 <code>/etc/hosts</code> 中添加映射。</p><p>如果一切正常的话现在我们可以在浏览器中来查看我们部署的 web 应用程序了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkylSD4SV0tgeuyviclKwuKicCJ6LyPqRsZ9oV9fvd7NuxFUNnZ5bf6Q0A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt>Dev web app</p><p>然后我们来尝试修改下代码，编辑 main.go 文件，将变量 welcome 中的 <code>GITOPS</code> 修改为 <code>GITOPS-K8S</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">   welcome := Welcome&#123;&quot;GITOPS-K8S&quot;, time.Now().Format(time.Stamp), os.Getenv(&quot;HOSTNAME&quot;)</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>然后提交代码到 master 分支，然后进入 GitLab 项目 -&gt; CI/CD -&gt; Pipelines，就可以看到一个新的流水线开始构建了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkEz8Po0y2e3IqlSNH8u1WmgBHfokg8zoKXkALuE0Xhac19cXaLymHfA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>等待一会儿，正常情况下会执行到 dev 的部署阶段，然后变成 <code>skipped</code> 的状态，此时流水线已经将代码中的 dev 下的资源清单文件已经更新了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkCyPfBKa2csl9PfKNYkN64sYIBjllnhicEu4jcV6ianUaTiab38K0T0LNw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitLab CI/CD Pipeline</p><p>然后 Argo CD 在自动同步模式下在一分钟内变会更新 Kubernetes 的资源对象，我们也可以在 Argo CD 的页面中看到进度。当 Argo CD 中同步完成后我们再去查看 DEV 环境的应用，就可以看到页面上面的信息已经变成了 <code>GITOPS-K8S</code> 了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUk5aXjc4XibnmywxOzt1dBr9aR3gPMOqUFU4E7lxXzIRQSia8Nrs8t1yjQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt>Update Dev Web APP</p><p>最后如果需要部署到 prod 环境，我们只需要在 GitLab 的流水线中手动触发即可，之后，prod 中的镜像也会被更新。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkInCosUJFMHtjiaO1quRoQohfICvaa1QO7aZyKs15yKOR4GUyUI0EwYQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitLab CI/CD Prod deployment</p><p>下面是同步时 Argo CD 更新的页面状态变化图。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkFgM9ujWCgKRLHVJCJxQDcFwAO4BictzbeuO52PTlH3OWIOf4o7lpricw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>Argo CD Sync Workflow</p><p>到这里，我们就使用 GitOps 成功的将我们的应用部署到了开发和生产环境之中了。</p></blockquote><blockquote><p> 转发来源  阳明 k8s技术圈 <a href="https://mp.weixin.qq.com/s/tCY86QZ_K3STiN5ZvfhjpQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/tCY86QZ_K3STiN5ZvfhjpQ</a></p></blockquote><blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://www.weave.works/technologies/gitops/" target="_blank" rel="noopener">https://www.weave.works/technologies/gitops/</a></li></ul><ul><li><a href="https://argoproj.github.io/argo-cd/" target="_blank" rel="noopener">https://argoproj.github.io/argo-cd/</a></li></ul><ul><li><a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">https://docs.gitlab.com/ee/ci/yaml/</a></li></ul><ul><li><a href="https://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6" target="_blank" rel="noopener">https://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6</a></li></ul><ul><li><a href="https://github.com/cnych/gitops-webapp-demo" target="_blank" rel="noopener">https://github.com/cnych/gitops-webapp-demo</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h1 id=&quot;使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践&quot;&gt;&lt;a href=&quot;#使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践&quot; class=&quot;headerlink&quot; title=&quot;使用 GitLab
      
    
    </summary>
    
      <category term="devops" scheme="http://zhang-yu.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhang-yu.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>理解Kubernetes架构</title>
    <link href="http://zhang-yu.me/2020/07/03/%E7%90%86%E8%A7%A3Kubernetes%E6%9E%B6%E6%9E%84/"/>
    <id>http://zhang-yu.me/2020/07/03/理解Kubernetes架构/</id>
    <published>2020-07-03T03:00:00.000Z</published>
    <updated>2020-08-12T08:48:51.849Z</updated>
    
    <content type="html"><![CDATA[<p>架构师波波</p><p><a href="https://blog.csdn.net/yang75108/article/details/100215486" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/100215486</a> </p><blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>理解K8s的架构是运用好K8s的基础，本文波波帮助大家梳理一下K8s的架构。我们先会对K8s的架构进行一个概览，然后分别剖析Master和Worker节点的组件构成，然后把这些组件再集成起来，通过一个发布样例展示这些组件是如何配合工作的，最后展示K8s集群的总体架构。</p><h2 id="架构概览"><a href="#架构概览" class="headerlink" title="架构概览"></a>架构概览</h2><p><img src="https://img-blog.csdnimg.cn/20190902113048702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图是K8s架构的概览。K8s集群中主要有两类角色，一类是Master节点，另外一类是Worker节点，简单讲，Master节点主要用来管理和调度集群资源的，而Worker节点则是提供资源的。在一个高可用的K8s集群中，Master和Worker一般都有多个节点构成，这些节点可以是物理机，也可以是虚拟机。</p><p>Worker节点提供的资源单位称为Pod，简单理解，Pod就是K8s云平台提供的虚拟机。Pod里头住的是应用容器，比如Docker容器，容器是CPU/Mem资源隔离单位。大部分场景下，一个Pod只住一个应用容器，但是也有一些场景，一个Pod里头可以住多个容器，其中一个是主容器，其它则是辅助容器。一个Pod里头的容器共享Pod的网络栈和存储资源。</p><p>K8s主要解决集群资源调度的问题。简单讲，就是当有应用发布请求过来的时候，K8s需要根据集群资源空闲现状，将这个应用的Pods合理的分配到空闲的Worker节点上去。同时，K8s需要时刻监控集群，如果有节点或者Pods挂了，它要能够重新协调和启动Pods，保证应用高可用，这个术语叫自愈。还有，K8s需要管理集群网络，保证Pod/服务之间可以互通互联。</p><h2 id="Master节点组件"><a href="#Master节点组件" class="headerlink" title="Master节点组件"></a>Master节点组件</h2><p><img src="https://img-blog.csdnimg.cn/2019090211310821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>Master节点是K8s集群大脑，它由如下组件构成：</p><ol><li><strong>Etcd</strong>： 它是K8s的集中状态存储，所有的集群状态数据，例如节点，Pods，发布，配置等等，最终都存储在Etcd中。Etcd是一个分布式KV数据库，采用Raft分布式一致性算法。Etcd高可用部署一般需要至少三个节点。Etcd集群可以独立部署，也可以和Master节点住在一起。</li><li><strong>API server</strong>： 它是K8s集群的接口和通讯总线。用户通过kubectl，dashboard或者sdk等方式操作K8s，背后都通过API server和集群进行交互。集群内的其它组件，例如Kubelet/Kube-Proxy/Scheduler/Controller-Manager等，都通过API server和集群进行交互。API server可以认为是Etcd的一个代理Proxy，它是唯一能够访问操作Etcd数据库的组件，其它组件，都必须通过API server间接操作Etcd。API server不仅接受其它组件的API请求，它还是集群的事件总线，其它组件可以订阅在API server上，当有新事件发生时候，API server会将相关事件通知到感兴趣的组件。</li><li><strong>Scheduler</strong>： 它是K8s集群负责调度决策的组件。Scheduler掌握当前的集群资源使用情况，当有新的应用发布请求被提交到K8s集群，它负责决策相应的Pods应该分布到哪些空闲节点上去。K8s中的调度决策算法是可以扩展的。</li><li><strong>Controller Manager</strong>： 它是保证集群状态最终一致的组件。它通过API server监控集群状态，确保实际状态和预期状态最终一致，如果一个应用要求发布十个Pods，Controller Manager保证这个应用最终启动十个Pods，如果中间有Pods挂了，Controller Manager会负责协调重启Pods，如果Pods启多了，Controller Manager会负责协调关闭多余Pods。也即是说，K8s采用最终一致调度策略，它是集群自愈的背后实现机制。</li></ol><h2 id="Worker节点组件"><a href="#Worker节点组件" class="headerlink" title="Worker节点组件"></a>Worker节点组件</h2><p><img src="https://img-blog.csdnimg.cn/20190902113122457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>Worker节点是K8s集群资源的提供者，它由如下组件构成：</p><ol><li><strong>Kubelet</strong>: 它是Worker节点资源的管理者，相当于一个Agent角色。它监听API server的事件，根据Master节点的指示启动或者关闭Pod等资源，也将本节点状态数据汇报给Master节点。如果说Master节点是K8s集群的大脑，那么Kubelet就是Worker节点的小脑。</li><li><strong>Container Runtime</strong>: 它是节点容器资源的管理者，如果采用Docker容器，那么它就是Docker Engine。Kubelet并不直接管理节点的容器资源，它委托Container Runtime进行管理，比如启动或者关闭容器，收集容器状态等。Container Runtime在启动容器时，如果本地没有镜像缓存，则需要到Docker Registry(或Docker Hub)去拉取相应镜像，然后缓存本地。</li><li><strong>Kube-Proxy</strong>: 它是管理K8s中的服务(Service)网络的组件。Pod在K8s中是ephemeral的概念，也就是不固定的，PodIP可能会变(包括预期和非预期的)。为了屏蔽PodIP的可能的变化，K8s中引入了Servie概念，它可以屏蔽应用的PodIP，并且在调用时进行负载均衡。Kube-Proxy是实现K8s服务(Service)网络的背后机制。另外，当需要把K8s中的服务(Service)暴露给外网时，也需要通过Kube-Proxy进行代理转发。</li></ol><h2 id="流程样例"><a href="#流程样例" class="headerlink" title="流程样例"></a>流程样例</h2><p><img src="https://img-blog.csdnimg.cn/20190902113135346.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>如果我们把Master节点和Worker节点集成起来，就构成上图所示的K8s集群。下面我们通过一个发布流程，展示上面介绍的这些组件是如何配合工作的。</p><ol><li>假设管理员要发布一个新应用，他通过Kubectl命令行工具将发布请求提交到API server，API server将请求存储到Etcd数据库中。</li><li>Scheduler通过API server监听到有新的应用发布请求，它通过调度算法决策，选择若干可发布的空闲节点，并将发布决策更新到API server。</li><li>被选中的Worker节点上的Kubelet通过API server监听到有给自己的新发布任务，它根据任务指示在本地启动相应的Pods(间接通过Container Runtime启动容器)，并将任务执行成功情况报告给API server。</li><li>所有Worker节点上Kube-Proxy通过API server监听到有新的发布，它获取应用的PodIP/ClusterIP/端口等相关数据，更新本地的iptables表规则，让本地的Pods可以通过iptables转发方式，访问到新发布应用的Pods。</li><li>Controller Manager通过API server，时刻监控新发应用的健康状况，保证实际状态和预期状态最终一致。</li></ol><h2 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h2><p><img src="https://img-blog.csdnimg.cn/20190902113147939.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图是一个K8s集群的总体架构。实际K8s集群中还有一个覆盖(Overlay)网络，集群中的Pods通过覆盖网络可以实现IP寻址和通讯。实现覆盖网络的技术有很多，例如Flannel/VxLan/Calico/Weave-Net等等。外网流量如果要访问K8s集群内部的服务，一般要走负载均衡器(Load Balancer)，背后流量会通过Kube-Proxy间接转发到服务Pods上。</p><p>除了上述组件，K8s外围一般还有存储，监控，日志和分析等配套支持服务。</p><h2 id="总结和课程推荐"><a href="#总结和课程推荐" class="headerlink" title="总结和课程推荐"></a>总结和课程推荐</h2><p>下表我把本文讲到的一些K8s关键组件的作用做一个梳理总结，方便大家理解记忆。</p><p><img src="https://img-blog.csdnimg.cn/20190902113204818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;架构师波波&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/100215486&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/yan
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>微服务为什么要配置中心</title>
    <link href="http://zhang-yu.me/2020/06/28/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    <id>http://zhang-yu.me/2020/06/28/微服务为什么要配置中心/</id>
    <published>2020-06-28T03:00:00.000Z</published>
    <updated>2020-06-28T05:45:36.794Z</updated>
    
    <content type="html"><![CDATA[<p>微服务为什么要配置中心?</p><p>架构师波波的专栏</p><p><a href="https://blog.csdn.net/yang75108/article/details/86987941" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/86987941</a></p><blockquote><h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>在系统架构中，和安全、日志、监控等非功能需求一样，配置管理也是一种非功能需求。配置中心是整个微服务基础架构体系中的一个组件，如下图，它的功能看上去并不起眼，无非就是简单配置的管理和存取，但它是整个微服务架构中不可或缺的一环。另外，配置中心如果真得用好了，它还能推动技术组织持续交付和DevOps文化转型。<br><img src="https://img-blog.csdnimg.cn/20200212132045204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" width="100%" height="100%"></p><p>本文介绍在分布式微服务环境下，应用配置管理背后的业务需求，配置的各种分类和一些高级应用场景。</p><h2 id="二、配置定义和形态"><a href="#二、配置定义和形态" class="headerlink" title="二、配置定义和形态"></a>二、配置定义和形态</h2><p><strong>配置其实是独立于程序的可配变量</strong>，同一份程序在不同配置下会有不同的行为，常见的配置有连接字符串，应用配置和业务配置等。</p><p>配置有多种形态，下面是一些常见的：</p><ul><li><strong>程序内部hardcode</strong>，这种做法是反模式，一般我们<strong>不建议！</strong></li><li><strong>配置文件</strong>，比如Spring应用程序的配置一般放在<code>application.properties</code>文件中。</li><li><strong>环境变量</strong>，配置可以预置在操作系统的环境变量里头，程序运行时读取，这是很多PaaS平台，比如Heroku推荐的做法，参考12 factor app[附录9.1]。</li><li><strong>启动参数</strong>，可以在程序启动时一次性提供参数，例如java程序启动时可以通过<code>java -D</code>方式配启动参数。</li><li><strong>基于数据库</strong>，有经验的开发人员会把易变配置放在数据库中，这样可以在运行期灵活调整配置，这个做法和配置中心的思路已经有点接近了。</li></ul><p><img src="https://img-blog.csdnimg.cn/20200212132102689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9ECzTdDT-1581484808516)(http://jskillcloud.com/img/post/2018060701/config_format.png#pic_center)]"></p><h2 id="三、传统应用配置的痛点"><a href="#三、传统应用配置的痛点" class="headerlink" title="三、传统应用配置的痛点"></a>三、传统应用配置的痛点</h2><p>在没有引入配置中心之前，一般企业研发都会面临如下痛点：</p><h3 id="1-配置散乱格式不标准"><a href="#1-配置散乱格式不标准" class="headerlink" title="1. 配置散乱格式不标准"></a><strong>1. 配置散乱格式不标准</strong></h3><p>有的用properties格式，有的用xml格式，还有的存DB，团队倾向自造轮子，做法五花八门。</p><h3 id="2-主要采用本地静态配置，配置修改麻烦"><a href="#2-主要采用本地静态配置，配置修改麻烦" class="headerlink" title="2. 主要采用本地静态配置，配置修改麻烦"></a><strong>2. 主要采用本地静态配置，配置修改麻烦</strong></h3><p>配置修改一般需要经过一个较长的测试发布周期。在分布式微服务环境下，当服务实例很多时，修改配置费时费力。</p><h3 id="3-易引发生产事故"><a href="#3-易引发生产事故" class="headerlink" title="3. 易引发生产事故"></a><strong>3. 易引发生产事故</strong></h3><p>这个是我亲身经历，之前在一家互联网公司，有团队在发布的时候将测试环境的配置带到生产上，引发百万级资损事故。</p><h3 id="4-配置缺乏安全审计和版本控制功能"><a href="#4-配置缺乏安全审计和版本控制功能" class="headerlink" title="4. 配置缺乏安全审计和版本控制功能"></a><strong>4. 配置缺乏安全审计和版本控制功能</strong></h3><p>谁改的配置？改了什么？什么时候改的？无从追溯，出了问题也无法及时回滚。</p><h2 id="四、现代应用配置核心需求"><a href="#四、现代应用配置核心需求" class="headerlink" title="四、现代应用配置核心需求"></a>四、现代应用配置核心需求</h2><p>近年，持续交付和DevOps理念开始逐步被一线企业接受，微服务架构和容器云也逐渐在一线企业落地，这些都对应用配置管理提出了更高的要求：</p><p><img src="https://img-blog.csdnimg.cn/20200212132118751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-q00lcjAM-1581484808517)(http://jskillcloud.com/img/post/2018060701/core_requirements.png#pic_center)]"></p><h3 id="1-交付件和配置分离"><a href="#1-交付件和配置分离" class="headerlink" title="1. 交付件和配置分离"></a><strong>1. 交付件和配置分离</strong></h3><p>传统做法应用在打包部署时，会为不同环境打出不同配置的包，例如为开发/测试/UAT/生产环境分别制作发布包，每个包里头包含环境特定配置。</p><p>现代微服务提倡云原生(Cloud Native)和不可变基础设施（Immutable Infrastructure）的理念，推荐采用如容器镜像这种方式打包和交付微服务，应用镜像一般只打一份，可以部署到不同环境。这就要求交付件（比如容器镜像）和配置进行分离，交付件只制作一份，并且是不可变的，可以部署到任意环境，而配置由配置中心集中管理，所有环境的配置都可以在配置中心集中配，运行期应用根据自身环境到配置中心动态拉取相应的配置。</p><h3 id="2-抽象标准化"><a href="#2-抽象标准化" class="headerlink" title="2. 抽象标准化"></a><strong>2. 抽象标准化</strong></h3><p>企业应该由框架或者中间件团队提供标准化的配置中心服务(Configuration as a Service)，封装屏蔽配置管理的细节和配置的不同格式，方便用户进行自助式的配置管理。一般用户只需要关注两个抽象和标准化的接口：</p><ol><li>配置管理界面UI，方便应用开发人员管理和发布配置，</li><li>封装好的客户端API，方便应用集成和获取配置。</li></ol><h3 id="3-多环境多集群"><a href="#3-多环境多集群" class="headerlink" title="3. 多环境多集群"></a><strong>3. 多环境多集群</strong></h3><p>现代微服务应用大都采用多环境部署，一般标准化的环境有开发/测试/UAT/生产等，有些应用还需要多集群部署，例如支持跨机房或者多版本部署。配置中心需要支持对多环境和多集群应用配置的集中式管理。</p><h3 id="4-高可用"><a href="#4-高可用" class="headerlink" title="4. 高可用"></a><strong>4. 高可用</strong></h3><p>配置中心必须保证高可用，不能随便挂，否则可能大面积影响微服务。在极端的情况下，如果配置中心不可用，客户端也需要有降级策略，保证应用可以不受影响。</p><h3 id="5-实时性"><a href="#5-实时性" class="headerlink" title="5. 实时性"></a><strong>5. 实时性</strong></h3><p>配置更新需要尽快通知到客户端，这个周期不能太长，理想应该是实时的。有些配置的实时性要求很高，比方说主备切换配置或者蓝绿部署配置，需要秒级切换配置的能力。</p><h3 id="6-治理"><a href="#6-治理" class="headerlink" title="6. 治理"></a><strong>6. 治理</strong></h3><p>配置需要治理，具体包括：</p><ul><li>配置审计，谁、在什么时间、修改了什么配置，需要详细的审计，方便出现问题时能够追溯。</li><li>配置版本控制，每次变更需要版本化，出现问题时候能够及时回滚到上一版本。</li><li>配置权限控制，配置变更发布需要认证授权，不是所有人都能修改和发布配置。</li><li>灰度发布，高级的配置治理支持灰度发布，配置发布时可以先让少数实例生效，确保没有问题再逐步放量。</li></ul><h2 id="五、配置分类"><a href="#五、配置分类" class="headerlink" title="五、配置分类"></a>五、配置分类</h2><p>配置目前还没有特别标准的分类方法，我简单把配置分为静态和动态两大类，每一类再分为若干子类，如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200212132133488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-57ZCzoGH-1581484808518)(http://jskillcloud.com/img/post/2018060701/config_category.png#pic_center)]"></p><h3 id="1-静态配置"><a href="#1-静态配置" class="headerlink" title="1. 静态配置"></a><strong>1. 静态配置</strong></h3><p>所谓静态配置，就是在程序启动前一次性配好，启动时一次性生效，在程序运行期一般不会变化的配置。具体包括：</p><h4 id="1-1-环境相关配置"><a href="#1-1-环境相关配置" class="headerlink" title="1.1 环境相关配置"></a>1.1 环境相关配置</h4><p>有些配置是和环境相关的，每个环境的配置不一样，例如数据库、中间件和其它服务的连接字符串配置。这些配置一次性配好，运行期一般不变。</p><h4 id="1-2-安全配置"><a href="#1-2-安全配置" class="headerlink" title="1.2 安全配置"></a>1.2 安全配置</h4><p>有些配置和安全相关，例如用户名，密码，访问令牌，许可证书等，这些配置也是一次性配好，运行期一般不变。因为涉及安全，相关信息一般需要加密存储，对配置访问需要权限控制。</p><h3 id="2-动态配置"><a href="#2-动态配置" class="headerlink" title="2. 动态配置"></a><strong>2. 动态配置</strong></h3><p>所谓动态配置，就是在程序的运行期可以根据需要动态调整的配置。动态配置让应用行为和功能的调整变得更加灵活，是持续交付和DevOps的最佳实践。具体包括：</p><h4 id="2-1-应用配置"><a href="#2-1-应用配置" class="headerlink" title="2.1 应用配置"></a>2.1 应用配置</h4><p>和应用相关的配置，例如服务请求超时，线程池和队列的大小，缓存过期时间，数据库连接池的容量，日志输出级别，限流熔断阀值，服务安全黑白名单等。一般开发或者运维会根据应用的实际运行情况调整这些配置。</p><h4 id="2-2-业务配置"><a href="#2-2-业务配置" class="headerlink" title="2.2 业务配置"></a>2.2 业务配置</h4><p>和业务相关的一些配置，例如促销规则，贷款额度，利率等业务参数，A/B测试参数等。一般产品运营或开发人员会根据实际的业务需求，动态调整这些参数。</p><h4 id="2-3-功能开关"><a href="#2-3-功能开关" class="headerlink" title="2.3 功能开关"></a>2.3 功能开关</h4><p>在英文中也称Feature Flag/Toggle/Switch，简单的只有真假两个值，复杂的可以是多值参数。功能开关是DevOps的一种最佳实践，在运维中有很多应用场景，比如蓝绿部署，灰度开关，降级开关，主备切换开关，数据库迁移开关等。功能开关在国外互联网公司用得比较多，国内还没有普及开，所以我在下一节会给出一些功能开关的高级应用场景。</p><h2 id="六、配置中心高级应用场景"><a href="#六、配置中心高级应用场景" class="headerlink" title="六、配置中心高级应用场景"></a>六、配置中心高级应用场景</h2><h3 id="场景一、蓝绿部署"><a href="#场景一、蓝绿部署" class="headerlink" title="场景一、蓝绿部署"></a>场景一、蓝绿部署</h3><p>蓝绿部署的传统做法是通过负载均衡器切流量来实现，如下图左边所示。这种做法一般研发人员无法自助操作，需要提交工单由运维介入操作，操作和反馈周期比较长，出了问题回退还需运维人员介入，所以回退也比较慢，总体风险比较高。</p><p><img src="https://img-blog.csdnimg.cn/20200212132148357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ZFkvyLnE-1581484808519)(http://jskillcloud.com/img/post/2018060701/blue_green_deployment.png#pic_center)]"></p><p>蓝绿部署也可以通过配置中心+功能开关的方式来实现，如上图右边所示。开发人员在上线新功能时先将新功能隐藏在动态开关后面，开关的值在配置中心里头配。刚上线时新功能暂不启用，走老功能逻辑，然后开发人员通过配置中心打开开关，这个时候新功能就启用了。一旦发现新功能有问题，可以随时把开关关掉切回老功能。这种做法开发人员可以全程自助实现蓝绿部署，不需要运维人员介入，反馈周期短效率高。</p><h3 id="场景二、限流降级"><a href="#场景二、限流降级" class="headerlink" title="场景二、限流降级"></a>场景二、限流降级</h3><p>当业务团队在搞促销，或者是系统受DDOS攻击的时候，如果没有好的限流降级机制，则系统很容易被洪峰流量冲垮，这个时候所有用户无法访问，体验糟糕，如下图左边所示。</p><p><img src="https://img-blog.csdnimg.cn/20200212132159343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ieWQQ5Mr-1581484808520)(http://jskillcloud.com/img/post/2018060701/rate_limiting_degrade.png#pic_center)]"></p><p>所以我们需要限流降级机制来应对流量洪峰。常见做法，我们一般会在应用的过滤器层或者是网关代理层添加限流降级逻辑，并且和配置中心配合，实现限流降级开关和参数的动态调整。如果促销出现流量洪峰，我们可以通过配置中心启动限流降级策略，比如对于普通用户，我们可以先给出“网络不给力，请稍后再试”的友好提示，对于高级VIP用户，我们仍然保证他们的正常访问。</p><p>国内电商巨头阿里，它内部的系统大量采用<font color="red">限流降级机制，实现方式基于其内部的diamond+sentinel配置管理系统。</font>如果没有限流降级机制的保护，则阿里的系统也无法抵御双十一带来的洪峰流量冲击。</p><h3 id="场景三、数据库迁移"><a href="#场景三、数据库迁移" class="headerlink" title="场景三、数据库迁移"></a>场景三、数据库迁移</h3><p>LaunchDarkly是一家提供配置既服务(Configuration as a Service)的SAAS服务公司，它在其博客上给出了一片关于使用功能开关实现数据库迁移的案例文章，该案例基于其内部一次成功的数据库迁移实践，从MongdoDB迁移到DynamoDB[参考附录9.2]，下图是展示了一个简化的迁移流程：</p><p><img src="https://img-blog.csdnimg.cn/20200212132212489.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-N4Q8XNdr-1581484808520)(http://jskillcloud.com/img/post/2018060701/ff_database_migration-768x1024.jpg#pic_center)]"></p><p>简化迁移腾挪流程如下：</p><ol><li>开发人员先在应用端的DAO层埋好数据双写双读、以及数据比对逻辑。双写双读逻辑由开关控制，开关的值可在配置中心配。</li><li>先保证应用100%读写mongoDB，然后先放开10%的DynamoDB双写，也称金丝雀写(Canary Write)，确保金丝雀写没有功能和性能问题。</li><li>逐步放量DyanamoDB写到100%，确保全量双写没有功能和性能问题。</li><li>放开10%的DynamoDB双读，也称金丝雀读(Canary Read)，通过比对逻辑确保金丝雀读没有逻辑和性能问题。</li><li>逐步放量DynamoDB读到100%，通过比对逻辑确保全量双读没有逻辑和性能问题。</li><li>关闭对mongoDB的读写，迁移完成。</li></ol><p>整个迁移流程受配置中心的开关控制，可以灵活调整开关和参数，有问题可以随时回滚，大大降低迁移风险。</p><h3 id="场景四、A-B测试"><a href="#场景四、A-B测试" class="headerlink" title="场景四、A/B测试"></a>场景四、A/B测试</h3><p>如果我们需要对电商平台的结账(checkout)功能进行改版，考虑到结账功能业务影响面大，一下子上线风险大，为了减低风险，我们可以在配置中心配合下，对结账功能进行A/B测试，简化逻辑如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200212132223128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-xlzC9otk-1581484808521)(http://jskillcloud.com/img/post/2018060701/ab_test.png#pic_center)]"></p><p>我们在配置中心中增加一个<code>ab_test_flag</code>开关，控制A/B测试逻辑：</p><ol><li>如果A/B测试开关是关闭的(<code>ab_test_flag==false</code>)，那么就走老的结账逻辑。</li><li>如果A/B测试开关是打开的(<code>ab_test_flag==true</code>，并且是普通用户(<code>user==regular</code>，可以检查数据库中用户类型)，那么就走老的结账逻辑。</li><li>如果A/B测试开关是打开的(<code>ab_test_flag==true</code>)，并且是beta用户（<code>user==beta</code>），那么就走改版后的新结账逻辑。</li></ol><p>通过配置中心，我们可以灵活调整开关，先对新功能进行充分的beta试验，再考虑全量上线，大大降低关键业务新功能的上线风险。</p><h2 id="七、公司案例和产品"><a href="#七、公司案例和产品" class="headerlink" title="七、公司案例和产品"></a>七、公司案例和产品</h2><p>在一线前沿的互联网公司，配置中心都是其技术体系中的关键基础服务，下图给出一些公司案例产品：</p><p><img src="https://img-blog.csdnimg.cn/20200212132236228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-MA6Ky9GG-1581484808521)(http://jskillcloud.com/img/post/2018060701/config_center_products.png#pic_center)]"></p><ol><li>阿里巴巴中间件部门很早就自研了配置中心Diamond，并且是开源的。Diamond对阿里系统的灵活稳定性发挥了至关重要的作用。开源版本的Diamond由于研发时间比较早，使用的技术比较老，功能也不够完善，目前社区不热已经不维护了。</li><li>Facebook内部也有一整套完善的配置管理体系[可参考其论文，附录9.3]，其中一个产品叫Gatekeeper，目前没有开源。</li><li>Netflix内部有大量的微服务，它的服务的稳定灵活性也重度依赖于配置中心。Netflix开源了它的配置中心的客户端，叫变色龙Archaius[参考附录9.4]，比较可惜的是，Netflix没有开源它的配置中心的服务器端。</li><li>Apollo[参考附录9.5]是携程框架部研发并开源的一款配置中心产品，企业级治理功能完善，目前社区比较火，在github上有超过5k星，在国内众多互联网公司有落地案例。<strong>如果企业打算引入开源的配置中心，那么Apollo是我推荐的首选</strong>。</li><li>百度之前也开源过一个叫Disconf[参考附录9.6]的配置中心产品，作者是前百度资深工程师廖绮绮。在Apollo没有出来之前，Disconf在社区是比较火的，但是自从廖琦琦离开百度之后，他好像没有足够精力投入维护这个项目，目前社区活跃度已经大不如前。</li></ol><h2 id="八、结论"><a href="#八、结论" class="headerlink" title="八、结论"></a>八、结论</h2><ol><li>配置中心是微服务基础架构中不可或缺的核心组件，现代微服务架构和云原生环境，对应用配置管理提出了更高的要求。</li><li>配置中心有众多的应用场景，<strong>配置中心+功能开关是DevOps最佳实践</strong>。用好配置中心，它能帮助技术组织实现持续交付和DevOps文化转型。</li><li>携程开源的Apollo配置中心，企业级功能完善，经过大规模生产验证，社区活跃度高，是开源配置中心产品的首选。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;微服务为什么要配置中心?&lt;/p&gt;
&lt;p&gt;架构师波波的专栏&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/86987941&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;htt
      
    
    </summary>
    
      <category term="配置中心" scheme="http://zhang-yu.me/categories/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
    
      <category term="配置中心" scheme="http://zhang-yu.me/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>我为啥暂不看好ServiceMesh</title>
    <link href="http://zhang-yu.me/2020/06/28/%E6%88%91%E4%B8%BA%E5%95%A5%E6%9A%82%E4%B8%8D%E7%9C%8B%E5%A5%BDServiceMesh/"/>
    <id>http://zhang-yu.me/2020/06/28/我为啥暂不看好ServiceMesh/</id>
    <published>2020-06-28T03:00:00.000Z</published>
    <updated>2020-06-28T08:26:44.713Z</updated>
    
    <content type="html"><![CDATA[<p>我为啥暂不看好ServiceMesh?</p><p><a href="https://blog.csdn.net/yang75108/article/details/87266458" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/87266458</a></p><blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>过去的2018年，ServiceMesh(服务网格)概念在社区里头非常火，有人提出2018年是ServiceMesh年，还有人提出ServiceMesh是下一代的微服务架构基础。作为架构师，如果你现在还不了解ServiceMesh的话，是否感觉有点落伍了？</p><p>那么到底什么是ServiceMesh？它诞生的背景是什么？它解决什么问题？企业是否适合引入ServiceMesh？根据近年在一线互联网企业的实践和思考，从个人视角出发，我为大家一一解答这些问题。</p><h2 id="微服务架构的核心技术问题"><a href="#微服务架构的核心技术问题" class="headerlink" title="微服务架构的核心技术问题"></a>微服务架构的核心技术问题</h2><p>在业务规模化和研发效能提升等因素的驱动下，从单块应用向微服务架构的转型(如下图所示)，已经成为很多企业(尤其是互联网企业)数字化转型的趋势。</p><p><img src="https://img-blog.csdnimg.cn/20200211204043954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在微服务模式下，企业内部服务少则几个到几十个，多则上百个，每个服务一般都以集群方式部署，这时自然产生两个问题(如下图所示)：</p><p><img src="https://img-blog.csdnimg.cn/20200211204322439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p><strong>一、服务发现</strong>：服务的消费方(Consumer)如何发现服务的提供方(Provider)？</p><p><strong>二、负载均衡</strong>：服务的消费方如何以某种负载均衡策略访问集群中的服务提供方实例？</p><p>作为架构师，如果你理解了这两个问题，可以说就理解了微服务架构在技术上的最核心问题。</p><h2 id="三种服务发现模式"><a href="#三种服务发现模式" class="headerlink" title="三种服务发现模式"></a>三种服务发现模式</h2><p>服务发现和负载均衡并不是新问题，业界其实已经探索和总结出一些常用的模式，这些模式的核心其实是代理(Proxy，如下图所以)，以及代理在架构中所处的位置，</p><p><img src="https://img-blog.csdnimg.cn/20200211204059487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在服务消费方和服务提供方之间增加一层代理，由代理负责服务发现和负载均衡功能，消费方通过代理间接访问目标服务。根据代理在架构上所处的位置不同，当前业界主要有三种不同的服务发现模式：</p><h3 id="模式一：传统集中式代理"><a href="#模式一：传统集中式代理" class="headerlink" title="模式一：传统集中式代理"></a>模式一：传统集中式代理</h3><p><img src="https://img-blog.csdnimg.cn/20200211204117995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，F5(4层负载)+Nginx(7层负载)这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。</p><p>这种方式通常在DNS域名服务器的配合下实现服务发现，服务注册(建立服务域名和IP地址之间的映射关系)一般由运维人员在代理上手工配置，服务消费方仅依赖服务域名，这个域名指向代理，由代理解析目标地址并做负载均衡和调用。</p><p>国外知名电商网站eBay，虽然体量巨大，但其内部的服务发现机制仍然是基于这种传统的集中代理模式，国内公司如携程，也是采用这种模式。</p><h3 id="模式二：客户端嵌入式代理"><a href="#模式二：客户端嵌入式代理" class="headerlink" title="模式二：客户端嵌入式代理"></a>模式二：客户端嵌入式代理</h3><p><img src="https://img-blog.csdnimg.cn/2020021120413546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。</p><p>Netflix开源的Eureka(注册中心)和Ribbon(客户端代理)是这种模式的典型案例，国内阿里开源的Dubbo也是采用这种模式。</p><h3 id="模式三：主机独立进程代理"><a href="#模式三：主机独立进程代理" class="headerlink" title="模式三：主机独立进程代理"></a>模式三：主机独立进程代理</h3><p>这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同模式二。</p><p><img src="https://img-blog.csdnimg.cn/20200211204151399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>Airbnb的SmartStack是这种模式早期实践产品，国内公司唯品会对这种模式也有探索和实践。</p><h2 id="三种服务发现模式的比较"><a href="#三种服务发现模式的比较" class="headerlink" title="三种服务发现模式的比较"></a>三种服务发现模式的比较</h2><p>上面介绍的三种服务发现模式各有优劣，没有绝对的好坏，可以认为是三种不同的架构风格，在不同的公司都有成功实践。下表总结三种服务发现模式的优劣比较，业界案例和适用场景建议，供架构师选型参考：</p><p><img src="https://img-blog.csdnimg.cn/20200211204205282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><h2 id="服务网格ServiceMesh"><a href="#服务网格ServiceMesh" class="headerlink" title="服务网格ServiceMesh"></a>服务网格ServiceMesh</h2><p>所谓的ServiceMesh，其实本质上就是上面提到的模式三~主机独立进程模式，这个模式其实并不新鲜，业界(国外的Airbnb和国内的唯品会等)早有实践，那么为什么现在这个概念又流行起来了呢？我认为主要原因如下：</p><ol><li>上述模式一和二有一些固有缺陷，模式一相对比较重，有单点问题和性能问题；模式二则有客户端复杂，支持多语言困难，无法集中治理的问题。模式三是模式一和二的折中，弥补了两者的不足，它是纯分布式的，没有单点问题，性能也OK，应用语言栈无关，可以集中治理。</li><li>微服务化、多语言和容器化发展的趋势，企业迫切需要一种轻量级的服务发现机制，ServiceMesh正是迎合这种趋势诞生，当然这还和一些大厂(如Google/IBM等)的背后推动有关。</li></ol><p>模式三(ServiceMesh)也被形象称为边车(Sidecar)模式，如下图，早期有一些摩托车，除了主驾驶位，还带一个边车位，可以额外坐一个人。在模式三中，业务代码进程(相当于主驾驶)共享一个代理(相当于边车)，代理除了负责服务发现和负载均衡，还负责动态路由、容错限流、监控度量和安全日志等功能，这些功能是具体业务无关的，属于跨横切面关注点(Cross-Cutting Concerns)范畴。</p><p><img src="https://img-blog.csdnimg.cn/20200211204224981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在新一代的ServiceMesh架构中(下图上方)，服务的消费方和提供方主机(或者容器)两边都会部署代理SideCar。ServiceMesh比较正式的术语也叫数据平面(DataPlane)，与数据平面对应的还有一个独立部署的控制平面(ControlPlane)，用来集中配置和管理数据平面，也可以对接各种服务发现机制(如K8S服务发现)。术语数据平面和控制平面，估计是偏网络SDN背景的人提出来的。</p><p><img src="https://img-blog.csdnimg.cn/20200211204234911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>上图左下角，每个主机上同时居住了业务逻辑代码(绿色表示)和代理(蓝色表示)，服务之间通过代理发现和调用目标服务，形成服务之间的一种网络状依赖关系，控制平面则可以配置这种依赖调用关系，也可以调拨路由流量。如果我们把主机和业务逻辑剥离，就出现一种网格状架构(上图右下角)，服务网格由此得名。</p><p><img src="https://img-blog.csdnimg.cn/20200211204245278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>Istio是Google/IBM等大厂支持和推进的一个ServiceMesh标准化工作组，上图是Istio给出的ServiceMesh参考架构(注意这个是老版架构，新版有一些调整，但是大框架没变)。Istio专注在控制平面的架构、功能、以及控制平面和数据平面之间API的标准化，它的控制平面功能主要包括：</p><ul><li>Istio-Manager：负责服务发现，路由分流，熔断限流等配置数据的管理和下发</li><li>Mixer：负责收集代理上采集的度量数据，进行集中监控</li><li>Istio-Auth：负责安全控制数据的管理和下发</li></ul><p>Envoy是目前Istio主力支持的数据平面代理，其它主流代理如nginx/kong等也正在陆续加入这个阵营。kubernetes是目前Isito主力支持的容器云环境。</p><h2 id="我的建议"><a href="#我的建议" class="headerlink" title="我的建议"></a>我的建议</h2><p>目前我本人并不特别看好ServiceMesh，也不是特别建议企业在生产上试水ServiceMesh，主要原因如下：</p><ol><li>ServiceMesh其实并不是什么新东西，本质就是上面提到的服务发现模式三~主机独立进程模式，这个模式很早就有公司在探索和实践，但是一直没有普遍流行起来，说明这个模式也是存在落地挑战的。从表面上看，模式三是模式一和模式二的折中，同时解决了模式一和模式二存在的问题，但是在每个主机上独立部署一个代理进程，是有很大运维管理开销的，一方面是规模化部署的问题(考虑服务很多，机器也很多的场景)；另一方面是如何监控治理的问题，代理挂了怎么办？你的团队是否具备自动化运维和监控的能力？另外开发人员在服务调试的时候，会依赖于这个独立的代理，调试排错比较麻烦，这个问题怎么解决？</li><li>Istio的确做了一些标准化工作，但是没有什么特别的创新，可是说换汤不换药，就是把模式三规范化和包装了一下。透过现象看本质，Google/IBM等行业大厂在背后推Isito/ServiceMesh，背后有一些市场利益诉求考虑，例如Google要推进它的kubernates和公有云生态。</li><li>ServiceMesh在年初声音比较大，最近渐渐安静下来，我听到国内只有一些大厂(华为，新浪微博，蚂蚁金服等)在试水，实际生产级落地的案例聊聊无几。大多数企业对ServiceMesh只是观望，很多架构师对ServiceMesh实际落地都存在疑虑。</li></ol><p>所以我的个人建议，对于大部分企业(一般运维和研发能力不是特别强)，采用模式一~集中代理模式就足够了。这个模式比较传统不新鲜，但是在很多一线企业已经切实落地，我甚至认为，除了一些大厂，大部分中小企业的服务发现架构采用的就是集中代理。我本人经历过三家互联网公司，大的有eBay，中等有携程，小的有拍拍贷，都是采用集中式代理模式，而且玩得都很好。我的架构理念很简单，对于生产级应用，不追新，老实采用大部分企业落地过的方案。</p><p>模式一的最大好处是集中治理，应用不侵入，语言栈无关，另外因为模式一是集中部署的，不像模式三是分布式部署，所以模式一的运维开销也远小于模式三。对于模式一，大家最大的顾虑是性能和单点问题，其实性能还是OK的，如果架构和容量规划合理的话，实际生产中经过集中代理的性能开销一般可以控制在小于10个ms，eBay和携程等大流量企业的成功实践已经验证了这点。单点问题一般建议采用两层负载结构，例如硬件F5+软件nginx两层负载，F5以主从HA部署，nginx则以集群多实例部署，这种架构兼顾了高可用和配置的灵活性。</p><p>另外，模式一还可以和服务注册中心结合，从而降低手工配置的复杂性，实现DevOps研发自助部署，一种方案如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200211204300581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>服务启动时自动注册到服务注册中心并定期报心跳，Proxy则定期到服务注册中心同步实例。这种方式下，不需要为每个服务申请一个域名，只需一个泛域名即可，消费者访问服务时采用服务名+泛域名即可，整个服务上线流程可以做到DevOps研发自助。目前社区流行的一些开源代理如traefik和kong\等都支持和多种服务注册中心(Consul/Eureka/Etcd/Zookeeper等)进行集成。目前这种方案在拍拍贷有初步成功实践，采用kong和自研服务注册中心Radar同时和容器云调度平台配合，实现了研发全自助式发布上线。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>服务注册发现和负载均衡是微服务架构在技术上的根本问题，解决的办法是采用代理Proxy。根据代理在架构上的位置不同，服务发现代理一般有三种模式：</li></ol><ul><li>模式一：集中式代理</li><li>模式二：客户端嵌入式代理</li><li>模式三：主机独立进程代理<br>这三种模式没有绝对的好还之分，只是三种不同的架构风格，各有优劣和适用场景，在不同企业都有成功落地案例。</li></ul><ol start="2"><li>ServiceMesh本质上就是模式三~主机独立进程代理，它结合了模式一和模式二的优势，但是分布式部署运维管理开销大。Istio对ServiceMesh的架构、功能和API进行了标准化。</li></ol></blockquote><blockquote><ol start="3"><li>ServiceMesh还在演进中，生产落地仍有挑战，一般企业不建议生产级使用。集中式代理最成熟，对于一般中小企业，建议从集中式代理开始，等达到一定规模和具备一定的研发运维能力，再根据需要考虑其它服务发现模式.</li><li>架构师不要盲目追新，在理解微服务架构原理的基础上，可以学习和试点新技术，但是对于生产级应用，应该以成熟稳定，有大规模落地案例作为选型第一准则。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我为啥暂不看好ServiceMesh?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/87266458&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog
      
    
    </summary>
    
      <category term="ServiceMesh" scheme="http://zhang-yu.me/categories/ServiceMesh/"/>
    
    
      <category term="ServiceMesh" scheme="http://zhang-yu.me/tags/ServiceMesh/"/>
    
  </entry>
  
  <entry>
    <title>小米Redis的K8s容器化部署实践</title>
    <link href="http://zhang-yu.me/2020/06/22/%E5%B0%8F%E7%B1%B3Redis%E7%9A%84Kubernetes%E5%AE%B9%E5%99%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zhang-yu.me/2020/06/22/小米Redis的Kubernetes容器化部署实践/</id>
    <published>2020-06-22T03:00:00.000Z</published>
    <updated>2020-06-28T08:09:43.707Z</updated>
    
    <content type="html"><![CDATA[<p>小米Redis的K8s容器化部署实践</p><p>原创 崔凯峰 小米云技术 </p><p><a href="https://mp.weixin.qq.com/s/WrUU3C-C8TBgJfGuOv3qGQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/WrUU3C-C8TBgJfGuOv3qGQ</a></p><blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>小米的Redis使用规模很大，现在有数万个实例，并且每天有百万亿次的访问频率，支撑了几乎所有的产品线和生态链公司。之前所有的Redis都部署在物理机上，也没有做资源隔离，给管理治理带来了很大的困难。我们的运维人员工作压力很大，机器宕机网络抖动导致的Redis节点下线都经常需要人工介入处理。由于没有做CPU的资源隔离，slave节点打RDB或者由于流量突增导致节点QPS升高造成的节点CPU使用率升高，都可能对本集群或其他集群的节点造成影响，导致无法预测的时延增加。</p><p>Redis分片方式采用社区的Redis Cluster协议，集群自主分片。Redis Cluster带来了一定的易用性的同时，也提高了应用开发的门槛，应用开发人员需要一定程度上了解Redis Cluster，同时需要使用智能客户端访问Redis Cluster。这些智能客户端配置参数繁多，应用开发人员并无法完全掌握并设置这些参数，踩了很多坑。同时，由于智能客户端需要做分片计算，给应用端的机器也带来了一定的负载。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcVtxn9dUon7LbJnGJOOQkEZziaRaedAMJIkY8TaKru5STJXBtKibNxt7Ow/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Why-K8S"><a href="#Why-K8S" class="headerlink" title="Why K8S"></a>Why K8S</h2><h2 id="资源隔离"><a href="#资源隔离" class="headerlink" title="资源隔离"></a><strong>资源隔离</strong></h2><p>当前的Redis Cluster部署在物理机集群上，为了提高资源利用率节约成本，多业务线的Redis集群都是混布的。由于没有做CPU的资源隔离，经常出现某Redis节点CPU使用率过高导致其他Redis集群的节点争抢不到CPU资源引起时延抖动。因为不同的集群混布，这类问题很难快速定位，影响运维效率。K8s容器化部署可以指定 CPU request 和 CPU limit ，在提高资源利用率的同时避免了资源争抢。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="自动化部署"><a href="#自动化部署" class="headerlink" title="自动化部署"></a><strong>自动化部署</strong></h2><p>自动化部署。当前Redis Cluster在物理机上的部署过程十分繁琐，需要通过查看元信息数据库查找有空余资源的机器，手动修改很多配置文件再逐个部署节点，最后使用redis_trib工具创建集群，新集群的初始化工作经常需要一两个小时。</p><p>K8s通过StatefulSet部署Redis集群，使用configmap管理配置文件，新集群部署时间只需要几分钟，大大提高了运维效率。</p><h2 id="How-K8S"><a href="#How-K8S" class="headerlink" title="How K8S"></a>How K8S</h2><p>客户端通过LVS的VIP统一接入，通过Redis Proxy转发服务请求到Redis Cluster集群。这里我们引入了Redis Proxy来转发请求。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcV9yKs0YScPnxtplmyDUUxl1miaEib7Ekn9L73dKS2Tia7fA6XCOd3CDTNA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Redis-Cluster部署方式"><a href="#Redis-Cluster部署方式" class="headerlink" title="Redis Cluster部署方式"></a><strong>Redis Cluster部署方式</strong></h2><p>Redis部署为StatefulSet，作为有状态的服务，选择StatefulSet最为合理，可以将节点的RDB/AOF持久化到分布式存储中。当节点重启漂移到其他机器上时，可通过挂载的PVC(PersistentVolumeClaim)拿到原来的RDB/AOF来同步数据。我们选择的持久化存储PV(PersistentVolume)是Ceph Block Service。Ceph的读写性能低于本地磁盘，会带来100~200ms的读写时延。但由于Redis的RDB/AOF的写出都是异步的，分布式存储带来的读写延迟对服务并没有影响。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcVPzdQYX28SbZqwYl5tlFBkqfpxM3xOC3IQa3qEq2hnEYISoMWmrXFqA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="Proxy选型"><a href="#Proxy选型" class="headerlink" title="Proxy选型"></a><strong>Proxy选型</strong></h2><p>## </p><p>开源的Redis Proxy有很多，常见的开源Redis Proxy如下:</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRrJAgO3tqmgH2KF0EcfGqqW2pJcs1rgdmyqibPpJbRM4PiabXStSXeyObE7y8wqvZqeLZ7N4RVNfIog/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>我们希望能够继续使用Redis Cluster来管理Redis集群，所以Codis和Twemproxy不再考虑。redis-cluster-proxy是Redis官方在6.0版本推出的支持Redis Cluster协议的Proxy，但是目前还没有稳定版，暂时也无法大规模应用。</p><p>备选就只有Cerberus和Predixy两种。我们在K8s环境上对Cerberus和Predixy进行了性能测试，结果如下:</p><h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a><strong>测试环境</strong></h3><p>测试工具: redis-benchmark</p><p>Proxy CPU: 2 core</p><p>Client CPU: 2 core</p><p>Redis Cluster: 3 master nodes, 1 CPU per node</p><h3 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a><strong>测试结果</strong></h3><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRrJAgO3tqmgH2KF0EcfGqqWdUeLMTycZ2acica48JEiahLtSuFbULukibvVAFqbDOC4fcQRqKhx8RqgQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRpnDfGTj2hakEapXh4jdmcVNwHM46icecJg2NzJJQMZqP9CicfnyK7LeTAt3kPoGe3ldvpRgZX5erwA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>## </p><p>在相同workload和配置下，Predixy的最高QPS要优于Cerberus，时延也比较接近。综合来看，Predixy比Cerberus的性能要高33%~60%，并且数据的key/value越大，Predixy优势越明显，所以最后我们选择了Predixy。</p><p>为了适应业务和K8s环境，在上线前我们对Predixy做了大量的改动，增加了很多新的功能，比如动态切换后端Redis Cluster、黑白名单、异常操作审计等。</p><h2 id="Proxy部署方式"><a href="#Proxy部署方式" class="headerlink" title="Proxy部署方式"></a><strong>Proxy部署方式</strong></h2><p>Proxy作为deployment部署，无状态轻量化，通过LB对外提供服务，很容易做到动态扩缩容。同时，我们为Proxy开发了动态切换后端Redis Cluster的功能，可实现在线添加和切换Redis Cluster。</p><h2 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h2><h2 id="Proxy自动扩缩容方式"><a href="#Proxy自动扩缩容方式" class="headerlink" title="Proxy自动扩缩容方式"></a><strong>Proxy自动扩缩容方式</strong></h2><p>我们使用K8s原生的HPA(Horizontal Pod Autoscaler)来实现Proxy的动态扩缩容。当Proxy所有pod的平均CPU使用率超过一定阈值时，会自动触发扩容，HPA会将Proxy的replica数加1，之后LVS就会探测到新的Proxy pod并将一部分流量切过去。如果扩容后CPU使用率仍然超过规定的阈值，会继续触发扩容逻辑。但是在扩容成功5分钟内，不论CPU使用率降到多低，都不会触发缩容逻辑，这样就避免了频繁的扩缩容给集群稳定性带来的影响。</p><p>HPA可配置集群的最少(MINPODS)和最多(MAXPODS)pod数量，集群负载再低也不会缩容到MINPODS以下数量的pods。建议客户可以根据自己的实际业务情况来决定MINPODS和MAXPODS的值。</p><h2 id="Why-Proxy"><a href="#Why-Proxy" class="headerlink" title="Why Proxy"></a>Why Proxy</h2><h2 id="Redis-pod重启可导致IP变化"><a href="#Redis-pod重启可导致IP变化" class="headerlink" title="Redis pod重启可导致IP变化"></a><strong>Redis pod重启可导致IP变化</strong></h2><p>使用Redis Cluster的Redis客户端，都需要配置集群的部分IP和Port，用于客户端重启时查找Redis Cluster的入口。对于物理机集群部署的Redis节点，即便遇到实例重启或者机器重启，IP和Port都可以保持不变，客户端依然能够找到Redis Cluster的拓扑。但是部署在K8s上的Redis Cluster，pod重启是不保证IP不变的(即便是重启在原来的K8s node上)，这样客户端重启时，就可能会找不到Redis Cluster的入口。</p><p>通过在客户端和Redis Cluster之间加上Proxy，就对客户端屏蔽了Redis Cluster的信息，Proxy可以动态感知Redis Cluster的拓扑变化，客户端只需要将LVS的IP:Port作为入口，请求转发到Proxy上，即可以像使用单机版Redis一样使用Redis Cluster集群，而不需要Redis智能客户端。</p><h2 id="Redis处理连接负载高"><a href="#Redis处理连接负载高" class="headerlink" title="Redis处理连接负载高"></a><strong>Redis处理连接负载高</strong></h2><p>在6.0版本之前，Redis都是单线程处理大部分任务的。当Redis节点的连接较高时，Redis需要消耗大量的CPU资源处理这些连接，导致时延升高。有了Proxy之后，大量连接都在Proxy上，而Proxy跟Redis实例之间只保持很少的连接，这样降低了Redis的负担，避免了因为连接增加而导致的Redis时延升高。</p><h2 id="集群迁移切换需要应用重启"><a href="#集群迁移切换需要应用重启" class="headerlink" title="集群迁移切换需要应用重启"></a><strong>集群迁移切换需要应用重启</strong></h2><p>在使用过程中，随着业务的增长，Redis集群的数据量会持续增加，当每个节点的数据量过高时，BGSAVE的时间会大大延长，降低集群的可用度。同时QPS的增加也会导致每个节点的CPU使用率增高。这都需要增加扩容集群来解决。目前Redis Cluster的横向扩展能力不是很好，原生的slots搬移方案效率很低。新增节点后，有些客户端比如Lettuce，会因为安全机制无法识别新节点。另外迁移时间也完全无法预估，迁移过程中遇到问题也无法回退。</p><p>当前物理机集群的扩容方案是：</p><ol><li><p>按需创建新集群</p></li><li><p>使用同步工具将数据从老集群同步到新集群</p></li><li><p>确认数据无误后，跟业务沟通，重启服务切换到新集群</p></li></ol><p>整个过程繁琐而且风险较大，还需要业务重启服务。</p><p>有了Proxy层，可以将后端的创建、同步和切换集群对客户端屏蔽掉。新老集群同步完成之后，向Proxy发送命令就可以将连接换到新集群，可以实现对客户端完全无感知的集群扩缩容。</p><h2 id="数据安全风险"><a href="#数据安全风险" class="headerlink" title="数据安全风险"></a><strong>数据安全风险</strong></h2><p>Redis是通过AUTH来实现鉴权操作，客户端直连Redis，密码还是需要在客户端保存。而使用Proxy，客户端只需要通过Proxy的密码来访问Proxy，不需要知道Redis的密码。Proxy还限制了FLUSHDB、CONFIG SET等操作，避免了客户误操作清空数据或修改Redis配置，大大提高了系统的安全性。</p><p>同时，Redis并没有提供审计功能。我们在Proxy上增加了高危操作的日志保存功能，可以在不影响整体性能的前提下提供审计能力。</p><h2 id="Proxy-带来的问题"><a href="#Proxy-带来的问题" class="headerlink" title="Proxy 带来的问题"></a>Proxy 带来的问题</h2><h2 id="多一跳带来的时延"><a href="#多一跳带来的时延" class="headerlink" title="多一跳带来的时延"></a><strong>多一跳带来的时延</strong></h2><p>Proxy在客户端和Redis实例之间，客户端访问Redis数据需要先访问Proxy再访问Redis节点，多了一跳，会导致时延增加。经测试，多一跳会增加0.2~0.3ms的时延，不过通常这对业务来说是可以接受的。</p><h2 id="Pod漂移造成IP变化"><a href="#Pod漂移造成IP变化" class="headerlink" title="Pod漂移造成IP变化"></a><strong>Pod漂移造成IP变化</strong></h2><p>Proxy在K8s上是通过deployment部署的，一样会有节点重启导致IP变化的问题。我们K8s的LB方案可以感知到Proxy的IP变化，动态的将LVS的流量切到重启后的Proxy上。</p><h2 id="LVS带来的时延"><a href="#LVS带来的时延" class="headerlink" title="LVS带来的时延"></a><strong>LVS带来的时延</strong></h2><p>LVS也会带来时延，如下表中的测试，不同的数据长度get/set操作，LVS引入的时延小于0.1ms。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/sz_mmbiz_png/Re4KW51oYRrJAgO3tqmgH2KF0EcfGqqWNBgfVsEkDqxONt4P9Wicn7rg44gSnFS6GgK2bt97NbeWGQmiaFc6PQiaA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="K8S-带来的好处"><a href="#K8S-带来的好处" class="headerlink" title="K8S 带来的好处"></a>K8S 带来的好处</h2><h2 id="部署方便"><a href="#部署方便" class="headerlink" title="部署方便"></a><strong>部署方便</strong></h2><p>通过运维平台调用K8s API部署集群，大大提高了运维效率。</p><h2 id="解决端口管理问题"><a href="#解决端口管理问题" class="headerlink" title="解决端口管理问题"></a><strong>解决端口管理问题</strong></h2><p>目前小米在物理机上部署Redis实例是通过端口来区分的，并且下线的端口不能复用，也就是说整个公司每个Redis实例都有唯一的端口号。目前65535个端口已经用到了40000多，按现在的业务发展速度，将在两年内耗尽端口资源。而通过K8s部署，每一个Redis实例对应的K8s pod都有独立的IP，不存在端口耗尽问题和复杂的管理问题。</p><h2 id="降低客户使用门槛"><a href="#降低客户使用门槛" class="headerlink" title="降低客户使用门槛"></a><strong>降低客户使用门槛</strong></h2><p>对应用来说，只需要使用单机版的非智能客户端连接VIP，降低了使用门槛，避免了繁琐复杂的参数设置。同时由于VIP和端口是固定不变的，应用程序不再需要自己管理Redis Cluster的拓扑。</p><h2 id="提高客户端性能"><a href="#提高客户端性能" class="headerlink" title="提高客户端性能"></a><strong>提高客户端性能</strong></h2><p>使用非智能客户端还可以降低客户端的负载，因为智能客户端需要在客户端对key进行hash以确定将请求发送到哪个Redis节点，在QPS比较高的情况下会消耗客户端机器的CPU资源。当然，为了降低客户端应用迁移的难度，我们让Proxy也支持了智能客户端协议。</p><h2 id="动态升级和扩缩容"><a href="#动态升级和扩缩容" class="headerlink" title="动态升级和扩缩容"></a><strong>动态升级和扩缩容</strong></h2><p>Proxy支持动态添加切换Redis Cluster的功能，这样Redis Cluster的集群升级和扩容切换过程可以做到对业务端完全无感知。例如，业务方使用30个节点的Redis Cluster集群，由于业务量的增加，数据量和QPS都增长的很快，需要将集群规模扩容两倍。如果在原有的物理机上扩容，需要以下过程:</p><ol><li><p>协调资源，部署60个节点的新集群</p></li><li><p>手动配置迁移工具，将当前集群的数据迁移到新集群</p></li><li><p>验证数据无误后，通知业务方修改Redis Cluster连接池拓扑，重启服务</p></li></ol><p>虽然Redis Cluster支持在线扩容，但是扩容过程中slots搬移会对线上业务造成影响，同时迁移时间不可控，所以现阶段很少采用这种方式，只有在资源严重不足时才会偶尔使用。</p><p>在新的K8s架构下，迁移过程如下：  </p><ol><li><p>通过API接口一键创建60个节点的新集群</p></li><li><p>同样通过API接口一键创建集群同步工具，将数据迁移到新集群</p></li><li><p>验证数据无误后，向Proxy发送命令添加新集群信息并完成切换</p></li></ol><p>整个过程对业务端完全无感知。</p><p>集群升级也很方便：如果业务方能接受一定的延迟毛刺，可以在低峰时通过StatefulSet滚动升级的方式来实现；如果业务对延迟有要求，可以通过创建新集群迁移数据的方式来实现。</p><h2 id="提高服务稳定性和资源利用率"><a href="#提高服务稳定性和资源利用率" class="headerlink" title="提高服务稳定性和资源利用率"></a><strong>提高服务稳定性和资源利用率</strong></h2><p>通过K8s自带的资源隔离能力，实现和其他不同类型应用混部，在提高资源利用率的同时，也能保证服务稳定性。</p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h2 id="Pod重启导致数据丢失"><a href="#Pod重启导致数据丢失" class="headerlink" title="Pod重启导致数据丢失"></a><strong>Pod重启导致数据丢失</strong></h2><p>K8s的pod碰到问题重启时，由于重启速度过快，会在Redis Cluster集群发现并切主前将pod重启。如果pod上的Redis是slave，不会造成什么影响。但如果Redis是master，并且没有AOF，重启后原先内存的数据都被清空，Redis会reload之前存储的RDB文件，但是RDB文件并不是实时的数据。之后slave也会跟着把自己的数据同步成之前的RDB文件中的数据镜像，会造成部分数据丢失。</p><p>StatefulSet是有状态服务，部署的pod名是固定格式(StatefulSet名+编号)。我们在初始化Redis Cluster时，将相邻编号的pod设置为主从关系。在重启pod时，通过pod名确定它的slave，在重启pod前向从节点发送cluster failover命令，强制将活着的从节点切主。这样在重启后，该节点会自动以从节点方式加入集群。</p><p>LVS映射时延</p><p>Proxy的pod是通过LVS实现负载均衡的，LVS对后端IP:Port的映射生效有一定的时延，Proxy节点突然下线会导致部分连接丢失。为减少Proxy运维对业务造成影响，我们在Proxy的deployment模板中增加了如下选项：</p><pre><code>lifecycle:  preStop:    exec:      command:      - sleep      - &quot;171&quot;</code></pre><p>对于正常的Proxy pod下线，例如集群缩容、滚动更新Proxy版本以及其它K8s可控的pod下线，在pod下线前会发消息给LVS并等待171秒，这段时间足够LVS将这个pod的流量逐渐切到其他pod上，对业务无感知。</p><h2 id="K8s-StatefulSet无法满足Redis-Cluster部署要求"><a href="#K8s-StatefulSet无法满足Redis-Cluster部署要求" class="headerlink" title="K8s StatefulSet无法满足Redis Cluster部署要求"></a><strong>K8s StatefulSet无法满足Redis Cluster部署要求</strong></h2><p>K8s原生的StatefulSet不能完全满足Redis Cluster部署的要求：</p><ol><li><p>Redis Cluster不允许同为主备关系的节点部署在同一台机器上。这个很好理解，如果该机器宕机，会导致这个数据分片不可用。</p></li><li><p>Redis Cluster不允许集群超过一半的主节点失效，因为如果超过一半主节点失效，就无法有足够的节点投票来满足gossip协议的要求。因为Redis Cluster的主备是可能随时切换的，我们无法避免同一个机器上的所有节点都是主节点这种情况，所以在部署时不能允许集群中超过1/4的节点部署在同一台机器上。</p></li></ol><p>为了满足上面的要求，原生StatefulSet可以通过 anti-affinity 功能来保证相同集群在同一台机器上只部署一个节点，但是这样机器利用率很低。</p><p>因此我们开发了基于StatefulSet的CRD：RedisStatefulSet，会采用多种策略部署Redis节点。同时，还在RedisStatefulSet中加入了一些Redis管理功能。这些我们将会在其他文章中来继续详细探讨。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前集团内部已经有多个业务的数十个Redis集群部署到了K8s上并运行了半年多。得益于K8s的快速部署和故障迁移能力，这些集群的运维工作量比物理机上的Redis集群低很多，稳定性也得到了充分的验证。</p><p>在运维过程中我们也遇到了不少问题，文章中提到的很多功能都是根据实际需求提炼出来的。目前还是有很多问题需要在后续逐步解决，以进一步提高资源利用率和服务质量。</p><h2 id="混布-Vs-独立部署"><a href="#混布-Vs-独立部署" class="headerlink" title="混布 Vs. 独立部署"></a><strong>混布 Vs. 独立部署</strong></h2><p>物理机的Redis实例是独立部署的，单台物理机上部署的都是Redis实例，这样有利于管理，但是资源利用率并不高。Redis实例使用了CPU、内存和网络IO，但存储空间基本都是浪费的。在K8s上部署Redis实例，其所在的机器上可能也会部署其他任意类型的服务，这样虽然可以提高机器的利用率，但是对于Redis这样的可用性和时延要求都很高的服务来说，如果因为机器内存不足而被驱逐，是不能接受的。这就需要运维人员监控所有部署了Redis实例的机器内存，一旦内存不足，就切主和迁移节点，但这样又增加运维的工作量。</p><p>同时，如果混部的其他服务是网络吞吐很高的应用，也可能对Redis服务造成影响。虽然K8s的 anti-affinity 功能可以将Redis实例有选择地部署到没有这类应用的机器上，但是在机器资源紧张时，还是无法避免这种情况。</p><h2 id="Redis-Cluster管理"><a href="#Redis-Cluster管理" class="headerlink" title="Redis Cluster管理"></a><strong>Redis Cluster管理</strong></h2><p>Redis Cluster是一个P2P无中心节点的集群架构，依靠gossip协议传播协同自动化修复集群的状态，节点上下线和网络问题都可能导致Redis Cluster的部分节点状态出现问题，例如会在集群拓扑中出现failed或者handshake状态的节点，甚至脑裂。对这种异常状态，我们可以在Redis CRD上增加更多的功能来逐步解决，进一步提高运维效率。</p><h2 id="审计与安全"><a href="#审计与安全" class="headerlink" title="审计与安全"></a><strong>审计与安全</strong></h2><p>Redis本身只提供了Auth密码认证保护功能，没有权限管理，安全性较差。通过Proxy，我们可以通过密码区分客户端类型，管理员和普通用户使用不同的密码登录，可执行的操作权限也不同，这样就可以实现权限管理和操作审计等功能。</p><h2 id="支持多Redis-Cluster"><a href="#支持多Redis-Cluster" class="headerlink" title="支持多Redis Cluster"></a><strong>支持多Redis Cluster</strong></h2><p>单个Redis Cluster由于gossip协议的限制，横向扩展能力有限，集群规模在300个节点时，节点选主这类拓扑变更的效率就明显降低。同时，由于单个Redis实例的容量不宜过高，单个Redis Cluster也很难支持TB以上的数据规模。通过Proxy，我们可以对key做逻辑分片，这样单个Proxy就可以接入多个Redis Cluster，从客户端的视角来看，就相当于接入了一个能够支持更大数据规模的Redis集群。</p><p>最后，像Redis这种有状态服务的容器化部署在国内大厂都还没有非常成熟的经验，小米云平台也是在摸索中逐步完善。目前我们新增集群已经大部分部署在K8s上，更计划在一到两年内将集团内大部分的物理机Redis集群都迁移到K8s上。这样就可以有效地降低运维人员的负担，在不显著增加运维人员的同时维护更多的Redis集群。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;小米Redis的K8s容器化部署实践&lt;/p&gt;
&lt;p&gt;原创 崔凯峰 小米云技术 &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/WrUU3C-C8TBgJfGuOv3qGQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;
      
    
    </summary>
    
      <category term="redis" scheme="http://zhang-yu.me/categories/redis/"/>
    
    
      <category term="redis" scheme="http://zhang-yu.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>k8s桌面客户端Lens</title>
    <link href="http://zhang-yu.me/2020/06/18/K8s%E6%A1%8C%E9%9D%A2%E5%AE%A2%E6%88%B7%E7%AB%AFLens/"/>
    <id>http://zhang-yu.me/2020/06/18/K8s桌面客户端Lens/</id>
    <published>2020-06-18T03:00:00.000Z</published>
    <updated>2020-08-12T08:49:23.514Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 的桌面客户端有那么几个，  Kubernetic  (<a href="https://kubernetic.com/)应该是最好用的，但是收费。" target="_blank" rel="noopener">https://kubernetic.com/)应该是最好用的，但是收费。</a></p><p>最近有个叫 Lens 的 APP 改变了这个格局，功能比 Kubernetic 多，使用体验更好，适合广大系统重启工程师装逼。<br><a href="https://github.com/lensapp/lens" target="_blank" rel="noopener">https://github.com/lensapp/lens</a></p><p><a href="https://k8slens.dev/" target="_blank" rel="noopener">https://k8slens.dev/</a></p><p>Lens 就是一个强大的 IDE，可以实时查看集群状态，实时查看日志流，方便排查故障。<br>日志流界面可以选择显示或隐藏时间戳，也可以指定显示的行数</p><p>Lens 可以管理多集群，它使用内置的 kubectl 通过 kubeconfig  来访问集群，支持本地集群和外部集群（如EKS、AKS、GKE、Pharos、UCP、Rancher 等）</p><p>Lens 内置了资源利用率的仪表板，支持多种对接 Prometheus 的方式：</p><p>Lens 内置了 kubectl，它的内置终端会确保集群的 API Server 版本与 kubectl 版本兼容，所以你不需要在本地安装 kubectl。 </p><p>Lens 内置了 helm 模板商店，可直接点击安装：</p><p>下载安装<br><a href="https://github.com/lensapp/lens/releases/download/v3.5.0/Lens-Setup-3.5.0.exe" target="_blank" rel="noopener">https://github.com/lensapp/lens/releases/download/v3.5.0/Lens-Setup-3.5.0.exe</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Kubernetes 的桌面客户端有那么几个，  Kubernetic  (&lt;a href=&quot;https://kubernetic.com/)应该是最好用的，但是收费。&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://kubernetic.c
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>java线程池里面到底该设置多少个线程</title>
    <link href="http://zhang-yu.me/2020/06/16/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%87%8C%E9%9D%A2%E5%88%B0%E5%BA%95%E8%AF%A5%E8%AE%BE%E7%BD%AE%E5%A4%9A%E5%B0%91%E4%B8%AA%E7%BA%BF%E7%A8%8B/"/>
    <id>http://zhang-yu.me/2020/06/16/Java线程池里面到底该设置多少个线程/</id>
    <published>2020-06-16T03:00:00.000Z</published>
    <updated>2020-06-18T14:07:24.226Z</updated>
    
    <content type="html"><![CDATA[<p>根据CPU核心数确定线程池并发线程数</p><p><a href="https://www.cnblogs.com/dennyzhangdd/p/6909771.html" target="_blank" rel="noopener">https://www.cnblogs.com/dennyzhangdd/p/6909771.html</a></p><blockquote><h2 id="一、抛出问题"><a href="#一、抛出问题" class="headerlink" title="一、抛出问题"></a>一、抛出问题</h2><p>关于如何计算并发线程数，一般分两派，来自两本书，且都是好书，到底哪个是对的？问题追踪后，整理如下：</p><p>第一派：《Java Concurrency in Practice》即《java并发编程实践》，如下图：</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526162253247-2075463115.png" alt></p><p>如上图，在《Java Concurrency in Practice》一书中，给出了估算线程池大小的公式：</p><p>Nthreads=Ncpu*Ucpu*(1+w/c)，其中</p><p>Ncpu=CPU核心数</p><p>Ucpu=cpu使用率，0~1</p><p>W/C=等待时间与计算时间的比率</p><p>第二派：《Programming Concurrency on the JVM Mastering》即《Java 虚拟机并发编程》</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526170508450-925520860.png" alt></p><p>线程数=Ncpu/（1-阻塞系数）</p></blockquote><blockquote><h2 id="二、分析"><a href="#二、分析" class="headerlink" title="二、分析"></a>二、分析</h2><p>对于派系一，假设cpu100%运转，即撇开CPU使用率这个因素，线程数\=Ncpu*(1+w/c)。</p><p>现在假设将派系二的公式等于派系一公式，即Ncpu/（1-阻塞系数）=Ncpu*(1+w/c),===》阻塞系数=w/(w+c)，即阻塞系数=阻塞时间/（阻塞时间+计算时间），这个结论在派系二后续中得到应征，如下图：</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526171225919-888895376.png" alt></p><p>由此可见，派系一和派系二其实是一个公式……这样我就放心了……</p></blockquote><blockquote><h2 id="三、实际应用"><a href="#三、实际应用" class="headerlink" title="三、实际应用"></a>三、实际应用</h2><p>那么实际使用中并发线程数如何设置呢？分析如下（我们以派系一公式为例）：</p><p>Nthreads=Ncpu*(1+w/c)</p><p>IO密集型：一般情况下，如果存在IO，那么肯定w/c&gt;1（阻塞耗时一般都是计算耗时的很多倍）,但是需要考虑系统内存有限（每开启一个线程都需要内存空间），这里需要上服务器测试具体多少个线程数适合（CPU占比、线程数、总耗时、内存消耗）。如果不想去测试，保守点取1即，Nthreads=Ncpu*(1+1)=2Ncpu。这样设置一般都OK。</p><p>计算密集型：假设没有等待w=0，则W/C=0. Nthreads=Ncpu。</p><p><strong>至此结论就是：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; </span><br><span class="line">&gt; ```计算密集型\=Ncpu（常出现于线程中：复杂算法）</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>java中：Ncpu=<code>Runtime.getRuntime().availableProcessors()</code></strong></p><p>=========================此处可略过=============================================</p><p>当然派系一种《Java Concurrency in Practice》还有一种说法，</p><p><img src="https://images2015.cnblogs.com/blog/584866/201705/584866-20170526173437372-976370152.png" alt></p><p>即对于计算密集型的任务，在拥有N个处理器的系统上，当线程池的大小为N+1时，通常能实现最优的效率。(即使当计算密集型的线程偶尔由于缺失故障或者其他原因而暂停时，这个额外的线程也能确保CPU的时钟周期不会被浪费。)</p><p><code>即，计算密集型\=Ncpu+1，但是这种做法导致的多一个cpu上下文切换是否值得</code>，这里不考虑。读者可自己考量。  </p><p>======================================================================</p></blockquote><blockquote><h2 id="四、总结："><a href="#四、总结：" class="headerlink" title="四、总结："></a><strong>四、总结</strong>：</h2><p>选择线程池并发线程数的因素很多：任务类型、内存等线程中使用到所有资源都需要考虑</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;根据CPU核心数确定线程池并发线程数&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/dennyzhangdd/p/6909771.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblog
      
    
    </summary>
    
      <category term="java" scheme="http://zhang-yu.me/categories/java/"/>
    
    
      <category term="java" scheme="http://zhang-yu.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>做容灾，冷备是不是个好方案</title>
    <link href="http://zhang-yu.me/2020/05/06/%E5%81%9A%E5%AE%B9%E7%81%BE%EF%BC%8C%E5%86%B7%E5%A4%87%E6%98%AF%E4%B8%8D%E6%98%AF%E4%B8%AA%E5%A5%BD%E6%96%B9%E6%A1%88/"/>
    <id>http://zhang-yu.me/2020/05/06/做容灾，冷备是不是个好方案/</id>
    <published>2020-05-06T06:00:00.000Z</published>
    <updated>2020-05-06T06:05:22.279Z</updated>
    
    <content type="html"><![CDATA[<p> 做容灾，冷备是不是个好方案？ </p><p> 原创 Cheng哥 成哥的世界 2018-09-30 </p><blockquote><p>主备、冷备、热备、双活、多活、同城、异地、多云，等等等等，这些保证业务高可用和容灾名词，我们经常会听到，不绝于耳。  </p><p>但是，真的当我们自己要去建设，选择方案时，就发现不知道该怎么选择和搭配了。</p><p>结合近期我们的一些讨论，准备用几篇文章简单分享下我们的理解，今天先聊冷备。</p><p><strong>冷备是不是个好方案？</strong></p><p>这里的<strong>冷备我们可以理解为，是主站系统核心链路的镜像站点</strong>，应用、各类分布式服务以及底层基础设施都是独立，且启动的。</p><p><strong>它跟主站唯一的差别就是，正常情况下，不承载任何线上流量。</strong></p><p>理论上，只要有状态的数据（也就是各类分布式服务，如数据库、缓存、消息等组件）同步好，接入层流量能够灵活调度，当出现问题的时候，切入口流量，就可以顺畅的切过去。</p><p>看上去很美好，但是<strong>实际操作起来，基本不可行</strong>。</p><p>这里有<strong>一个关键点，就是业务应用</strong>，应用的代码和配置是<strong>随时在变化的。</strong></p><p>原则上，我们可以通过持续交付和运维自动化等等手段，确保每次变更都能够同步到备站点，并通过流程约束不允许有外部操作。</p><p>所以，手段上，我们可以做到非常完备，流程上，我们可以设计的非常严密。</p><p>但是，我们始终绕不开的一个命题，<strong>只要不承载真实的线上业务流量，我们就无法证明这个系统是可用的。</strong></p><p>何况，有可能是好几个月我们都不会发生真实的切换动作，所以，一个几个月没有经过线上流量检验的系统，在真正需要切换时，不会有任何人敢决策直接切换的。</p><p>当然，以上是我们的直接推断，确实行不通。但是我们仍然要经过一些详细的论证，从其它角度看是否有解。</p><p><strong>从另外一个角度的论证过程</strong></p><p>当时我们讨论在冷备的前提下，应该怎么保证系统的可用性，没想到，论证的过程，<strong>反而进一步证实了冷备只是一个美好的愿望</strong>。</p><p><strong>1、通过模拟压测的方式。</strong></p><p>但是我们知道，压测的模型是根据线上业务模型来定制的，但是业务场景和逻辑每天都在发生变化，压测模型的同步有时是跟不上业务模型变化的。</p><p>况且这个日常工作量要靠人，无法做到自动生成，所以基本不可持续。</p><p>再就是，<strong>压测的结果检验是通过技术指标衡量**</strong>，而非业务指标，**也就是是否200ok，或者出现5xx之类的错误。</p><p>业务逻辑上是否正确，并没有办法确保。这种情况就极易造成数据污染。数据故障的影响范围远远超过服务不可用的影响。</p><p>所以，<strong>压测可以最大程度评估系统容量，但是无法保证系统业务正确性。</strong></p><p><strong>2、切换后，接入线上流量前，QA介入验证。</strong></p><p>理由同上，工作量大，也无法覆盖到所有场景，时间不可控，完全起不到冷备节点的快速承载业务效果。</p><p><strong>3、定期模拟演练，确保系统周期范围内可用</strong></p><p>但是这里就有一个前提，冷备站点的建设目标，并不是全量建设，而是在极端状况下，确保核心业务临时可用，当主站点恢复后，仍然要切回去。</p><p>这里暗含的一个意思就是，一旦需要做这个动作，业务必然有损，而且涉及范围非常大，这就意味着，每一次演练都要付出极大的业务代价。</p><p>从这个角度，产品运营及决策者们是不会允许你经常干这种事情的。</p><p>到这里，你会发现，连日常演练的条件都不具备了。</p><p><strong>4、一个绕不开的限制条件</strong></p><p>数据同步必然是单向的，为了保证数据一致，通常要确保备用站点是禁写的，以防止各类误操作引起的数据污染。</p><p>所以，即使上面几个方案可行，基础条件上又不满足，因为根本无法写入数据，关键的业务逻辑根本不具备验证条件。</p><p><strong>最后，结论</strong></p><p><strong>冷备只能是冷备</strong>，关键时刻并不能起到快速承载业务的效果，<strong>在业务容灾建设时，这个思路其实是不可行的。</strong></p><p>但是对于部分组件，比如数据库、大数据、文件，这些存储类的部件，做冷备是有重大意义的。</p><p>也就是，后面我们在提到冷备时，应该叫做数据冷备、文件冷备、源代码冷备才有意义，或许会更准确些。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 做容灾，冷备是不是个好方案？ &lt;/p&gt;
&lt;p&gt; 原创 Cheng哥 成哥的世界 2018-09-30 &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;主备、冷备、热备、双活、多活、同城、异地、多云，等等等等，这些保证业务高可用和容灾名词，我们经常会听到，不绝于耳。  &lt;/p&gt;

      
    
    </summary>
    
      <category term="容灾" scheme="http://zhang-yu.me/categories/%E5%AE%B9%E7%81%BE/"/>
    
    
      <category term="容灾" scheme="http://zhang-yu.me/tags/%E5%AE%B9%E7%81%BE/"/>
    
  </entry>
  
  <entry>
    <title>做容灾，双活、多活、同城、异地、多云，到底应该怎么选</title>
    <link href="http://zhang-yu.me/2020/05/06/%E5%81%9A%E5%AE%B9%E7%81%BE%EF%BC%8C%E5%8F%8C%E6%B4%BB%E3%80%81%E5%A4%9A%E6%B4%BB%E3%80%81%E5%90%8C%E5%9F%8E%E3%80%81%E5%BC%82%E5%9C%B0%E3%80%81%E5%A4%9A%E4%BA%91%EF%BC%8C%E5%88%B0%E5%BA%95%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E9%80%89/"/>
    <id>http://zhang-yu.me/2020/05/06/做容灾，双活、多活、同城、异地、多云，到底应该怎么选/</id>
    <published>2020-05-06T06:00:00.000Z</published>
    <updated>2020-05-06T06:05:38.644Z</updated>
    
    <content type="html"><![CDATA[<p> 做容灾，双活、多活、同城、异地、多云，到底应该怎么选？<br><a href="https://mp.weixin.qq.com/s/_NGqq-xvDtFrvKwbdGIhwQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/_NGqq-xvDtFrvKwbdGIhwQ</a> </p><p> 原创 Cheng哥 成哥的世界 2019-03-11 </p><blockquote><p>去年写过一篇 &lt;&lt;做容灾，冷备是不是个好方案？&gt;&gt;，当时提出来，冷备或者主备，其实并不是一个理想的方案，而且绝大多数情况下，只能是一个心理安慰，真正发生故障的情况下，这样的容灾模式根本起不到作用。  </p><p>原因我就不重复了，大家如果有兴趣可以直接看那篇文章。</p><p>最近，公有云又出了些大故障，各大群和朋友圈又开始沸沸扬扬，但是整体看下来，声音无非两种：</p><ul><li><p>单站点不靠谱，要有容灾，出现这种情况就得马上切，所以回去赶紧建设容灾站点；</p></li><li><p>鸡蛋不能放在一个篮子里，单云不靠谱，要多云。所以，多云就要选我们家的xx云，或者我们提供xx多云服务。</p></li></ul><p>我在我的一个讨论群里就提出来，第一种声音是有意识的建设，有这个意识很好，但是把这个事情想得太简单了。第二种声音，基本就是不动脑子的瞎BB，原因我下面讲。</p><p>转回正题来，既然上篇提到主备模式不靠谱，那到底怎么选？而且整天见各类技术文章，不是双活，就是多活，不是同城，就是异地，现在又出来个多云，好复杂。</p><p>下面我就谈谈我的理解：</p><p>首先，这么多名词是什么含义，要搞清楚，然后再看适不适合。</p><p>先讲相对简单的双活（简不简单，看后面就明白了），其实就是两个站点，同时承载业务流量，可以根据用户ID、地域或者其他业务属性也决定怎么分担流量，当一个站点故障时，可以快速（分钟级）切换到另一个站点，理想情况下，对业务基本是无损或者非常小的。</p><p>这里就跟前面讲的主备不同了，主备的另一个站点完全是不承载任何流量的。</p><p>这里再往深里看一眼，同时承载流量，也要看承载到那一层，也就是流量在统一站点内闭环，所有调用都是本机房内完成，还是只有应用层这样的无状态组件双活，但是数据访问、异步消息这些有状态的部件还是回到主站点调用，这两种模式又是不一样的。</p><p>其实第二种，就比前面讲的主备模式要好一些，因为这样至少可以保证应用层随时可用，不过真出故障的时候，还是少不了数据层的切换，这个其实是非常耗时的。跟主备模式一样，基本无法演练，因为代价太高，数据会有损。（如果数据层没有这么复杂，只有几个数据库，那是没问题问题的，但是分布式的场景下，上百个，几百个实例切换，这个代价和成本还是很大的。）</p><p>所以，再往下推导，如果想要做到有效果的双活，就必须保证每个站点，都是独立运行，所有的调用都是本机房调用且闭环，底层做好数据同步即可。</p><p>只有做到这个程度，当一个站点发生故障不可用时，就可以从接入层把故障站点的流量切换到另一个站点，双活的效果也就有了。</p><p>不过，做到这个程度，就不是说我们想要做就能做到的，如果您做个类似的架构设计，你会知道这里有三个关键的技术点：</p><p><strong>第一个，本机房调用</strong></p><p>也就是一个分布式请求不能跨机房调来调去，这个是不行的，必须要保证本机房调用闭环。所以从分布式服务的路由策略上，以及服务化框架上，必须得支持这也中调用模式，同理，数据访问层，以及消息组件也要支持这种特性。</p><p><strong>第二个，数据分片和一致性</strong></p><p>为什么要做这个事情？我们知道一个系统中数据准确性、完整性和一致性是非常关键的，放到双活这个场景下，最关键的就是数据一致性，我们不能允许有同一个记录两边同时在变更，还要双向同步，比如用户交易和支付类的数据，同时变更的情况下，我们无法确认哪边是准确的。</p><p>前面提到，两个站点是同时承载不同的流量的，这就要根据一些业务属性来分配，比如用户ID、所属地域等等策略，这里为的就是能够在数据层面也要做好隔离，一个站点内只提供固定部分的用户访问。</p><p>这样就保证了单站点内同一分片的数据，不会在另外一个站点被变更，后续的同步也可以做到单向。</p><p>所以，这里的关键，就是数据要做分片，就要用到分布式的数据中间件，要做数据访问的路由设计，数据要同机房读写，还要做数据拆分这样的工作，技术门槛和工作量也不低。</p><p><strong>这两点如果能够做到，其实就是我们经常说的“单元化”架构达成了，理论上，我们可以选择任何一个机房和地域，把系统搭建起来，就可以提供业务访问了。</strong></p><p>但现实是更为复杂的，因为用户业务系统产生的数据，有可能会被其它系统用到，比如商品库存这样的系统，这就要涉及异步消息和数据的同步问题，而<strong>数据同步不仅仅是一个技术问题，而是个物理问题</strong>，我们接下来讲。</p><p><strong>第三个，数据同步。</strong></p><p>其实单从同步角度而言，目前很多的同步工具和开源产品已经比较完善，所以这里最大的问题，其实不在技术层面，而是在物理层面。</p><p>准确点，就是物理距离上的时延问题，这个无论是双活、多活，还是同城、异地，都绕不开的痛苦问题。</p><p>既然要双活，必然会选择另一个跟当前机房有一定距离的机房（同城或异地），而且距离必须得拉开才有意义，如果都在一个园区里面，就没有任何容灾意义了。</p><p>距离一旦拉开，物理距离就出来了，即使是专线相连，中间也要经过很多网络设备，如果是云化的网络架构下，经过的软硬设备就更多，还有可能涉及协议转换，如果中途跨运营商，就更难保障，这样一来时延肯定是几倍、十几倍，甚至是上百倍的上涨，直接从0.x毫秒，上涨到秒级别。</p><p>对于同城来说，这个问题还好，但是一旦跨省就完全不可控，特别是机房如果不是自己的，根本无法控制。所以，想大公司自建机房，一定会在这个层面做大量的优化，尽最大可能降低时延。</p><p>就以淘宝、天猫为例，按照之前了解的情况，基本也是杭州和上海这两个城市为主做双活，再远时延这个问题就绕不开了。</p><p>数据同步及时性为什么这么重要，一个是业务体验，不能说库存都没了，其他用户看到的还是有货，这个是不会被接受的。</p><p>再就是故障时，如果同步不及时，极有可能造成几秒钟内的交易数据丢失，或者不一致，像淘宝这样每秒4位数订单量的系统，丢几秒钟数据，造成的损失也是巨大的。所以，这里就必须要建设有一整套的数据完整性和一致性保障措施，尽最大程度降低业务损失。  </p><p>所以，数<strong>据同步所依赖的时延问题，其实就已经超出了绝大部分公司所能掌控的范畴，也不是单纯靠自身技术能解决的问题，要看天时和地利。</strong></p><p>讲到这里，我想多活就不用讲了，时延这个问题解决不了，多活就是扯淡，至于同城和异地，我想看明白的读者，也知道怎么选择了，其实一样，还是取决于时延。</p><p><strong>我们可以得出的几个结论：</strong></p><ul><li><p>不管怎么选择容灾方案，我们自己的业务系统，从<strong>自身架构上，一定要支持单元化，一定要支持数据同步才行</strong>，如果这都不支持，讲双活和多活，就是特么的扯淡。所以，打算搞双活，先从这里下手，当然牵出来就要涉及到分布式，还有很多大量细节技术问题。</p></li><li><p><strong>一个合理的建设节奏应该是，同城双活—异地双活—两地三中心（同城双活+异地多活）</strong>，因为你要解决的问题的复杂度和难度也是在逐步上升的，不可能一蹴而就。</p></li><li><p><strong>题目里这些个名词，不是孤立的</strong>，而是从不同维度看到的结论，但是如果你偏离自己的业务场景去看，孤立的去看，就一定会被带到沟里去，而且不知道该如何下手，所以，一定别偏离你的业务场景，然后把它们联系起来。</p></li><li><p><strong>一切都是ROI</strong>，为了保证高可用，就一定会有成本，高可用程度越高，成本就一定越高，所以成本投入得到的收益到底划不划算，这个只能自家公司自家评判。</p></li><li><p>现实情况，比我写的要复杂的多的多，推荐大家看两个成功案例，<strong>一个是毕玄的异地多活数据中心，一个是饿了么异地多活</strong>，几个关键字google一下就有了，里面涉及到的场景化的细节对大家理解这件事情的复杂度会有更帮助。</p></li></ul><p>写的有点多了，关于多云先不写了，就当问题吧，大家觉得是不是需要多云建设？你怎么看？</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 做容灾，双活、多活、同城、异地、多云，到底应该怎么选？&lt;br&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/_NGqq-xvDtFrvKwbdGIhwQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mp.wei
      
    
    </summary>
    
      <category term="容灾" scheme="http://zhang-yu.me/categories/%E5%AE%B9%E7%81%BE/"/>
    
    
      <category term="容灾" scheme="http://zhang-yu.me/tags/%E5%AE%B9%E7%81%BE/"/>
    
  </entry>
  
  <entry>
    <title>MySQL如何优化cpu消耗</title>
    <link href="http://zhang-yu.me/2020/04/20/MySQL%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96cpu%E6%B6%88%E8%80%97/"/>
    <id>http://zhang-yu.me/2020/04/20/MySQL如何优化cpu消耗/</id>
    <published>2020-04-20T03:00:00.000Z</published>
    <updated>2020-04-20T03:14:15.521Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="谁在消耗cpu"><a href="#谁在消耗cpu" class="headerlink" title="谁在消耗cpu?"></a>谁在消耗cpu?</h2><p><em><strong>用户+系统+IO等待+软硬中断+空闲</strong></em><br><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220055993-65442699.png" alt></p><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220154673-599033146.png" alt></p><h2 id="祸首是谁？"><a href="#祸首是谁？" class="headerlink" title="祸首是谁？"></a>祸首是谁？</h2><h3 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h3><p><em><strong>用户空间CPU消耗，各种逻辑运算</strong></em></p><blockquote><p>正在进行大量tps<br>函数/排序/类型转化/逻辑IO访问…</p></blockquote><p><em><strong>用户空间消耗大量cpu，产生的系统调用是什么？那些函数使用了cpu周期？</strong></em><br>参考<a href="https://www.cnblogs.com/YangJiaXin/p/10928160.html" target="_blank" rel="noopener"><br>Linux 性能优化解析</a><br><a href="https://www.cnblogs.com/YangJiaXin/p/10853560.html" target="_blank" rel="noopener">MySQL 几种调式分析利器</a></p><h3 id="IO等待"><a href="#IO等待" class="headerlink" title="IO等待"></a>IO等待</h3><p><em><strong>等待IO请求的完成</strong></em></p><blockquote><p>此时CPU实际上空闲</p></blockquote><p><em><strong>如vmstat中的wa 很高。但IO等待增加，wa也不一定会上升（请求I/O后等待响应，但进程从核上移开了）</strong></em><br><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220232193-1150896123.png" alt></p><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220252309-1933130433.png" alt></p><h3 id="产生影响"><a href="#产生影响" class="headerlink" title="产生影响"></a>产生影响</h3><blockquote><p>用户和IO等待消耗了大部分cpu</p></blockquote><p><em><strong>吞吐量下降（tps）</strong></em><br><em><strong>查询响应时间增加</strong></em><br><em><strong>慢查询数增加</strong></em><br><em><strong>对mysql的并发陡增，也会产生上述影响</strong></em></p><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220350730-466848858.png" alt></p><h2 id="如何减少CPU消耗？"><a href="#如何减少CPU消耗？" class="headerlink" title="如何减少CPU消耗？"></a>如何减少CPU消耗？</h2><h3 id="减少等待"><a href="#减少等待" class="headerlink" title="减少等待"></a>减少等待</h3><p><em><strong>减少IO量</strong></em></p><blockquote><p>SQL/index，使用合适的索引减少扫描的行数（需平衡索引的正收益和维护开销，空间换时间）</p></blockquote><p><em><strong>提升IO处理能力</strong></em></p><blockquote><p>加cache/加磁盘/SSD</p></blockquote><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220402693-1264789500.png" alt></p><h3 id="减少计算"><a href="#减少计算" class="headerlink" title="减少计算"></a>减少计算</h3><h4 id="减少逻辑运算量"><a href="#减少逻辑运算量" class="headerlink" title="减少逻辑运算量"></a>减少逻辑运算量</h4><blockquote><ul><li><em><strong>避免使用函数</strong></em>，将运算转移至易扩展的应用服务器中<br>如substr等字符运算，dateadd/datesub等日期运算，abs等数学函数</li><li><em><strong>减少排序</strong></em>，利用索引取得有序数据或避免不必要排序<br>如union all代替 union，order by 索引字段等</li><li><em><strong>禁止类型转换</strong></em>，使用合适类型并保证传入参数类型与数据库字段类型绝对一致<br>如数字用tiny/int/bigint等，必需转换的在传入数据库之前在应用中转好</li><li><em><strong>简单类型</strong></em>，尽量避免复杂类型，降低由于复杂类型带来的附加运算。更小的数据类型占用更少的磁盘、内存、cpu缓存和cpu周期</li><li>….</li></ul></blockquote><h4 id="减少逻辑IO量"><a href="#减少逻辑IO量" class="headerlink" title="减少逻辑IO量"></a>减少逻辑IO量</h4><blockquote><ul><li><p><em><strong>index</strong></em>，优化索引，减少不必要的表扫描<br>如增加索引，调整组合索引字段顺序，去除选择性很差的索引字段等等</p></li><li><p><em><strong>table</strong></em>，合理拆分，适度冗余<br>如将很少使用的大字段拆分到独立表，非常频繁的小字段冗余到“引用表”</p></li><li><p><em><strong>SQL</strong></em>，调整SQL写法，充分利用现有索引，避免不必要的扫描，排序及其他操作<br>如减少复杂join，减少order by，尽量union all，避免子查询等</p></li><li><p><em><strong>数据类型</strong></em>，够用就好，减少不必要使用大字段<br>如tinyint够用就别总是int，int够用也别老bigint，date够用也别总是timestamp</p></li><li><p><em><strong>….</strong></em></p></li></ul></blockquote><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220428944-1005198827.png" alt></p><h4 id="减少query请求量（非数据库本身）"><a href="#减少query请求量（非数据库本身）" class="headerlink" title="减少query请求量（非数据库本身）"></a>减少query请求量（非数据库本身）</h4><blockquote><ul><li><em><strong>适当缓存</strong></em>，降低缓存数据粒度，对静态并被频繁请求的数据进行适当的缓存<br>如用户信息，商品信息等</li><li><em><strong>优化实现</strong></em>，尽量去除不必要的重复请求<br>如禁止同一页面多次重复请求相同数据的问题，通过跨页面参数传递减少访问等</li><li><em><strong>合理需求</strong></em>，评估需求产出比，对产出比极端底下的需求合理去除</li><li><em><strong>….</strong></em></li></ul></blockquote><p><img src="https://img2018.cnblogs.com/blog/1179590/201905/1179590-20190527220447492-64921986.png" alt></p><h3 id="升级cpu"><a href="#升级cpu" class="headerlink" title="升级cpu"></a>升级cpu</h3><ul><li><em><strong>若经过减少计算和减少等待后还不能满足需求，cpu利用率还高</strong></em></li><li>___是时候拿出最后的杀手锏了，<em>升级cpu</em>，是选择更快的cpu还是更多的cpu了？_**</li></ul><blockquote><ul><li><em>低延迟（快速响应）</em>，需要更快的cpu（每个查询只能使用一个cpu）</li><li><em>高吞吐</em>，同时运行很多查询语句，能从多个cpu处理查询中收益</li></ul></blockquote><p><em><strong>参考</strong></em><br><em><strong>《高性能MySQL》</strong></em><br><em><strong>《图解性能优化》</strong></em><br><em><strong>大部分整理自《MySQL Tuning For CPU Bottleneck》</strong></em></p><p>作者：jiaxin</p><p>出处：<a href="http://www.cnblogs.com/YangJiaXin/" target="_blank" rel="noopener">http://www.cnblogs.com/YangJiaXin/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;谁在消耗cpu&quot;&gt;&lt;a href=&quot;#谁在消耗cpu&quot; class=&quot;headerlink&quot; title=&quot;谁在消耗cpu?&quot;&gt;&lt;/a&gt;谁在消耗cpu?&lt;/h2&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;用户+系统+IO等待+软硬中断+空闲&lt;/str
      
    
    </summary>
    
      <category term="mysql" scheme="http://zhang-yu.me/categories/mysql/"/>
    
    
      <category term="mysql" scheme="http://zhang-yu.me/tags/mysql/"/>
    
  </entry>
  
</feed>
