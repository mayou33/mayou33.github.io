<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大雨哥</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhang-yu.me/"/>
  <updated>2020-11-18T06:09:27.720Z</updated>
  <id>http://zhang-yu.me/</id>
  
  <author>
    <name>大雨哥</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>nginx知识图谱</title>
    <link href="http://zhang-yu.me/2020/11/18/nginx%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    <id>http://zhang-yu.me/2020/11/18/nginx知识图谱/</id>
    <published>2020-11-17T16:00:00.000Z</published>
    <updated>2020-11-18T06:09:27.720Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20190116152033544.jpg" referrerpolicy="no-referrer" width="100%" height="100%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20190116152033544.jpg&quot; referrerpolicy=&quot;no-referrer&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="nginx" scheme="http://zhang-yu.me/categories/nginx/"/>
    
    
      <category term="nginx" scheme="http://zhang-yu.me/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>图解Kubernetes-Service</title>
    <link href="http://zhang-yu.me/2020/11/12/%E5%9B%BE%E8%A7%A3Kubernetes-Service/"/>
    <id>http://zhang-yu.me/2020/11/12/图解Kubernetes-Service/</id>
    <published>2020-11-12T03:00:00.000Z</published>
    <updated>2020-11-12T15:16:37.644Z</updated>
    
    <content type="html"><![CDATA[<p>图解Kubernetes Service</p><p><a href="https://blog.csdn.net/weixin_44692256/article/details/109640503" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44692256/article/details/109640503</a></p><blockquote><p>在 Kubernetes 中 Service 主要有4种不同的类型，其中的 ClusterIP 是最基础的，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/dd145643860273527e7955a4aab3c0df.png" alt></p><p>当我们创建一个 NodePort 的 Service 时，它也会创建一个 ClusterIP，而如果你创建一个 LoadBalancer，它就会创建一个 NodePort，然后创建一个 ClusterIP</p><p>此外我们还需要明白 Service 是指向 pods 的，Service 不是直接指向 Deployments 或 ReplicaSets，而是直接使用 labels 标签指向 Pod，这种方式就提供了极大的灵活性，因为通过什么方式创建的 Pod 其实并不重要。接下来我们通过一个简单的例子开始，我们用不同的 Service 类型来逐步扩展，看看这些 Service 是如何建立的。</p><h2 id="No-Services"><a href="#No-Services" class="headerlink" title="No Services"></a>No Services</h2><p>最开始我们没有任何的 Services。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/8aeb928fe5a3680a7402f94a9db6e503.png" alt></p><p>我们有两个节点，一个 Pod，节点有外网（4.4.4.1、4.4.4.2）和内网（1.1.1.1、1.1.1.2）的 IP 地址，pod-python 这个 Pod 只有一个内部的 IP 地址。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/35000ad9cc7d48a3903be87fad16c551.png" alt></p><p>现在我们添加第二个名为 pod-nginx 的 Pod，它被调度在 node-1 节点上。在 Kubernetes 中，所有的 Pod 之间都可以通过 Pod 的 IP 进行通信，不管它们运行在哪个节点上。这意味着 pod-nginx 可以使用其内部IP 1.1.1.3 来 ping 和连接 pod-python 这个 Pod。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/357d63461afd5b6f57848b56d5a22ba7.png" alt></p><p>现在如果 pod-python 挂掉了重新创建了一个新的 pod-python 出来（本文不涉及如何管理和控制 pods），重新分配了一个新的 1.1.1.5 的 Pod IP 地址，这个时候 pod-nginx 就无法再达到 1.1.1.3 这个之前的地址了，为了防止这种情况发生，我们就需要创建一个 Service 服务了！</p><h2 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/a80d96fbe7518ef4902968bce1d63c13.png" alt></p><p>和上面同样的场景，但是我们创建了一个名为 service-python 类型为 ClusterIP 的 Service 服务，一个 Service 并不像 Pod 那样运行在一个特定的节点上，这里我们可以假设一个 Service 只是在整个集群内部的内存中可用就可以了。</p><p>pod-nginx 可以安全地连接到 1.1.10.1 这个 ClusterIP 或直接通过 dns 名service-python 进行通信，并被重定向到后面一个可用的 Pod 上去。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/d7a5a1079e97e6268ffc73490140149d.png" alt></p><p>现在我们来稍微扩展下这个示例，启动3个 python 实例，现在我们来显示所有 Pod 和 Service 内部 IP 地址的端口。</p><p>集群内部的所有 Pods 都可以通过 <code>http://1.1.10.1:3000</code> 或者 <code>http://service-python:3000</code> 来访问到后面的 python pods 的443端口。</p><p><strong>service-python</strong> 这个 Service  是随机或轮询的方式来转发请求的，这个就是 ClusterIP Service 的作用，它通过一个名称和一个 IP 让集群内部的 Pods 可用。</p><p>上图中的 service-python 这个 Service 可以用下面的 yaml 文件来创建：</p><pre><code>apiVersion: v1kind: Servicemetadata:  name: service-pythonspec:  ports:  - port: 3000    protocol: TCP    targetPort: 443  selector:    run: pod-python  type: ClusterIP</code></pre><p>创建后，可以用 <code>kubectl get svc</code> 命令来查看：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/3bb199a50498dea18ed0c34540f39a6b.png" alt></p><h2 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h2><p>现在我们想让 ClusterIP Service 可以从集群外部进行访问，为此我们需要把它转换成 NodePort 类型的 Service，在我们的例子中，我们只需要简单修改上面的 <strong>service-python</strong> 这个 Service 服务即可：</p><pre><code>apiVersion: v1kind: Servicemetadata:  name: service-pythonspec:  ports:  - port: 3000    protocol: TCP    targetPort: 443    nodePort: 30080  selector:    run: pod-python  type: NodePort</code></pre><p>更新完成后，如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/img_convert/f60cb9e8acbf624de5122882d1c7eaf0.png" alt></p><p>这意味着我们的内部的 <strong>service-python</strong> 这个 Service 现在也可以通过30080 端口从<strong>每个节点</strong>的内部和外部 IP 地址进行访问了。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/4a6a903eecdc3504c284d1a1da66b839.png" alt></p><p>集群内部的 Pod 也可以通过内网节点 IP 连接到 30080 端口。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/56a4e494d456e4222bdbd763edb83c62.png" alt></p><p>运行 <code>kubectl get svc</code> 命令来查看这个 NodePort 的 Service，可以看到同样有一个 ClusterIP，只是类型和额外的节点端口不同。在内部，NodePort 服务仍然像之前的 ClusterIP 服务一样。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/b14c53e92796f05d99637566c746ec5f.png" alt></p><h2 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h2><p>如果我们希望有一个单独的 IP 地址，将请求分配给所有的外部节点IP（比如使用 round robin），我们就可以使用 LoadBalancer 服务，所以它是建立在 NodePort 服务之上的。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/6318d888d54822364ad7b775cdd035f4.png" alt></p><p>一个 LoadBalancer 服务创建了一个 NodePort 服务，NodePort 服务创建了一个 ClusterIP 服务。我们也只需要将服务类型更改为 LoadBalancer 即可。</p><pre><code>apiVersion: v1kind: Servicemetadata:  name: service-pythonspec:  ports:  - port: 3000    protocol: TCP    targetPort: 443    nodePort: 30080  selector:    run: pod-python  type: LoadBalancer</code></pre><p>LoadBalancer 服务所做的就是创建一个 NodePort 服务，此外，它还会向托管 Kubernetes 集群的提供商发送一条消息，要求设置一个指向所有外部节点 IP 和特定 nodePort 端口的负载均衡器，当然前提条件是要提供商支持。</p><p>现在运行 <code>kubectl get svc</code> 可以看到新增了 external-IP 和 LoadBalancer 的类型。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/bdd8c8716b2e7923a8371f9241ae61c4.png" alt></p><p>LoadBalancer 服务仍然像和以前一样在节点内部和外部 IP 上打开 30080 端口。</p><h2 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a>ExternalName</h2><p>最后是 ExternalName 服务，这个服务和前面的几种类型的服务有点分离。它创建一个内部服务，其端点指向一个 DNS 名。</p><p>我们假设 pod-nginx 运行在 Kubernetes 集群中，但是 python api 服务在集群外部。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/6cf994490f034e1aa382ea45dbe75790.png" alt></p><p>这里 <strong>pod-nginx</strong> 这个 Pod 可以直接通过 <a href="http://remote.server.url.com" target="_blank" rel="noopener">http://remote.server.url.com</a> 连接到外部的 python api 服务上去，但是如果我们考虑到以后某个时间节点希望把这个 python api 服务集成到 Kubernetes 集群中去，还不希望去更改连接的地址，这个时候我们就可以创建一个 ExternalName 类型的 Service 服务了。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/bfeee36892871d388542bb34e4e9cb16.png" alt></p><p>对应的 YAML 资源清单文件如下所示：</p><pre><code>kind: ServiceapiVersion: v1metadata:  name: service-pythonspec:  ports:  - port: 3000    protocol: TCP    targetPort: 443  type: ExternalName  externalName: remote.server.url.com</code></pre><p>现在 <strong>pod-nginx</strong> 就可以很方便地通过 <code>http://service-python:3000</code> 进行通信了，就像使用 ClusterIP 服务一样，当我们决定将 python api 这个服务也迁移到我们 Kubernetes 集群中时，我们只需要将服务改为 ClusterIP 服务，并设置正确的标签即可，其他都不需要更改了。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/1637ac7b0a54e12fc9795d5f7ba70d17.png" alt></p><p>到这里我们就用几张图将 Kubernetes 中的 Service 解释得明明白白清清楚楚真真切切了~~~</p><blockquote><p>原文链接：<a href="https://medium.com/swlh/kubernetes-services-simply-visually-explained-2d84e58d70e5" target="_blank" rel="noopener">https://medium.com/swlh/kubernetes-services-simply-visually-explained-2d84e58d70e5</a></p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;图解Kubernetes Service&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_44692256/article/details/109640503&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>图解Kubernetes-Ingress</title>
    <link href="http://zhang-yu.me/2020/11/12/%E5%9B%BE%E8%A7%A3Kubernetes-Ingress/"/>
    <id>http://zhang-yu.me/2020/11/12/图解Kubernetes-Ingress/</id>
    <published>2020-11-12T03:00:00.000Z</published>
    <updated>2020-11-12T15:56:25.041Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://new.qq.com/omn/20201109/20201109A065L600.html" target="_blank" rel="noopener">https://new.qq.com/omn/20201109/20201109A065L600.html</a></p><blockquote><p>Kubernetes Ingress 只是 Kubernetes 中的一个普通资源对象，需要一个对应的 Ingress 控制器来解析 Ingress 的规则，暴露服务到外部，比如 <code>ingress-nginx</code>，本质上来说它只是一个 Nginx Pod，然后将请求重定向到其他内部（ClusterIP）服务去，这个 Pod 本身也是通过 Kubernetes 服务暴露出去，最常见的方式是通过 LoadBalancer 来实现的。同样本文我们希望用一个简单清晰的概述，让你来了解 Kubernetes Ingress 背后的东西，让你更容易理解使用的 Ingress。  </p><p>我们可以使用 Ingress 来使内部服务暴露到集群外部去，它为你节省了宝贵的静态 IP，因为你不需要声明多个 LoadBalancer 服务了，此次，它还可以进行更多的额外配置。下面我们通过一个简单的示例来对 Ingress 进行一些说明吧。</p><h2 id="简单-HTTP-server"><a href="#简单-HTTP-server" class="headerlink" title="简单 HTTP server"></a>简单 HTTP server</h2><p>首先，我们先回到容器、Kubernetes 之前的时代。</p><p>之前我们更多会使用一个（Nginx）HTTP server 来托管我们的服务，它可以通过 HTTP 协议接收到一个特定文件路径的请求，然后在文件系统中检查这个文件路径，如果存在则就返回即可。</p><p><img src="https://miro.medium.com/max/1000/1*SUicvV80aX-ytzYxUTfDLA.png" alt></p><p>例如，在 Nginx 中，我们可以通过下面的配置来实现这个功能。</p><p> location /folder {<br>  root /var/www/;<br> index index.html;<br>}  </p><p>除了上面提到的功能之外，我们可以当 HTTP server 接收到请求后，将该请求重定向到另一个服务器（意味着它作为代理）去，然后将该服务器的响应重定向到客户端去。对于客户端来说，什么都没有改变，接收到的结果仍然还是请求的文件（如果存在的话）。</p><p><img src="https://miro.medium.com/max/1000/1*2wlHmuF_leK_W1RyWxTmLA.png" alt></p><p>同样如果在 Nginx 中，重定向可以配置成下面的样子：</p><p> location /folder {<br> proxy_pass <a href="http://second-nginx-server:8000" target="_blank" rel="noopener">http://second-nginx-server:8000</a>;<br>}  </p><p>这意味着 Nginx 可以从文件系统中提供文件，或者通过代理将响应重定向到其他服务器并返回它们的响应。</p><h2 id="简单的-Kubernetes-示例"><a href="#简单的-Kubernetes-示例" class="headerlink" title="简单的 Kubernetes 示例"></a>简单的 Kubernetes 示例</h2><h3 id="使用-ClusterIP-服务"><a href="#使用-ClusterIP-服务" class="headerlink" title="使用 ClusterIP 服务"></a>使用 ClusterIP 服务</h3><p>在 Kubernetes 中部署应用后  。比如我们有两个 worker 节点，有两个服务 <strong>service-nginx</strong> 和 <strong>service-python</strong>，它们指向不同的 pods。这两个服务没有被调度到任何特定的节点上，也就是在任何节点上都有可能，如下图所示：</p><p><img src="https://miro.medium.com/max/1000/1*ySPjrqpwQLx8nTQuXhJbdA.png" alt></p><p>在集群内部我们可以通过他们的 Service 服务来请求到 Nginx pods 和 Python pods 上去，现在我们想让这些服务也能从集群外部进行访问，按照前文提到的我们就需要将这些服务转换为 LoadBalancer 服务。</p><h3 id="使用-LoadBalancer-服务"><a href="#使用-LoadBalancer-服务" class="headerlink" title="使用 LoadBalancer 服务"></a>使用 LoadBalancer 服务</h3><p>当然使用 LoadBalancer 服务的前提是我们的 Kubernetes 集群的托管服务商要能支持才行，如果支持我们可以将上面的 ClusterIP 服务转换为 LoadBalancer 服务，可以创建两个外部负载均衡器，将请求重定向到我们的节点 IP，然后重定向到内部的 ClusterIP 服务。</p><p><img src="https://miro.medium.com/max/1000/1*NJFzaf9rEGwFMmgh2_wqGQ.png" alt></p><p>我们可以看到两个 LoadBalancers 都有自己的 IP，如果我们向 LoadBalancer <code>22.33.44.55</code> 发送请求，它请被重定向到我们的内部的 <strong>service-nginx</strong> 服务去。如果发送请求到 77.66.55.44，它将被重定向到我们的内部的 <strong>service-python</strong> 服务。</p><p>这个确实很方便，但是要知道 IP 地址是比较稀有的，而且价格可不便宜。想象下我们 Kubernetes 集群中不只是两个服务，有很多的话，我们为这些服务创建 LoadBalancers 成本是不是就成倍增加了。</p><p>那么是否有另一种解决方案可以让我们只使用一个 LoadBalancer 就可以把请求转发给我们的内部服务呢？我们先通过手动（非 Kubernetes）的方式来探讨下这个问题。</p><h3 id="手动配置-Nginx-代理服务"><a href="#手动配置-Nginx-代理服务" class="headerlink" title="手动配置 Nginx 代理服务"></a>手动配置 Nginx 代理服务</h3><p>我们知道 Nginx 可以作为一个代理使用，所以我们可以很容易想到运行一个 Nginx 来代理我们的服务。如下图所示，我们新增了一个名为 <strong>service-nginx-proxy</strong> 的新服务，它实际上是我们唯一的一个 LoadBalancer 服务。service-nginx-proxy 仍然会指向一个或多个 <strong>Nginx-pod-endpoints</strong>（为了简单没有在图上标识），之前的另外两个服务转换为简单的 ClusterIP 服务了。</p><p><img src="https://miro.medium.com/max/1000/1*NH4oVWnr6lpEGMi2iHhApw.png" alt></p><p>可以看到我们只分配了一个 IP 地址为 <code>11.22.33.44</code> 的负载均衡器，对于不同的 http 请求路径我们用黄色来进行标记，他们的目标是一致的，只是包含的不同的请求 URL。</p><p><strong>service-nginx-proxy</strong> 服务会根据请求的 URL 来决定他们应该将请求重定向到哪个服务去。</p><p>在上图中我们有两个背后的服务，分别用红色和蓝色进行了标记，红色会重定向到 <strong>service-nginx</strong> 服务，蓝色重定向到 <strong>service-python</strong> 服务。对应的 Nginx 代理配置如下所示：</p><p> location /folder {<br> proxy_pass <a href="http://service-nginx:3001" target="_blank" rel="noopener">http://service-nginx:3001</a>;<br>}<br>location /other {<br> proxy_pass <a href="http://service-python:3002" target="_blank" rel="noopener">http://service-python:3002</a>;<br>}  </p><p>只是目前我们需要去手动配置 <strong>service-nginx-proxy</strong> 服务，比如新增了一个请求路径需要路由到其他服务去，我们就需要去重新配置 Nginx 的配置让其生效，但是这个确实是一个可行的解决方案，只是有点麻烦而已。</p><p>而 Kubernetes Ingress 就是为了让我们的配置更加容易、更加智能、更容易管理出现的，所以在 Kubernetes 集群中我们会用 Ingress 来代替上面的手动配置的方式将服务暴露到集群外去。</p><h2 id="使用-Kubernetes-Ingress"><a href="#使用-Kubernetes-Ingress" class="headerlink" title="使用 Kubernetes Ingress"></a>使用 Kubernetes Ingress</h2><p>现在我们将上面手动配置代理的方式转换为 Kubernetes Ingress 的方式，如下图所示，我们只是使用了一个预先配置好的 Nginx（Ingress），它已经为我们做了所有的代理重定向工作，这为我们节省了大量的手动配置工作了。</p><p><img src="https://miro.medium.com/max/1000/1*rUQI9dSxEJ4MXNV1Zt7dwg.png" alt></p><p>这其实就已经说明了 Kubernetes Ingress 是什么，下面让我们来看看一些配置实例吧。</p><h3 id="安装-Ingress-控制器"><a href="#安装-Ingress-控制器" class="headerlink" title="安装 Ingress 控制器"></a>安装 Ingress 控制器</h3><p>Ingress 只是 Kubernetes 的一种资源对象而已，在这个资源中我们可以去配置我们的服务路由规则，但是要真正去实现识别这个 Ingress 并提供代理路由功能，还需要安装一个对应的控制器才能实现。</p><p> kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/mandatory.yaml" target="_blank" rel="noopener">https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/mandatory.yaml</a>  </p></blockquote><blockquote><p>kubectl apply -f<br><a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.2" target="_blank" rel="noopener">https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.2</a> </p><p>使用下面的命令，可以看到安装在命名空间 <strong>ingress-nginx</strong> 中的 k8s 资源。<br>kubectl get svc,pod –namespace=ingress-nginx<br><img src="https://miro.medium.com/max/1000/1*NV4HknTA1LCoL7B0KSFgDA.png" alt></p><p>我们可以看到一个正常的 LoadBalancer 服务，有一个外部 IP 和一个所属的 pod，我们可以使用命令 <code>kubectl exec</code> 进入该 pod，里面包含一个预配置的 Nginx 服务器。</p><p><img src="https://miro.medium.com/max/1000/1*lpckG2-HqEZ_3ildoSBh9g.png" alt></p><p>其中的 <code>nginx.conf</code> 文件就包含各种代理重定向设置和其他相关配置。</p><h3 id="Ingress-配置示例"><a href="#Ingress-配置示例" class="headerlink" title="Ingress 配置示例"></a>Ingress 配置示例</h3><p>我们所使用的 Ingress yaml 例子可以是这样的。</p><h1 id="just-example-not-tested"><a href="#just-example-not-tested" class="headerlink" title="just example, not tested"></a>just example, not tested</h1><p>apiVersion: networking.k8s.io/v1beta1<br>kind: Ingress<br>metadata:<br>  annotations:<br>    kubernetes.io/ingress.class: nginx<br>  namespace: default<br>  name: test-ingress<br>spec:<br>  rules:  </p><ul><li>http:<br>  paths:  <ul><li>path: /folder<br>backend:<br>  serviceName: service-nginx<br>  servicePort: 3001  </li></ul></li><li>http:<br>  paths:  <ul><li>path: /other<br>backend:<br>  serviceName: service-python<br>  servicePort: 3002  </li></ul></li></ul><p>和其他资源对象一样，通过 <code>kubectl create -f ingress.yaml</code> 来创建这个资源对象即可，创建完成后这个 Ingress 对象会被上面安装的 Ingress 控制器转换为对应的 Nginx 配置。</p><p>如果你的一个内部服务，即 Ingress 应该重定向到的服务，是在不同的命名空间里，怎么办？因为我们定义的 Ingress 资源是命名空间级别的。在 Ingress 配置中，<strong>只能重定向到同一命名空间的服务</strong>。</p><p>如果你定义了多个 Ingress yaml 配置，那么这些配置会被一个单一的Ingress 控制器合并成一个 Nginx 配置。也就是说所有的人都在使用同一个 LoadBalancer IP。</p><h2 id="配置-Ingress-Nginx"><a href="#配置-Ingress-Nginx" class="headerlink" title="配置 Ingress Nginx"></a>配置 Ingress Nginx</h2><p>有时候我们需要对 Ingress Nginx 进行一些微调配置，我们可以通过 Ingress 资源对象中的 annotations 注解来实现，比如我们可以配置各种平时直接在 Nginx 中的配置选项。</p><p>kind: Ingress<br>metadata:<br>  name: ingress<br>  annotations:<br>      kubernetes.io/ingress.class: nginx<br>      nginx.ingress.kubernetes.io/proxy-connect-timeout: ‘30’<br>      nginx.ingress.kubernetes.io/proxy-send-timeout: ‘500’<br>      nginx.ingress.kubernetes.io/proxy-read-timeout: ‘500’<br>      nginx.ingress.kubernetes.io/send-timeout: “500”<br>      nginx.ingress.kubernetes.io/enable-cors: “true”<br>      nginx.ingress.kubernetes.io/cors-allow-methods: “<em>“<br>      nginx.ingress.kubernetes.io/cors-allow-origin: “</em>“<br>…  </p><p>此外也可以做更细粒度的规则配置，如下所示：</p><p> nginx.ingress.kubernetes.io/configuration-snippet: |<br>  if ($host = ‘<a href="http://www.qikqiak.com&#39;" target="_blank" rel="noopener">www.qikqiak.com&#39;</a> ) {<br>    rewrite ^ <a href="https://qikqiak.com$request_uri" target="_blank" rel="noopener">https://qikqiak.com$request_uri</a> permanent;  </p><p>这些注释都将被转换成 Nginx 配置，你可以通过手动连接(<code>kubectl exec</code>)到 nginx pod 中检查这些配置。</p><p>关于 ingress-nginx 更多的配置使用可以参考官方文档相关说明：</p><ul><li><p><a href="https://github.com/kubernetes/ingress-nginx/tree/master/docs/user-guide/nginx-configuration" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/tree/master/docs/user-guide/nginx-configuration</a></p></li><li><p><a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md#lua-resty-waf" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md#lua-resty-waf</a></p></li></ul><h3 id="查看-ingress-nginx-日志"><a href="#查看-ingress-nginx-日志" class="headerlink" title="查看 ingress-nginx 日志"></a>查看 ingress-nginx 日志</h3><p>要排查问题，通过查看 Ingress 控制器的日志非常有帮助。<br>kubectl logs -n ingress-nginx ingress-nginx-controller-6cfd5b6544-k2r4n<br><img src="https://miro.medium.com/max/1000/1*72EW9YWQPwqV0YBEdaJibA.png" alt></p><h3 id="使用-Curl-测试"><a href="#使用-Curl-测试" class="headerlink" title="使用 Curl 测试"></a>使用 Curl 测试</h3><p>如果我们想测试 Ingress 重定向规则，最好使用 <code>curl -v [yourhost.com](http://yourhost.com)</code> 来代替浏览器，可以避免缓存等带来的问题。</p><h3 id="重定向规则"><a href="#重定向规则" class="headerlink" title="重定向规则"></a>重定向规则</h3><p>在本文的示例中我们使用 <code>/folder</code> 和 <code>/other/directory</code> 等路径来重定向到不同的服务，此外我们也可以通过主机名来区分请求，比如将 api.myurl.com 和 site.myurl.com 重定向到不同的内部 ClusterIP 服务去。</p><p> apiVersion: networking.k8s.io/v1beta1<br>kind: Ingress<br>metadata:<br>  name: simple-fanout-example<br>spec:<br>  rules:  </p><ul><li>host: api.myurl.com<br>http:<br>  paths:  <ul><li>path: /foo<br>backend:<br>  serviceName: service1<br>  servicePort: 4200  </li><li>path: /bar<br>backend:<br>  serviceName: service2<br>  servicePort: 8080  </li></ul></li><li>host: website.myurl.com<br>http:<br>  paths:  <ul><li>path: /<br>backend:<br>  serviceName: service3<br>  servicePort: 3333  </li></ul></li></ul><h3 id="SSL-HTTPS"><a href="#SSL-HTTPS" class="headerlink" title="SSL/HTTPS"></a>SSL/HTTPS</h3><p>可能我们想让网站使用安全的 HTTPS 服务，Kubernetes Ingress 也提供了简单的 TLS 校验，这意味着它会处理所有的 SSL 通信、解密/校验 SSL 请求，然后将这些解密后的请求发送到内部服务去。</p><p>如果你的多个内部服务使用相同（可能是通配符）的 SSL 证书，这样我们就只需要在 Ingress 上配置一次，而不需要在内部服务上去配置，Ingress 可以使用配置的 TLS Kubernetes Secret 来配置 SSL 证书。</p><p> apiVersion: networking.k8s.io/v1beta1<br>kind: Ingress<br>metadata:<br>  name: tls-example-ingress<br>spec:<br>  tls:  </p><ul><li>hosts:  <ul><li>sslexample.foo.com<br>secretName: testsecret-tls<br>rules:  </li><li>host: sslexample.foo.com<br>http:<br>  paths:  <ul><li>path: /<br>backend:<br>  serviceName: service1<br>  servicePort: 80  </li></ul></li></ul></li></ul><p>不过需要注意的是如果你在不同的命名空间有多个 Ingress 资源，那么你的 TLS secret 也需要在你使用的 Ingress 资源的所有命名空间中可用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里我们简单介绍了 Kubernetes Ingress 的原理，简单来说：它不过是一种轻松配置 Nginx 服务器的方法，它可以将请求重定向到其他内部服务去。这为我们节省了宝贵的静态 IP 和 LoadBalancers 资源。</p><p>另外需要注意的是还有其他的 Kubernetes Ingress 类型，它们内部没有设置 Nginx 服务，但可能使用其他代理技术，一样也可以实现上面的所有功能。</p><blockquote><p>原文链接：<a href="https://codeburst.io/kubernetes-ingress-simply-visually-explained-d9cad44e4419" target="_blank" rel="noopener">https://codeburst.io/kubernetes-ingress-simply-visually-explained-d9cad44e4419</a></p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://new.qq.com/omn/20201109/20201109A065L600.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://new.qq.com/omn/20201109/20201109A0
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>图解Kubernetes-Serverless</title>
    <link href="http://zhang-yu.me/2020/11/12/%E5%9B%BE%E8%A7%A3Kubernetes-Serverless/"/>
    <id>http://zhang-yu.me/2020/11/12/图解Kubernetes-Serverless/</id>
    <published>2020-11-12T03:00:00.000Z</published>
    <updated>2020-11-15T04:22:49.811Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://itnext.io/kubernetes-serverless-simply-visually-explained-ccf7be05a689" target="_blank" rel="noopener">https://itnext.io/kubernetes-serverless-simply-visually-explained-ccf7be05a689</a><br><a href="https://mp.weixin.qq.com/s/j6Ox0Q6tA1vEkQtTFhF2SQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/j6Ox0Q6tA1vEkQtTFhF2SQ</a></p><p>Kubernetes上的Serverless技术以一种独立于云提供商的方式减少了重复配置。<br>这只是不断地使手动工作自动化的结果。</p><p>当我们在Kubernetes上谈论Serverless时，我们需要考虑两个不同的领域：</p><p>1.在群集中部署应用程序 Serverless（减少每个应用程序+自动构建容器所需的YAML文件数）</p><p>2.运行pods 容器Serverless而不管理节点/vm </p><p><strong>Serverless</strong></p><p>Serverless 是一种云原生开发模型，可使开发人员专注构建和运行应用，而无需管理服务器。简单来说 Serverless 就是让你不与或少与运行应用程序所需的服务器和基础设施进行交互，当今天我们提到 “serverless” 这个词的时候通常它可以指 CaaS 和 FaaS 这两种服务。</p><p><strong>CaaS —Container as a Service容器即服务</strong></p><p>当我们创建容器后，把它扔到 CaaS 上，它就会自动运行、服务和扩展，比如 Azure Container Instances、Google Cloud Run 或 AWS Fargate 这些服务。</p><p><img src="https://miro.medium.com/max/700/1*jSpEItYQsmXvUfkV_ypyyw.png" alt></p><p><strong>FaaS —Function as a Service函数即服务</strong></p><p>当我们写好代码，扔给 FaaS，它就会自动运行、服务和扩展。比如 Azure Functions、Google Functions 或者 AWS Lambda 这些服务。</p><p><img src="https://miro.medium.com/max/700/1*3Ozkuz_wLFHY5Or_8L7C9A.png" alt></p><p><strong>FaaS实施</strong></p><p>FaaS 可以用不同的方式来运行你的代码<br> <strong>一种方式</strong><br>可能是 FaaS 为每一次代码变化构建一个容器，就类似于使用 CaaS 这种服务。</p><p><img src="https://miro.medium.com/max/700/1*CW1EnqvBCUhBlYDrkkIM6g.png" alt><br>FaaS构建容器并将其发送到CaaS</p><p><strong>另一种方法</strong><br>FaaS 在启动过程中动态地将函数的源码拉到一个预定义的环境（容器）中，不同的语言会有不同的环境，当使用像 Go 这样的编译语言时，那么编译必须在启动时进行。</p><p><strong>事件/伸缩</strong></p><p>FaaS 大多数时候与函数实例的触发器事件系统一起使用，事件可以来源于 API网关、Github、Kafka、RabbitMQ、CronJobs 等。</p><p><img src="https://miro.medium.com/max/700/1*QxtIz2qk0M1iNeqrMWzx2w.png" alt></p><p>对于每个事件，将创建一个新的函数来处理它，如果有多个事件同时发生，将创建多个实例来处理这些事件。这样我们就有了自动伸缩的功能。</p><p>FaaS 与各种事件源进行通信，所以函数本身不需要去实现，它们只需要与 FaaS 使用的一种事件格式进行通信，比如 CloudEvents 或者通过 HTTP 传输。</p><p>有一个 CloudEvents项目 ，将事件的结构和元数据描述为“标准”。 它还包括数据和描述该数据的模式。 云事件是包裹事件数据的信封。 如果被许多供应商采用以实现互操作性，这可能会很棒。</p><p><strong>Kubernetes应用</strong></p><p>下面让我们来看看开发一个运行在 Kubernetes 上的传统非 serverless 应用需要的步骤。</p><p><img src="https://miro.medium.com/max/700/1*tRebIPaAjL8ZtTvu4CC_3w.png" alt> </p><p>我们需要构建一个容器，创建各种Kubernetes资源清单文件（YAML文件），然后决定我们需要多少个工作节点来运行我们的应用程序。</p><p>我们需要多少个工作节点的可以通过配置一个 Cluster/Node autoscaler来动态地处理，但是我们仍然必须配置它，并且需要设置一个最小+最大的节点数量。</p><p>我们使用这种传统方法与“服务器”进行了很多交互。 首先创建/构建容器，然后编写YAML文件，并定义节点的数量和资源。</p><p><strong>Kubernetes Serverless应用程序</strong></p><p>现在让我们探索为Kubernetes开发应用程序时的<strong>Serverless</strong>方法。</p><p><strong>CaaS —容器即服务</strong></p><p><img src="https://miro.medium.com/max/700/1*JfoJd33zAX5m98TVskY8JQ.png" alt></p><p>可以看到我们大大减少了需要创建的 Kubernetes 资源清单（YAML文件）的数量，CaaS 将为我们创建所有必要的子资源，比如 autoscaler、Ingress 或 Istio 路由</p><p>我们要做的就是提供一个（Docker）容器并创建一个单一的k8s资源，即通过 CRD 引入的 CaaS-容器资源。<br>CaaS 可以根据事件或我们自定义的方式，决定什么时候启动我们应用的实例，以及启动多少个实例，<br>我们必须确保我们构建的容器能够接收和处理来自 CaaS 的事件，例如可以通过 HTTP 或 CloudEvents，这可能需要容器内部的某些库支持。</p><p>示例： CaaS Knative （Knative 就是一个典型的 CaaS 服务，Knative 提供了灵活的构建模块，其他解决方案也可以使用和依赖它。）。</p><p><strong>FaaS —函数即服务</strong></p><p><img src="https://miro.medium.com/max/700/1*NkrEERZgTqEpaXMmBKVzNg.png" alt></p><p>现在，我们还可以自动执行构建过程，并且可能会有一个不错的FaaS Web界面</p><p>在 FaaS 服务中的 function.yml 文件中将包含一个来自 FaaS 系统的 K8s 资源，通过 CRD 引入，在该资源中，我们可以配置函数名称、源代码位置、语言运行时和触发事件等内容。</p><p>如果我们通过网络界面上传代码来创建FaaS，则 function.yaml是不需要的 。 但是将函数保留为代码应该是一种好习惯。 Web界面非常适合原型设计或测试修改。</p><p>借助FaaS，我们还可以获得CaaS解决方案所提供的一切。 但是现在我们进一步减少了工作量，因为我们有工具在 Kubernetes 集群中运行，可以直接执行/构建我们的应用源代码</p><p>为我们构建的容器已经包含必要的库，例如HTTP或CloudEvents，以从FaaS接收事件。 所以我们不必担心这个问题。</p><p>源码可能存储在 Git 仓库中，也可能是通过 web 界面上传的，或者是在其他地方提供的。FaaS 将访问代码，监听变化，构建容器，然后将其传递给 CaaS，用于服务终端事件.</p><p><img src="https://miro.medium.com/max/700/1*U4fIQGNsVBTXBZp1zuGTBA.png" alt></p><p>TriggerMesh的示例Web界面，用于上传代码并作为功能进行部署</p><p>FaaS示例：</p><pre><code>TriggerMesh (uses Knative as CaaS)OpenFaaS (can use Knative)KubelessFission (works with environments rather than immutable function containers, more further down)OpenWhiskoverview and comparison of various K8s FaaS</code></pre><p><strong>冷热启动</strong></p><p>冷启动将意味着没有 Pod 已经在运行处理事件，所以需要一些时间来创建它。通常情况下，这些 Pod 在最后一次使用后会保持一段时间，可以重复使用。在 “已经运行” 期间的调用被称为热启动，热启动的速度较快，但也会消耗资源。</p><p><strong>基于FaaS的裂变/环境</strong></p><p><strong>Fission / Environment based FaaS</strong></p><p><img src="https://miro.medium.com/max/700/1*k0cm1bFT1ZIJUl71bfHx2g.png" alt><br>裂变架构</p><p>Fission 是一个典型的运行在 Kubernetes 环境下面的 Faas 服务，实际上并没有为每个函数的代码变化构建一个不可变的容器，而是使用了可变的环境容器（”Generic pods”）的概念，动态地将代码拉进来，然后将这些容器转换成 “Specific Function pods”，这也是 AWS Lambda 使用用 AWS Firecracker 的工作方式。</p><p><strong>可观察性</strong></p><p>从容器化的微服务转向函数，可能会导致不得不管理比以前更多、更小的服务。这是因为创建小型函数是很容易的，这些函数只是监听和处理一个单一事件。</p><p>  当我创建与 相同的演示应用程序时，我自己注意到了这一点 第1部分中的Container Microservices 和 第2部分中的AWS Serverless 。</p><p>为了管理更多的服务或功能，所以非常有必要保持可观察性（指标、日志、跟踪），这就是为什么大多数 Kubernetes 的 FaaS 和 CaaS 已经与Prometheus、Jaeger 和 Istio 或 Linkerd 等服务网格进行了集成。</p><p><strong>Kubernetes Serverless节点</strong></p><p>上面我们谈到了 K8s Serverless 应用，我们看到了使用 CaaS 或 FaaS 时的工作流程，这些服务减少了我们很多重复性的工作。</p><p>但是开发人员或运维人员仍然在与服务器交互：作为集群中的工作节点的虚拟机，他们仍然需要指定有多少节点以及它们的资源（CPU/内存）。</p><p>下面我们来看下使用 Virtual Kubelet 让实际的底层 Kubernetes 节点成为 Serverless 节点。<br><img src="https://miro.medium.com/max/700/1*SthgjT7yZXgQxkYzkuIAng.png" alt></p><p>FaaS 在 Kubernetes 之上，以 Virtual Kubelet 为节点</p><p> Virtual Kubelet 给 Kubernetes 模拟了一个 worker 节点，然后可以调度 Pods 到上面来工作，就像其他普通节点一样。虽然 Pods 的容器不是运行在虚拟机上，<br> 但是在无服务器容器产品中，AWS Fargate，Google Cloud Run或Azure容器实例等云提供商都提供了该产品。</p><p>在我 阅读更多 有关Virtual Kubelet的详细文章中 内容 。<br>K8s Serverless 应用和 K8s Serverless 节点可能是一个强大的组合，但是，如果我们把所有的东西都 serverless 化了，那为什么还要使用 K8s呢？</p><p><strong>为什么仍然使用Kubernetes？</strong></p><p>Kubernetes 提供了强大而灵活的构建功能，而不是为了方便交互和终端用户而生的的。这使得 K8s 变得很复杂，直接使用时需要大量的重复性工作。<br>Kubernetes 成为一个独立于云提供商的标准，在上面使用 serverless 框架，在使用 serverless 时保持这种独立性是有意义的，如果有必要的话，我们可以随时对我们的应用进行更加详细的定义，因为它的下面仍然只是运行 K8s 而已。<br>通过在 K8s 上使用 serverless，我们可以减少很多重复性工作，这样我们就可以花更多的时间来构建实际的应用了。</p><p><strong>回顾</strong></p><p>最后我们来回顾下传统的 K8s 应用和 serverless 应用的区别。<br>传统 K8s 应用</p><p><img src="https://miro.medium.com/max/700/1*tRebIPaAjL8ZtTvu4CC_3w.png" alt></p><p>Serverless 应用</p><p><img src="https://miro.medium.com/max/700/1*SthgjT7yZXgQxkYzkuIAng.png" alt></p><p><strong>结论</strong></p><p>我认为现代 serverless 事件驱动的架构已经证明了自己的能力，并且在未来几年会越来越普遍。Kubernetes上的 Serverless 只是持续自动化掉手工作业的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://itnext.io/kubernetes-serverless-simply-visually-explained-ccf7be05a689&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://itnext.io
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>图解Kubernetes-Istio</title>
    <link href="http://zhang-yu.me/2020/11/12/%E5%9B%BE%E8%A7%A3Kubernetes-Istio/"/>
    <id>http://zhang-yu.me/2020/11/12/图解Kubernetes-Istio/</id>
    <published>2020-11-12T03:00:00.000Z</published>
    <updated>2020-11-12T15:34:00.988Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Istio 是一个服务网格，它允许集群中的 pods 和服务之间进行更详细、复杂和可观察的通信。</p><p>它通过使用 CRD 扩展 Kubernetes API 来进行管理，它将代理容器注入到所有 pods 中，然后由这些 pods 来控制集群中的流量。</p><h2 id="Kubernetes-Services"><a href="#Kubernetes-Services" class="headerlink" title="Kubernetes Services"></a>Kubernetes Services</h2><p>前面我们已经了解了 Kubernetes Services，我们可以再简短地说明下如何实现 Kubernetes Services，这这有助于理解 Istio 如何工作的。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/5ee57b3ec4844b1df477574621fc49cc.png" alt></p><p>图1: Kubernetes native service request</p><p>上图的 Kubernetes 集群中一共有两个节点和 4 个 pod，每个 pod 都有一个容器。服务 <code>service-nginx</code> 指向 nginx pods，服务 <code>service-python</code> 指向 python pods。红线显示了从 <code>pod1-nginx</code> 中的 nginx 容器向 <code>service-python</code> 服务发出的请求，该服务将请求重定向到 <code>pod2-python</code>。</p><p>默认情况下，ClusterIP 服务进行简单的随机或轮询转发请求，Kubernetes 中的 Services 并不存在于特定的节点上，而是存在于整个集群中。我们可以在下图 中看到更多细节:</p><p><img src="https://img-blog.csdnimg.cn/img_convert/df59fd13fca0a0ca118df5adaddf56e8.png" alt></p><p>图2: Kubernetes native service request with kube-proxy</p><p>上图要更详细点，Kubernetes 中的服务是由运行在每个节点上的 <code>kube-proxy</code> 组件实现的，该组件创建 iptables 规则，并将请求重定向到 Pod。因此，服务就是 iptables 规则。(还有其他不使用 iptables 的代理模式，但过程是相同的。)</p><h2 id="Kubernetes-Istio"><a href="#Kubernetes-Istio" class="headerlink" title="Kubernetes Istio"></a>Kubernetes Istio</h2><p>现在我们来看一个配置了 Istio 的相同示例:</p><p><img src="https://img-blog.csdnimg.cn/img_convert/0b06dd1708a9634377d1c32c45893a06.png" alt></p><p>图3: Istio Control Plane programs istio-proxy</p><p>上图中可以看到集群中安装了 Istio，每个 pod 都有第二个称为 <code>istio-proxy</code> 的 sidecar 容器，该容器在创建期间会自动将其注入到 pods 中。</p><p>Istio 最常见的代理是 <strong>Envoy</strong>，当然也可以使用其他代理（如 Nginx），所以我们将代理称为<code>istio-proxy</code>。</p><p>我们可以看到不再显示 <code>kube-proxy</code> 组件，这样做是为了保持图像的整洁，这些组件仍然存在，但是拥有 <code>istio-proxy</code> 的 pods 将不再使用 <code>kube-proxy</code> 组件了。</p><p>每当配置或服务发生变化时，Istio 控制平面就会对所有 <code>istio-proxy</code> sidecars 进行处理，类似于图 2 中 Kubernetes API 处理所有 <code>kube-proxy</code> 组件的方式。Istio 控制平面使用现有的 Kubernetes 服务来接收每个服务点所指向的所有 pods ，通过使用 pod IP 地址，Istio 实现了自己的路由。</p><p>在 Istio 控制平面对所有 <code>istio-proxy</code> sidecars 处理之后，它看起来是这样的:</p><p><img src="https://img-blog.csdnimg.cn/img_convert/7b568acb26e073d9789502483fe5cbb8.png" alt></p><p>图4: Istio Control Plane programmed all istio-proxys</p><p>在图 4 中，我们看到 Istio 控制平面如何将当前配置应用到集群中的所有 <code>istio-proxy</code> 容器，Istio 将把 Kubernetes 服务声明转换成它自己的路由声明。</p><p>让我们看看如何使用 Istio 发出请求:</p><p><img src="https://img-blog.csdnimg.cn/img_convert/3c61fd7c7d26b8472657f5ccd807665a.png" alt></p><p>图5: Request made with Istio</p><p>在上图中，所有的 <code>istio-proxy</code> 容器已经被 Istio 控制平面所管控，并包含所有必要的路由信息，如图 3/4 所示，来自 <code>pod1-nginx</code> 的 nginx 容器向 <code>service-python</code> 发出请求。</p><p>请求被 <code>pod1-nginx</code> 的 <code>istio-proxy</code> 容器拦截，并被重定向到一个 python pod 的 <code>istio-proxy</code> 容器，该容器随后将请求重定向到 python 容器。</p><h2 id="发生了什么？"><a href="#发生了什么？" class="headerlink" title="发生了什么？"></a>发生了什么？</h2><p>图 1-5 显示了使用 nginx 和 python pod 的 Kubernetes 应用程序的相同示例，我们已经看到了使用默认的 Kubernetes 服务和使用 Istio 是如何发生请求的。</p><p>重要的是：无论使用什么方法，结果都是相同的，并且不需要更改应用程序本身，只需要更改基础结构代码。</p><h2 id="为什么要使用-Istio"><a href="#为什么要使用-Istio" class="headerlink" title="为什么要使用 Istio?"></a>为什么要使用 Istio?</h2><p>如果在使用 Istio 的时候没有什么变化（nginx pod 仍然可以像以前一样连接到 python pod），那么我们为什么还要使用 Istio 呢？</p><p>其惊人的优势是，现在所有流量都通过每个 Pod 中的 <code>istio-proxy</code> 容器进行路由，每当 <code>istio-proxy</code> 接收并重定向一个请求时，它还会将有关该请求的信息提交给 Istio 控制平面。因此 Istio 控制平面可以准确地知道该请求来自哪个 pod、存在哪些 HTTP 头、从一个<code>istio-proxy</code> 到另一个 <code>istio-proxy</code> 的请求需要多长时间等等。在具有彼此通信的服务的集群中，这可以提高可观察性并更好地控制所有流量。</p><p>先进的路由，Kubernetes 内部 Services 只能对 pods 执行轮询或随机分发请求，使用 Istio 可以实现更复杂的方式。比如，如果发生错误，根据请求头进行重定向，或者重定向到最少使用的服务。</p><p>部署，它允许将一定比例的流量路由到特定的服务版本，因此允许绿色/蓝色和金丝雀部署。</p><p>加密，可以对 pods 之间从 <code>istio-proxy</code> 到 <code>istio-proxy</code> 的集群内部通信进行加密。</p><p>监控/图形，Istio 可以连接到 Prometheus 等监控工具，也可以与 Kiali 一起展示所有的服务和他们的流量。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/664dabc74baf1450227fbffe97e50467.png" alt></p><p>追踪，因为 Istio 控制平面拥有大量关于请求的数据，所以可以使用 Jaeger 等工具跟踪和检查这些数据。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/cc77a8ed747473ef804a87baae7c5f32.png" alt></p><p>多集群 mesh，Istio 有一个内部服务注册中心，它可以使用现有的 Kubernetes  服务，但是也可以从集群外部添加资源，甚至将不同的集群连接到一个网格中。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/3b88df7f1b358097c1738aa404fd7e9c.png" alt></p><p>Sidecar 注入，为了使 Istio 工作，每一个作为网状结构一部分的 pod 都需要注入一个 <code>istio-proxy</code>sidecar，这可以在 pod 创建期间为整个命名空间自动完成（也可以手动完成）。</p><h2 id="Istio-会取代-Kubernetes-的服务吗？"><a href="#Istio-会取代-Kubernetes-的服务吗？" class="headerlink" title="Istio 会取代 Kubernetes 的服务吗？"></a>Istio 会取代 Kubernetes 的服务吗？</h2><p>当然不会，当我开始使用 Istio 时，我问自己的一个问题是它是否会取代现有的 Kubernetes 服务，答案是否定的，因为 Istio 会使用现有的 Kubernetes 服务获取它们的所有 <code>endpoints/pod IP</code> 地址。</p><h2 id="Istio-取代了-Kubernetes-的-Ingress-吗？"><a href="#Istio-取代了-Kubernetes-的-Ingress-吗？" class="headerlink" title="Istio 取代了 Kubernetes 的 Ingress 吗？"></a>Istio 取代了 Kubernetes 的 Ingress 吗？</h2><p>##是的<br>Istio 提供了新的 CRD 资源，比如 <code>Gateway</code> 和 <code>VirtualService</code>，甚至还附带了 ingress 转换器 <code>istioctl convert-ingress</code>，下图显示了 Istio 网关如何处理进入流量，网关本身也是一个 <code>istio-proxy</code> 组件。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/36752b60c629bd198b858b66e3b52eaa.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Istio 无疑在 Kubernetes 之上又增加了另一层次的复杂性，但是对于现代微服务架构来说，它实际上提供了一种比必须在应用程序代码本身中实现跟踪或可观察性更简单的方法。</p><p>“</p><p>原文链接：<a href="https://itnext.io/kubernetes-istio-simply-visually-explained-58a7d158b83f" target="_blank" rel="noopener">https://itnext.io/kubernetes-istio-simply-visually-explained-58a7d158b83f</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Istio 是一个服务网格，它允许集群中的 pods 和服务之间进行更详细、复杂和可观察的通信。&lt;/p&gt;
&lt;p&gt;它通过使用 CRD 扩展 Kubernetes API 来进行管理，它将代理容器注入到所有 pods 中，然后由这些 pods 来控制集
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>如何做好一名稳定性SRE</title>
    <link href="http://zhang-yu.me/2020/11/02/%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E4%B8%80%E5%90%8D%E7%A8%B3%E5%AE%9A%E6%80%A7SRE/"/>
    <id>http://zhang-yu.me/2020/11/02/如何做好一名稳定性SRE/</id>
    <published>2020-11-02T02:00:00.000Z</published>
    <updated>2020-11-03T15:04:33.775Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://developer.aliyun.com/article/776448?spm=a2c6h.12873581.0.dArticle776448.4f1d6446TFJ8Rn" target="_blank" rel="noopener">https://developer.aliyun.com/article/776448?spm=a2c6h.12873581.0.dArticle776448.4f1d6446TFJ8Rn</a></p><p><a href="https://developer.aliyun.com/article/776448?spm=a2c6h.12873581.0.dArticle776448.4f1d6446TFJ8Rn" target="_blank" rel="noopener">如何做好一名稳定性SRE–业务团队系统稳定性的思与行-阿里云开发者社区</a></p><blockquote><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>2013年，当我第一次接触稳定性的时候，我是有些懵的，当时完全不知道稳定性是什么，也不清楚要做什么。在接下来的8年里，我先后在菜鸟、天猫、盒马从事中间件、业务系统、架构等方面的工作，期间一直穿插着负责稳定性和大促的保障工作。我的心态，大致经历过以下几个阶段：</p><ul><li>low：完全不懂，觉得稳定性就是做别人安排好的一些表格和梳理，不知道自己该做啥，稳定性好low；</li><li>烦：各种重复的会议，做好了是应该的，做不好就是责任，很烦很焦虑；</li><li>知道该做什么，但是累：各种报警和风险，每天需要担心，想要不管又过不了自己心里那关；大促时候天天熬夜，各种压测，最终自己累得够呛；</li><li>发现结合点：发现在采用系统化思维之后，稳定性与系统自身的结合变得紧密，稳定性成为一种基线，找到了稳定性中的关键点和重点。</li><li>主动驱动：发现线上业务和线下业务的稳定性差别，理解并主动调整在不同业务团队采取的稳定性策略，探究在稳定性中的自动化、工具化，系统化建立稳定性机制；</li><li>形成体系：形成稳定性体系化思考，明确稳定性每一个点在业务团队大图中的位置，探究系统弹性建设；<br>近两年来，稳定性不再仅仅局限于之前的大促保障和平时的稳定性轮值，越来越体系化，在保障体系、监控体系、资源体系、质量保障、变更管控等多个方面，越来越系统。阿里的各个事业部，也纷纷成立专职的SRE安全生产团队。然而仍有很多人和业务团队，对于稳定性的理解和认知未形成一个体系化的机制，下面就结合我在业务团队系统稳定性上的认识，以及最近2年在盒马的一些思考，做一个分享。</li></ul><h3 id="什么是SRE"><a href="#什么是SRE" class="headerlink" title="什么是SRE"></a>什么是SRE</h3><p>SRE（Site Reliability Engineering，站点可靠性/稳定性工程师），与普通的开发工程师（Dev）不同，也与传统的运维工程师（Ops）不同，SRE更接近是两者的结合，也就是2008年末提出的一个概念：DevOps，这个概念最近也越来越流行起来。SRE模型是Google对Dev+Ops模型的一种实践和拓展（可以参考《Google运维解密》一书），SRE这个概念我比较喜欢，因为这个词不简单是两个概念的叠加，而是一种对系统稳定性、高可用、团队持续迭代和持续建设的体系化解决方案；<br>那么要如何做好一个SRE呢，这是本文要探讨的话题。</p><h3 id="1，心态-amp-态度"><a href="#1，心态-amp-态度" class="headerlink" title="1，心态&amp;态度"></a>1，心态&amp;态度</h3><h4 id="1-1，谁适合做稳定性？"><a href="#1-1，谁适合做稳定性？" class="headerlink" title="1.1，谁适合做稳定性？"></a>1.1，谁适合做稳定性？</h4><p>就像前言里我做稳定性前期的心态一样，稳定性最初上手，是提心吊胆、不得其门而入的，所以想要做好稳定性，心态最重要，业务团队想要找到合适做稳定性的人，态度也很重要。对于业务团队，要如何挑选和培养团队中最合适做稳定性的人呢？</p><ul><li>必须选择负责任的人，负责任是第一要素，主动承担，对报警、工单、线上问题、风险主动响应，不怕吃苦；一个不负责任的人，遇到问题与我无关的人，边界感太强的人，难以做好稳定性的工作；</li><li>原则上不要选择新人，对于团队leader而言，“用新人做别人不愿意做的工作”，这个决定比较容易做出，但是这也相当于是把团队的稳定性放在了一定程度的风险上，用新人做稳定性，其实只是用新人占了稳定性的一个坑而已。新人不熟悉业务，不了解上下游，最多只能凭借一腔热血，对业务和系统感知不足，容易导致线上风险无法被快速发现、故障应急无法迅速组织。</li><li>不要用过于”老实”的人，这里的“老实”的定义是不去主动想优化的办法，不主动出头解决问题，但是很能吃苦，任劳任怨，也很能忍耐系统的腐烂和低效；这样的人平时很踏实，用起来也顺手，但是却无法主动提高系统稳定性，有的时候反而会给系统稳定性造成伤害（稳定性就像大堤，不主动升级，就早晚会腐烂）。</li></ul><h4 id="1-2，业务团队如何支持稳定性SRE人员"><a href="#1-2，业务团队如何支持稳定性SRE人员" class="headerlink" title="1.2，业务团队如何支持稳定性SRE人员"></a>1.2，业务团队如何支持稳定性SRE人员</h4><ul><li>给资源，稳定性从来不只是稳定性负责人的事情，而是全团队的事情，稳定性负责人要做的是建立机制，主动承担，但是稳定性意识，要深入到团队所有人脑子里，稳定性的事情，要能够调动团队一切资源参与。</li><li>给空间，做稳定性的人，往往面临一个尴尬场景：晋升困难，主要是因为在技术深度和业务价值两个方面，很容易被挑战，对于业务团队，一定要留给做稳定性的人足够的思考和上升空间，将稳定性与团队的技术架构升级、业务项目结合起来，共同推动。经过集团安全生产团队的推动，目前在阿里，SRE已经有了自己专门的晋升体系。</li><li>区分责任，当出现故障时，区分清楚责任，到底是稳定性工作没有做到位，还是做到位了，但是团队同学疏忽了，还是说只是单纯的业务变化；</li></ul><h4 id="1-3，开发和SRE的区别"><a href="#1-3，开发和SRE的区别" class="headerlink" title="1.3，开发和SRE的区别"></a>1.3，开发和SRE的区别</h4><p>都是做技术的，很多开发刚刚转向负责稳定性时，有些弯转不过来。<br>举个例子：对于“问题”，传统的开发人员更多的倾向于是“bug/错误”，而SRE倾向于是一种“风险/故障”，所以，两者对“问题”的处理方法是不一样的：</p><ul><li>开发：了解业务 -&gt; 定位问题 -&gt; 排查问题 -&gt; 解决问题</li><li>SRE：了解业务归属 -&gt; 快速定位问题范围 -&gt; 协调相关人投入排查 -&gt; 评估影响面 -&gt; 决策恢复手段<br>可见，开发人员面对问题，会首先尝试去探究根因，研究解决方案；而SRE人员首先是评估影响，快速定位，快速止损恢复。目标和侧重点的不同，造成了SRE思考问题的特殊性。</li></ul><p>所以，成为一名SRE，就一定要从态度和方式上进行转变，切换到一个“团队稳定性负责人”的角度上去思考问题。</p><h4 id="1-4，SRE心态上的一些释疑"><a href="#1-4，SRE心态上的一些释疑" class="headerlink" title="1.4，SRE心态上的一些释疑"></a>1.4，SRE心态上的一些释疑</h4><p>下面这些疑惑，有很多是我最初做稳定性的时候面临的问题，这里给大家分享和解释一下我的解决方法：</p><h5 id="1-4-1，疑惑1：做好了是应该的，出了问题就要负责任"><a href="#1-4-1，疑惑1：做好了是应该的，出了问题就要负责任" class="headerlink" title="1.4.1，疑惑1：做好了是应该的，出了问题就要负责任"></a>1.4.1，疑惑1：做好了是应该的，出了问题就要负责任</h5><p>不出问题，就是稳定性的基线，也是SRE的基本目标，所以这个话虽然残酷，但是也不能说错，关键在于：你要如何去做。<br>如果抱着一个“背锅” / “打杂”的思想去做稳定性，那么“做好没好处、做不好背锅”这句话就会成为击垮心理防线的最重的稻草。<br>应对这种心态的最关键一点，在于“做好”不出问题这条基线，要从下面3个方面去做：<br><strong>1. 及时、快速的响应</strong><br>这是最关键的一点，作为一个SRE，能够及时、快速的响应是第一要务，遇到报警、工单、线上问题，能够第一时间冲上去，不要去问是不是自己的，而是要问这个事情的影响是什么，有没有坑，有没有需要优化的风险？这是对自己负责；<br>同时，快速的响应，还需要让你的老板第一时间知悉，这个不是在老板面前爱表现拍马屁，而是要让你的老板第一时间了解风险的发生，一个好的团队leader，一定是对质量、稳定性和风险非常敏感的leader，所以，你要将风险第一时间反馈。这是对老板负责。<br>反馈也是有技巧的，不仅仅是告知这么简单，你需要快速的说明以下几个信息：</p><ul><li>尽快告知当前告警已经有人接手，是谁接手的，表明问题有人在处理了。（这一步叫“响应”）</li><li>组织人员，快速定位问题，告知问题初步定位原因（这一步叫“定位”）。</li><li>初步影响范围是什么？给出大致数据（这一步方便后面做决策）</li><li>有哪些需要老板、产品、业务方决策的？你的建议是什么？（这一步很关键，很多时候是：两害相权取其轻，你的评估和建议，直接影响老板的决策）</li><li>当前进展如何，是否已经止血？（这一步是“恢复”，要给出“进展”，让决策者和业务方了解情况）</li></ul><p>需要注意的是：如果你响应了，但是没有及时的同步出来，等于没响应，默默把事情做了，是开发者（Dev）的思维，作为SRE，风险和进展的及时组织和通报，才是你应该做的。<br>当然，你的通报要注意控制范围，最好优先同步给你的主管和产品进行评估，避免范围过大引起恐慌，要根据事情的严重程度来共同决定，这是对团队负责。<br>及时、快速的响应，是保证不出问题的关键，也是SRE人员赢得领导、业务方、产品和其他合作方信任的关键，赢得信任，是解决“做好没好处、做不好背锅”的基石。<br><strong>2. 把机制建立好，切实落地</strong><br>前面已经说过，“稳定性从来不只是稳定性负责人的事情”，这一点，要深入到团队每个人的心里，更要深入到SRE自己心里，一人抗下所有，不是英雄的行为，在SRE工作中，也不值得赞许，很多时候一人抗下所有只会让事情变得更糟糕。</p><p>作为一个SRE，想做到“不出问题”这个基线，关键还是要靠大家，如何靠大家呢？就是要落地一套稳定性的机制体系，用机制的严格执行来约束大家，这套机制也必须得到团队leader的全力支持，不然无法展开，这套机制包括：</p><ul><li>稳定性意识</li><li>日常值班机制</li><li>报警响应机制</li><li>复盘机制</li><li>故障演练机制</li><li>故障奖惩机制</li><li>大促保障机制</li></ul><p>比如，如果总是SRE人员去响应报警和值班，就会非常疲惫劳累，人不可能永远关注报警，那怎么办呢？可以从报警机制、自动化、值班机制3个方面入手：<br>一方面，让报警更加准确和完善，减少误报和漏报，防止大家不必要的介入，另一方面产出自动化机器人，自动进行一些机器重启，工单查询，问题简单排查之类的工作，还有就是建立值班轮班，让每个人都参与进来，既能让大家熟悉业务，又能提高每个人的稳定性意识。<br>对于SRE来说，指定机制并且严格落地，比事必躬亲更加重要。上面这些机制，将在后面的章节中详细论述。<br><strong>3. 主动走到最前线</strong><br>SRE工作，容易给人一种错觉：“是做后勤保障的”，如果有这种思想，是一定做不好的，也会把“做好没好处、做不好背锅”这个疑惑无限放大。作为SRE人员，一定要主动走到最前线，把责任担起来，主动做以下几个事情：</p><ul><li>梳理。主动梳理团队的业务时序、核心链路流程、流量地图、依赖风险，通过这个过程明确链路风险，流量水位，时序冗余；</li><li>治理。主动组织风险治理，将梳理出来的风险，以专项的形式治理掉，防患于未然。</li><li>演练。把风险化成攻击，在没有故障时制造一些可控的故障点，通过演练来提高大家响应的能力和对风险点的认知。这一点将在后面详述。</li><li>值班。不能仅仅为了值班而值班，值班不止是解决问题，还要能够发现风险，发现问题之后，推动上下游解决，减少值班中的重复问题，才是目标。</li><li>报警。除了前面说过的主动响应之外，还要经常做报警保险和机制调整，保证报警的准确度和大家对报警的敏感度。同时也要做到不疏忽任何一个点，因为疏忽的点，就可能导致问题。</li></ul><h5 id="1-4-2，疑惑2：稳定性总是做擦屁股的工作"><a href="#1-4-2，疑惑2：稳定性总是做擦屁股的工作" class="headerlink" title="1.4.2，疑惑2：稳定性总是做擦屁股的工作"></a>1.4.2，疑惑2：稳定性总是做擦屁股的工作</h5><p>这么想，是因为没有看到稳定性的前瞻性和价值，如果你走在系统的后面，你能看到的就只有系统的屁股，也只能做擦屁股的工作，如果你走到了系统的前面，你就能看到系统的方向，做的也就是探索性的工作。<br>所以，要让稳定性变成不“擦屁股”的工作，建议从下面2个方面思考：</p><p><strong>1. 不能只做当下，要看到未来的风险，善于总结</strong></p><blockquote><p>暖曰：“ 王独不闻魏文王之问扁鹊耶？曰：‘子昆弟三人其孰最善为医？’扁鹊曰：‘长兄最善，中兄次之，扁鹊最为下。’魏文侯曰：‘可得闻邪？’扁鹊曰：‘长兄于病视神，未有形而除之，故名不出于家。中兄治病，其在毫毛，故名不出于闾。若扁鹊者，镵血脉，投毒药，副肌肤，闲而名出闻于诸侯。’魏文侯曰：‘善。使管子行医术以扁鹊之道，曰桓公几能成其霸乎！’凡此者不病病，治之无名，使之无形，至功之成，其下谓之自然。故良医化之，拙医败之，虽幸不死，创伸股维。” —-《鶡冠子·卷下·世贤第十六》</p></blockquote><p>与扁鹊三兄弟一样，如果想要让稳定性有价值，SRE同学一定不能站到系统的屁股后面等着擦屁股，必须走到前面，看到未来的风险。既要在发生问题时快速解决问题（做扁鹊），也要把风险归纳总结，推动解决（做二哥），还要在系统健康的时候评估链路，发现隐藏的问题（做大哥）；</p><pre><code>1. 做扁鹊大哥：在系统健康时发现问题2. 做扁鹊二哥：在系统有隐患时发现问题3. 做扁鹊：在系统发生问题时快速解决问题</code></pre><p><strong>2. 自动化、系统化、数据化</strong><br>SRE不是在做一种收尾型、擦屁股的工作，而是在做一种探索性、前瞻性的工作，但SRE不可避免的，会面对很多重复性的工作，所以除了要在组织和机制上做好分工，让恰当的人做恰当的事之外，SRE人员要经常思考产品的系统化和弹性化，要常常思考下面几个问题：</p><ul><li>常常思考产品和系统哪里有问题，如何优化，如何体系化？</li><li>常常思考有没有更好的办法，有没有提高效率的办法？</li><li>常常思考如何让稳定性本身更加有价值，有意义？</li></ul><p>这3个问题，我觉得可以从3个方面着手：</p><p>1，【自动化】<br>这里自动化，包括自动和自助2个部分。自动是指能够系统能够对一些异常自动恢复、自动运维，这部分，也可以叫做“弹性”，它一方面包括兜底、容灾，另一方面也包括智能化、机器人和规则判断。比如，对一些可能导致问题的服务失败，能够自动走兜底处理逻辑，能够建立一个调度任务，自动对这部分数据进行调度处理；对一些机器的load飚高、服务抖动等，能自动重启，自动置换机器。<br>自助是让你的客户自己动手，通过提供机器人，自动识别订单类型，自动排查订单状态和节点，自动告知服务规则特征，自动匹配问题类型给出排查结果或排查过程等。<br>Google SRE设置了一个50%的上限值，要求SRE人员最多只在手工处理上花费50%的时间，其他时间都用来编码或者自动化处理。这个可以供我们参考。<br>2，【系统化】<br>系统化，可以体现在SRE工作的方方面面，我觉得，可以主要在“监控、链路治理、演练” 3方面入手。这3个方面也正好对应着“发现问题、解决风险、因事修人” 3个核心。通过系统化，目的是让我们SRE的工作形成体系，不再是一个个“点”的工作，而是能够连成“面”，让SRE工作不再局限于“后期保障/兜底保障”，而是能够通过监控体系、链路风险、演练体系发现问题。<br>监控、链路治理和演练的系统化，将在后面的章节中详细探讨。<br>3，【数据化】<br>稳定性工作，如果要拿到结果，做到可量化，可度量，就一定要在数据化上下功夫，这个数据化，包括如下几个方面：</p><ol><li>数据驱动：包括日志标准化和错误码标准化，能够对日志和错误码反馈的情况进行量化；</li><li>数据对账：包括上下游对账、业务对账，能够通过对账，保障域内数据校准；</li><li>轨迹跟踪：包括变更轨迹和数据轨迹，目标是实现数据的可跟踪，和变更的可回溯、可回滚；</li><li>数据化运营：主要是将稳定性的指标量化，比如工单解决时间、工单数、报警数、报警响应时间、故障风险数、代码CR量，变更灰度时长等，通过量化指标，驱动团队同学建立量化意识，并且能给老板一份量化数据。<br>1.4.3，疑惑3：稳定性似乎总是新人的垃圾场</li></ol><p>虽然前文中说过，对于团队而言，最好不要让新人从事稳定性工作，但是稳定性毕竟是很多希望“专注工作”的开发人员不愿意做的，这个时候，团队leader很容易做出让一个刚进入团队的人从事稳定性工作，毕竟其他核心开发岗位的人似乎对团队更加重要，也不能调开去从事这种“重要不紧急”的工作，不是吗？<br>所以这个时候，新人被安排了稳定性工作，也是敢怒不敢言，充满抱怨的做已经约定好的工作，或者浑浑噩噩的划划水，只在需要“应急”的时候出现一下。<br>这个现状要解决，就要涉及到一个人的“被认可度”，也是我们经常说一个人的价值（在个人自我感知上，我们认为这是“成就感”），很多人可能觉得一个人是因为有价值，才会被认可。而我认为，一个人是因为被认可，才会觉得自己有价值，这样才会产生做一件事情的成就感。<br>毕竟，能一开始就找到自己喜欢并且愿意去创造价值的事情，是很少的。大多数人是在不情不愿的去做自己并不知道方向也无所谓成败的事情。这个时候，是做的事情被认可，让自己感觉有价值，产生兴趣，而不是反过来，爱一行做一行是幸运的，做一行爱一行是勇敢的。<br>那么对于稳定性的新人，如果你“被安排”从事了稳定性，那么首先要注意下面3个点：<br>1，对于稳定性新人，一定要优先考虑如何响应问题，而不是如何解决问题<br>2，稳定性从来都不是简单的，他的关键，是要做细，这需要细心和耐心<br>3，稳定性不是一个人的事情，要团结团队内的同学，上下游的同学<br>在有了上面3点心理建设之后，要开始在自己的心里，构建3张图，3张表：<br>3张图：</p><pre><code>1，系统间依赖图（也包括业务时序，熟悉业务流程），参考5.4节系统依赖梳理方法；2，流量地图（知道上下游系统，团队内系统的流量关系和流量水位，也同时把控系统架构），参考5.3节流量地图；3，系统保障图（知道稳定性保障的步骤和打法），参考5.2节作战地图；</code></pre><p>3张表:</p><pre><code>1，机器资源表（做到占用多少资源，了然于胸，团队需要时能拿得出来），参考第4章资源管控2，异常场景应急表（出现问题时知道怎么应对，演练知道哪里容易出问题），参考3.2节故障场景梳理；3，业务近30日单量表（知道哪些业务影响大，哪些业务是重点），参考6.1节黄金链路治理；</code></pre><p>心中3张图，3张表，可以让自己心中有数，不会抓瞎，这就像林彪在《怎样当好一个师长》一文中写的那样，心里要有个“活地图”。这样，一个新人才能快速熟悉起团队的业务和系统，明白风险在哪里，要往哪里打。才能让自己的工作变得被认可，直击痛点，有价值。<br>2，监控<br>再牛的SRE，也不可能对整个复杂系统了如指掌，也不可能做到对每次变更和发布，都在掌控之内，所以对于SRE人员来说，就必须要有一双敏锐的“眼睛”，这双“眼睛”，无论是要快速响应，还是要发现风险，都能快速发现问题，这就是“监控”。<br>从运维意义上讲，“发现问题”的描述 和 “监控”的实现之间的对应关系如下：<br>发现问题的需求描述<br>监控的实现<br>减少人力发现成本<br>自动监控、多种报警手段<br>及时、准确<br>实时监控、同比、环比、对账<br>防止出错<br>减少误报、同比环比、削峰<br>不遗漏<br>减少漏报，多维监控<br>直观评估影响面<br>对账&amp;统计<br>2.1，监控的5个维度<br>监控的核心目标，是快速发现“异常”。那如何定位异常呢？是不是低于我们设置的阈值的，都是异常？如果要是这么定义的话，你会发现，报警非常多，应接不暇。<br>要定义异常，就要考虑一个问题：兼容系统的弹性，也就是系统要有一定的容错能力和自愈能力，不然就会非常脆弱和敏感。因此，我对“异常”的定义，是：在服务(体验)、数据、资金3个方面中至少1个方面出现了损失 或 错误。我认为，一个系统，如果在下面3个方面没有出现问题，那么即使中间过程出现了偏差，或者没有按既定路径达到最终结果，我也认为没有出现“异常”（这也是一种弹性）：<br>• 在服务方面没有异常（我把服务错误造成的用户体验，也认为是服务异常）<br>• 在数据上没有出错（我把订单超时等体验，也认为是数据出现了偏差）<br>• 在资金上没有资损（走了兜底逻辑，且按照业务可接受的预定范围兜底造成的损失，不算资损，如兜底运费）</p><p>所以监控一个系统是否具有健壮性（即：弹性(Resilient)，这一点在后面【弹性建设】中详细论述），就要从这3个最终目标去实现，为了达到这3个目标，我们可以从 系统自身、服务接口、业务特征、数据、资金对账 5个维度保障监控的准确性。<br>下图详细解释了这5个维度：<br>2.2，监控大盘<br>建立监控大盘的目的，是在大促等关键时期，在一张图上能够看到所有的关键指标。所以大盘的key point应该是“直观简洁、指标核心、集中聚焦”。在大盘上，我认为要包括以下要素：</p><ol><li>最核心业务入口的qps、rt、错误数、成功率，从这个维度可以看到入口流量的大小和相应时间，成功率。这一点，是在知道入口的健康情况；</li><li>错误码top N，这个维度可以看到系统运行过程中最核心的错误，快速直观定位问题原因（这个需要打通上下游错误码透传）。这一点，是在快速知晓问题出在哪里；</li><li>按业务维度（业务身份、行业、仓储、地区等，根据实际需要决定）分类统计计算的单量、或分钟级下单数量，用于确定核心业务的单量趋势。这一点，只在知道自身业务的健康情况；</li><li>核心下游依赖接口、tair、db的qps、rt、错误数、成功率，需要注意的是，这个一般比较多，建议只放最核心、量最大的几个。这一点，是在知道下游依赖的健康情况；</li><li>其他影响系统稳定性的核心指标，如限单量，核心计数器等，根据各个团队的核心来决定。这一点，是在个性化定义关键影响点的监控情况；<br>2.3，避免监控信息爆炸</li></ol><p>在SRE的实践过程中，为了保证监控的全面，往往会增加很多报警项，报警多了之后，就会像洪水一样，渐渐的SRE对于监控就不再敏感了，让SRE比较烦恼的一个问题，就是如何做监控报警瘦身？<br>目前一般来说，我们的监控报警至少包括2种方式：</p><ol><li>推送到手机的报警，如电话、短信报警；</li><li>推送到钉钉的报警，如报警小助手、报警；<br>我个人的建议是：</li></ol><p>1，【谨慎使用电话报警】，因为这会让人非常疲惫，尤其是夜间，而且容易导致接收者将电话加入骚扰拦截，当真正需要电话报警的时候，就会通知不到位；因此电话报警，一定要设置在不处理要死人的大面积/关键问题上；<br>2， 【设置专门的唯一的钉钉报警群】一定一定要建设专门钉钉报警群，而且1个团队只能建1个群，中间可以用多个报警机器人进行区分。报警群的目的只有1个：让所有的报警能够在这个群里通知出来。只建一个群，是为了报警集中，且利于值班同学在报警群中集中响应。<br>3，【报警留底】所有报警，一定要能留底，也就是有地方可以查到历史报警，所以建议所有报警，不管最终用什么方式通知，都要在钉钉报警群里同时通知一份，这样大家只看这个群，也能查到历史报警。在进行复盘的时候，历史报警作用非常关键，可以看到问题发现时间，监控遗漏，问题恢复时间。<br>4， 【日常报警数量限制】一般来说，如果一段时间内，报警短信的数量超过99条，显示了99+，大家就会失去查看报警的兴趣，因此，一定要不断调整报警的阈值，使其在业务正常的情况下，不会频繁报警。在盒马履约，我们基本可以做到24小时内，报警群内的报警总数，在不出故障/风险的情况下小于100条；这样的好处是明显的，因为我们基本上可以做到1个小时以上才查看报警群，只要看到报警群的新增条数不多（比如只有10条左右），就能大致判断过去的一个小时内，没有严重的报警发生；减少报警的方法，可以采用如下手段：</p><ul><li>对于系统监控报警，采用范围报警，比如load，设置集群内超过N %且机器数大于M的机器load都升高了才报警。毕竟对于一个集群而言，偶尔一台机器的load抖动，是不需要响应的。</li><li>对于业务报警，一定要做好同比，不但要同比昨天，还要同比上周，通过对比确认，对于一些流量不是很大的业务来说，这一点尤其重要，有些时候错误高，纯粹是ERROR级别日志过度打印，所以只要相对于昨天和上周没有明显增加，就不用报警；</li><li>对于qps、rt等服务报警，要注意持续性，一般来说，要考虑持续N分钟，才需要报警，偶尔的抖动，是不用报警的。当然，对于成功率下跌，异常数增加，一般要立即报出来；</li><li>复合报警，比如一方面要持续N分钟，一方面要同比昨天和上周，这样来减少一些无需报警的情况；</li><li>根据需要设置报警的阈值，避免设置&gt;0就报警这种，这种报警没有意义，一般来说，如果一个报警，连续重复报10条以上，都没有处理，一般是这个报警的通知级别不够，但是如果一个报警，重复10条以上，经过处理人判断，不需要处理，那就肯定是这个报警的阈值有问题；<br>5，【报警要能够互补】我们经常提到监控的覆盖率，但是覆盖还是不够的，因为监控可能出现多种可能性的缺失（丢日志、通信异常等），因此要能够从多个维度覆盖，比如，除了要直接用指标覆盖qps，还需要通过日志来覆盖一遍，除了要用日志覆盖一些订单趋势，还要从db统计上覆盖一遍，这样一个报警丢失，还至少有另外一个报警可以backup；</li></ul><p>2.4，有效发现监控问题<br>作为一个SRE人员，很容易发现一个点，如果有几次线上问题或报警响应不及时，就会被老板和同事质疑。同样的，如果每次线上问题都能先于同事们发现和响应，就会赢得大家信任，那要如何做到先于大家发现呢？我的建议是：像刷抖音一样刷监控群和值班群；<br>一般来说，一个团队的稳定性问题在3类群里发现：BU级消防群、团队的监控报警群、业务值班群；所以没有必要红着眼睛盯着监控大盘，也没必要对每个报警都做的好像惊弓之鸟，这样很快自己就会疲惫厌烦。<br>我的经验是按下面的步骤：</p><ol><li>首先当然是要监控治理，做到监控准确，全面，然后按照前面说的，控制报警数量，集中报警群，做到可控、合理；</li><li>然后像刷抖音一样，隔三差五（一般至少1个小时要有一次）刷一下报警群，如果报警群里的新增条数在20条以内，问题一般不大，刷一刷就行；</li><li>如果突然一段时间内报警陡增，就要看一下具体是什么问题了，小问题直接处理，大问题分工组织协调；</li><li>消防群中的问题，要及时同步到团队中；</li><li>值班群中的工单，需要关注，并有一个初步的判断：是否是大面积出现的业务反馈；是否有扩大的隐患；<br>要做到“有效”两个字，SRE人员，需要有一个精确的判断：当前报警是否需要处理？当前报警是否意味着问题？当前报警的影响范围和涉及人员是谁？当前工单/问题是否可能进一步扩大，不同的判断，采取的行动是不同的。</li></ol><p>3，故障应急<br>前面1.4.1中，有提到如何及时、快速的响应，这一点是作为SRE人员在故障应急时的关键，也是平时处理线上问题的关键。除此之外，在应对故障方面，还有很多事情需要做。<br>3.1，系统可用性的定义<br>ufried 在2017年的经典弹性设计PPT：《Resilient software design in a nutshell》中，对系统可用性的定义如下：<br>可见，影响系统可用性的指标包括2个方面：MTTF（不出故障的时间）和MTTR（出故障后的恢复时间），所以，要提高系统可用性，要从2个方面入手：1，尽量增加无故障时间，2，尽量缩短出故障后的恢复时间；<br>对故障应急来说，也要从这两个方面入手，首先要增加无故障时间，包括日常的风险发现和风险治理，借大促机会进行的链路梳理和风险治理。只有不断的发现风险，治理风险，才能防止系统稳定性腐烂，才能增加无故障时间。<br>其次，要缩短出故障之后的恢复时间，这一点上，首先要把功夫花在平时，防止出现故障时的慌张无助。平时的功夫，主要就是场景梳理和故障演练。<br>3.2，场景梳理<br>故障场景梳理，重点在于要把可能出现故障的核心场景、表现、定位方法、应对策略梳理清楚，做到应对人员烂熟于心，为演练、故障应急提供脚本；<br>业务域<br>关键场景<br>问题表现<br>问题定位<br>止血措施<br>预案执行<br>业务影响<br>上游影响<br>下游影响<br>数据影响（操作人）<br>服务侧、业务侧应对策略<br>产品端应对策略<br>相关域，要分别梳理上游和下游<br>服务场景，每行列出一个场景，要列出所有可能的场景<br>逐条列出当前场景的所有可能表现<br>对应前面的问题表现，列出每一个表现的定位方法和指标<br>对每个定位的原因，给出快速止血的措施<br>逐条列出可以执行的预案<br>逐条列出可能导致的业务影响和严重程度、范围<br>逐条列出在上游的影响<br>逐条列出对下游的影响<br>逐条列出数据的影响（qps、rt、单量），以及捞取数据的方法、订正数据的方法<br>列出服务端、业务侧的应对话术和退款、赔付、补偿方案；<br>列出产品侧对业务侧的沟通方案、客服话术、产品降级方案；<br>通过这种程度的梳理，SRE以及其掌控的故障应对人员，能够快速的明确发生问题的场景，以及场景下的影响、表现、定位方法、应对策略。当然，如果要把这些场景牢记，做到快速应对，就需要依靠：演练。<br>3.3，故障演练<br>演练对故障应急无比重要，但是，我个人十分反对把演练作为解决一切问题的手段。演练本身，应该是验证可行性和增加成熟度的方式，只能锦上添花，而不能解决问题，真正解决问题的应该是方案本身。<br>• 不要进行无场景演练：有些演练，不设置场景，纯粹考察大家的反应，这种演练，上有政策下有对策，表面上是在搞突然袭击，其实已经预设了时间段，预设了参加的域，不太可能做到完全毫无准备，到了演练的时间点，大家可以通过死盯着报警群，调整各种报警阈值的方式，更快的发现问题；而且完全无场景的演练，一般只能演练如fullGC，线程池满，机器load高，接口注入异常，对于一些数据错误，消息丢失，异步任务积压等场景，很难演练。<br>针对性的，我建议多进行场景演练，各域要提前进行3.2节这种详细的场景梳理，通过场景攻击，提高大家的应对成熟度。事实上，现在横向安全生产团队不对各个业务团队进行场景攻击的原因，也是因为横向安全生产团队自己也不熟悉各个业务团队的业务场景，这个就需要加强对业务场景攻击方式的规范化，横向安全生产团队也要加强机制建设，让纵向业务团队能够产出场景，而不是每次都在线程池、fullGC、磁盘空间这些方面进行攻击。<br>• 也不要无意义的提速演练：演练本身虽然确实有一个重要目的是提高应对熟练度，但是不同的业务是有区别的，有些业务的发现本身，就不止1分钟（比如某些单据积压场景，消息消费场景），这些场景，如果不参加评比，或者流于形式了，就会让攻击本身没有意义。<br>针对性的，我建议各个业务根据各自的特点，定制演练。如：普通电商业务，关注下单成功率，有大量的实时同步调用；新零售业务，关注单据履约效率，有大量的异步调度；每个业务，根据实际场景和业务需要，制定“有各自特色的要求”的演练标准，演练不一定要千篇一律，但是一定要达到业务的需求标准。这样也更加有利于演练场景的落地，有利于蓝军针对性的制定攻击策略。<br>各个SRE同学，不管大的政策怎么样，还是要关注团队内部的场景本身：</p><ol><li>对于系统性故障注入（load、cpu、fullGC、线程池等），直接套用集团的mk注入即可；</li><li>对于服务型故障注入（下游异常、超时，接口超时、限流），mk也有比较好的支持；</li><li>对于订单异常型故障注入，要自主开发较好的错误订单生成工具，注入异常订单，触发故障报警；</li><li>对于调度、积压型故障注入，要关注schedulex、异步消息的故障注入方式，同时防止积压阻塞正常订单影响真正的线上业务；<br>同时，在演练前后，要注意跟老板的沟通，要让老板理解到你组织的演练的目标和效果，不然就不是演习，而是演戏了。要和老板的目标契合，在演练过程中，通过演练提高大家对业务场景的理解深度和对问题的应对速度，增加大家的稳定性意识，达到“因事修人”的目的。</li></ol><p>3.4，故障应急过程<br>如果不幸真的产生了故障，作为SRE，要记得如下信息：</p><ol><li>冷静。作为SRE，首先不能慌，没有什么比尽快定位和止损更重要的事情</li><li>拉电话会议同步给大家信息。记住，在出现故障时，没什么比电话会议更加高效的沟通方式了；</li><li>参考前面1.4.1节中的SRE人员快速响应流程，在电话会议中同步给大家：<br>• 尽快告知当前告警已经有人接手，是谁接手的，表明问题有人在处理了。（这一步叫“响应”）</li></ol><p>• 组织人员，快速定位问题，告知问题初步定位原因（这一步叫“定位”）。<br>• 初步影响范围是什么？给出大致数据（这一步方便后面做决策）<br>• 有哪些需要老板、产品、业务方决策的？你的建议是什么？（这一步很关键，很多时候是：两害相权取其轻，你的评估和建议，直接影响老板的决策）<br>• 当前进展如何，是否已经止血？（这一步是“恢复”，要给出“进展”，让决策者和业务方了解情况）</p><ol><li>组织大家按照故障场景梳理的应对方案进行应对，如果没有在故障场景列表中，一定要组织最熟练的人员进行定位和恢复。</li><li>故障过程中，对外通信要跟团队和老板统一评估过再说；</li><li>处理故障过程中，要随时组织同学们进行影响数据捞取和评估，捞出来的数据，要优先跟老板、业务熟练的同学一起评估是否有错漏。</li><li>在处理完故障后，要及时组织复盘（不管GOC是不是统一组织复盘，内部都要更加深刻的复盘），复盘流程至少包括：详细的时间线，详细的原因，详细的定位和解决方案，后续action和改进措施，本次故障的处理结果。<br>我个人其实不太赞同预案自动化和强运营的故障应急方案，这一点也是给安全生产同学的建议，比如预案自动化，有很强的局限性，只有在明确预案的执行肯定不会有问题、或者明显有优化作用的情况下，才能自动执行。否则都应该有人为判断。</li></ol><p>强运营类的工作，会导致人走茶凉，比如GOC上自动推送的预案，故障场景关联的监控这种，一方面应该尽量减少强运营的工作，另一方面应该定期组织维护一些必要预案。<br>3.5，与兄弟团队的关系<br>如果兄弟团队发生故障，一定注意：</p><ol><li>不能嘲笑别人，看笑话；</li><li>不能当没事人，高高挂起，要检查自身；</li><li>不能话说的太满，比如说我肯定没故障。<br>尤其是1和3，非常邪性，嘲笑别人的团队，或者觉得自己万事大吉，很容易沾染故障。（其实本身是由科学依据的，嘲笑别人的，一般容易放松警惕）</li></ol><p>4，资源管控<br>作为一个SRE，在资源管控领域，一定要保证自己域有足够的机器，同时又不会浪费太多。我个人的建议是，核心应用，应该控制load在1-1.5左右（日常峰值或A级活动场景下），控制核心应用在10个以内，非核心应用，应该控制load在1.5-2左右（日常峰值或A级活动场景下）。目前集团很多应用load不到1，甚至只有0.几，其实很浪费的。<br>同时，一个团队的SRE，至少随时手上应该握有20%左右的空余额度buffer，方便随时扩容，或者应对新业务增长。这些额度，目前按照集团的预算策略，只要不真的扩容上去，都是不收费的，所以应当持有。<br>除了机器以外，tair、db、消息、精卫等，也要如上操作，除了年初准备好一年的预算，还要额外准备20%左右的buffer。<br>SRE要自己梳理一份资源表，表中一方面要明确有哪些资源，余量多少，另一方面要明确资源的当前水位、压力。<br>比如机器资源，要关注当前机器数、额度、load，如：<br>再比如对数据库资源，要关注数据库的配置、空间、日常和峰值qps、单均访问量（创建一个订单，要读和写DB多少次，这一点很关键）：<br>5，大促保障机制<br>对于SRE来说，大促、尤其是双十一，是一年一度的沉淀系统、防止腐烂、摸清水位、剔除风险的最佳时机，所以一定要加以利用。对于团队的其他开发，大促是一次活动而已，对于SRE来说，大促是系统稳定性提升的过程，与其说是保障大促稳定，不如说是“借大促，修系统”。<br>所以，一定要转变思想，我们做大促保障，不仅是要“保持系统稳定性”的，更是要“提升系统稳定性”的。保持重在压测、摸排水位、限流、预案，而提升重在链路排查、风险治理、流量提升、兜底容灾。<br>一般的A级大促，或间隔较小的S级大促，应以“保持”稳定为主，但对于双十一、618，这种S级大促，应当以“提升”稳定为主；下面，我们重点就要介绍如何借助大促的机会，来提升系统稳定性。<br>5.1，业务团队大促保障的一般流程<br>一个完善的大促保障包括如下步骤（可参考下面的作战地图）：</p><ol><li>明确本次大促的作战地图，明确时间节点和步骤；</li><li>输入活动玩法和节点，明确关键时间点和GMV目标、单量峰值；</li><li>SRE产出备战报告，其中包括保障目标，大促保障时间节奏，作战地图，流量地图（如果已经绘制出来的话），资源规划地图，业务新的变化和技术的挑战，上下游链路依赖图，核心风险和专项分工（精确到人和完成时间），同时SRE要指定监控、压测、演练、预案专项负责人（leader要为SRE放权和背书）。</li><li>SRE绘制流量地图，明确接口流量，模块链路，关键风险。</li><li>SRE和开发同学共同梳理上下游接口依赖流量和峰值，给出限流阈值并沟通上下游；</li><li>开始链路梳理，一般由熟悉业务和系统的开发同学梳理，然后拉上上下游、梳理同学、测试同学、SRE、leader一起review，review时，SRE要产出5个点：强弱依赖、风险点、限流、降级预案、新业务特征；</li><li>根据梳理出来的风险点展开集中治理，大的风险点要开专项治理，这一阶段要全员听调，风险点要各自去做，SRE只负责把控全局、跟踪进度、验收结果。</li><li>治理完成后，开展监控走查，更新监控大盘，建议由SRE指定监控专人负责；</li><li>压测开始前配置限流，压测过程中还要不断根据情况调节限流值；</li><li>开始压测，分为专项重点压测（一般单接口、单机），上下游压测，全链路压测，建议由SRE指定压测责任人；</li><li>录入预案，并对预案进行测试和验证，拉上业务、产品、测试一起，组织预案演练，验证预案可行性，要求业务方知晓预案执行后的影响。建议由SRE指定预案和演练责任人。</li><li>上述过程中都要记录未完成点和check点，在大促前，要对checklist 逐项check；</li><li>产出作战手册，包括值班表，工具清单，大促期间作战流程（精确到分钟级的操作时间点和人员），再次通知业务侧相关预案的影响。</li><li>大促开始前一天，SRE要进行战前宣讲，一般包括大促期间发布流程、审批流程、白名单人员名单，工单汇报方法，大促交流群，大促期间的红线和注意事项。</li><li>大促结束后，要进行复盘，复盘内容包括：目标是否达到，大促期间达到系统指标、单量，系统、业务的能力亮点，大促保障期间大家做的工作汇总，保障期间的产出亮点，后续action项，未来保障的思考和计划。<br>5.2，作战地图</li></ol><p>下面一张图，比较详细的介绍了整个双十一作战的过程：一次大促保障，大致分为前期的容量评估和准备阶段，中间的系统健壮性提升阶段，后面的压测摸底、预案演练阶段，最后的战前check和值班阶段。其中前2个阶段才是核心。<br>在这张作战地图中：</p><ol><li>容量准备阶段，重在根据业务活动节点，输入流量和单量，梳理上下游流量压力，绘制流量地图，统计上下游接口压力，评估限流阈值和资源缺口，同时准备资源。这个阶段是关键阶段</li><li>系统健壮性提升阶段，重在链路梳理和风险发现，对发现的风险进行专项治理。这个阶段是最核心阶段</li><li>在压测阶段，不能只被动的参加全链路压测，还要优先在自己域内进行单点压测和上下游链路压测，这样在全链路压测的时候才不会掉链子。同时，压测要关注几个事情：1，有没有达到流量预期；2，有哪些点是瓶颈点；3，对瓶颈点如何解决，如何降级或限流？4，上下游在压测过程中有没有可能与自己域相互影响的地方；</li><li>在预案&amp;演练阶段，要注重实效，不要走形式化，演练的目的是验证预案的可行性和可操作性，不是为了证明“我演练过了”，如果预案不进行演练，在大促时就有可能出现：1，不敢执行预案，因为不知道会发生什么；2，执行了预案之后发现有坑；3，执行了预案之后发现无效；</li><li>在战前准备阶段，重在检查和宣讲，检查要查缺补漏，要有checklist，一项项check；宣讲要有作战手册和红线，作战手册要精确到时间点和人，每个时间点由谁做什么要明确；红线一定要宣讲清楚，大促是一场战役，如何报备，如何响应工单，各自分工是什么，哪些红线不能踩，要明确到位，避免低级骚操作带来的风险。<br>下面重点介绍容量准备阶段（流量地图和评估）和系统健壮性提升阶段（梳理和治理）；</li></ol><p>5.3，流量地图&amp;流量评估<br>绘制流量地图的目的，是让SRE人员对于域内和上下游流量有明确的了解，在心中有一张全局流量的“图”。流量地图应该包括几个要素：</p><ol><li>系统核心模块和模块间的依赖关系；（用方块和连线标识）</li><li>核心功能流量流向；（用不同颜色的连线区分）</li><li>核心接口/功能的单量或qps；（在接口上方标注）</li><li>链路上的主要风险点；（用红色方块标注）<br>在进行流量评估时，关键在于要明白上下游依赖、是否有缓存、平时单量和大促单量：</li></ol><p>本域应用名<br>对方域<br>对方应用host<br>业务场景<br>服务接口<br>对方owner<br>依赖方式、强弱<br>日常峰值<br>大促预估峰值<br>缓存击穿情况下的悲观峰值<br>appname<br>商品域<br>xxx-itemhost，精确到host<br>时间片商品查询<br>service.method~params<br>某某<br>hsf、强，有缓存<br>1000<br>2000<br>5000<br>e.g.<br>流量评估除了要跟域内对齐外，更加重要的是上下游的沟通，这一点非常重要，要相互明确各自域的瓶颈、限流、承诺；在对上做出流量承诺的时候，一定要优先考虑保护自己域；在对下提出流量要求的时候，要同时提出有保护措施（如缓存）情况下的正常流量和无保护措施下最糟糕流量。<br>5.4，链路梳理&amp;治理<br>这个是SRE能够借双十一提升系统性能的重中之重，一般，我建议遵循下面的方法：<br>大促保障准备时间有限，所以不能等全都梳理完了之后才开始治理。一般来说是先根据经验和日常的发现，治理一波，同时进行梳理，然后对梳理进行review时，发现新的风险点，并进行治理。<br>一般由熟悉业务和系统的开发同学梳理，然后拉上上下游、梳理同学、测试同学、SRE、团队leader一起review，review时，SRE要重点关注5个点：<br>• 强弱依赖<br>• 风险点<br>• 限流<br>• 降级预案<br>• 新业务特征<br>功能<br>细粒度场景<br>风险点action<br>限流<br>降级预案<br>强弱依赖<br>新业务<br>功能描述，根据功能进行划分<br>将场景细粒度细分，列表化<br>可能存在的风险点<br>当前场景的限流评估<br>当前场景可能存在的降级预案<br>上下游间的强弱依赖，强依赖的要关联预案、限流、降级等<br>当前场景从上次大促到现在的增量变更<br>梳理的目的不是仅仅评估风险，更重要的是治理，治理不一定是代码层面的升级，可能有如下方式：</p><ol><li>对可能有流量尖峰、造成系统冲击的接口，加特殊限流，如全局限流、线程数限流；</li><li>对流量尖峰，加dts等异步任务，进行削峰填谷；</li><li>对需要做降级的地方加降级预案；</li><li>对需要兜底容灾的地方加自动兜底；</li><li>对强依赖的下游接口，加本地缓存或tair缓存（慎之又慎，同时下游绝对不能期待上游加缓存来降低访问量）；</li><li>治理的时候，要注意几个最容易出问题的地方：缓存、异步消息、异步任务、数据库量级、数据库关联查询量或批量更新量（比如1主单关联N子单的情况）、接口超时时间、重试次数、幂等、sql limit和查询上限；</li><li>需要提前禁写的，要产出禁写预案</li><li>对大促期间可能随时调整的业务参数，要留出口子，方便调整，做好审批流程报备流程、check流程；<br>在链路治理时，建议可以建立一个aone项目空间或者lark表格，将所有要做的事情，列成工作项，每完成一项勾掉一项，当所有工作项都完成时，项目就结束了，治理也达到了一个水平。</li></ol><p>5.5，压测<br>在大促保障过程中，提到压测，有同学可能习惯性的将其归结到“全链路压测”中，这个是不全面的，压测应该贯穿整个大促保障的过程。<br>压测分为3种，分别是单点压测（如：单机压测和单接口压测），单链路压测，全链路压测。<br>• 单点压测的目标就是考察单点性能，主要用于发现单机或者单接口的性能水位和性能热点，为了后面计算限流阈值，评估集群规模做准备；应该在大促前就开始，随时可以进行，不一定要在正式环境。如果只是发现性能热点，可以使用火焰图分析；如果是评估性能水位，就要进行摸高，一般对于4核8g的机器，至少要摸到load 到5，cpu利用率到75%，内存到70%，线程数超过800个，这4个指标至少要达到1个才能算。<br>• 单链路压测的目标是考虑单链路多个应用间多级调用的性能，用于评估某个功能点的水位，为的是应对活动过程中某个业务的量级，评估的目标是整个链路中至少一个节点达到性能瓶颈为止。整个链路的性能水位，是由最低的那个节点接口/应用 决定的。在压测过程中，建议摸高，一直摸到其中一个节点达到瓶颈。单链路压测建议在业务给出活动目标后，就要梳理出核心链路有那几条，这几条核心链路是一定要压测的。需要注意的是，如果有非核心链路影响了核心链路，那么大促时这个非核心链路要么做降级预案，要么改为核心链路死保。<br>• 全链路压测的目标是预演，而不是压测，这一点跟前面的压测是完全不同的，需要特别注意，虽然全链路压测本身也会摸高，但是它摸高是预设目标的，比如本次大促50wqps创建，预计摸高也就是120%，不会一直摸高。全链路压测其实是在将各个团队压测的结果进行汇总并验证，所以现在全链路压测一般要去一次通过。这就要求各个团队要提前调通内部各条单链路。<br>另外，如果链路有涉及外部partner，建议一定要在压测中走通，不要轻易相信外部partner自己的压测结果，也不能认为“他们承诺了，他们要做到”，这种想法，在大促的时候，partner的承诺无法做到的比比皆是，不要最后坑了自己的业务，导致业务单子积压。<br>6，日常稳定性机制<br>对于SRE来说，在日常稳定性保障过程中，建立一个行之有效的机制体系，比事必躬亲更加重要。一般来说，SRE在日常稳定性保障过程中，要建立的机制如下：</p><ol><li>黄金链路识别治理机制</li><li>值班机制</li><li>复盘机制</li><li>日常资源（机器、中间件）管控和记账机制</li><li>日常风险和问题报备机制</li><li>团队权限管控机制</li><li>日常演练机制<br>下面重点说一下黄金链路和值班：</li></ol><p>6.1，黄金链路治理<br>黄金链路，就是核心链路，或者说，是团队的生命线链路，由最核心的应用，最关键的DB，最需要死保的接口，支撑的最核心业务。所以黄金链路的治理就一个目标：不要让非核心的东西，影响了核心的；这里的“东西”，包括业务、系统、db；<br>黄金链路治理机制，是要在日常工作中，做如下工作：</p><ol><li>每隔一段时间（半个月左右），要统计一次线上订单各业务单量，有条件的团队，建议分配专门负责数据驱动的人员，建立fbi报表，或者建立数据分析系统。这些数据产出的目标，是统计业务的量级和趋势，报告最核心单量最大的业务、最容易出问题的业务、和发展最快的业务；</li><li>对业务和应用链路，按重要性进行划分。把重要的挑出来，把不重要的，不死人的，可以降级的摘出去；</li><li>开始治理，要求核心链路上的系统，不能依赖非核心的接口，不能依赖非核心的db。非核心链路上的任何降级措施，不能影响核心链路的功能；</li><li>核心链路和非核心链路，也不能依赖共同的基础组件，注意，核心链路不等同于核心应用，现在很多核心应用里的非核心功能太多了。导致每次改非核心功能，要发布核心应用，甚至两者共用底层组件，导致底层发布影响上层。比如，非核心功能要改一个消息发送接口，正好核心功能也依赖了这个接口，非核心功能改动里的bug，可能导致核心功能挂掉；</li><li>核心链路和非核心链路，要有2套发布等级，2种监控等级。防止相互影响。<br>黄金链路治理做好了，团队出现高等级（P1/P2）故障的可能性会大大降低。</li></ol><p>6.2，值班机制建设<br>值班，既能让大家熟悉业务，又能让SRE不那么劳累，因此，值班人员一方面要响应报警，另一方面要响应工单。SRE要做的事情，是安排好值班表和值班机制，明确值班人员的职责。一般来说，可以如下建设：<br>• 事前：<br>• 值班人员必须参与故障演练（包括故障止血方法）以及熟练使用各种故障排查工具。<br>• 值班人员需要明确值班的范围，包括预警群、工单群、线上问题反馈群、答疑群等；<br>• 值班人员在值班周期内，应该减少业务工作安排；<br>• 值班人员的值班周期不宜过长或者过短，以一周为宜；<br>• SRE应该尽可能的多值班，只有对业务熟悉的人，才能更加敏锐的发现系统的问题；<br>• 新进入团队的同学，应该先值几个月的轮班，通过值班熟悉业务，是最快的方式；<br>• 事中：不管是工单问题还是报警，如果短时间无法定位原因的情况，立即把相关人员拉入电话会议，如果遇到卡点，需要把接力棒明确交接给下一位，事后再回顾卡点的原因。对于会影响上下游的问题（事故），需要立刻通知上下游，可能引起故障的，需要GOC报备。<br>• 值班人员自己发现问题后，应该第一时间在群里反馈说处理中，签到通知其他人已经在处理<br>• 关闭当前报警的通知（关闭方法集中沉淀到常见问题处理手册），防止电话打爆掩盖其他更重要的报警，事后再重开报警（由当前值班人员保证）<br>• 事后：值班人员和SRE一起组织问题Review，并把涉及到稳定性的操作内容记录在稳定性流程中。对于常见问题的排查沉淀到一处，后续工具化和演练。<br>值得注意的是，值班不应该是简单的人力消耗，应该花费时间开发工具平台，包括问题智能排查、订单详情查询，业务日志轨迹、数据变更轨迹查询，并且开发问题自动排查、问题解决方案自动推荐机器人，做到自动答疑、自助答疑，减少工单数量，提高问题排查效率。<br>7，弹性建设<br>系统的健壮不是没有报警，也不是不出故障，服务、数据、体验都不受影响，我们才认为系统是健壮的。一个系统想要健壮，应该具有一定的弹性，系统的弹性体现在系统是：容灾的、可自愈的、一定程度上容错的、可运营的。<br>这里，有ufried 在2017年的弹性设计PPT：《Resilient software design in a nutshell》，这个PPT非常清晰准确的阐述了“弹性软件设计”的概念，下图取自该PPT：<br>对于我个人的实践而言，我自己倾向于重点做好其中的发现、恢复、预防、缓解4个部分。<br>8，价值建设<br>我想把“价值建设”这个事情，放到本文的最后，作为对SRE人员提升工作信心的鼓励。<br>开发人员容易趋向于实现什么，如何做到，Dev工作的目标，是电脑屏幕和系统功能；而SRE，很多都是电脑外的工作：上下游沟通，流量评估，演练组织，资源调度，故障应急，系统治理。工作内容既有代码里的ifelse，也有Excel上的各种统计，还有PPT上的各种曲线。很多人觉得这样的事情很boring，倾向于做一个专心写代码的美男子，但是一个开发人员，偶尔抬起头看着窗外，心里对那些横跨多域，调度多个资源，影响力辐射多个团队甚至BU的人，也有一丝羡慕和无力感。<br>SRE恰恰给了这样一个机会，它能让一个级别不怎么高的开发人员，调度域内尽可能多的资源，从事多种能够辐射影响力的机会。<br>SRE最容易拿到数据，就要去考虑一个问题：价值导向，当前做的事情，有多少数据量，有哪些是无用的长尾的，有哪些可以下线掉。哪些是造成资损的，哪些是需要推动解决的异常订单，哪些是有巨大价值的。哪些是客户抱怨最多的，耗时耗人力最大的。<br>开发人员容易正向思维，而SRE人员，一定要去做反向思维，通过价值来考量，通过价值反推意义，提效降本。<br>绝大多数情况下，一个好的SRE，和一个差的SRE，差距就体现在3个方面：好的SRE会随时发现和响应问题，会建立体系化，会极其细致的落地解决方案。 这其中，随时发现和响应，是第一态度，是要持之以恒的心态，而不是一个作秀和表现；落地是基础，是保障稳定的生命线，细致的落地，是决定你是走走过场，还是真的做深的根本；体系化是框架，是决定机制能否持久以及你自己累不累的原因。<br>另外，一个团队（尤其是业务团队）的SRE是极度需要老板的鼎力支持和权力背书的，SRE往往没有组织架构上的权力，这个时候，老板真正意义上的支持就很重要，如果一个SRE，你的老板总是口头上跟你说稳定性不能松懈，但是几乎从来不愿意在稳定性上给你支持，一心只想做业务，你就要考虑2个方面：1，是不是你做的不落地，没有得到老板信任；2，换个老板。<br>但是，如果老板愿意给你时间和空间，也愿意投入资源，给你权力，那你就要认真做好SRE这份工作，做出价值。一般来说，老板愿意给你权力的时候，就意味着老板愿意帮你背责任，所以不要怕出错，要先把态度做出来，老板会支持你的。但是注意：老板给你的权力用好了，老板才会给你背责任，用不好，责任就是你自己的。这一点无论在哪个团队，无论在哪个层级，都是一样。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/article/776448?spm=a2c6h.12873581.0.dArticle776448.4f1d6446TFJ8Rn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;h
      
    
    </summary>
    
      <category term="devops" scheme="http://zhang-yu.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhang-yu.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>CPU飙高如何排查</title>
    <link href="http://zhang-yu.me/2020/10/29/CPU%E9%A3%99%E9%AB%98%E5%A6%82%E4%BD%95%E6%8E%92%E6%9F%A5/"/>
    <id>http://zhang-yu.me/2020/10/29/CPU飙高如何排查/</id>
    <published>2020-10-29T03:00:00.000Z</published>
    <updated>2020-11-03T15:15:05.811Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;groupCode=alitech" target="_blank" rel="noopener">https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;groupCode=alitech</a></p><p><a href="https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;groupCode=alitech" target="_blank" rel="noopener">CPU飙高，系统性能问题如何排查？-阿里云开发者社区</a></p><blockquote><p><img src="https://ucc.alicdn.com/pic/developer-ecology/d0cf7464f1af4e23b3543cf5cfb4540e.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h3 id="一-背景知识"><a href="#一-背景知识" class="headerlink" title="一 背景知识"></a>一 背景知识</h3><h4 id="LINUX进程状态"><a href="#LINUX进程状态" class="headerlink" title="LINUX进程状态"></a>LINUX进程状态</h4><p>LINUX 2.6以后的内核中，进程一般存在7种基础状态：D-不可中断睡眠、R-可执行、S-可中断睡眠、T-暂停态、t-跟踪态、X-死亡态、Z-僵尸态，这几种状态在PS命令中有对应解释。</p><p><img src="https://ucc.alicdn.com/pic/developer-ecology/51ff89c423b24966bbd88490cc8ff158.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><ul><li><p>D (TASK_UNINTERRUPTIBLE)，不可中断睡眠态。顾名思义，位于这种状态的进程处于睡眠中，并且不允许被其他进程或中断(异步信号)打断。因此这种状态的进程，是无法使用kill -9杀死的(kill也是一种信号)，除非重启系统(没错，就是这么头硬)。不过这种状态一般由I/O等待(比如磁盘I/O、网络I/O、外设I/O等)引起，出现时间非常短暂，大多很难被PS或者TOP命令捕获(除非I/O HANG死)。SLEEP态进程不会占用任何CPU资源。</p></li><li><p>R (TASK_RUNNING)，可执行态。这种状态的进程都位于CPU的可执行队列中，正在运行或者正在等待运行，即不是在上班就是在上班的路上。</p></li><li><p>S (TASK_INTERRUPTIBLE)，可中断睡眠态。不同于D，这种状态的进程虽然也处于睡眠中，但是是允许被中断的。这种进程一般在等待某事件的发生（比如socket连接、信号量等），而被挂起。一旦这些时间完成，进程将被唤醒转为R态。如果不在高负载时期，系统中大部分进程都处于S态。SLEEP态进程不会占用任何CPU资源。</p></li><li><p>T&amp;t (__TASK_STOPPED &amp; __TASK_TRACED)，暂停or跟踪态。这种两种状态的进程都处于运行停止的状态。不同之处是暂停态一般由于收到SIGSTOP、SIGTSTP、SIGTTIN、SIGTTOUT四种信号被停止，而跟踪态是由于进程被另一个进程跟踪引起(比如gdb断点）。暂停态进程会释放所有占用资源。</p></li><li><p>Z (EXIT_ZOMBIE), 僵尸态。这种状态的进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID等）。僵尸态进程会释放除进程入口之外的所有资源。</p></li><li><p>X (EXIT_DEAD), 死亡态。进程的真正结束态，这种状态一般在正常系统中捕获不到。</p></li></ul><h4 id="Load-Average-amp-CPU使用率"><a href="#Load-Average-amp-CPU使用率" class="headerlink" title="Load Average &amp; CPU使用率"></a>Load Average &amp; CPU使用率</h4><p>谈到系统性能，Load和CPU使用率是最直观的两个指标，那么这两个指标是怎么被计算出来的呢？是否能互相等价呢？</p><p><strong>Load Average</strong></p><p>不少人都认为，Load代表正在CPU上运行&amp;等待运行的进程数，即<br><img src="https://ucc.alicdn.com/pic/developer-ecology/96cb14429ca744a4b69c4b49801225dd.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>但Linux系统中，这种描述并不完全准确。</p><p>以下为Linux内核源码中Load Average计算方法，可以看出来，因此除了可执行态进程，不可中断睡眠态进程也会被一起纳入计算，即：<br><img src="https://ucc.alicdn.com/pic/developer-ecology/16c209bcb66f41508572a829a3081367.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><pre><code>602staticunsignedlongcount_active_tasks(void)603 {604structtask_struct*p;605unsignedlongnr=0;606607read_lock(&amp;tasklist_lock);608for_each_task(p) {609if ((p-&gt;state==TASK_RUNNING610 (p-&gt;state&amp;TASK_UNINTERRUPTIBLE)))611nr+=FIXED_1;612 }613read_unlock(&amp;tasklist_lock);614returnnr;615 }......625staticinlinevoidcalc_load(unsignedlongticks)626 {627unsignedlongactive_tasks; /* fixed-point */628staticintcount=LOAD_FREQ;629630count-=ticks;631if (count&lt;0) {632count+=LOAD_FREQ;633active_tasks=count_active_tasks();634CALC_LOAD(avenrun[0], EXP_1, active_tasks);635CALC_LOAD(avenrun[1], EXP_5, active_tasks);636CALC_LOAD(avenrun[2], EXP_15, active_tasks);637 }638 }</code></pre><p>在前文 Linux进程状态 中有提到过，不可中断睡眠态的进程(TASK_UNINTERRUTED)一般都在进行I/O等待，比如磁盘、网络或者其他外设等待。由此我们可以看出，Load Average在Linux中体现的是整体系统负载，即CPU负载 + Disk负载 + 网络负载 + 其余外设负载，并不能完全等同于CPU使用率(这种情况只出现在Linux中，其余系统比如Unix，Load还是只代表CPU负载)。</p><p><strong>CPU使用率</strong></p><p>CPU的时间分片一般可分为4大类：用户进程运行时间 - User Time, 系统内核运行时间 - System Time, 空闲时间 - Idle Time, 被抢占时间 - Steal Time。除了Idle Time外，其余时间CPU都处于工作运行状态。<br><img src="https://ucc.alicdn.com/pic/developer-ecology/b4155049419b40cd9755adf475011979.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>通常而言，我们泛指的整体CPU使用率为User Time 和 Systime占比之和(例如tsar中CPU util)，即：<br><img src="https://ucc.alicdn.com/pic/developer-ecology/309084761b534400a9cfd95619bad787.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>为了便于定位问题，大多数性能统计工具都将这4类时间片进一步细化成了8类，如下为TOP对CPU时间片的分类。<br><img src="https://ucc.alicdn.com/pic/developer-ecology/228ef627fdeb4c59aaa2769c22ded5d1.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p><ul><li>us：用户进程空间中未改变过优先级的进程占用CPU百分比</li><li>sy：内核空间占用CPU百分比</li><li>ni：用户进程空间内改变过优先级的进程占用CPU百分比</li><li>id：空闲时间百分比</li><li>wa：空闲&amp;等待I/O的时间百分比</li><li>hi：硬中断时间百分比</li><li>si：软中断时间百分比</li><li>st：虚拟化时被其余VM窃取时间百分比</li></ul><p>这8类分片中，除wa和id外，其余分片CPU都处于工作态。</p><h3 id="二-资源-amp-瓶颈分析"><a href="#二-资源-amp-瓶颈分析" class="headerlink" title="二 资源&amp;瓶颈分析"></a>二 资源&amp;瓶颈分析</h3><p>从上文我们了解到，Load Average和CPU使用率可被细分为不同的子域指标，指向不同的资源瓶颈。总体来说，指标与资源瓶颈的对应关系基本如下图所示。<br><img src="https://ucc.alicdn.com/pic/developer-ecology/17e703e3393c453b95848736cdf7ce65.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h4 id="Load高-amp-CPU高"><a href="#Load高-amp-CPU高" class="headerlink" title="Load高 &amp; CPU高"></a>Load高 &amp; CPU高</h4><p>这是我们最常遇到的一类情况，即load上涨是CPU负载上升导致。根据CPU具体资源分配表现，可分为以下几类：</p><p><strong>CPU sys高</strong></p><p>这种情况CPU主要开销在于系统内核，可进一步查看上下文切换情况。</p><ul><li><p>如果非自愿上下文切换较多，说明CPU抢占较为激烈，大量进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。</p></li><li><p>如果自愿上下文切换较多，说明可能存在I/O、内存等系统资源瓶颈，大量进程无法获取所需资源，导致的上下文切换。</p></li></ul><p><strong>CPU si高</strong></p><p>这种情况CPU大量消耗在软中断，可进一步查看软中断类型。一般而言，网络I/O或者线程调度引起软中断最为常见：</p><ul><li><p>NET_TX &amp; NET_RX。NET_TX是发送网络数据包的软中断，NET_RX是接收网络数据包的软中断，这两种类型的软中断较高时，系统存在网络I/O瓶颈可能性较大。</p></li><li><p>SCHED。SCHED为进程调度以及负载均衡引起的中断，这种中断出现较多时，系统存在较多进程切换，一般与非自愿上下文切换高同时出现，可能存在CPU瓶颈。</p></li></ul><p><strong>CPU us高</strong></p><p>这种情况说明资源主要消耗在应用进程，可能引发的原因有以下几类：</p><ul><li><p>死循环或代码中存在CPU密集计算。这种情况多核CPU us会同时上涨。</p></li><li><p>内存问题，导致大量FULLGC，阻塞线程。这种情况一般只有一核CPU us上涨。</p></li><li><p>资源等待造成线程池满，连带引发CPU上涨。这种情况下，线程池满等异常会同时出现。</p></li></ul><p><strong>Load高 &amp; CPU低</strong></p><p>这种情况出现的根本原因在于不可中断睡眠态(TASK_UNINTERRUPTIBLE)进程数较多，即CPU负载不高，但I/O负载较高。可进一步定位是磁盘I/O还是网络I/O导致。</p><h3 id="三-排查策略"><a href="#三-排查策略" class="headerlink" title="三 排查策略"></a>三 排查策略</h3><p>利用现有常用的工具，我们常用的排查策略基本如下图所示：<br><img src="https://ucc.alicdn.com/pic/developer-ecology/335db4d413bb4b1fafceb9d0cb5cdcd9.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>从问题发现到最终定位，基本可分为四个阶段：</p><h4 id="资源瓶颈定位"><a href="#资源瓶颈定位" class="headerlink" title="资源瓶颈定位"></a>资源瓶颈定位</h4><p>这一阶段通过全局性能检测工具，初步定位资源消耗异常位点。</p><p>常用的工具有：</p><ul><li>top、vmstat、tsar(历史)</li><li><ul><li>中断：/proc/softirqs、/proc/interrupts</li></ul></li><li><ul><li>I/O：iostat、dstat</li></ul></li></ul><h4 id="热点进程定位"><a href="#热点进程定位" class="headerlink" title="热点进程定位"></a>热点进程定位</h4><p>定位到资源瓶颈后，可进一步分析具体进程资源消耗情况，找到热点进程。</p><p>常用工具有：</p><ul><li>上下文切换：pidstat -w</li><li>CPU：pidstat -u</li><li>I/O：iotop、pidstat -d</li><li>僵尸进程：ps</li></ul><h4 id="线程-amp-进程内部资源定位"><a href="#线程-amp-进程内部资源定位" class="headerlink" title="线程&amp;进程内部资源定位"></a>线程&amp;进程内部资源定位</h4><p>找到具体进程后，可细化分析进程内部资源开销情况。</p><p>常用工具有：</p><ul><li>上下文切换：pidstat -w -p [pid]</li><li>CPU：pidstat -u -p [pid]</li><li>I/O: lsof</li></ul><h4 id="热点事件-amp-方法分析"><a href="#热点事件-amp-方法分析" class="headerlink" title="热点事件&amp;方法分析"></a>热点事件&amp;方法分析</h4><p>获取到热点线程后，我们可用trace或者dump工具，将线程反向关联，将问题范围定位到具体方法&amp;堆栈。</p><p>常用的工具有：</p><ul><li><p>perf：Linux自带性能分析工具，功能类似hotmethod，基于事件采样原理，以性能事件为基础，支持针对处理器相关性能指标与操作系统相关性能指标的性能剖析。</p></li><li><p>jstack</p></li><li><ul><li>结合ps -Lp或者pidstat -p一起使用，可初步定位热点线程。</li></ul></li><li><ul><li>结合zprofile-threaddump一起使用，可统计线程分布、等锁情况，常用与线程数增加分析。</li></ul></li><li><p>strace：跟踪进程执行时的系统调用和所接收的信号。</p></li><li><p>tcpdump：抓包分析，常用于网络I/O瓶颈定位。</p></li></ul><blockquote><p>相关阅读</p></blockquote><p>[1]Linux Load Averages: Solving the Mystery<br><a href="http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html" target="_blank" rel="noopener">http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html</a><br>[2]What exactly is a load average?<br><a href="http://linuxtechsupport.blogspot.com/2008/10/what-exactly-is-load-average.html" target="_blank" rel="noopener">http://linuxtechsupport.blogspot.com/2008/10/what-exactly-is-load-average.html</a>   </p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://developer.aliyun.com/article/774833?spm=a2c6h.12873581.0.0.2c8f6446nP5yVa&amp;amp;groupCode=alitech&quot; target=&quot;_blank&quot; rel=&quot;no
      
    
    </summary>
    
      <category term="linux" scheme="http://zhang-yu.me/categories/linux/"/>
    
    
      <category term="linux" scheme="http://zhang-yu.me/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第一篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E4%B8%80%E7%AF%87/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第一篇/</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:00:21.573Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p>互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第一篇  </p><blockquote><h1 id="背景引入"><a href="#背景引入" class="headerlink" title="背景引入"></a>背景引入</h1><p>这篇文章，我们来聊聊在线上生产环境使用消息中间件技术的时候，从前到后的全链路到底如何保证数据不能丢失。</p><p>这个问题，在互联网公司面试的时候高频出现，而且也是非常现实的生产环境问题。</p><p>如果你的简历中写了自己熟悉MQ技术（RabbitMQ、RocketMQ、Kafka），而且在项目里有使用的经验，那么非常实际的一个生产环境问题就是：投递消息到MQ，然后从MQ消费消息来处理的这个过程，数据到底会不会丢失。</p><p>面试官此时会问：如果数据会丢失的话，你们项目生产部署的时候，是通过什么手段保证基于MQ传输的数据100%不会丢失的？麻烦结合你们线上使用的消息中间件来具体说说你们的技术方案。</p><p>这个其实就是非常区分面试候选人技术水平的一个问题。</p><p>实际上相当大比例的普通工程师，哪怕是在一些中小型互联网公司里工作过的，也就是基于公司部署的MQ集群简单的使用一下罢了，可能代码层面就是基本的发送消息和消费消息，基本没考虑太多的技术方案。</p><p>但是实际上，对于MQ、缓存、分库分表、NoSQL等各式各类的技术以及中间件在使用的时候，都会有对应技术相关的一堆生产环境问题。</p><p>那么针对这些问题，就必须要有相对应的一整套技术方案来保证系统的健壮性、稳定性以及高可用性。</p><p>所以其实中大型互联网公司的面试官在面试候选人的时候，如果考察对MQ相关技术的经验和掌握程度，十有八九都会抛出这个使用MQ时一定会涉及的数据丢失问题。因为这个问题，能够非常好的区分候选人的技术水平。</p><p>所以这篇文章，我们就来具体聊聊基于RabbitMQ这种消息中间件的背景下，从投递消息到MQ，到从MQ消费消息出来，这个过程中有哪些数据丢失的风险和可能。</p><p>然后我们再一起来看看，应该如何结合MQ自身提供的一些技术特性来保证数据不丢失？</p><h1 id="前情回顾"><a href="#前情回顾" class="headerlink" title="前情回顾"></a>前情回顾</h1><p>首先给大伙一点提醒，有些新同学可能还对MQ相关技术不太了解，建议看一下之前的MQ系列文章，看看MQ的基本使用和原理：</p><ul><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484149&amp;idx=1&amp;sn=98186297335e13ec7222b3fd43cfae5a&amp;chksm=fba6eaf6ccd163e0c2c3086daa725de224a97814d31e7b3f62dd3ec763b4abbb0689cc7565b0&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《哥们，你们的系统架构中为什么要引入消息中间件？》</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484157&amp;idx=1&amp;sn=f4644be2db6b1c230846cb4d62ae5be9&amp;chksm=fba6eafeccd163e817b420d57478829d92251a6a5fd446f81805f0983a0d95cb6853a6735c4b&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《哥们，那你说说系统架构引入消息中间件有什么缺点？》</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484191&amp;idx=1&amp;sn=fac0c513cf9ad480fc39c5b51f6c4fde&amp;chksm=fba6eb1cccd1620aa0b48c72d1c6b51706400f24268db6c774bac6ec02a0f31885db9b7d6cad&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《哥们，消息中间件在你们项目里是如何落地的？》</a></p></li></ul><p>另外，其实之前我们有过2篇文章是讨论消息中间件的数据不丢失问题的。</p><p>我们分别从消费者突然宕机可能导致数据丢失，以及集群突然崩溃可能导致的数据丢失两个角度讨论了一下数据如何不丢失。</p><p>只不过仅仅那两个方案还无法保证全链路数据不丢失，但是大家如果没看过的建议也先回过头看看：</p><ul><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484204&amp;idx=1&amp;sn=6fc43b0620857b653dbef20693d1c6c6&amp;chksm=fba6eb2fccd16239056e4b52dc0895585292b830bfd2652dea81b7360556fe36aceac0951761&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《扎心！线上服务宕机时，如何保证数据100%不丢失？》</a></p></li><li><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484257&amp;idx=1&amp;sn=e7704f92a1008ab7a292e2826bd079aa&amp;chksm=fba6eb62ccd1627451d439bbc21e46e6fc1d7bfbe2a431fd887cf974a7bd0d9d482697f0e4fd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《消息中间件集群崩溃，如何保证百万生产数据不丢失？》</a></p></li></ul><p>总之，希望对MQ不太熟悉的同学，先把前面那些系列文章熟悉一下，然后再来一起系统性的研究一下MQ数据如何做到100%不丢失。</p><h1 id="目前已有的技术方案"><a href="#目前已有的技术方案" class="headerlink" title="目前已有的技术方案"></a>目前已有的技术方案</h1><p>经过之前几篇文章的讨论，目前我们已经初步知道，第一个会导致数据丢失的地方，就是消费者获取到消息之后，没有来得及处理完毕，自己直接宕机了。</p><p>此时RabbitMQ的自动ack机制会通知MQ集群这条消息已经处理好了，MQ集群就会删除这条消息。</p><p>那么这条消息不就丢失了么？不会有任何一个消费者处理到这条消息了。</p><p>所以之前我们详细讨论过，通过在消费者服务中调整为手动ack机制，来确保消息一定是已经成功处理完了，才会发送ack通知给MQ集群。</p><p>否则没发送ack之前消费者服务宕机，此时MQ集群会自动感知到，然后重发消息给其他的消费者服务实例。</p><p><a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484204&amp;idx=1&amp;sn=6fc43b0620857b653dbef20693d1c6c6&amp;chksm=fba6eb2fccd16239056e4b52dc0895585292b830bfd2652dea81b7360556fe36aceac0951761&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《扎心！线上服务宕机时，如何保证数据100%不丢失？》</a>这篇文章，详细讨论了这个问题，手动ack机制之下的架构图如下所示：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111212900106-165253661.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>当时除了这个数据丢失问题之外，还有另外一个问题，就是MQ集群自身如果突然宕机，是不是会导致数据丢失？</p><p>默认情况下是肯定会的，因为queue和message都没采用持久化的方式来投递，所以MQ集群重启会导致部分数据丢失。</p><p>所以<a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484257&amp;idx=1&amp;sn=e7704f92a1008ab7a292e2826bd079aa&amp;chksm=fba6eb62ccd1627451d439bbc21e46e6fc1d7bfbe2a431fd887cf974a7bd0d9d482697f0e4fd&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《消息中间件集群崩溃，如何保证百万生产数据不丢失？》</a>这篇文章，我们分析了如何采用持久化的方式来创建queue，同时采用持久化的方式来投递消息到MQ集群，这样MQ集群会将消息持久化到磁盘上去。</p><p>此时如果消息还没来得及投递给消费者服务，然后MQ集群突然宕机了，数据是不会丢失的，因为MQ集群重启之后会自动从磁盘文件里加载出来没投递出去的消息，然后继续投递给消费者服务。</p><p>同样，该方案沉淀下来的系统架构图，如下所示：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111212925797-142776737.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="数据100-不丢失了吗？"><a href="#数据100-不丢失了吗？" class="headerlink" title="数据100%不丢失了吗？"></a>数据100%不丢失了吗？</h1><p>大家想一想，到目前为止，咱们的架构一定可以保证数据不丢失了吗？</p><p>其实，现在的架构，还是有一个数据可能会丢失的问题。</p><p>那就是上面作为生产者的订单服务把消息投递到MQ集群之后，暂时还驻留在MQ的内存里，还没来得及持久化到磁盘上，同时也还没来得及投递到作为消费者的仓储服务。</p><p>此时要是MQ集群自身突然宕机，咋办呢？</p><p>尴尬了吧，驻留在内存里的数据是一定会丢失的，我们来看看下面的图示。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213004493-756910885.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="按需制定技术方案"><a href="#按需制定技术方案" class="headerlink" title="按需制定技术方案"></a>按需制定技术方案</h1><p>现在，我们需要考虑的技术方案是：订单服务如何保证消息一定已经持久化到磁盘？</p><p>实际上，作为生产者的订单服务把消息投递到MQ集群的过程是很容易丢数据的。</p><p>比如说网络出了点什么故障，数据压根儿没传输过去，或者就是上面说的消息刚刚被MQ接收但是还驻留在内存里，没落地到磁盘上，此时MQ集群宕机就会丢数据。</p><p>所以首先，我们得考虑一下作为生产者的订单服务要如何利用RabbitMQ提供的相关功能来实现一个技术方案。</p><p>这个技术方案需要保证：<strong>只要订单服务发送出去的消息确认成功了，此时MQ集群就一定已经将消息持久化到磁盘了</strong>。</p><p>我们必须实现这样的一个效果，才能保证投递到MQ集群的数据是不会丢失的。</p><h1 id="需要研究的技术细节"><a href="#需要研究的技术细节" class="headerlink" title="需要研究的技术细节"></a>需要研究的技术细节</h1><p>这里我们需要研究的技术细节是：仓储服务手动ack保证数据不丢失的实现原理。</p><p>之前，笔者就收到很多同学提问：</p><ul><li>仓储服务那块到底是如何基于手动ack就可以实现数据不丢失的？</li><li>RabbitMQ底层实现的细节和原理到底是什么？</li><li>为什么仓储服务没发送ack就宕机了，RabbitMQ可以自动感知到他宕机了，然后自动重发消息给其他的仓储服务实例呢？</li></ul><p>这些东西背后的实现原理和底层细节，到底是什么？</p><p>大伙儿稍安勿躁，接下来，咱们会通过一系列文章，仔细探究一下这背后的原理。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt;互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第一篇  &lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;背景引入&quot;&gt;&lt;a href=&quot;#背景引入&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第三篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E4%B8%89%E7%AF%87/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第三篇/</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:01:18.909Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p>互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第三篇 </p><blockquote><h1 id="前情提示"><a href="#前情提示" class="headerlink" title="前情提示"></a>前情提示</h1><p>上一篇文章：<a href="https://www.cnblogs.com/jajian/p/10293093.html" target="_blank" rel="noopener">&lt;&lt;互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第二篇&gt;&gt;</a>，我们分析了 ack 机制的底层实现原理（<code>delivery tag</code>机制），还有消除处理失败时的nack机制如何触发消息重发。</p><p>通过这个，已经让大家进一步对消费端保证数据不丢失的方案的理解更进一层了。</p><p>这篇文章，我们将会对 ack 底层的<code>delivery tag</code>机制进行更加深入的分析，让大家理解的更加透彻一些。</p><p>面试时，如果被问到消息中间件数据不丢失问题的时候，可以更深入到底层，给面试官进行分析。</p><h1 id="unack消息的积压问题"><a href="#unack消息的积压问题" class="headerlink" title="unack消息的积压问题"></a>unack消息的积压问题</h1><p>首先，我们要给大家介绍一下RabbitMQ的<code>prefetch count</code>这个概念。</p><p>大家看过上篇文章之后应该都知道了，对每个 <code>channel</code>（其实对应了一个消费者服务实例，你大体可以这么来认为），RabbitMQ 投递消息的时候，都是会带上本次消息投递的一个<code>delivery tag</code>的，唯一标识一次消息投递。</p><p>然后，我们进行 ack 时，也会带上这个 <code>delivery tag</code>，基于同一个 <code>channel</code> 进行 ack，ack 消息里会带上 <code>delivery tag</code> 让RabbitMQ知道是对哪一次消息投递进行了 ack，此时就可以对那条消息进行删除了。</p><p>大家先来看一张图，帮助大家回忆一下这个<code>delivery tag</code>的概念。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119201956874-1938458409.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>所以大家可以考虑一下，对于每个<code>channel</code>而言（你就认为是针对每个消费者服务实例吧，比如一个仓储服务实例），其实都有一些处于<code>unack</code>状态的消息。</p><p>比如RabbitMQ正在投递一条消息到<code>channel</code>，此时消息肯定是<code>unack</code>状态吧？</p><p>然后仓储服务接收到一条消息以后，要处理这条消息需要耗费时间，此时消息肯定是<code>unack</code>状态吧？</p><p>同时，即使你执行了<code>ack</code>之后，你要知道这个<code>ack</code>他默认是异步执行的，尤其如果你开启了批量<code>ack</code>的话，更是有一个延迟时间才会<code>ack</code>的，此时消息也是<code>unack</code>吧？</p><p>那么大家考虑一下，RabbitMQ 他能够无限制的不停给你的消费者服务实例推送消息吗？</p><p>明显是不能的，如果 RabbitMQ 给你的消费者服务实例推送的消息过多过快，比如都有几千条消息积压在某个消费者服务实例的内存中。</p><p>那么此时这几千条消息都是<code>unack</code>的状态，一直积压着，是不是有可能会导致消费者服务实例的内存溢出？内存消耗过大？甚至内存泄露之类的问题产生？</p><p>所以说，RabbitMQ 是必须要考虑一下消费者服务的处理能力的。</p><p>大家看看下面的图，感受一下如果消费者服务实例的内存中积压消息过多，都是<code>unack</code>的状态，此时会怎么样。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202028721-10842096.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="如何解决unack消息的积压问题"><a href="#如何解决unack消息的积压问题" class="headerlink" title="如何解决unack消息的积压问题"></a>如何解决unack消息的积压问题</h1><p>正是因为这个原因，RabbitMQ基于一个<code>prefetch count</code>来控制这个<code>unack message</code>的数量。</p><p>你可以通过<code>“channel.basicQos(10)”</code>这个方法来设置当前<code>channel</code>的<code>prefetch count</code>。</p><p>举个例子，比如你要是设置为10的话，那么意味着当前这个<code>channel</code>里，<code>unack message</code>的数量不能超过10个，以此来避免消费者服务实例积压unack message过多。</p><p>这样的话，就意味着RabbitMQ正在投递到channel过程中的<code>unack message</code>，以及消费者服务在处理中的<code>unack message</code>，以及异步ack之后还没完成 ack 的<code>unack message</code>，所有这些message 加起来，一个 channel 也不能超过10个。</p><p>如果你要简单粗浅的理解的话，也大致可以理解为这个<code>prefetch count</code>就代表了一个消费者服务同时最多可以获取多少个 message 来处理。所以这里也点出了 prefetch 这个单词的意思。</p><p>prefetch 就是预抓取的意思，就意味着你的消费者服务实例预抓取多少条 message 过来处理，但是最多只能同时处理这么多消息。</p><p>如果一个 channel 里的<code>unack message</code>超过了<code>prefetch count</code>指定的数量，此时RabbitMQ就会停止给这个 channel 投递消息了，必须要等待已经投递过去的消息被 ack 了，此时才能继续投递下一个消息。</p><p>老规矩，给大家上一张图，我们一起来看看这个东西是啥意思。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202058498-1935372139.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="高并发场景下的内存溢出问题"><a href="#高并发场景下的内存溢出问题" class="headerlink" title="高并发场景下的内存溢出问题"></a>高并发场景下的内存溢出问题</h1><p>好！现在大家对 ack 机制底层的另外一个核心机制：prefetch 机制也有了一个深刻的理解了。</p><p>此时，咱们就应该来考虑一个问题了。就是如何来设置这个<code>prefetch count</code>呢？这个东西设置的过大或者过小有什么影响呢？</p><p>其实大家理解了上面的图就很好理解这个问题了。</p><p>假如说我们把 <code>prefetch count</code> 设置的很大，比如说3000，5000，甚至100000，就这样特别大的值，那么此时会如何呢？</p><p>这个时候，在高并发大流量的场景下，可能就会导致消费者服务的内存被快速的消耗掉。</p><p>因为假如说现在MQ接收到的流量特别的大，每秒都上千条消息，而且此时你的消费者服务的<code>prefetch count</code>还设置的特别大，就会导致可能一瞬间你的消费者服务接收到了达到<code>prefetch count</code>指定数量的消息。</p><p>打个比方，比如一下子你的消费者服务内存里积压了10万条消息，都是unack的状态，反正你的prefetch count设置的是10万。</p><p>那么对一个channel，RabbitMQ就会最多容忍10万个unack状态的消息，在高并发下也就最多可能积压10万条消息在消费者服务的内存里。</p><p>那么此时导致的结果，就是消费者服务直接被击垮了，内存溢出，OOM，服务宕机，然后大量unack的消息会被重新投递给其他的消费者服务，此时其他消费者服务一样的情况，直接宕机，最后造成雪崩效应。</p><p>所有的消费者服务因为扛不住这么大的数据量，全部宕机。</p><p>大家来看看下面的图，自己感受一下现场的氛围。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202121988-833235887.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="低吞吐量问题"><a href="#低吞吐量问题" class="headerlink" title="低吞吐量问题"></a>低吞吐量问题</h1><p>那么如果反过来呢，我们要是把prefetch count设置的很小会如何呢？</p><p>比如说我们把 prefetch count 设置为1？此时就必然会导致消费者服务的吞吐量极低。因为你即使处理完一条消息，执行ack了也是异步的。</p><p>给你举个例子，假如说你的 <code>prefetch count = 1</code>，RabbitMQ最多投递给你1条消息处于 unack 状态。</p><p>此时比如你刚处理完这条消息，然后执行了 ack 的那行代码，结果不幸的是，ack需要异步执行，也就是需要100ms之后才会让RabbitMQ感知到。</p><p>那么100ms之后RabbitMQ感知到消息被ack了，此时才会投递给你下一条消息！</p><p>这就尴尬了，在这100ms期间，你的消费者服务是不是啥都没干啊？</p><p>这不就直接导致了你的消费者服务处理消息的吞吐量可能下降10倍，甚至百倍，千倍，都有这种可能！</p><p>大家看看下面的图，感受一下低吞吐量的现场。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119202156488-260456800.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="合理的设置prefetch-count"><a href="#合理的设置prefetch-count" class="headerlink" title="合理的设置prefetch count"></a>合理的设置prefetch count</h1><p>所以鉴于上面两种极端情况，RabbitMQ官方给出的建议是prefetch count一般设置在100~300之间。</p><p>也就是一个消费者服务最多接收到100~300个message来处理，允许处于unack状态。</p><p>这个状态下可以兼顾吞吐量也很高，同时也不容易造成内存溢出的问题。</p><p>但是其实在我们的实践中，这个prefetch count大家完全是可以自己去压测一下的。</p><p>比如说慢慢调节这个值，不断加大，观察高并发大流量之下，吞吐量是否越来越大，而且观察消费者服务的内存消耗，会不会OOM、频繁FullGC等问题。</p><h1 id="阶段性总结"><a href="#阶段性总结" class="headerlink" title="阶段性总结"></a>阶段性总结</h1><p>其实通过最近几篇文章，基本上已经把消息中间件的消费端如何保证数据不丢失这个问题剖析的较为深入和透彻了。</p><p>如果你是基于RabbitMQ来做消息中间件的话，消费端的代码里，必须考虑三个问题：手动ack、处理失败的nack、prefetch count的合理设置</p><p>这三个问题背后涉及到了各种机制：</p><ul><li>自动ack机制</li><li>delivery tag机制</li><li>ack批量与异步提交机制</li><li>消息重发机制</li><li>手动nack触发消息重发机制</li><li>prefetch count过大导致内存溢出问题</li><li>prefetch count过小导致吞吐量过低</li></ul><p>这些底层机制和问题，咱们都一步步分析清楚了。</p><p>所以到现在，单论消费端这块的数据不丢失技术方案，相信大家在面试的时候就可以有一整套自己的理解和方案可以阐述了。</p><p>接下来下篇文章开始，我们就来具体聊一聊：消息中间件的生产端如何保证数据不丢失。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt;互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第三篇 &lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;前情提示&quot;&gt;&lt;a href=&quot;#前情提示&quot; class=&quot;headerlink&quot; title=&quot;前
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第二篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E4%BA%8C%E7%AF%87%20/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第二篇 /</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:00:55.862Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p> 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第二篇  </p><blockquote><p>上一篇文章<a href="https://www.cnblogs.com/jajian/p/10257555.html" target="_blank" rel="noopener">《互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第一篇》</a>,我们初步介绍了之前制定的那些消息中间件数据不丢失的技术方案遗留的问题。</p><p>一个最大的问题，就是生产者投递出去的消息，可能会丢失。</p><p>丢失的原因有很多，比如消息在网络传输到一半的时候因为网络故障就丢了，或者是消息投递到MQ的内存时，MQ突发故障宕机导致消息就丢失了。</p><p>针对这种生产者投递数据丢失的问题，RabbitMQ实际上是提供了一些机制的。</p><p>比如，有一种重量级的机制，就是<strong>事务消息机制</strong>。采用类事务的机制把消息投递到MQ，可以保证消息不丢失，但是性能极差，经过测试性能会呈现几百倍的下降。</p><p>所以说现在一般是不会用这种过于重量级的机制，而是会用<strong>轻量级的confirm机制</strong>。</p><p>但是我们这篇文章还不能直接讲解生产者保证消息不丢失的confirm机制，因为这种confirm机制实际上是采用了类似消费者的ack机制来实现的。</p><p>所以，要深入理解confirm机制，我们得先从这篇文章开始，深入的分析一下消费者手动ack机制保证消息不丢失的底层原理。</p><h1 id="ack机制回顾"><a href="#ack机制回顾" class="headerlink" title="ack机制回顾"></a>ack机制回顾</h1><p>其实手动ack机制非常的简单，必须要消费者确保自己处理完毕了一个消息，才能手动发送ack给MQ，MQ收到ack之后才会删除这个消息。</p><p>如果消费者还没发送ack，自己就宕机了，此时MQ感知到他的宕机，就会重新投递这条消息给其他的消费者实例。</p><p>通过这种机制保证消费者实例宕机的时候，数据是不会丢失的。</p><p>再次提醒一下大家，如果还对手动ack机制不太熟悉的同学，可以回头看一下之前的一篇文章：<a href="http://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&amp;mid=2247484204&amp;idx=1&amp;sn=6fc43b0620857b653dbef20693d1c6c6&amp;chksm=fba6eb2fccd16239056e4b52dc0895585292b830bfd2652dea81b7360556fe36aceac0951761&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">《扎心！线上服务宕机时，如何保证数据100%不丢失？》</a>。然后这篇文章，我们将继续深入探讨一下ack机制的实现原理。</p><h1 id="ack机制实现原理：delivery-tag"><a href="#ack机制实现原理：delivery-tag" class="headerlink" title="ack机制实现原理：delivery tag"></a>ack机制实现原理：delivery tag</h1><p>如果你写好了一个消费者服务的代码，让他开始从RabbitMQ消费数据，这时这个消费者服务实例就会自己注册到RabbitMQ。</p><p>所以，RabbitMQ其实是知道有哪些消费者服务实例存在的。</p><p>大家看看下面的图，直观的感受一下：<br> <img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213354238-905111691.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>接着，RabbitMQ就会通过自己内部的一个“basic.delivery”方法来投递消息到仓储服务里去，让他消费消息。</p><p>投递的时候，会给这次消息的投递带上一个重要的东西，就是“delivery tag”，你可以认为是本次消息投递的一个唯一标识。</p><p>这个所谓的唯一标识，有点类似于一个ID，比如说消息本次投递到一个仓储服务实例的唯一ID。通过这个唯一ID，我们就可以定位一次消息投递。</p><p>所以这个delivery tag机制不要看很简单，实际上他是后面要说的很多机制的核心基础。</p><p>而且这里要给大家强调另外一个概念，就是每个消费者从RabbitMQ获取消息的时候，都是通过一个channel的概念来进行的。</p><p>大家回看一下下面的消费者代码片段，我们必须是先对指定机器上部署的RabbitMQ建立连接，然后通过这个连接获取一个channel。</p><p>ConnectionFactory factory = new ConnectionFactory(); </p></blockquote><blockquote><p>factory.setHost(“localhost”); </p></blockquote><blockquote><p>Connection connection = factory.newConnection();</p></blockquote><blockquote><p>Channel channel = connection.createChannel();</p><p>而且如果大家还有点印象的话，我们在仓储服务里对消息的消费、ack等操作，全部都是基于这个channel来进行的，channel又有点类似于是我们跟RabbitMQ进行通信的这么一个句柄，比如看看下面的代码：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111214453240-695115936.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><blockquote><p>另外这里提一句：之前写那篇文章讲解手动ack保证数据不丢失的时候，有很多人提出疑问：为什么上面代码里直接是try finally，如果代码有异常，那还是会直接执行finally里的手动ack？其实很简单，自己加上catch就可以了。</p></blockquote><p>好的，咱们继续。你大概可以认为这个channel就是进行数据传输的一个管道吧。对于每个channel而言，一个“delivery tag”就可以唯一的标识一次消息投递，这个delivery tag大致而言就是一个不断增长的数字。</p><p>大家来看看下面的图，相信会很好理解的：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213504848-417638307.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>如果采用手动ack机制，实际上仓储服务每次消费了一条消息，处理完毕完成调度发货之后，就会发送一个ack消息给RabbitMQ服务器，这个ack消息是会带上自己本次消息的delivery tag的。</p><p>咱们看看下面的ack代码，是不是带上了一个delivery tag？</p><pre><code>channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);</code></pre><p>然后，RabbitMQ根据哪个channel的哪个delivery tag，不就可以唯一定位一次消息投递了？</p><p>接下来就可以对那条消息删除，标识为已经处理完毕。</p><p>这里大家必须注意的一点，就是delivery tag仅仅在一个channel内部是唯一标识消息投递的。</p><p>所以说，你ack一条消息的时候，必须是通过接受这条消息的同一个channel来进行。</p><p>大家看看下面的图，直观的感受一下。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213612518-291570713.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>其实这里还有一个很重要的点，就是我们可以设置一个参数，然后就批量的发送ack消息给RabbitMQ，这样可以提升整体的性能和吞吐量。</p><p>比如下面那行代码，把第二个参数设置为true就可以了。</p><pre><code>channel.basicAck(delivery.getEnvelope().getDeliveryTag(), true);</code></pre><p>看到这里，大家应该对这个ack机制的底层原理有了稍微进一步的认识了。起码是知道delivery tag是啥东西了，他是实现ack的一个底层机制。</p><p>然后，我们再来简单回顾一下自动ack、手动ack的区别。</p><p>实际上默认用自动ack，是非常简单的。RabbitMQ只要投递一个消息出去给仓储服务，那么他立马就把这个消息给标记为删除，因为他是不管仓储服务到底接收到没有，处理完没有的。</p><p>所以这种情况下，性能很好，但是数据容易丢失。</p><p>如果手动ack，那么就是必须等仓储服务完成商品调度发货以后，才会手动发送ack给RabbitMQ，此时RabbitMQ才会认为消息处理完毕，然后才会标记消息为删除。</p><p>这样在发送ack之前，仓储服务宕机，RabbitMQ会重发消息给另外一个仓储服务实例，保证数据不丢。</p><h1 id="RabbitMQ如何感知到仓储服务实例宕机"><a href="#RabbitMQ如何感知到仓储服务实例宕机" class="headerlink" title="RabbitMQ如何感知到仓储服务实例宕机"></a>RabbitMQ如何感知到仓储服务实例宕机</h1><p>之前就有同学提出过这个问题，但是其实要搞清楚这个问题，其实不需要深入的探索底层，只要自己大致的思考和推测一下就可以了。</p><p>如果你的仓储服务实例接收到了消息，但是没有来得及调度发货，没有发送ack，此时他宕机了。</p><p>我们想一想就知道，RabbitMQ之前既然收到了仓储服务实例的注册，因此他们之间必然是建立有某种联系的。</p><p>一旦某个仓储服务实例宕机，那么RabbitMQ就必然会感知到他的宕机，而且对发送给他的还没ack的消息，都发送给其他仓储服务实例。</p><p>所以这个问题以后有机会我们可以深入聊一聊，在这里，大家其实先建立起来这种认识即可。</p><p>我们再回头看看下面的架构图：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213730961-1499375762.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="仓储服务处理失败时的消息重发"><a href="#仓储服务处理失败时的消息重发" class="headerlink" title="仓储服务处理失败时的消息重发"></a>仓储服务处理失败时的消息重发</h1><p>首先，我们来看看下面一段代码：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111214524017-261713897.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>假如说某个仓储服务实例处理某个消息失败了，此时会进入catch代码块，那么此时我们怎么办呢？难道还是直接ack消息吗？</p><p>当然不是了，你要是还是ack，那会导致消息被删除，但是实际没有完成调度发货。</p><p>这样的话，数据不是还是丢失了吗？因此，合理的方式是使用nack操作。</p><p>就是通知RabbitMQ自己没处理成功消息，然后让RabbitMQ将这个消息再次投递给其他的仓储服务实例尝试去完成调度发货的任务。</p><p>我们只要在catch代码块里加入下面的代码即可：</p><pre><code>channel.basicNack(delivery.getEnvelope().getDeliveryTag(),  true);</code></pre><p>注意上面第二个参数是true，意思就是让RabbitMQ把这条消息重新投递给其他的仓储服务实例，因为自己没处理成功。</p><p>你要是设置为false的话，就会导致RabbitMQ知道你处理失败，但是还是删除这条消息，这是不对的。</p><p>同样，我们还是来一张图，大家一起来感受一下：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190111213911750-654802068.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="阶段总结"><a href="#阶段总结" class="headerlink" title="阶段总结"></a>阶段总结</h1><p>这篇文章对之前的ack机制做了进一步的分析，包括底层的delivery tag机制，以及消息处理失败时的消息重发。</p><p>通过ack机制、消息重发等这套机制的落地实现，就可以保证一个消费者服务自身突然宕机、消息处理失败等场景下，都不会丢失数据。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt; 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第二篇  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上一篇文章&lt;a href=&quot;https://www.cnblogs.com/jajian/p/10257
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>如何保证消息中间件全链路数据不丢失-第四篇</title>
    <link href="http://zhang-yu.me/2020/10/10/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%85%A8%E9%93%BE%E8%B7%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1-%E7%AC%AC%E5%9B%9B%E7%AF%87/"/>
    <id>http://zhang-yu.me/2020/10/10/如何保证消息中间件全链路数据不丢失-第四篇/</id>
    <published>2020-10-10T03:00:00.000Z</published>
    <updated>2020-11-03T14:01:41.064Z</updated>
    
    <content type="html"><![CDATA[<p>来源：【微信公众号 - 石杉的架构笔记】</p><p> 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第四篇 </p><blockquote><h1 id="前情提示"><a href="#前情提示" class="headerlink" title="前情提示"></a>前情提示</h1><p>上篇文章：<a href="https://www.cnblogs.com/jajian/p/10293391.html" target="_blank" rel="noopener">《互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第三篇》</a>，我们分析了 RabbitMQ 开启手动ack机制保证消费端数据不丢失的时候，prefetch 机制对消费者的吞吐量以及内存消耗的影响。</p><p>通过分析，我们知道了 prefetch 过大容易导致内存溢出，prefetch 过小又会导致消费吞吐量过低，所以在实际项目中需要慎重测试和设置。</p><p>这篇文章，我们转移到消息中间件的生产端，一起来看看如何保证投递到 MQ 的数据不丢失。</p><p>如果投递出去的消息在网络传输过程中丢失，或者在 RabbitMQ 的内存中还没写入磁盘的时候宕机，都会导致生产端投递到MQ的数据丢失。</p><p>而且丢失之后，生产端自己还感知不到，同时还没办法来补救。</p><p>下面的图就展示了这个问题。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214653541-577481334.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>所以本文呢，我们就来逐步分析一下。</p><h1 id="保证投递消息不丢失的-confirm-机制"><a href="#保证投递消息不丢失的-confirm-机制" class="headerlink" title="保证投递消息不丢失的 confirm 机制"></a>保证投递消息不丢失的 confirm 机制</h1><p>其实要解决这个问题，相信大家看过之前的消费端 ack 机制之后，也都猜到了。</p><p>很简单，就是生产端（比如上图的订单服务）首先需要开启一个 confirm 模式，接着投递到 MQ 的消息，如果 MQ 一旦将消息持久化到磁盘之后，必须也要回传一个 confirm 消息给生产端。</p><p>这样的话，如果生产端的服务接收到了这个 confirm 消息，就知道是已经持久化到磁盘了。</p><p>否则如果没有接收到confirm消息，那么就说明这条消息半路可能丢失了，此时你就可以重新投递消息到 MQ 去，确保消息不要丢失。</p><p>而且一旦你开启了confirm模式之后，每次消息投递也同样是有一个 <code>delivery tag</code> 的，也是起到唯一标识一次消息投递的作用。</p><p>这样，MQ回传ack给生产端的时候，会带上这个 <code>delivery tag</code>。你就知道具体对应着哪一次消息投递了，可以删除这条消息。</p><p>此外，如果 RabbitMQ 接收到一条消息之后，结果内部出错发现无法处理这条消息，那么他会回传一个 nack 消息给生产端。此时你就会感知到这条消息可能处理有问题，你可以选择重新再次投递这条消息到MQ去。</p><p>或者另一种情况，如果某条消息很长时间都没给你回传 ack/nack，那可能是极端意外情况发生了，数据也丢了，你也可以自己重新投递消息到 MQ 去。</p><p>通过这套 confirm 机制，就可以实现生产端投递消息不会丢失的效果。大家来看看下面的图，一起来感受一下。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214724244-1755660067.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p><h1 id="confirm机制的代码实现"><a href="#confirm机制的代码实现" class="headerlink" title="confirm机制的代码实现"></a>confirm机制的代码实现</h1><p>下面，我们再来看看confirm机制的代码实现：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214748749-1779146961.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="confirm机制投递消息的高延迟性"><a href="#confirm机制投递消息的高延迟性" class="headerlink" title="confirm机制投递消息的高延迟性"></a>confirm机制投递消息的高延迟性</h1><p>这里有一个很关键的点，就是一旦启用了 confirm 机制投递消息到 MQ 之后，MQ 是不保证什么时候会给你一个ack或者nack的。</p><p>因为 RabbitMQ 自己内部将消息持久化到磁盘，本身就是通过异步批量的方式来进行的。</p><p>正常情况下，你投递到 RabbitMQ 的消息都会先驻留在内存里，然后过了几百毫秒的延迟时间之后，再一次性批量把多条消息持久化到磁盘里去。</p><p>这样做，是为了兼顾高并发写入的吞吐量和性能的，因为要是你来一条消息就写一次磁盘，那么性能会很差，每次写磁盘都是一次 fsync 强制刷入磁盘的操作，是很耗时的。</p><p>所以正是因为这个原因，你打开了 confirm 模式之后，很可能你投递出去一条消息，要间隔几百毫秒之后，MQ 才会把消息写入磁盘，接着你才会收到 MQ 回传过来的 ack 消息，这个就是所谓confirm机制投递消息的高延迟性。</p><p>大家看看下面的图，一起来感受一下。<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214823183-1953400051.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="高并发下如何投递消息才能不丢失"><a href="#高并发下如何投递消息才能不丢失" class="headerlink" title="高并发下如何投递消息才能不丢失"></a>高并发下如何投递消息才能不丢失</h1><p>大家可以考虑一下，在生产端高并发写入 MQ 的场景下，你会面临两个问题：</p><ul><li>1、你每次写一条消息到 MQ，为了等待这条消息的ack，必须把消息保存到一个存储里。</li></ul><p>并且这个存储不建议是内存，因为高并发下消息是很多的，每秒可能都几千甚至上万的消息投递出去，消息的 ack 要等几百毫秒的话，放内存可能有内存溢出的风险。</p><ul><li>2、绝对不能以同步写消息 + 等待 ack 的方式来投递，那样会导致每次投递一个消息都同步阻塞等待几百毫秒，会导致投递性能和吞吐量大幅度下降。</li></ul><p>针对这两个问题，相对应的方案其实也呼之欲出了。</p><p>首先，用来临时存放未 ack 消息的存储需要承载高并发写入，而且我们不需要什么复杂的运算操作，这种存储首选绝对不是 MySQL 之类的数据库，而建议采用 kv 存储。kv 存储承载高并发能力极强，而且 kv 操作性能很高。</p><p>其次，投递消息之后等待 ack 的过程必须是异步的，也就是类似上面那样的代码，已经给出了一个初步的异步回调的方式。</p><p>消息投递出去之后，这个投递的线程其实就可以返回了，至于每个消息的异步回调，是通过在channel注册一个confirm监听器实现的。</p><p>收到一个消息 ack 之后，就从kv存储中删除这条临时消息；收到一个消息 nack 之后，就从 kv 存储提取这条消息然后重新投递一次即可；也可以自己对 kv 存储里的消息做监控，如果超过一定时长没收到 ack，就主动重发消息。</p><p>大家看看下面的图，一起来体会一下：<br><img src="https://img2018.cnblogs.com/blog/1162587/201901/1162587-20190119214848776-1757813065.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><h1 id="消息中间件全链路100-数据不丢失能做到吗"><a href="#消息中间件全链路100-数据不丢失能做到吗" class="headerlink" title="消息中间件全链路100%数据不丢失能做到吗"></a>消息中间件全链路100%数据不丢失能做到吗</h1><p>到此为止，我们已经把生产端和消费端如何保证消息不丢失的相关技术方案结合RabbitMQ这种中间件都给大家分析过了。</p><p>其实，架构思想是通用的， 无论你用的是哪一种MQ中间件，他们提供的功能是不太一样的，但是你都需要考虑如下几点：</p><ol><li><p>生产端如何保证投递出去的消息不丢失：消息在半路丢失，或者在 MQ 内存中宕机导致丢失，此时你如何基于 MQ 的功能保证消息不要丢失？</p></li><li><p>MQ 自身如何保证消息不丢失：起码需要让 MQ 对消息是有持久化到磁盘这个机制。</p></li><li><p>消费端如何保证消费到的消息不丢失：如果你处理到一半消费端宕机，导致消息丢失，此时怎么办？</p></li></ol><p>目前来说，我们初步的借着 RabbitMQ 举例，已经把从前到后一整套技术方案的原理、设计和实现都给大家分析了一遍了。</p><p>但是此时真的能做到100%数据不丢失吗？恐怕未必，大家再考虑一下个特殊的场景。</p><p>生产端投递了消息到 MQ，而且持久化到磁盘并且回传ack给生产端了。</p><p>但是此时 MQ 还没投递消息给消费端，结果 MQ 部署的机器突然宕机，而且因为未知的原因磁盘损坏了，直接在物理层面导致 MQ 持久化到磁盘的数据找不回来了。</p><p>这个大家千万别以为是开玩笑的，大家如果留意留意行业新闻，这种磁盘损坏导致数据丢失的是真的有的。</p><p>那么此时即使你把 MQ 重启了，磁盘上的数据也丢失了，数据是不是还是丢失了？</p><p>你说，我可以用 MQ 的集群机制啊，给一个数据做多个副本，比如后面我们就会给大家分析 RabbitMQ 的镜像集群机制，确实可以做到数据多副本。</p><p>但是即使数据多副本，一定可以做到100%数据不丢失？</p><p>比如说你的机房突然遇到地震，结果机房里的机器全部没了，数据是不是还是全丢了？</p><p>说这个，并不是说要抬杠。而是告诉大家，技术这个东西，100%都是理论上的期望。</p><p>应该说，我们凡事都朝着100%去做，但是理论上是不可能完全做到100%保证的，可能就是做到99.9999%的可能性数据不丢失，但是还是有千万分之一的概率会丢失。</p><p>当然，从实际的情况来说，能做到这种地步，其实基本上已经基本数据不会丢失了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;来源：【微信公众号 - 石杉的架构笔记】&lt;/p&gt;
&lt;p&gt; 互联网面试必杀：如何保证消息中间件全链路数据100%不丢失：第四篇 &lt;/p&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;前情提示&quot;&gt;&lt;a href=&quot;#前情提示&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="中间件" scheme="http://zhang-yu.me/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="中间件" scheme="http://zhang-yu.me/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch性能优化</title>
    <link href="http://zhang-yu.me/2020/09/28/Elasticsearch%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://zhang-yu.me/2020/09/28/Elasticsearch性能优化/</id>
    <published>2020-09-28T03:00:00.000Z</published>
    <updated>2020-11-03T14:03:32.192Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/jajian/category/1280015.html" target="_blank" rel="noopener">https://www.cnblogs.com/jajian/category/1280015.html</a><br><a href="https://www.cnblogs.com/jajian/p/10465519.html" target="_blank" rel="noopener">https://www.cnblogs.com/jajian/p/10465519.html</a></p><blockquote><h1 id="硬件选择"><a href="#硬件选择" class="headerlink" title="硬件选择"></a>硬件选择</h1><p>Elasticsearch（后文简称 ES）的基础是 Lucene，所有的索引和文档数据是存储在本地的磁盘中，具体的路径可在 ES 的配置文件<code>../config/elasticsearch.yml</code>中配置，如下：</p></blockquote><p><a href="https://www.cnblogs.com/jajian/p/10465519.html" target="_blank" rel="noopener">Elasticsearch 技术分析（七）： Elasticsearch 的性能优化 - JaJian - 博客园</a></p><pre><code># ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /path/to/data## Path to log files:#path.logs: /path/to/logs</code></pre><blockquote><p>磁盘在现代服务器上通常都是瓶颈。Elasticsearch 重度使用磁盘，你的磁盘能处理的吞吐量越大，你的节点就越稳定。这里有一些优化磁盘 I/O 的技巧：</p><ul><li>使用 SSD。就像其他地方提过的， 他们比机械磁盘优秀多了。</li><li>使用 RAID 0。条带化 RAID 会提高磁盘 I/O，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验 RAID 因为副本已经提供了这个功能。</li><li>另外，使用多块硬盘，并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面。</li><li>不要使用远程挂载的存储，比如 NFS 或者 SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的。</li><li>如果你用的是 EC2，当心 EBS。即便是基于 SSD 的 EBS，通常也比本地实例的存储要慢。</li></ul><h1 id="内部压缩"><a href="#内部压缩" class="headerlink" title="内部压缩"></a>内部压缩</h1><p>硬件资源比较昂贵，一般不会花大成本去购置这些，可控的解决方案还是需要从软件方面来实现性能优化提升。</p><p>其实，对于一个分布式、可扩展、支持PB级别数据、实时的搜索与数据分析引擎，ES 本身对于索引数据和文档数据的存储方面内部做了很多优化，具体体现在对数据的压缩，那么是如何压缩的呢？介绍前先要说明下 <strong>Postings lists</strong> 的概念。</p><h2 id="倒排列表-postings-list"><a href="#倒排列表-postings-list" class="headerlink" title="倒排列表 - postings list"></a>倒排列表 - postings list</h2><p>搜索引擎一项很重要的工作就是高效的压缩和快速的解压缩一系列有序的整数列表。我们都知道，Elasticsearch 基于 Lucene，一个 Lucene 索引 我们在 Elasticsearch 称作 _分片_ ， 并且引入了 <strong>按段搜索</strong> 的概念。</p><p>新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段。在每个 segment 内文档都会有一个 0 到文档个数之间的标识符（最高值 2^31 -1），称之为 <strong>doc ID</strong>。这在概念上类似于数组中的索引：它本身不做存储，但足以识别每个item 数据。</p><p>Segments 按顺序存储有关文档的数据，在一个Segments 中 doc ID 是 文档的索引。因此，segment 中的第一个文档的 doc ID 为0，第二个为1，等等。直到最后一个文档，其 doc ID 等于 segment 中文档的总数减1。</p><p>那么这些 doc ID 有什么用呢？倒排索引需要将 terms 映射到包含该单词 （term） 的文档列表，这样的映射列表我们称之为：<strong>倒排列表（postings list）</strong>。具体某一条映射数据称之为：<strong>倒排索引项（Posting）</strong>。</p><p>举个例子，文档和词条之间的关系如下图所示，右边的关系表即为倒排列表：</p><p><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154646364-1594632349.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><blockquote><p><strong>倒排列表</strong> 用来记录有哪些文档包含了某个单词（Term）。一般在文档集合里会有很多文档包含某个单词，每个文档会记录文档编号（doc ID），单词在这个文档中出现的次数（TF）及单词在文档中哪些位置出现过等信息，这样与一个文档相关的信息被称做 <strong>倒排索引项（Posting）</strong>，包含这个单词的一系列倒排索引项形成了列表结构，这就是某个单词对应的 <strong>倒排列表</strong> 。</p></blockquote><h2 id="Frame-Of-Reference"><a href="#Frame-Of-Reference" class="headerlink" title="Frame Of Reference"></a>Frame Of Reference</h2><p>了解了分词（Term）和文档（Document）之间的映射关系后，为了高效的计算交集和并集，我们需要倒排列表（postings lists）是有序的，这样方便我们压缩和解压缩。</p><p>针对倒排列表，Lucene 采用一种增量编码的方式将一系列 ID 进行压缩存储，即称为<strong>Frame Of Reference的压缩方式（FOR）</strong>，自Lucene 4.1以来一直在使用。</p><p>在实际的搜索引擎系统中，并不存储倒排索引项中的实际文档编号（Doc ID），而是代之以文档编号差值（D-Gap）。文档编号差值是倒排列表中相邻的两个倒排索引项文档编号的差值，一般在索引构建过程中，可以保证倒排列表中后面出现的文档编号大于之前出现的文档编号，所以文档编号差值总是大于0的整数。</p><p>如下图所示的例子中，原始的 3个文档编号分别是187、196和199，通过编号差值计算，在实际存储的时候就转化成了：187、9、3。<br><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154656895-261135422.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>之所以要对文档编号进行差值计算，主要原因是为了更好地对数据进行压缩，原始文档编号一般都是大数值，通过差值计算，就有效地将大数值转换为了小数值，而这有助于增加数据的压缩率。</p><p>比如一个词对应的文档ID 列表 <code>[73, 300, 302, 332,343, 372]</code> ，ID列表首先要从小到大排好序；</p><ul><li><strong>第一步：</strong> 增量编码就是从第二个数开始每个数存储与前一个id的差值，即<code>300-73=227</code>，<code>302-300=2</code>，…，一直到最后一个数。</li><li><strong>第二步：</strong> 就是将这些差值放到不同的区块，Lucene使用256个区块，下面示例为了方便展示使用了3个区块，即每3个数一组。</li><li><strong>第三步：</strong> 位压缩，计算每组3个数中最大的那个数需要占用bit位数，比如30、11、29中最大数30最小需要5个bit位存储，这样11、29也用5个bit位存储，这样才占用15个bit，不到2个字节，压缩效果很好。</li></ul><p>如下面原理图所示，这是一个区块大小为3的示例（实际上是256）：<br><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154706304-740632821.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>考虑到频繁出现的term（所谓low cardinality的值），比如gender里的男或者女。如果有1百万个文档，那么性别为男的 posting list 里就会有50万个int值。用 <strong>Frame of Reference</strong> 编码进行压缩可以极大减少磁盘占用。这个优化对于减少索引尺寸有非常重要的意义。</p><p>因为这个 <strong>FOR</strong> 的编码是有解压缩成本的。利用skip list(跳表)，除了跳过了遍历的成本，也跳过了解压缩这些压缩过的block的过程，从而节省了cpu。</p><h2 id="Roaring-bitmaps-（RBM）"><a href="#Roaring-bitmaps-（RBM）" class="headerlink" title="Roaring bitmaps （RBM）"></a>Roaring bitmaps （RBM）</h2><p>在 elasticsearch 中使用filters 优化查询，filter查询只处理文档是否匹配与否，不涉及文档评分操作，查询的结果可以被缓存。具体的 Filter 和Query 的异同读者可以自行网上查阅资料。</p><p>对于filter 查询，elasticsearch 提供了Filter cache 这种特殊的缓存，filter cache 用来存储 filters 得到的结果集。缓存 filters 不需要太多的内存，它只保留一种信息，即哪些文档与filter相匹配。同时它可以由其它的查询复用，极大地提升了查询的性能。</p><p>Frame Of Reference 压缩算法对于倒排表来说效果很好，但对于需要存储在内存中的 Filter cache 等不太合适。</p><p>倒排表和Filter cache两者之间有很多不同之处：</p><ul><li>倒排表存储在磁盘，针对每个词都需要进行编码，而Filter等内存缓存只会存储那些经常使用的数据。</li><li>针对Filter数据的缓存就是为了加速处理效率，对压缩算法要求更高。</li></ul><p>这就产生了下面针对内存缓存数据可以进行高效压缩解压和逻辑运算的roaring bitmaps算法。</p><p>说到Roaring bitmaps，就必须先从bitmap说起。Bitmap是一种数据结构，假设有某个posting list：</p><pre><code>[3,1,4,7,8]</code></pre><p>对应的Bitmap就是：</p><pre><code>[0,1,0,1,1,0,0,1,1]</code></pre><p>非常直观，用0/1表示某个值是否存在，比如8这个值就对应第8位，对应的bit值是1，这样用一个字节就可以代表8个文档id（1B = 8bit），旧版本(5.0之前)的Lucene就是用这样的方式来压缩的。但这样的压缩方式仍然不够高效，Bitmap自身就有压缩的特点，其用一个byte就可以代表8个文档，所以100万个文档只需要12.5万个byte。但是考虑到文档可能有数十亿之多，在内存里保存Bitmap仍然是很奢侈的事情。而且对于个每一个filter都要消耗一个Bitmap，比如age=18缓存起来的话是一个Bitmap，18&lt;=age&lt;25是另外一个filter缓存起来也要一个Bitmap。</p><p>Bitmap的缺点是存储空间随着文档个数线性增长，所以秘诀就在于需要有一个数据结构打破这个魔咒，那么就一定要用到某些指数特性：</p><ul><li>可以很压缩地保存上亿个bit代表对应的文档是否匹配filter；</li><li>这个压缩的Bitmap仍然可以很快地进行AND和 OR的逻辑操作。</li></ul><p>Lucene使用的这个数据结构叫做 <strong>Roaring Bitmap</strong>，即位图压缩算法，简称<strong>BMP</strong>。<br><img src="https://img2018.cnblogs.com/blog/1162587/201903/1162587-20190311154718895-359906247.png" referrerpolicy="no-referrer" width="100%" height="100%"></p><p>其压缩的思路其实很简单。与其保存100个0，占用100个bit。还不如保存0一次，然后声明这个0重复了100遍。</p><p>这两种合并使用索引的方式都有其用途。Elasticsearch 对其性能有详细的对比，可阅读 <a href="https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps" target="_blank" rel="noopener">Frame of Reference and Roaring Bitmaps</a>。</p><h1 id="分片策略"><a href="#分片策略" class="headerlink" title="分片策略"></a>分片策略</h1><h2 id="合理设置分片数"><a href="#合理设置分片数" class="headerlink" title="合理设置分片数"></a>合理设置分片数</h2><p>创建索引的时候，我们需要预分配 ES 集群的分片数和副本数，即使是单机情况下。如果没有在 mapping 文件中指定，那么索引在默认情况下会被分配5个主分片和每个主分片的1个副本。</p><p>分片和副本的设计为 ES 提供了支持分布式和故障转移的特性，但并不意味着分片和副本是可以无限分配的。而且索引的分片完成分配后由于索引的路由机制，我们是不能重新修改分片数的。</p><p>例如某个创业公司初始用户的索引 _t_user_ 分片数为2，但是随着业务的发展用户的数据量迅速增长，这时我们是不能重新将索引 _t_user_ 的分片数增加为3或者更大的数。</p><p>可能有人会说，我不知道这个索引将来会变得多大，并且过后我也不能更改索引的大小，所以为了保险起见，还是给它设为 1000 个分片吧…</p><p>一个分片并不是没有代价的。需要了解：</p><ul><li>一个分片的底层即为一个 Lucene 索引，会消耗一定文件句柄、内存、以及 CPU 运转。</li><li>每一个搜索请求都需要命中索引中的每一个分片，如果每一个分片都处于不同的节点还好， 但如果多个分片都需要在同一个节点上竞争使用相同的资源就有些糟糕了。</li><li>用于计算相关度的词项统计信息是基于分片的。如果有许多分片，每一个都只有很少的数据会导致很低的相关度。</li></ul><blockquote><p>适当的预分配是好的。但上千个分片就有些糟糕。我们很难去定义分片是否过多了，这取决于它们的大小以及如何去使用它们。 一百个分片但很少使用还好，两个分片但非常频繁地使用有可能就有点多了。 监控你的节点保证它们留有足够的空闲资源来处理一些特殊情况。</p></blockquote><p>一个业务索引具体需要分配多少分片可能需要架构师和技术人员对业务的增长有个预先的判断，横向扩展应当分阶段进行。为下一阶段准备好足够的资源。 只有当你进入到下一个阶段，你才有时间思考需要作出哪些改变来达到这个阶段。</p><p>一般来说，我们遵循一些原则：</p><ol><li><p>控制每个分片占用的硬盘容量不超过ES的最大JVM的堆空间设置（一般设置不超过32G，参考下文的JVM设置原则），因此，如果索引的总容量在500G左右，那分片大小在16个左右即可；当然，最好同时考虑原则2。</p></li><li><p>考虑一下node数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了1个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以， 一般都设置分片数不超过节点数的3倍。</p></li><li><p>主分片，副本和节点最大数之间数量，我们分配的时候可以参考以下关系：</p></li></ol><ol><li>节点数&lt;=主分片数*（副本数+1）</li></ol><p>创建索引的时候需要控制分片分配行为，合理分配分片，如果后期索引所对应的数据越来越多，我们还可以通过<em>索引别名</em>等其他方式解决。</p><h2 id="调整分片分配器的类型"><a href="#调整分片分配器的类型" class="headerlink" title="调整分片分配器的类型"></a>调整分片分配器的类型</h2><p>以上是在创建每个索引的时候需要考虑的优化方法，然而在索引已创建好的前提下，是否就是没有办法从分片的角度提高了性能了呢？当然不是，首先能做的是调整分片分配器的类型，具体是在 <code>elasticsearch.yml</code> 中设置<code>cluster.routing.allocation.type</code> 属性，共有两种分片器<code>even_shard</code>，<code>balanced（默认）</code>。</p><p><strong>even_shard</strong> 是尽量保证每个节点都具有相同数量的分片，<strong>balanced</strong> 是基于可控制的权重进行分配，相对于前一个分配器，它更暴漏了一些参数而引入调整分配过程的能力。</p><p>每次ES的分片调整都是在ES上的数据分布发生了变化的时候进行的，最有代表性的就是有新的数据节点加入了集群的时候。当然调整分片的时机并不是由某个阈值触发的，ES内置十一个裁决者来决定是否触发分片调整，这里暂不赘述。另外，这些分配部署策略都是可以在运行时更新的，更多配置分片的属性也请大家自行查阅网上资料。</p><h2 id="推迟分片分配"><a href="#推迟分片分配" class="headerlink" title="推迟分片分配"></a>推迟分片分配</h2><p>对于节点瞬时中断的问题，默认情况，集群会等待一分钟来查看节点是否会重新加入，如果这个节点在此期间重新加入，重新加入的节点会保持其现有的分片数据，不会触发新的分片分配。这样就可以减少 ES 在自动再平衡可用分片时所带来的极大开销。</p><p>通过修改参数 <code>delayed_timeout</code> ，可以延长再均衡的时间，可以全局设置也可以在索引级别进行修改:</p><p>PUT /_all/_settings { “settings”: { “index.unassigned.node_left.delayed_timeout”: “5m” }</p><pre><code>}</code></pre><p>通过使用 _all 索引名，我们可以为集群里面的所有的索引使用这个参数，默认时间被延长成了 5 分钟。</p><p>这个配置是动态的，可以在运行时进行修改。如果你希望分片立即分配而不想等待，你可以设置参数： <code>delayed_timeout: 0</code>。</p><blockquote><p>延迟分配不会阻止副本被提拔为主分片。集群还是会进行必要的提拔来让集群回到 yellow 状态。缺失副本的重建是唯一被延迟的过程。</p></blockquote><h1 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h1><h2 id="Mapping建模"><a href="#Mapping建模" class="headerlink" title="Mapping建模"></a>Mapping建模</h2><ol><li><p>尽量避免使用nested或 parent/child，能不用就不用；</p><p>nested query慢， parent/child query 更慢，比nested query慢上百倍；因此能在mapping设计阶段搞定的（大宽表设计或采用比较smart的数据结构），就不要用父子关系的mapping。</p></li><li><p>如果一定要使用nested fields，保证nested fields字段不能过多，目前ES默认限制是50。参考：</p></li></ol><ul><li>index.mapping.nested_fields.limit ：50</li></ul><pre><code>因为针对1个document, 每一个nested field, 都会生成一个独立的document, 这将使Doc数量剧增，影响查询效率，尤其是Join的效率。</code></pre><ul><li><p>避免使用动态值作字段(key)，动态递增的mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。</p><p>控制索引的字段数量、mapping深度、索引字段的类型，对于ES的性能优化是重中之重。以下是ES关于字段数、mapping深度的一些默认设置：</p></li></ul><p>index.mapping.nested_objects.limit :10000 index.mapping.total_fields.limit:1000</p><ol><li>index.mapping.depth.limit: 20</li></ol><ol start="2"><li><p>不需要做模糊检索的字段使用 <code>keyword</code>类型代替 <code>text</code> 类型，这样可以避免在建立索引前对这些文本进行分词。</p></li><li><p>对于那些不需要聚合和排序的索引字段禁用Doc values。</p><p>Doc Values 默认对所有字段启用，除了 <code>analyzed strings</code>。也就是说所有的数字、地理坐标、日期、IP 和不分析（ <code>not_analyzed</code> ）字符类型都会默认开启。</p><p>因为 Doc Values 默认启用，也就是说ES对你数据集里面的大多数字段都可以进行聚合和排序操作。但是如果你知道你永远也不会对某些字段进行聚合、排序或是使用脚本操作， 尽管这并不常见，这时你可以通过禁用特定字段的 Doc Values 。这样不仅节省磁盘空间，也会提升索引的速度。</p><p>要禁用 Doc Values ，在字段的映射（mapping）设置 <code>doc_values: false</code> 即可。</p></li></ol><h2 id="索引设置"><a href="#索引设置" class="headerlink" title="索引设置"></a>索引设置</h2><ol><li><p>如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 <code>index.refresh_interval</code> 改到 30s或者更大。 如果你是在做大批量导入，设置 <code>refresh_interval</code> 为-1，同时设置 <code>number_of_replicas</code> 为0，通过关闭 refresh 间隔周期，同时不设置副本来提高写性能。</p><p>文档在复制的时候，整个文档内容都被发往副本节点，然后逐字的把索引过程重复一遍。这意味着每个副本也会执行分析、索引以及可能的合并过程。</p><p>相反，如果你的索引是零副本，然后在写入完成后再开启副本，恢复过程本质上只是一个字节到字节的网络传输。相比重复索引过程，这个算是相当高效的了。</p></li><li><p>修改 <code>index_buffer_size</code> 的设置，可以设置成百分数，也可设置成具体的大小，最多给512M，大于这个值会触发refresh。默认值是JVM的内存10%，但是是所有切片共享大小。可根据集群的规模做不同的设置测试。</p></li></ol><p>indices.memory.index_buffer_size：10%（默认） indices.memory.min_index_buffer_size： 48mb（默认）</p><ol><li>indices.memory.max_index_buffer_size</li></ol><ol start="2"><li>修改 translog 相关的设置：</li></ol><ul><li>a. 控制数据从内存到硬盘的操作频率，以减少硬盘IO。可将 <code>sync_interval</code> 的时间设置大一些。</li></ul><ul><li>index.translog.sync_interval：5s(默认)。</li></ul><ul><li>b. 控制 tranlog 数据块的大小，达到 threshold 大小时，才会 flush 到 lucene 索引文件。</li></ul><ul><li>index.translog.flush_threshold_size：512mb(默认)</li></ul><ol start="4"><li><p>_id字段的使用，应尽可能避免自定义_id, 以避免针对ID的版本管理；建议使用ES的默认ID生成策略或使用数字类型ID做为主键，包括零填充序列 ID、UUID-1 和纳秒；这些 ID 都是有一致的，压缩良好的序列模式。相反的，像 UUID-4 这样的 ID，本质上是随机的，压缩比很低，会明显拖慢 Lucene。</p></li><li><p><code>_all</code> 字段及 <code>_source</code> 字段的使用，应该注意场景和需要，_all字段包含了所有的索引字段，方便做全文检索，如果无此需求，可以禁用；_source存储了原始的document内容，如果没有获取原始文档数据的需求，可通过设置includes、excludes 属性来定义放入_source的字段。</p></li><li><p>合理的配置使用index属性，analyzed 和not_analyzed，根据业务需求来控制字段是否分词或不分词。只有 groupby需求的字段，配置时就设置成not_analyzed, 以提高查询或聚类的效率。</p></li></ol><h1 id="查询效率"><a href="#查询效率" class="headerlink" title="查询效率"></a>查询效率</h1><ol><li><p>使用批量请求，批量索引的效率肯定比单条索引的效率要高。</p></li><li><p><code>query_string</code> 或 <code>multi_match</code> 的查询字段越多， 查询越慢。可以在 mapping 阶段，利用 copy_to 属性将多字段的值索引到一个新字段，<code>multi_match</code>时，用新的字段查询。</p></li><li><p>日期字段的查询， 尤其是用now 的查询实际上是不存在缓存的，因此， 可以从业务的角度来考虑是否一定要用now, 毕竟利用 <code>query cache</code> 是能够大大提高查询效率的。</p></li><li><p>查询结果集的大小不能随意设置成大得离谱的值， 如<code>query.setSize</code>不能设置成 <code>Integer.MAX_VALUE</code>， 因为ES内部需要建立一个数据结构来放指定大小的结果集数据。</p></li><li><p>尽量避免使用 <code>script</code>，万不得已需要使用的话，选择<code>painless &amp; experssions</code> 引擎。一旦使用 script 查询，一定要注意控制返回，千万不要有死循环（如下错误的例子），因为ES没有脚本运行的超时控制，只要当前的脚本没执行完，该查询会一直阻塞。如：</p></li></ol><p>{ “script_fields”：{ “test1”：{ “lang”：“groovy”， “script”：“while（true）{print ‘don’t use script’}” } }-       }</p><ul><li><p>避免层级过深的聚合查询， 层级过深的group by , 会导致内存、CPU消耗，建议在服务层通过程序来组装业务，也可以通过pipeline 的方式来优化。</p></li><li><p>复用预索引数据方式来提高 AGG 性能：</p><p>如通过 <code>terms aggregations</code> 替代 <code>range aggregations</code>， 如要根据年龄来分组，分组目标是: 少年（14岁以下） 青年（14-28） 中年（29-50） 老年（51以上）， 可以在索引的时候设置一个age_group字段，预先将数据进行分类。从而不用按age来做range aggregations, 通过age_group字段就可以了。</p></li><li><p>Cache的设置及使用：</p><p>**a) QueryCache: **ES查询的时候，使用filter查询会使用query cache, 如果业务场景中的过滤查询比较多，建议将querycache设置大一些，以提高查询速度。</p></li></ul><ol><li>indices.queries.cache.size： 10%（默认），//可设置成百分比，也可设置成具体值，如256mb。</li></ol><pre><code>当然也可以禁用查询缓存（默认是开启）， 通过`index.queries.cache.enabled：false`设置。\*\*b) FieldDataCache: \*\*在聚类或排序时，field data cache会使用频繁，因此，设置字段数据缓存的大小，在聚类或排序场景较多的情形下很有必要，可通过indices.fielddata.cache.size：30% 或具体值10GB来设置。但是如果场景或数据变更比较频繁，设置cache并不是好的做法，因为缓存加载的开销也是特别大的。\*\*c) ShardRequestCache: \*\*查询请求发起后，每个分片会将结果返回给协调节点(Coordinating Node), 由协调节点将结果整合。如果有需求，可以设置开启; 通过设置`index.requests.cache.enable: true`来开启。不过，shard request cache 只缓存 hits.total, aggregations, suggestions 类型的数据，并不会缓存hits的内容。也可以通过设置`indices.requests.cache.size: 1%（默认）`来控制缓存空间大小。</code></pre><h1 id="ES的内存设置"><a href="#ES的内存设置" class="headerlink" title="ES的内存设置"></a>ES的内存设置</h1><p>由于ES构建基于lucene, 而lucene设计强大之处在于lucene能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能。lucene的索引文件segements是存储在单文件中的，并且不可变，对于OS来说，能够很友好地将索引文件保持在cache中，以便快速访问；因此，我们很有必要将一半的物理内存留给lucene ; 另一半的物理内存留给ES（JVM heap )。所以， 在ES内存设置方面，可以遵循以下原则：</p><ol><li><p>当机器内存小于64G时，遵循通用的原则，50%给ES，50%留给lucene。</p></li><li><p>当机器内存大于64G时，遵循以下原则：</p><ul><li>a. 如果主要的使用场景是全文检索, 那么建议给ES Heap分配 4~32G的内存即可；其它内存留给操作系统, 供lucene使用（segments cache), 以提供更快的查询性能。</li><li>b. 如果主要的使用场景是聚合或排序， 并且大多数是numerics, dates, geo_points 以及not_analyzed的字符类型， 建议分配给ES Heap分配 4~32G的内存即可，其它内存留给操作系统，供lucene使用(doc values cache)，提供快速的基于文档的聚类、排序性能。</li><li>c. 如果使用场景是聚合或排序，并且都是基于analyzed 字符数据，这时需要更多的 heap size, 建议机器上运行多ES实例，每个实例保持不超过50%的ES heap设置(但不超过32G，堆内存设置32G以下时，JVM使用对象指标压缩技巧节省空间)，50%以上留给lucene。</li></ul></li><li><p>禁止swap，一旦允许内存与磁盘的交换，会引起致命的性能问题。 通过： 在elasticsearch.yml 中 bootstrap.memory_lock: true， 以保持JVM锁定内存，保证ES的性能。</p></li><li><p>GC设置原则：</p><ul><li>a. 保持GC的现有设置，默认设置为：Concurrent-Mark and Sweep (CMS)，别换成G1GC，因为目前G1还有很多BUG。</li><li>b. 保持线程池的现有设置，目前ES的线程池较1.X有了较多优化设置，保持现状即可；默认线程池大小等于CPU核心数。如果一定要改，按公式（（CPU核心数* 3）/ 2）+ 1 设置；不能超过CPU核心数的2倍；但是不建议修改默认配置，否则会对CPU造成硬伤。</li></ul></li></ol><h1 id="调整JVM设置"><a href="#调整JVM设置" class="headerlink" title="调整JVM设置#"></a>调整JVM设置<a href="https://www.cnblogs.com/jajian/p/10465519.html#调整jvm设置" target="_blank" rel="noopener">#</a></h1><p>ES 是在 lucene 的基础上进行研发的，隐藏了 lucene 的复杂性，提供简单易用的 RESTful Api接口。ES 的分片相当于 lucene 的索引。由于 lucene 是 Java 语言开发的，是 Java 语言就涉及到 JVM，所以 ES 存在 JVM的调优问题。</p><ul><li>调整内存大小。当频繁出现full gc后考虑增加内存大小，但是堆内存和堆外内存不要超过32G。</li><li>调整写入的线程数和队列大小。不过线程数最大不能超过33个（es控制死）。</li><li>ES非常依赖文件系统缓存，以便快速搜索。一般来说，应该至少确保物理上有一半的可用内存分配到文件系统缓存。</li></ul><p>参考文档：</p><ol><li><a href="https://blog.csdn.net/ok0011/article/details/82185133" target="_blank" rel="noopener">elasticsearch倒排表压缩及缓存合并策略</a></li><li><a href="https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps" target="_blank" rel="noopener">Frame of Reference and Roaring Bitmaps</a></li><li><a href="https://zhuanlan.zhihu.com/p/33671444" target="_blank" rel="noopener">elasticsearch 倒排索引原理</a></li><li><a href="https://zhuanlan.zhihu.com/p/43437056" target="_blank" rel="noopener">Elasticsearch性能优化总结</a></li><li><a href="https://blog.51cto.com/13527416/2132270?source=dra" target="_blank" rel="noopener">亿级 Elasticsearch 性能优化</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/jajian/category/1280015.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/jajian/category/128
      
    
    </summary>
    
      <category term="elasticsearch" scheme="http://zhang-yu.me/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="http://zhang-yu.me/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>三次握手-四次挥手-你真的懂吗</title>
    <link href="http://zhang-yu.me/2020/09/28/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B-%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B-%E4%BD%A0%E7%9C%9F%E7%9A%84%E6%87%82%E5%90%97/"/>
    <id>http://zhang-yu.me/2020/09/28/三次握手-四次挥手-你真的懂吗/</id>
    <published>2020-09-28T03:00:00.000Z</published>
    <updated>2020-11-03T13:59:35.881Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/qcrao-2018/p/10182185.html#%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8Bredis%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90" target="_blank" rel="noopener">https://www.cnblogs.com/qcrao-2018/p/10182185.html#%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8Bredis%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90</a></p><p>“三次握手，四次挥手”你真的懂吗？ </p><blockquote><p>记得刚毕业找工作面试的时候，经常会被问到：你知道“3次握手，4次挥手”吗？这时候我会“胸有成竹”地“背诵”前期准备好的“答案”，第一次怎么怎么，第二次……答完就没有下文了，面试官貌似也没有深入下去的意思，深入下去我也不懂，皆大欢喜！</p><p>作为程序员，要有“刨根问底”的精神。知其然，更要知其所以然。这篇文章希望能抽丝剥茧，还原背后的原理。  </p><p>目录</p><ul><li>[什么是“3次握手，4次挥手”]<ul><li>[TCP服务模型] </li><li>[TCP头部]</li><li>[状态转换]</li></ul></li><li>[为什么要“三次握手，四次挥手”]<ul><li>[三次握手]</li><li>[四次挥手]</li></ul></li><li>[“三次握手，四次挥手”怎么完成？]<ul><li>[三次握手]</li><li>[四次挥手]</li><li>[为什么建立连接是三次握手，而关闭连接却是四次挥手呢？]</li></ul></li><li>[“三次握手，四次挥手”进阶]<ul><li>[ISN]</li><li>[序列号回绕]</li><li>[syn flood攻击]<ul><li>[无效连接的监视释放]</li><li>[延缓TCB分配方法]<ul><li>[Syn Cache技术]</li><li>[Syn Cookie技术]</li></ul></li><li>[使用SYN Proxy防火墙]</li></ul></li><li>[连接队列]<ul><li>[半连接队列满了]</li><li>[全连接队列满了]</li><li>[命令]</li><li>[小结]</li></ul></li></ul></li><li>[“三次握手，四次挥手”redis实例分析]</li><li>[总结]</li><li>[参考资料]</li></ul><h1 id="什么是“3次握手，4次挥手”"><a href="#什么是“3次握手，4次挥手”" class="headerlink" title="什么是“3次握手，4次挥手”"></a>什么是“3次握手，4次挥手”</h1><p>TCP是一种面向连接的单播协议，在发送数据前，通信双方必须在彼此间建立一条连接。所谓的“连接”，其实是客户端和服务器的内存里保存的一份关于对方的信息，如ip地址、端口号等。</p><p>TCP可以看成是一种字节流，它会处理IP层或以下的层的丢包、重复以及错误问题。在连接的建立过程中，双方需要交换一些连接的参数。这些参数可以放在TCP头部。</p><p>TCP提供了一种可靠、面向连接、字节流、传输层的服务，采用三次握手建立一个连接。采用4次挥手来关闭一个连接。</p><h2 id="TCP服务模型"><a href="#TCP服务模型" class="headerlink" title="TCP服务模型"></a>TCP服务模型</h2><p>在了解了建立连接、关闭连接的“三次握手和四次挥手”后，我们再来看下TCP相关的东西。</p><p>一个TCP连接由一个4元组构成，分别是两个IP地址和两个端口号。一个TCP连接通常分为三个阶段：启动、数据传输、退出（关闭）。</p><p>当TCP接收到另一端的数据时，它会发送一个确认，但这个确认不会立即发送，一般会延迟一会儿。ACK是累积的，一个确认字节号N的ACK表示所有直到N的字节（不包括N）已经成功被接收了。这样的好处是如果一个ACK丢失，很可能后续的ACK就足以确认前面的报文段了。</p><p>一个完整的TCP连接是双向和对称的，数据可以在两个方向上平等地流动。给上层应用程序提供一种<code>双工服务</code>。一旦建立了一个连接，这个连接的一个方向上的每个TCP报文段都包含了相反方向上的报文段的一个ACK。</p><p>序列号的作用是使得一个TCP接收端可丢弃重复的报文段，记录以杂乱次序到达的报文段。因为TCP使用IP来传输报文段，而IP不提供重复消除或者保证次序正确的功能。另一方面，TCP是一个字节流协议，绝不会以杂乱的次序给上层程序发送数据。因此TCP接收端会被迫先保持大序列号的数据不交给应用程序，直到缺失的小序列号的报文段被填满。</p><h2 id="TCP头部"><a href="#TCP头部" class="headerlink" title="TCP头部"></a>TCP头部</h2><p><img src="https://upload-images.jianshu.io/upload_images/12234098-40089b5b24b9d38b.png" referrerpolicy="no-referrer" width="50%" height="50%"><br>源端口和目的端口在TCP层确定双方进程，序列号表示的是报文段数据中的第一个字节号，ACK表示确认号，该确认号的发送方期待接收的下一个序列号，即最后被成功接收的数据字节序列号加1，这个字段只有在ACK位被启用的时候才有效。</p><p>当新建一个连接时，从客户端发送到服务端的第一个报文段的SYN位被启用，这称为SYN报文段，这时序列号字段包含了在本次连接的这个方向上要使用的第一个序列号，即初始序列号<code>ISN</code>，之后发送的数据是ISN加1，因此SYN位字段会<code>消耗</code>一个序列号，这意味着使用重传进行可靠传输。而不消耗序列号的ACK则不是。</p><p>头部长度（图中的数据偏移）以32位字为单位，也就是以4bytes为单位，它只有4位，最大为15，因此头部最大长度为60字节，而其最小为5，也就是头部最小为20字节（可变选项为空）。</p><p>ACK —— 确认，使得确认号有效。<br>RST —— 重置连接（经常看到的reset by peer）就是此字段搞的鬼。<br>SYN —— 用于初如化一个连接的序列号。<br>FIN —— 该报文段的发送方已经结束向对方发送数据。</p><p>当一个连接被建立或被终止时，交换的报文段只包含TCP头部，而没有数据。</p><h2 id="状态转换"><a href="#状态转换" class="headerlink" title="状态转换"></a>状态转换</h2><p>三次握手和四次挥手的状态转换如下图。<br><img src="https://upload-images.jianshu.io/upload_images/12234098-40f65020a755ca18" referrerpolicy="no-referrer" width="50%" height="50%"> </p><h1 id="为什么要“三次握手，四次挥手”"><a href="#为什么要“三次握手，四次挥手”" class="headerlink" title="为什么要“三次握手，四次挥手”"></a>为什么要“三次握手，四次挥手”</h1><h2 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h2><p>换个易于理解的视角来看为什么要3次握手。</p><p>客户端和服务端通信前要进行连接，“3次握手”的作用就是<code>双方都能明确自己和对方的收、发能力是正常的</code>。</p><p><code>第一次握手</code>：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。</p><p><code>第二次握手</code>：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。  </p></blockquote><blockquote><p>从客户端的视角来看，我接到了服务端发送过来的响应数据包，说明服务端接收到了我在第一次握手时发送的网络包，并且成功发送了响应数据包，这就说明，服务端的接收、发送能力正常。而另一方面，我收到了服务端的响应数据包，说明我第一次发送的网络包成功到达服务端，这样，我自己的发送和接收能力也是正常的。</p><p><code>第三次握手</code>：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力，服务端的发送、接收能力是正常的。  </p></blockquote><blockquote><p>第一、二次握手后，服务端并不知道客户端的接收能力以及自己的发送能力是否正常。而在第三次握手时，服务端收到了客户端对第二次握手作的回应。从服务端的角度，我在第二次握手时的响应数据发送出去了，客户端接收到了。所以，我的发送能力是正常的。而客户端的接收能力也是正常的。</p><p>经历了上面的三次握手过程，客户端和服务端都确认了自己的接收、发送能力是正常的。之后就可以正常通信了。</p><p>每次都是接收到数据包的一方可以得到一些结论，发送的一方其实没有任何头绪。我虽然有发包的动作，但是我怎么知道我有没有发出去，而对方有没有接收到呢？</p><p>而从上面的过程可以看到，最少是需要三次握手过程的。两次达不到让双方都得出自己、对方的接收、发送能力都正常的结论。其实每次收到网络包的一方至少是可以得到：对方的发送、我方的接收是正常的。而每一步都是有关联的，下一次的“响应”是由于第一次的“请求”触发，因此每次握手其实是可以得到额外的结论的。比如第三次握手时，服务端收到数据包，表明看服务端只能得到客户端的发送能力、服务端的接收能力是正常的，但是结合第二次，说明服务端在第二次发送的响应包，客户端接收到了，并且作出了响应，从而得到额外的结论：客户端的接收、服务端的发送是正常的。</p></blockquote><blockquote><h2 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h2><p>TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，等到发送完了所有的数据后，会发送一个FIN段来关闭此方向上的连接。接收方发送ACK确认关闭连接。注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解<code>“上层的意志”</code>。</p><h1 id="“三次握手，四次挥手”怎么完成？"><a href="#“三次握手，四次挥手”怎么完成？" class="headerlink" title="“三次握手，四次挥手”怎么完成？"></a>“三次握手，四次挥手”怎么完成？</h1><p>其实3次握手的目的并不只是让通信双方都了解到一个连接正在建立，还在于利用数据包的选项来传输特殊的信息，交换初始序列号ISN。</p><p>3次握手是指发送了3个报文段，4次挥手是指发送了4个报文段。注意，SYN和FIN段都是会利用重传进行可靠传输的。<br><img src="https://upload-images.jianshu.io/upload_images/12234098-8604b533d42457b5.png0" referrerpolicy="no-referrer" width="100%" height="100%"> </p><h2 id="三次握手-1"><a href="#三次握手-1" class="headerlink" title="三次握手"></a>三次握手</h2><ol><li>客户端发送一个SYN段，并指明客户端的初始序列号，即ISN.</li><li>服务端发送自己的SYN段作为应答，同样指明自己的ISN+1作为ACK数值。这样，每发送一个SYN，序列号就会加1. 如果有丢失的情况，则会重传。</li><li>为了确认服务器端的SYN，客户端将ISN+1作为返回的ACK数值。</li></ol><h2 id="四次挥手-1"><a href="#四次挥手-1" class="headerlink" title="四次挥手"></a>四次挥手</h2><p><img src="https://upload-images.jianshu.io/upload_images/12234098-3754de754cbcf2af.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p></blockquote><blockquote><ol><li>客户端发送一个FIN段，并包含一个希望接收者看到的自己当前的序列号K. 同时还包含一个ACK表示确认对方最近一次发过来的数据。</li><li>服务端将K值加1作为ACK序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。</li><li>服务端发起自己的FIN段，ACK=K+1, Seq=L</li><li>客户端确认。ACK=L+1</li></ol><h2 id="为什么建立连接是三次握手，而关闭连接却是四次挥手呢？"><a href="#为什么建立连接是三次握手，而关闭连接却是四次挥手呢？" class="headerlink" title="为什么建立连接是三次握手，而关闭连接却是四次挥手呢？"></a>为什么建立连接是三次握手，而关闭连接却是四次挥手呢？</h2><p>这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方是否现在关闭发送数据通道，需要上层应用来决定，因此，己方ACK和FIN一般都会分开发送。</p><h1 id="“三次握手，四次挥手”进阶"><a href="#“三次握手，四次挥手”进阶" class="headerlink" title="“三次握手，四次挥手”进阶"></a>“三次握手，四次挥手”进阶</h1><h2 id="ISN"><a href="#ISN" class="headerlink" title="ISN"></a>ISN</h2><p>三次握手的一个重要功能是客户端和服务端交换ISN, 以便让对方知道接下来接收数据的时候如何按序列号组装数据。</p><p>如果ISN是固定的，攻击者很容易猜出后续的确认号。</p><pre><code>ISN = M + F</code></pre><p>M是一个计时器，每隔4微秒加1。<br>F是一个Hash算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值。要保证hash算法不能被外部轻易推算得出。</p><h2 id="序列号回绕"><a href="#序列号回绕" class="headerlink" title="序列号回绕"></a>序列号回绕</h2><p>因为ISN是随机的，所以序列号容易就会超过2^31 -1. 而tcp对于丢包和乱序等问题的判断都是依赖于序列号大小比较的。此时就出现了所谓的tcp序列号回绕（sequence wraparound）问题。怎么解决？</p><pre><code>/** The next routines deal with comparing 32 bit unsigned ints* and worry about wraparound .*/static inline int before{    return  &lt; 0;}#define after</code></pre><p>上述代码是内核中的解决回绕问题代码。__s32是有符号整型的意思，而__u32则是无符号整型。序列号发生回绕后，序列号变小，相减之后，把结果变成有符号数了，因此结果成了负数。</p><pre><code>假设seq1=255， seq2=1（发生了回绕）。seq1 = 1111 1111 seq2 = 0000 0001我们希望比较结果是 seq1 - seq2= 1111 1111-0000 0001----------- 1111 1110由于我们将结果转化成了有符号数，由于最高位是1，因此结果是一个负数，负数的绝对值为 0000 0001 + 1 = 0000 0010 = 2因此seq1 - seq2 &lt; 0</code></pre><h2 id="syn-flood攻击"><a href="#syn-flood攻击" class="headerlink" title="syn flood攻击"></a>syn flood攻击</h2><p>最基本的DoS攻击就是利用合理的服务请求来占用过多的服务资源，从而使合法用户无法得到服务的响应。syn flood属于Dos攻击的一种。</p><p>如果恶意的向某个服务器端口发送大量的SYN包，则可以使服务器打开大量的半开连接，分配TCB（Transmission Control Block）, 从而消耗大量的服务器资源，同时也使得正常的连接请求无法被相应。当开放了一个TCP端口后，该端口就处于Listening状态，不停地监视发到该端口的Syn报文，一 旦接收到Client发来的Syn报文，就需要为该请求分配一个TCB，通常一个TCB至少需要280个字节，在某些操作系统中TCB甚至需要1300个字节，并返回一个SYN ACK命令，立即转为SYN-RECEIVED即半开连接状态。系统会为此耗尽资源。</p><p>常见的防攻击方法有：</p><h3 id="无效连接的监视释放"><a href="#无效连接的监视释放" class="headerlink" title="无效连接的监视释放"></a>无效连接的监视释放</h3><p>监视系统的半开连接和不活动连接，当达到一定阈值时拆除这些连接，从而释放系统资源。这种方法对于所有的连接一视同仁，而且由于SYN Flood造成的半开连接数量很大，正常连接请求也被淹没在其中被这种方式误释放掉，因此这种方法属于入门级的SYN Flood方法。</p><h3 id="延缓TCB分配方法"><a href="#延缓TCB分配方法" class="headerlink" title="延缓TCB分配方法"></a>延缓TCB分配方法</h3><p>消耗服务器资源主要是因为当SYN数据报文一到达，系统立即分配TCB，从而占用了资源。而SYN Flood由于很难建立起正常连接，因此，当正常连接建立起来后再分配TCB则可以有效地减轻服务器资源的消耗。常见的方法是使用Syn Cache和Syn Cookie技术。</p><h4 id="Syn-Cache技术"><a href="#Syn-Cache技术" class="headerlink" title="Syn Cache技术"></a>Syn Cache技术</h4><p>系统在收到一个SYN报文时，在一个专用HASH表中保存这种半连接信息，直到收到正确的回应ACK报文再分配TCB。这个开销远小于TCB的开销。当然还需要保存序列号。</p><h4 id="Syn-Cookie技术"><a href="#Syn-Cookie技术" class="headerlink" title="Syn Cookie技术"></a>Syn Cookie技术</h4><p>Syn Cookie技术则完全不使用任何存储资源，这种方法比较巧妙，它使用一种特殊的算法生成Sequence Number，这种算法考虑到了对方的IP、端口、己方IP、端口的固定信息，以及对方无法知道而己方比较固定的一些信息，如MSS、时间等，在收到对方 的ACK报文后，重新计算一遍，看其是否与对方回应报文中的（Sequence Number-1）相同，从而决定是否分配TCB资源。</p><h3 id="使用SYN-Proxy防火墙"><a href="#使用SYN-Proxy防火墙" class="headerlink" title="使用SYN Proxy防火墙"></a>使用SYN Proxy防火墙</h3><p>一种方式是防止墙dqywb连接的有效性后，防火墙才会向内部服务器发起SYN请求。防火墙代服务器发出的SYN ACK包使用的序列号为c, 而真正的服务器回应的序列号为c’, 这样，在每个数据报文经过防火墙的时候进行序列号的修改。另一种方式是防火墙确定了连接的安全后，会发出一个safe reset命令，client会进行重新连接，这时出现的syn报文会直接放行。这样不需要修改序列号了。但是，client需要发起两次握手过程，因此建立连接的时间将会延长。</p><h2 id="连接队列"><a href="#连接队列" class="headerlink" title="连接队列"></a>连接队列</h2><p>在外部请求到达时，被服务程序最终感知到前，连接可能处于SYN_RCVD状态或是ESTABLISHED状态，但还未被应用程序接受。<br>  <img src="https://upload-images.jianshu.io/upload_images/12234098-36b3c46688c685c7.png" referrerpolicy="no-referrer" width="100%" height="100%"> </p><p>对应地，服务器端也会维护两种队列，处于SYN_RCVD状态的半连接队列，而处于ESTABLISHED状态但仍未被应用程序accept的为全连接队列。如果这两个队列满了之后，就会出现各种丢包的情形。</p><pre><code>查看是否有连接溢出netstat -s | grep LISTEN</code></pre><h3 id="半连接队列满了"><a href="#半连接队列满了" class="headerlink" title="半连接队列满了"></a>半连接队列满了</h3><p>在三次握手协议中，服务器维护一个半连接队列，该队列为每个客户端的SYN包开设一个条目，该条目表明服务器已收到SYN包，并向客户发出确认，正在等待客户的确认包。这些条目所标识的连接在服务器处于Syn_RECV状态，当服务器收到客户的确认包时，删除该条目，服务器进入ESTABLISHED状态。</p><blockquote><p>目前，Linux下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s, 总共31s, 称为<code>指数退避</code>，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s, TCP才会把断开这个连接。由于，<code>SYN超时需要63秒</code>，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server，用于耗尽Server的SYN队列。对于应对SYN 过多的问题，linux提供了几个TCP参数：tcp_syncookies、tcp_synack_retries、tcp_max_syn_backlog、tcp_abort_on_overflow 来调整应对。</p></blockquote></blockquote><blockquote><p>tcp_syncookies</p><p>SYNcookie将连接信息编码在ISN中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。</p><p>tcp_syncookies</p><p>内核放弃建立连接之前发送SYN包的数量。</p><p>tcp_synack_retries</p><p>内核放弃连接之前发送SYN+ACK包的数量</p><p>tcp_max_syn_backlog</p><p>默认为1000. 这表示半连接队列的长度，如果超过则放弃当前连接。</p><p>tcp_abort_on_overflow</p><p>如果设置了此项，则直接reset. 否则，不做任何操作，这样当服务器半连接队列有空了之后，会重新接受连接。<code>Linux坚持在能力许可范围内不忽略进入的连接</code>。客户端在这期间会重复发送sys包，当重试次数到达上限之后，会得到<code>connection time out</code>响应。</p><h3 id="全连接队列满了"><a href="#全连接队列满了" class="headerlink" title="全连接队列满了"></a>全连接队列满了</h3><p>当第三次握手时，当server接收到ACK包之后，会进入一个新的叫 accept 的队列。</p><p>当accept队列满了之后，即使client继续向server发送ACK的包，也会不被响应，此时ListenOverflows+1，同时server通过tcp_abort_on_overflow来决定如何返回，0表示直接丢弃该ACK，1表示发送RST通知client；相应的，client则会分别返回<code>read timeout</code> 或者 <code>connection reset by peer</code>。另外，tcp_abort_on_overflow是0的话，server过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短，就很容易异常了。而客户端收到多个 SYN ACK 包，则会认为之前的 ACK 丢包了。于是促使客户端再次发送 ACK ，在 accept队列有空闲的时候最终完成连接。若 accept队列始终满员，则最终客户端收到 RST 包（此时服务端发送syn+ack的次数超出了tcp_synack_retries）。</p><p>服务端仅仅只是创建一个定时器，以固定间隔重传syn和ack到服务端</p></blockquote><blockquote><p>tcp_abort_on_overflow</p><p>如果设置了此项，则直接reset. 否则，不做任何操作，这样当服务器半连接队列有空了之后，会重新接受连接。<code>Linux坚持在能力许可范围内不忽略进入的连接</code>。客户端在这期间会重复发送sys包，当重试次数到达上限之后，会得到<code>connection time out</code>响应。</p><p>min</p><p>全连接队列的长度。</p><h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><p>netstat -s命令</p><pre><code>[root@server ~]#  netstat -s | egrep &quot;listen|LISTEN&quot; 667399 times the listen queue of a socket overflowed667399 SYNs to LISTEN sockets ignored</code></pre><p>上面看到的 667399 times ，表示全连接队列溢出的次数，隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了。</p><pre><code>[root@server ~]#  netstat -s | grep TCPBacklogDrop</code></pre><p>查看 Accept queue 是否有溢出</p><p>ss命令</p><pre><code>[root@server ~]#  ss -lntState Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN     0      128 *:6379 *:*LISTEN     0      128 *:22 *:*</code></pre><p>如果State是listen状态，Send-Q 表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少。<br>非 LISTEN 状态中 Recv-Q 表示 receive queue 中的 bytes 数量；Send-Q 表示 send queue 中的 bytes 数值。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>当外部连接请求到来时，TCP模块会首先查看max_syn_backlog，如果处于SYN_RCVD状态的连接数目超过这一阈值，进入的连接会被拒绝。根据tcp_abort_on_overflow字段来决定是直接丢弃，还是直接reset.</p><p>从服务端来说，三次握手中，第一步server接受到client的syn后，把相关信息放到半连接队列中，同时回复syn+ack给client. 第三步当收到客户端的ack, 将连接加入到全连接队列。</p><p>一般，全连接队列比较小，会先满，此时半连接队列还没满。如果这时收到syn报文，则会进入半连接队列，没有问题。但是如果收到了三次握手中的第3步，则会根据tcp_abort_on_overflow字段来决定是直接丢弃，还是直接reset.此时，客户端发送了ACK, 那么客户端认为三次握手完成，它认为服务端已经准备好了接收数据的准备。但此时服务端可能因为全连接队列满了而无法将连接放入，会重新发送第2步的syn+ack, 如果这时有数据到来，服务器TCP模块会将数据存入队列中。一段时间后，client端没收到回复，超时，连接异常，client会主动关闭连接。</p><h1 id="“三次握手，四次挥手”redis实例分析"><a href="#“三次握手，四次挥手”redis实例分析" class="headerlink" title="“三次握手，四次挥手”redis实例分析"></a>“三次握手，四次挥手”redis实例分析</h1><ol><li>我在dev机器上部署redis服务，端口号为6379,</li><li><p>通过tcpdump工具获取数据包，使用如下命令</p><p>tcpdump -w /tmp/a.cap port 6379 -s0<br>-w把数据写入文件，-s0设置每个数据包的大小默认为68字节，如果用-S 0则会抓到完整数据包</p></li></ol><ol start="3"><li>在dev2机器上用redis-cli访问dev:6379, 发送一个ping, 得到回复pong</li><li><p>停止抓包，用tcpdump读取捕获到的数据包</p><p>tcpdump -r /tmp/a.cap -n -nn -A -x| vim -<br>（-x 以16进制形式展示，便于后面分析）</p></li></ol><p>共收到了7个包。</p><p>抓到的是IP数据包，IP数据包分为IP头部和IP数据部分，IP数据部分是TCP头部加TCP数据部分。</p><p>IP的数据格式为：<br>  <img src="https://upload-images.jianshu.io/upload_images/12234098-e6b04f3e9bebdac4.png" referrerpolicy="no-referrer" width="100%" height="100%"><br>它由固定长度20B+可变长度构成。</p><pre><code>10:55:45.662077 IP dev2.39070 &gt; dev.6379: Flags [S], seq 4133153791, win 29200, options [mss 1460,sackOK,TS val 2959270704 ecr 0,nop,wscale 7], length 0        0x0000:  4500 003c 08cf 4000 3606 14a5 0ab3 b561        0x0010:  0a60 5cd4 989e 18eb f65a ebff 0000 0000        0x0020:  a002 7210 872f 0000 0204 05b4 0402 080a        0x0030:  b062 e330 0000 0000 0103 0307</code></pre><p>对着IP头部格式，来拆解数据包的具体含义。</p><p>字节值</p><p>字节含义</p><p>0x4</p><p>IP版本为ipv4</p><p>0x5</p><p>首部长度为5 * 4字节=20B</p><p>0x00</p><p>服务类型，现在基本都置为0</p><p>0x003c</p><p>总长度为3*16+12=60字节，上面所有的长度就是60字节</p><p>0x08cf</p><p>标识。同一个数据报的唯一标识。当IP数据报被拆分时，会复制到每一个数据中。</p><p>0x4000</p><p><code>3bit 标志 + 13bit 片偏移</code>。3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8 )</p><p>0x36</p><p>生存时间TTL。IP报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。 此处为54.</p><p>0x06</p><p>协议类型。指出IP报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。</p><p>0x14a5</p><p>16bitIP首部校验和。</p><p>0x0ab3 b561</p><p>32bit源ip地址。</p><p>0x0a60 5cd4</p><p>32bit目的ip地址。</p><p>剩余的数据部分即为TCP协议相关的。TCP也是20B固定长度+可变长度部分。</p><p>字节值</p><p>字节含义</p><p>0x989e</p><p>16bit源端口。1_16_16_16+8_16_16+14_16+11=39070</p><p>0x18eb</p><p>16bit目的端口6379</p><p>0xf65a ebff</p><p>32bit序列号。4133153791</p><p>0x0000 0000</p><p>32bit确认号。</p><p>0xa</p><p>4bit首部长度，以4byte为单位。共10*4=40字节。因此TCP报文的可选长度为40-20=20</p><p>0b000000</p><p>6bit保留位。目前置为0.</p><p>0b000010</p><p>6bitTCP标志位。从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。</p><p>0x7210</p><p>滑动窗口大小，滑动窗口即tcp接收缓冲区的大小，用于tcp拥塞控制。29200</p><p>0x872f</p><p>16bit校验和。</p><p>0x0000</p><p>紧急指针。仅在 URG = 1时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。</p><p>可变长度部分，协议如下：</p><p>字节值</p><p>字节含义</p><p>0x0204 05b4</p><p>最大报文长度为，05b4=1460. 即可接收的最大包长度，通常为MTU减40字节，IP头和TCP头各20字节</p><p>0x0402</p><p>表示支持SACK</p><p>0x080a b062 e330 0000 0000</p><p>时间戳。Ts val=b062 e330=2959270704, ecr=0</p><p>0x01</p><p>无操作</p><p>0x03 0307</p><p>窗口扩大因子为7. 移位7, 乘以128</p><p>这样第一个包分析完了。dev2向dev发送SYN请求。<code>也就是三次握手中的第一次了。</code><br><code>SYN seq=4133153791</code></p><p>第二个包，dev响应连接，ack=4133153792. 表明dev下次准备接收这个序号的包，用于tcp字节注的顺序控制。dev（也就是server端）的初始序号为seq=4264776963, syn=1.<br><code>SYN ack=seq=4264776963</code></p><p>第三个包，client包确认，这里使用了相对值应答。seq=4133153792, 等于第二个包的ack. ack=4264776964.<br><code>ack=seq+1</code><br>至此，三次握手完成。接下来就是发送ping和pong的数据了。</p><p>接着第四个包。</p><pre><code>10:55:48.090073 IP dev2.39070 &gt; dev.6379: Flags [P.], seq 1:15, ack 1, win 229, options [nop,nop,TS val 2959273132 ecr 3132256230], length 14        0x0000:  4500 0042 08d1 4000 3606 149d 0ab3 b561        0x0010:  0a60 5cd4 989e 18eb f65a ec00 fe33 5504        0x0020:  8018 00e5 4b5f 0000 0101 080a b062 ecac        0x0030:  bab2 6fe6 2a31 0d0a 2434 0d0a 7069 6e67        0x0040:  0d0a</code></pre><p>tcp首部长度为32B, 可选长度为12B. IP报文的总长度为66B, 首部长度为20B, 因此TCP数据部分长度为14B. seq=0xf65a ec00=4133153792<br>ACK, PSH. 数据部分为2a31 0d0a 2434 0d0a 7069 6e67 0d0a</p><pre><code>0x2a31         -&gt; *10x0d0a         -&gt; \r\n0x2434         -&gt; $40x0d0a         -&gt; \r\n0x7069 0x6e67  -&gt; ping0x0d0a         -&gt; \r\n</code></pre><p>dev2向dev发送了ping数据，第四个包完毕。</p><p>第五个包，dev2向dev发送ack响应。<br>序列号为0xfe33 5504=4264776964, ack确认号为0xf65a ec0e=4133153806=.</p><p>第六个包，dev向dev2响应pong消息。序列号fe33 5504，确认号f65a ec0e, TCP头部可选长度为12B, IP数据报总长度为59B, 首部长度为20B, 因此TCP数据长度为7B.<br>数据部分2b50 4f4e 470d 0a, 翻译过来就是<code>+PONG\r\n</code>.</p><p>至此，Redis客户端和Server端的三次握手过程分析完毕。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>“三次握手，四次挥手”看似简单，但是深究进去，还是可以延伸出很多知识点的。比如半连接队列、全连接队列等等。以前关于TCP建立连接、关闭连接的过程很容易就会忘记，可能是因为只是死记硬背了几个过程，没有深入研究背后的原理。</p><p>所以，“三次握手，四次挥手”你真的懂了吗？欢迎一起交流~~</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>【redis】[<a href="https://segmentfault.com/a/1190000015044878]" target="_blank" rel="noopener">https://segmentfault.com/a/1190000015044878]</a><br>【tcp option】[<a href="https://blog.csdn.net/wdscq1234/article/details/52423272]" target="_blank" rel="noopener">https://blog.csdn.net/wdscq1234/article/details/52423272]</a><br>【滑动窗口】[<a href="https://www.zhihu.com/question/32255109]" target="_blank" rel="noopener">https://www.zhihu.com/question/32255109]</a><br>【全连接队列】[<a href="http://jm.taobao.org/2017/05/25/525-1/]" target="_blank" rel="noopener">http://jm.taobao.org/2017/05/25/525-1/]</a><br>【client fooling】 [<a href="https://github.com/torvalds/linux/commit/5ea8ea2cb7f1d0db15762c9b0bb9e7330425a071]" target="_blank" rel="noopener">https://github.com/torvalds/linux/commit/5ea8ea2cb7f1d0db15762c9b0bb9e7330425a071]</a><br>【backlog RECV_Q】[<a href="http://blog.51cto.com/59090939/1947443]" target="_blank" rel="noopener">http://blog.51cto.com/59090939/1947443]</a><br>【定时器】[<a href="https://www.cnblogs.com/menghuanbiao/p/5212131.html]" target="_blank" rel="noopener">https://www.cnblogs.com/menghuanbiao/p/5212131.html]</a><br>【队列图示】[<a href="https://www.itcodemonkey.com/article/5834.html]" target="_blank" rel="noopener">https://www.itcodemonkey.com/article/5834.html]</a><br>【tcp flood攻击】[<a href="https://www.cnblogs.com/hubavyn/p/4477883.html]" target="_blank" rel="noopener">https://www.cnblogs.com/hubavyn/p/4477883.html]</a><br>【MSS MTU】[<a href="https://blog.csdn.net/LoseInVain/article/details/53694265]" target="_blank" rel="noopener">https://blog.csdn.net/LoseInVain/article/details/53694265]</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/qcrao-2018/p/10182185.html#%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8Bredis
      
    
    </summary>
    
      <category term="it" scheme="http://zhang-yu.me/categories/it/"/>
    
    
      <category term="it" scheme="http://zhang-yu.me/tags/it/"/>
    
  </entry>
  
  <entry>
    <title>线程数究竟设多少合理</title>
    <link href="http://zhang-yu.me/2020/09/28/%E7%BA%BF%E7%A8%8B%E6%95%B0%E7%A9%B6%E7%AB%9F%E8%AE%BE%E5%A4%9A%E5%B0%91%E5%90%88%E7%90%86/"/>
    <id>http://zhang-yu.me/2020/09/28/线程数究竟设多少合理/</id>
    <published>2020-09-28T03:00:00.000Z</published>
    <updated>2020-11-03T14:02:21.012Z</updated>
    
    <content type="html"><![CDATA[<p>58沈剑 架构师之路 2016-03-29</p><blockquote><p><strong>一、需求缘起</strong></p><p>Web-Server通常有个配置，<strong>最大工作线程数</strong>，后端服务一般也有个配置，工作线程池的<strong>线程数量</strong>，这个线程数的配置不同的业务架构师有不同的经验值，有些业务设置为CPU核数的2倍，有些业务设置为CPU核数的8倍，有些业务设置为CPU核数的32倍。</p><p>“工作线程数”的设置依据是什么，到底设置为多少能够最大化CPU性能，是本文要讨论的问题。</p><h2 id="二、一些共性认知"><a href="#二、一些共性认知" class="headerlink" title="二、一些共性认知"></a><strong>二、一些共性认知</strong></h2><p>在进行进一步深入讨论之前，先以提问的方式就一些共性认知达成一致。</p><p><strong>提问：工作线程数是不是设置的越大越好？</strong></p><p>回答：肯定不是的</p><p>1）一来服务器CPU核数有限，同时并发的线程数是有限的，1核CPU设置10000个工作线程没有意义</p><p>2）线程切换是有开销的，如果线程切换过于频繁，反而会使性能降低</p><p><strong>提问：调用sleep()**</strong>函数的时候，线程是否一直占用CPU<strong>**？</strong></p><p>回答：不占用，等待时会把CPU让出来，给其他需要CPU资源的线程使用</p><p>不止调用sleep()函数，在进行一些阻塞调用，例如网络编程中的<strong>阻塞</strong>accept()【等待客户端连接】和<strong>阻塞</strong>recv()【等待下游回包】也不占用CPU资源</p><p><strong>提问：如果CPU**</strong>是单核，设置多线程有意义么，能提高并发性能么？**</p><p>回答：即使是单核，使用多线程也是有意义的</p><p>1）多线程编码可以让我们的服务/代码更加清晰，有些IO线程收发包，有些Worker线程进行任务处理，有些Timeout线程进行超时检测</p><p>2）如果有一个任务一直占用CPU资源在进行计算，那么此时增加线程并不能增加并发，例如这样的一个代码</p><p> while(1){ i++; }</p><p>该代码一直不停的占用CPU资源进行计算，会使CPU占用率达到100%</p><p>3）通常来说，Worker线程一般不会一直占用CPU进行计算，此时即使CPU是单核，增加Worker线程也能够提高并发，因为这个线程在休息的时候，其他的线程可以继续工作</p><h2 id="三、常见服务线程模型"><a href="#三、常见服务线程模型" class="headerlink" title="三、常见服务线程模型"></a><strong>三、常见服务线程模型</strong></h2><p>了解常见的服务线程模型，有助于理解服务并发的原理，一般来说互联网常见的服务线程模型有如下两种</p><p><strong>IO**</strong>线程与工作线程通过队列解耦类模型**<br><img src="https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190514151730305-485918824.png" referrerpolicy="no-referrer" width="100%" height="100%"><br>如上图，大部分Web-Server与服务框架都是使用这样的一种“IO线程与Worker线程通过队列解耦”类线程模型：</p><p>1）<strong>有少数几个IO线程</strong>监听上游发过来的请求，并进行收发包（生产者）</p><p>2）<strong>有一个或者多个任务队列</strong>，作为IO线程与Worker线程异步解耦的数据传输通道（临界资源）</p><p>3）<strong>有多个工作线程</strong>执行正真的任务（消费者）</p><p>这个线程模型应用很广，符合大部分场景，这个线程模型的特点是，<strong>工作线程内部是同步阻塞执行任务的</strong>（回想一下tomcat线程中是怎么执行Java程序的，dubbo工作线程中是怎么执行任务的），因此可以通过增加Worker线程数来增加并发能力，今天要讨论的重点是“该模型Worker线程数设置为多少能达到最大的并发”。</p><p><strong>纯异步线程模型</strong></p><p>任何地方都没有阻塞，这种线程模型只需要设置很少的线程数就能够做到很高的吞吐量，Lighttpd有一种单进程单线程模式，并发处理能力很强，就是使用的的这种模型。该模型的缺点是：</p><p>1）如果使用单线程模式，难以利用多CPU多核的优势</p><p>2）程序员更习惯写同步代码，callback的方式对代码的可读性有冲击，对程序员的要求也更高</p><p>3）框架更复杂，往往需要server端收发组件，server端队列，client端收发组件，client端队列，上下文管理组件，有限状态机组件，超时管理组件的支持</p><p>however，这个模型不是今天讨论的重点。</p><h2 id="四、工作线程的工作模式"><a href="#四、工作线程的工作模式" class="headerlink" title="四、工作线程的工作模式"></a><strong>四、工作线程的工作模式</strong></h2><p>了解工作线程的工作模式，对量化分析线程数的设置非常有帮助：<br> <img src="https://img2018.cnblogs.com/blog/1162587/201905/1162587-20190514151718013-879900903.png" referrerpolicy="no-referrer" width="100%" height="100%"><br>上图是一个典型的工作线程的处理过程，从开始处理start到结束处理end，该任务的处理共有7个步骤：</p><p>1）从工作队列里拿出任务，进行一些<strong>本地</strong>初始化计算，例如http协议分析、参数解析、参数校验等</p><p>2）<strong>访问cache</strong>拿一些数据</p><p>3）拿到cache里的数据后，再进行一些<strong>本地</strong>计算，这些计算和业务逻辑相关</p><p>4）通过<strong>RPC调用下游service</strong>再拿一些数据，或者让下游service去处理一些相关的任务</p><p>5）RPC调用结束后，再进行一些<strong>本地</strong>计算，怎么计算和业务逻辑相关</p><p>6）<strong>访问DB</strong>进行一些数据操作</p><p>7）操作完数据库之后做一些收尾工作，同样这些收尾工作也是<strong>本地</strong>计算，和业务逻辑相关</p><p>分析整个处理的时间轴，会发现：</p><p>1）其中1，3，5，7步骤中【上图中粉色时间轴】，线程进行本地业务逻辑计算时<strong>需要占用**</strong>CPU**</p><p>2）而2，4，6步骤中【上图中橙色时间轴】，访问cache、service、DB过程中线程处于一个等待结果的状态，<strong>不需要占用**</strong>CPU**，进一步的分解，这个“等待结果”的时间共分为三部分：</p><p>2.1）请求在网络上传输到下游的cache、service、DB</p><p>2.2）下游cache、service、DB进行任务处理</p><p>2.3）cache、service、DB将报文在网络上传回工作线程</p><h2 id="五、量化分析并合理设置工作线程数"><a href="#五、量化分析并合理设置工作线程数" class="headerlink" title="五、量化分析并合理设置工作线程数"></a><strong>五、量化分析并合理设置工作线程数</strong></h2><p>最后一起来回答工作线程数设置为多少合理的问题。</p><p>通过上面的分析，Worker线程在执行的过程中，有一部计算时间需要占用CPU，另一部分等待时间不需要占用CPU，通过量化分析，例如打日志进行统计，可以统计出整个Worker线程执行过程中这两部分时间的比例，例如：</p><p>1）时间轴1，3，5，7【上图中粉色时间轴】的计算执行时间是100ms</p><p>2）时间轴2，4，6【上图中橙色时间轴】的等待时间也是100ms</p><p>得到的结果是，这个线程计算和等待的时间是1：1，即有50%的时间在计算（占用CPU），50%的时间在等待（不占用CPU）：</p><p>1）假设此时是<strong>单核</strong>，则设置为2个工作线程就可以把CPU充分利用起来，让CPU跑到100%</p><p>2）假设此时是<strong>N核</strong>，则设置为2N个工作现场就可以把CPU充分利用起来，让CPU跑到N*100%</p><p>结论：</p><p>N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。</p><p>经验：</p><p>一般来说，非CPU密集型的业务（加解密、压缩解压缩、搜索排序等业务是CPU密集型的业务），瓶颈都在后端数据库，本地CPU计算的时间很少，所以设置几十或者几百个工作线程也都是可能的。</p><h2 id="六、结论"><a href="#六、结论" class="headerlink" title="六、结论"></a><strong>六、结论</strong></h2><p>N核服务器，通过执行业务的单线程分析出本地计算时间为x，等待时间为y，则工作线程数（线程池线程数）设置为 N*(x+y)/x，能让CPU的利用率最大化。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;58沈剑 架构师之路 2016-03-29&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;一、需求缘起&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Web-Server通常有个配置，&lt;strong&gt;最大工作线程数&lt;/strong&gt;，后端服务一般也有个配置，工作线程池的&lt;str
      
    
    </summary>
    
      <category term="it" scheme="http://zhang-yu.me/categories/it/"/>
    
    
      <category term="it" scheme="http://zhang-yu.me/tags/it/"/>
    
  </entry>
  
  <entry>
    <title>如何在Kubernetes上部署MySQL数据库</title>
    <link href="http://zhang-yu.me/2020/08/12/%E5%9C%A8Kubernetes%E4%B8%8A%E9%83%A8%E7%BD%B2MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://zhang-yu.me/2020/08/12/在Kubernetes上部署MySQL数据库/</id>
    <published>2020-08-11T16:00:00.000Z</published>
    <updated>2020-11-03T14:01:57.070Z</updated>
    
    <content type="html"><![CDATA[<p>译者：王延飞</p><p>原文链接：<a href="https://www.magalix.com/blog/kubernetes-and-database" target="_blank" rel="noopener">https://www.magalix.com/blog/kubernetes-and-database</a></p><p><a href="https://mp.weixin.qq.com/s/j-Dbyp2xzk2zbYBZcA-lCQ" target="_blank" rel="noopener">如何在Kubernetes上部署MySQL数据库</a></p><blockquote><p>Kubernetes改变了开发的方式，数据库是应用程序的重要组成部分。在本文中，我们将展示如何在Kubernetes中部署数据库，以及可以使用哪些方法在Kubernetes中部署数据库。</p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p>数据库是一种用于在计算机系统上存储和处理数据的系统。数据库引擎可以在数据库上创建，读取，更新和删除。数据库由数据库管理系统（DBMS）控制。</p><p>在大多数数据库中，数据按行和列进行建模，称为关系型，这种类型的数据库在80年代占主导地位。在2000年代，非关系数据库开始流行，被称为No-SQL，它们使用不同的查询语言，并且这些类型的数据库可用于键值对。</p><h2 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h2><p>在本文中，我们将在Kubernetes中部署数据库，因此我们必须了解什么是StatefulSet。</p><p>StatefulSet是用于管理有状态应用程序的工作负载。它管理一组Pod的实现和扩展，并保证这些Pod的顺序和唯一性。</p><p>像Deployment一样，StatefulSet也管理具有相同容器规范的一组Pod。由StatefulSets维护的Pod具有唯一的，持久的身份和稳定的主机名，而不用管它们位于哪个节点上。如果我们想要一个跨存储的持久性，我们可以创建一个持久性卷并将StatefulSet用作解决方案的一部分。即使StatefulSet中的Pod容易发生故障，存储卷与新Pod进行匹配也很容易。</p><p>StatefulSet对于需要以下一项或多项功能的应用程序很有价值：</p><ul><li><p>稳定的唯一网络标识符。</p></li><li><p>稳定，持久的存储。</p></li><li><p>有序，顺畅的部署和扩展。</p></li><li><p>有序的自动滚动更新。</p></li></ul><p>在Kubernetes上部署数据库时，我们需要使用StatefulSet，但是使用StatefulSet有一些局限性：</p><ul><li><p>需要使用持久性存储卷为Pod提供存储。</p></li><li><p>删除副本或按比例缩小副本将不会删除附加到StatefulSet的存储卷。存储卷确保数据的安全性。</p></li><li><p>StatefulSet当前需要Headless Service 来负责Pod的网络标识。</p></li><li><p>与Deployment 不同，StatefulSet不保证删除StatefulSet资源时删除所有Pod，而Deployment在被删除时会删除与Deployment关联的所有Pod。在删除StatefulSet之前，你必须将pod副本数量缩小到0 。</p></li></ul><h2 id="Kubernetes上的数据库"><a href="#Kubernetes上的数据库" class="headerlink" title="Kubernetes上的数据库"></a>Kubernetes上的数据库</h2><p>我们可以将数据库作为有状态应用程序部署到Kubernetes。通常，当我们部署Pod时，它们具有自己的存储空间，但是该存储空间是短暂的-如果容器被杀死了，则其存储空间将随之消失。</p><p>因此，我们需要有一个Kubernetes资源对象来解决这种情况：当我们想要数据持久化时，我们就把Pod和持久化存储卷声明关联。通过这种方式，如果我们的容器被杀死了，我们的数据仍将位于集群中，新的pod也能够相应地访问数据。</p><p>Pod -&gt; PVC-&gt; PV</p><ul><li><p>PV =持久性存储</p></li><li><p>PVC =持久性存储声明</p></li></ul><h2 id="Operators将数据库部署到Kubernetes"><a href="#Operators将数据库部署到Kubernetes" class="headerlink" title="Operators将数据库部署到Kubernetes"></a>Operators将数据库部署到Kubernetes</h2><ul><li>我们可以使用由Oracle开发的Kubernetes Operators来部署MySQL数据库：</li></ul><pre><code>https://github.com/oracle/mysql-operator</code></pre><ul><li>使用Crunchydata开发的PostgreSQL Operators，、将PostgreSQL部署到Kubernetes：</li></ul><pre><code>https://github.com/CrunchyData/postgres-operator</code></pre><ul><li>使用MongoDB开发的Operators，可将MongoDB Enterprise部署到Kubernetes集群：</li></ul><pre><code>https://github.com/mongodb/mongodb-enterprise-kubernetes</code></pre><h2 id="在Kubernetes上部署数据库是否可行？"><a href="#在Kubernetes上部署数据库是否可行？" class="headerlink" title="在Kubernetes上部署数据库是否可行？"></a>在Kubernetes上部署数据库是否可行？</h2><p>在当今世界上，越来越多的公司致力于容器技术。在进行深入研究之前，让我们回顾一下用于运行数据库的选项。</p><h3 id="1-完全托管的数据库"><a href="#1-完全托管的数据库" class="headerlink" title="1.完全托管的数据库"></a>1.完全托管的数据库</h3><p>完全托管的数据库是那些不用自己来管理的数据库-这种管理可以由AWS Google，Azure或Digital Cloud等云提供商完成。托管数据库包括Amazon Web Services，Aurora DynamoDB或Google Spanner等。</p><p>使用这些完全托管的数据库的优势是操作少，云提供商可以处理许多维护任务，例如备份，扩展补丁等。你只需创建数据库即可构建应用程序，其他的由云提供商帮你处理。</p><h3 id="2-在VM或本地自行部署"><a href="#2-在VM或本地自行部署" class="headerlink" title="2.在VM或本地自行部署"></a>2.在VM或本地自行部署</h3><p>使用此选项，你可以将数据库部署到任何虚拟机（EC2或Compute Engine），并且将拥有完全控制权。你将能够部署任何版本的数据库，并且可以设置自己的安全性和备份计划。</p><p>另一方面，这意味着你将自行管理，修补，扩展或配置数据库。这将增加基础架构的成本，但具有灵活性的优势。</p><h3 id="3-在Kubernetes上运行"><a href="#3-在Kubernetes上运行" class="headerlink" title="3.在Kubernetes上运行"></a>3.在Kubernetes上运行</h3><p>在Kubernetes中部署数据库更接近full-ops选项，但是从Kubernetes提供的自动化方面来看，你将获得一些好处–能够保持数据库应用程序的正常运行。</p><p>要注意，pod是短暂的，因此数据库应用程序重新启动或失败的可能性更大。另外，你将负责更具体的数据库管理任务，例如备份，扩展等。</p><p>选择在Kubernetes上部署数据库时要考虑的一些重要点是：</p><ul><li><p>有一些自定义资源和 operators可用于在Kubernetes上管理数据库。</p></li><li><p>具有缓存层和瞬时态存储的数据库更适合Kubernetes。</p></li><li><p>你必须了解数据库中可用的复制模式。异步复制模式为数据丢失留有空间，因为事务可能会提交给主数据库，而不会提交给从数据库。</p><p><img src="https://www.magalix.com/hs-fs/hubfs/Google%20Drive%20Integration/kubernetes%20and%20database.png" referrerpolicy="no-referrer" width="50%" height="50%"><br>上面，我们用一个简单的图表来显示在Kubernetes上部署数据库时的决策。</p></li></ul><p>首先，我们需要尝试了解数据库是否具有与Kubernetes友好的功能，例如MySQL或PostgreSQL，然后我们查找kubernetes operators将数据库与其他功能打包在一起。</p><p>第二个问题是-考虑到在Kubernetes中部署数据库需要多少工作量，这是可以接受的？我们是否有一个运维团队，或者在托管数据库上部署数据库是否可行？</p><h2 id="在Kubernetes上部署有状态应用程序："><a href="#在Kubernetes上部署有状态应用程序：" class="headerlink" title="在Kubernetes上部署有状态应用程序："></a>在Kubernetes上部署有状态应用程序：</h2><h3 id="步骤1：部署MySQL服务"><a href="#步骤1：部署MySQL服务" class="headerlink" title="步骤1：部署MySQL服务"></a>步骤1：部署MySQL服务</h3><pre><code>apiVersion: v1</code></pre><p>首先，我们在端口3306上为MySQL数据库部署服务，所有Pod均具有标签键app: mysql。</p><p>接下来，创建以下资源：</p><pre><code>Kubectl create -f mysql_service.yaml</code></pre><h3 id="步骤2：部署MySQL-Deployment"><a href="#步骤2：部署MySQL-Deployment" class="headerlink" title="步骤2：部署MySQL Deployment"></a>步骤2：部署MySQL Deployment</h3><pre><code>apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2</code></pre><p>此Deployment在3306端口上创建带有MySQL5.6镜像和密码(使用secret)的Pod。我们还将附加一个持久卷mysql-pv-claim，将在接下来的步骤中进行显示。</p><p>创建资源：</p><pre><code>Kubectl create -f mysql_deployment.yaml</code></pre><h3 id="第3步：创建持久卷"><a href="#第3步：创建持久卷" class="headerlink" title="第3步：创建持久卷"></a>第3步：创建持久卷</h3><pre><code>apiVersion: v1</code></pre><p>这将创建一个持久卷，我们将使用它来附加到容器，以确保Pod重启时的数据安全。该持久卷具有ReadWriteOne访问模式，拥有20GB的存储空间，存放路径是/ mnt/data，我们所有的数据都将保存在该路径中。</p><p>创建以下资源：</p><pre><code>Kubectl create -f persistence_volume.yaml</code></pre><h3 id="第4步：创建持久卷声明"><a href="#第4步：创建持久卷声明" class="headerlink" title="第4步：创建持久卷声明"></a>第4步：创建持久卷声明</h3><pre><code>apiVersion: v1</code></pre><p>该声明从上面创建的“持久卷”中声明20GB，并具有与上面的“持久卷”相同的访问模式。</p><p>创建以下资源：</p><pre><code>Kubectl create -f pvClaim.yaml</code></pre><h3 id="步骤5：测试MySQL数据库"><a href="#步骤5：测试MySQL数据库" class="headerlink" title="步骤5：测试MySQL数据库"></a>步骤5：测试MySQL数据库</h3><pre><code>kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword</code></pre><p>此命令在运行MySQL的集群中创建一个新的Pod，并连接到MySQL服务器。如果连接成功，则说明你的MySQL数据库已启动并正在运行。</p><pre><code>Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false</code></pre><p>以上完整代码存放在这个位置：<a href="https://github.com/zarakM/mysql-k8.git" target="_blank" rel="noopener">https://github.com/zarakM/mysql-k8.git</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>有状态应用程序是存储用户会话状态的应用程序，保存的数据称为应用程序状态。</p></li><li><p>StatefulSet是一个Kubernetes资源对象，用于管理有状态应用程序，并提供有关Pod顺序和唯一性的保证。</p></li><li><p>通过删除StatefulSet，不会删除StatefulSet中的pod。相反如果删除，你必须将有状态应用程序副本数量缩小为0。</p></li><li><p>Kubernetes上的数据库部署有一个持久存储卷，只要你的集群正在运行，该存储卷就可以永久存储数据。这意味着它可以抵御pod的破坏，并且创建的任何新pod将能够再次使用该存储卷。</p></li><li><p>完全托管的数据库是由云提供商管理的数据库。我们不必管理数据库。这些数据库需要额外的费用，但是如果你想专注于应用程序，它们是最佳选择。</p></li><li><p>你可以通过VM部署数据库。但你将必须处理所有数据库操作，例如扩展，设置和修补。</p></li><li><p>最后，我们展示了如何在Kubernetes上部署数据库。</p></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;译者：王延飞&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://www.magalix.com/blog/kubernetes-and-database&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.magalix.com/
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>GitOps</title>
    <link href="http://zhang-yu.me/2020/07/13/GitOps/"/>
    <id>http://zhang-yu.me/2020/07/13/GitOps/</id>
    <published>2020-07-13T07:00:00.000Z</published>
    <updated>2020-07-13T09:14:41.549Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h1 id="GitOps-在-Kubernetes-中进行-DevOps-的方式"><a href="#GitOps-在-Kubernetes-中进行-DevOps-的方式" class="headerlink" title="GitOps - 在 Kubernetes 中进行 DevOps 的方式"></a>GitOps - 在 Kubernetes 中进行 DevOps 的方式</h1></blockquote><blockquote><h1 id="什么是-GitOps？"><a href="#什么是-GitOps？" class="headerlink" title="什么是 GitOps？"></a>什么是 GitOps？</h1><p>GitOps 是一个概念，将软件的端到端描述放置到 Git 中，然后尝试着让集群状态和 Git 仓库持续同步，其中有两个概念需要说明下。</p><ol><li><strong>软件的描述表示</strong>：Kubernetes、应用和底层基础架构之间的关系是一种声明式的，我们用声明式方式（YAML）来描述我们需要的基础架构。这些 YAML 的实现细节被底层的 Kubernetes 集群的 Controller、Schedulers、CoreDNS、Operator 等等抽象出来，这使得我们可以从传统的<strong>基础架构即代码</strong>转向<strong>基础架构即数据，</strong>我们也可以从 GitHub 上了解到更多相关的信息。这里的关键是，你需要的每一个应用声明的角色（应用开发者/应用运维/集群运维）都会被用到持续交互流水线的 YAML 中，最后被推送到 GitOps 仓库中去。</li></ol><ol start="2"><li><strong>持续同步</strong>：持续同步的意思是不断地检查 Git 仓库，将任何状态变化都反映到 Kubernetes 集群中。这种思路来自于 Flux 工具，Flux 使用 Kubernetes Operator 将自动化部署方式从 Kubernetes 集群外转移到集群内部来。</li></ol><p>GitOps 由以下4个主要的组件组成：</p><ol><li><strong>Git 仓库</strong>：用来存储我们的应用程序的声明式定义的 YAML 文件的源代码仓库。</li></ol><ol start="2"><li><strong>Kubernetes 集群</strong>：用于部署我们应用程序的底层集群。</li></ol><ol start="3"><li><strong>同步代理</strong>：Kubernetes Operator 扩展，它的工作是将 Git 仓库和应用状态持续同步到集群中。</li></ol><ol start="4"><li><strong>CD Pipeline</strong>：持续部署流水线，用来编排整个流程的持续部署流水线。</li></ol><p>关于这些组件如何协同工作来创建 GitOps 流程的架构图如下所示。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cezgWwU6NE50RPybSiauPxOMibG7ycygsHBhlesTN2Cw5JY3D38I5hlRg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>在上面的架构图中，YAML 文件的创建和修改分为应用开发、应用运维和集群运维三部分。根据我们的组织团队架构、集群多租户等需求，可以选择分一到两步进行。接下来我们来看看为什么需要使用 GitOps？</p><h1 id="为什么需要-GitOps？"><a href="#为什么需要-GitOps？" class="headerlink" title="为什么需要 GitOps？"></a>为什么需要 GitOps？</h1><p>GitOps 可以在很多方面都产生价值，下面我们来看看其中的一些关键的价值。</p><p><strong>应用交付速度</strong></p><p>持续的 GitOps 可以通过以下几个方面来提高产品交付速度。</p><ol><li>可以比较最终的 YAML 和集群状态的能力，这也可以作为批准发布的决策指南。</li></ol><ol start="2"><li>借助 Prometheus 的应用程序指标，通过自动化的蓝绿部署，非常容易进行部署。</li></ol><ol start="3"><li>根据策略自动更新容器镜像，例如，Istio sidecar 次要版本的发布是向后兼容的，可以自动更新。</li></ol><ol start="4"><li>GitOps 将以运维和开发为中心，提高效率。</li></ol><ol start="5"><li>应用团队可以接管一些运维工作，而运维团队则可以更加专注于平台建设。</li></ol><ol start="6"><li>GitOps 仓库可以绕过完整的持续部署流程进行紧急发布。</li></ol><p><strong>端到端的自动化</strong></p><p>在 GitOps 中，所有和应用开发、应用运维和集群运维相关的声明都通过 git 嵌入到 YAML 文件中，实现了端到端的自动化。</p><p><strong>安全、审计和合规性</strong></p><p>零手动更改到集群中应用的策略将大大增加集群的安全性，由于集群中的所有配置都在 git 中，我们将拥有一个完整的审计日志，记录集群中发生的事情。</p><p><strong>集群可观测性</strong></p><p>有了完整的审计日志，我们就可以很容易获得集群中发生的变化，来帮助调试一些问题。</p><p><strong>关注点分离和迁移</strong></p><p>GitOps 将应用开发者、应用运维和集群运维之间的关注点进行分离，这些团队中的依赖关系以声明式的方式注入到 git 中，这将大大缓解我们对底层 K8S 集群、治理策略等工具的迁移。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cEibKKfVx9n7ziaTLss4Ee8kpnAtaEg2iamLYNF7ecZOCtw0y4p67w5cHA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h1 id="如何采用-GitOps？"><a href="#如何采用-GitOps？" class="headerlink" title="如何采用 GitOps？"></a>如何采用 GitOps？</h1><p>我们将通过以下四个不同的方面进行阐述来帮助我们实现 GitOps 这一目标。</p><ul><li>GitOps 工作流的实现</li></ul><ul><li>管理声明式的 YAML</li></ul><ul><li>工具</li></ul><ul><li>从什么地方开始？</li></ul><h2 id="GitOps-工作流的实现"><a href="#GitOps-工作流的实现" class="headerlink" title="GitOps 工作流的实现"></a>GitOps 工作流的实现</h2><p>以下三个工作流程是我们在开始使用 GitOps 时要采用的比较流行的工作流程。</p><p><strong>工作流1</strong>：标准的 GitOps 流程</p><p>这是标准的 GitOps 工作流，我们将应用程序的 YAML 描述推送到 GitOps 仓库中，GitOps Agent 就会自动同步状态变化。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cCpLKt5qIvNNOM1x2c1iazNoYPV2WKme8Zmo2lZKrc9TMCxAHuQm2CcQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>工作流2</strong>：镜像自动更新</p><p>在这个工作流中，GitOps Agent 会根据指定的策略从容器镜像仓库中自动更新新版本的容器镜像，例如，我们可以设置这样的策略，如果镜像有一个小版本变化，我们就可以自动更新，因为它们是向后兼容的。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cq6k4LwFZwMibibjr5QsEOpw7dckGPTvbovB08OC57dwXwnSQP17NbdGw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>工作流3</strong>：自动化金丝雀部署</p><p>该工作流非常强大，我们可以在这里实现金丝雀自动化部署。有了这个，我们在用 Prometheus 测量 HTTP 请求成功率、请求平均持续时间和 Pods 健康状态等关键性能指标的同时，可以逐步将流量迁移到金丝雀实例上。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5crKDdUgNZDpCOyWicuISxVibCScHxLDXRDkMlPjibBHU8wUFDMGOSRSoBQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="管理声明式的-YAML"><a href="#管理声明式的-YAML" class="headerlink" title="管理声明式的 YAML"></a>管理声明式的 YAML</h2><p>假设我们有一个电子商务购物车应用，完整的应用程序定义如下所示。</p><ol><li>应用程序镜像。</li></ol><ol start="2"><li>Pod、Deployment、Service、Volume 和 ConfigMap 的 YAML 文件。</li></ol><ol start="3"><li>连接数据的一些 sealed secrets 对象。</li></ol><ol start="4"><li>标记将 Istio 作为默认的集群服务治理策略网格。</li></ol><ol start="5"><li>环境治理策略，比如 staging 环境1个副本，生产环境3个副本。</li></ol><ol start="6"><li>标记添加节点亲和性和容忍用于高可用的节点调度。</li></ol><ol start="7"><li>基于 Pod 标签的网络安全策略 YAML 声明。</li></ol><p>要构建一个最终的应用 YAML 描述文件，我们需要应用开发者、应用运维和集群运维人员的一些输入。</p><p>假设我们是一个比较小的团队并且管理了很多的 Pod，下面的流程就可以来表示如何构建一个持续部署的自动化流水线。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cb9oVNicve4A2ribtmX4CE3xC1DFUjqjqSNwibsp1qxBazPCxHiaIq6MyPw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>这种关注点分离的方式深受 OAM（开放应用模型）的影响，该模型试图为云原生应用开发提供一个完善的框架。</p><p>OAM(<a href="https://oam.dev/" target="_blank" rel="noopener">https://oam.dev/</a>) 描述了一种模式：</p><ul><li><strong>开发人员</strong>负责定义应用组件。</li></ul><ul><li><strong>应用运维人员</strong>负责创建这些组件的实例，并为其分配应用配置。</li></ul><ul><li><strong>基础设施运维人员</strong>负责声明、安装和维护平台上的可用的底层服务。</li></ul><p>通过使用 OAM 框架，会将 YAML 的贡献者责任进行分离，当然这可能会根据你的团队组织结构和使用的 Kubernetes 集群类型有所变化。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cjySamdPt5BtuuUstOzIm8C6uAOrkVcU0C67JQoyh3mppdlYnonPeEg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>如果你比较赞同 GitOps 的理念，那么下面就可以来选择一些需要用到的工具了。有很多工具可以支持我们去实现 GitOps 的不同功能。接下来我们简单介绍一些工具及其使用方法。</p><p><strong>Git</strong>：这是我们使用 GitOps 来存储 YAML 清单的基础。</p><p><strong>Helm &amp; Kustomize</strong>：这是一个强大的组合，可以帮助我们生成声明式的 YAML 资源清单文件，我们可以使用 Helm 打包应用程序和它的依赖关系。然后 Kustomize 会帮助我们自定义和修补 YAML 文件，而不需要去改变原来的 YAML 文件。单独使用 Helm 是不够的，特别是用于区分不同环境的资源清单的时候，我们还需要结合 Kustomize。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5c2mpOqEica4h1icq3G7crcqbDH0llawQMfOS7nuNXLCzDbPQvgsiaZYHRg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p><strong>Argo CD</strong>：这是一个 GitOps 持续交付工具，它可以作为一个 Agent，将 GitOps 仓库中的改动同步到 Kubernetes 集群中。</p><p><strong>Flux</strong>：这是另外一个 GitOps 持续交付的工具，功能和 Argo CD 类似。</p><p><strong>Flagger</strong>：这个工具和 Flux 配合使用，可以很好地实现金丝雀部署。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7YtzpvYZ3AW5MmTl26FAjX5cpMDSbOoen7N0BhnC7OAarv386DMCagr4f285nibvpL7M7abQFRD6rZg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="如何入手？"><a href="#如何入手？" class="headerlink" title="如何入手？"></a>如何入手？</h2><p>如果你正准备开始一个新的项目，那么从一开始就采用 GitOps 是比较容易的，我们所要做的就是选择我们的 CI/CD、声明式 YAML 文件管理以及 GitOps Agent 等工具来启动即可。</p><p>如果你想在现有的项目中实施 GitOps，下面的几点可以帮助你实施：</p><ol><li>你可以一次只选择一个应用，用成功的案例来推动其他应用的改造。</li></ol><ol start="2"><li>当选择第一个应用的时候，可以选择变化比较频繁的应用，这将有助于我们为成功案例建立一些可靠的指标说明。</li></ol><ol start="3"><li>选择一个经常出问题的应用，使用了 GitOps 过后，这些应用的问题频率应该会下降不少，当它出现问题的时候，应该有更好的可观测性了。</li></ol><ol start="4"><li>优先选择业务应用，而不是像 Istio、RBAC 集成的等运维复杂的应用。</li></ol><ol start="5"><li>如果需要的话可以暂时引入人工审批的步骤。</li></ol><p>  转发来源  阳明 k8s技术圈 </p></blockquote><blockquote><p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect</a></p><blockquote><p>原文链接：<a href="https://itnext.io/continuous-gitops-the-way-to-do-devops-in-kubernetes-896b0ea1d0fb" target="_blank" rel="noopener">https://itnext.io/continuous-gitops-the-way-to-do-devops-in-kubernetes-896b0ea1d0fb</a></p></blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h1 id=&quot;GitOps-在-Kubernetes-中进行-DevOps-的方式&quot;&gt;&lt;a href=&quot;#GitOps-在-Kubernetes-中进行-DevOps-的方式&quot; class=&quot;headerlink&quot; title=&quot;GitOps - 在 
      
    
    </summary>
    
      <category term="devops" scheme="http://zhang-yu.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhang-yu.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>使用GitLab-CI与Argo-CD进行GitOps实践</title>
    <link href="http://zhang-yu.me/2020/07/13/%E4%BD%BF%E7%94%A8GitLab-CI%E4%B8%8EArgo-CD%E8%BF%9B%E8%A1%8CGitOps%E5%AE%9E%E8%B7%B5/"/>
    <id>http://zhang-yu.me/2020/07/13/使用GitLab-CI与Argo-CD进行GitOps实践/</id>
    <published>2020-07-13T06:00:00.000Z</published>
    <updated>2020-07-13T09:14:52.452Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h1 id="使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践"><a href="#使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践" class="headerlink" title="使用 GitLab CI 与 Argo CD 进行 GitOps 实践"></a>使用 GitLab CI 与 Argo CD 进行 GitOps 实践</h1></blockquote><blockquote><p>在现在的云原生世界里面 GitOps 不断的被提及，这种持续交付的模式越来越受到了大家的青睐，<a href="http://mp.weixin.qq.com/s?__biz=MzU4MjQ0MTU4Ng==&amp;mid=2247485029&amp;idx=1&amp;sn=4e934616af9439ec746f2dc901c969e2&amp;chksm=fdb90978cace806eff3999fe40ec4bc5ba18edd8ee92998d4fe2073bb657e2d514d0a854314a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">我们前面也有文章详细讲解了 GitOps 的相关概念</a>，在网上也可以找到很多关于它的资源，但是关于 GitOps 相关的工作流实践的示例却并不多见，我们这里就将详细介绍一个使用示例，希望对大家实践 GitOps 有所帮助。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUk1FGGtWPmb7xyeiaOicS5Zwk9fyZJVgqVwL44SHpBOJw3u459iaYicAyfSg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitOps Workflow</p><p>上图是当前示例中的 GitOps 工作流程。GitLab 和 Argo CD 是两个主要的核心组件：</p><p><strong>Argo CD</strong> 是一个声明式、GitOps 持续交付的 Kubernetes 工具，它的配置和使用非常简单，并且自带一个简单易用的 Dashboard 页面，更重要的是 Argo CD 支持 kustomzie、helm、ksonnet 等多种工具。应用程序可以通过 Argo CD 提供的 CRD 资源对象进行配置，可以在指定的目标环境中自动部署所需的应用程序。关于 Argo CD 更多的信息可以查看官方文档了解更多。</p><p><strong>GitLab CI</strong> 是 GitLab 的持续集成和持续交付的工具，也是非常流行的 CI/CD 工具，相比 Jenkins 更加轻量级，更重要的是和 GitLab 天然集成在一起的，所以非常方便。</p><h2 id="Argo-CD-安装"><a href="#Argo-CD-安装" class="headerlink" title="Argo CD 安装"></a>Argo CD 安装</h2><p>当前前提条件是有一个可用的 Kubernetes 集群，通过 kubectl 可以正常访问集群，为了访问 Argo CD 的 Dashboard 页面，我们可以通过 Ingress 来暴露服务，为此需要在 Kubernetes 中安装一个 Ingress Controller，我这里已经提前安装了 <code>ingress-nginx</code>，接下来我们将 Helm3 来安装 Argo CD，关于 Helm 以及 ingress-nginx 的使用我们前面的文章中已经多次提到，这里就不再详细介绍他们的使用了。</p><p>首先创建一个 argocd 的命名空间：</p><pre><code>$ kubectl create ns argocd</code></pre><p>然后添加 argocd 的 chart 仓库地址：</p><pre><code>$ helm repo add argo https://argoproj.github.io/argo-helm</code></pre><p>接下来我们就可以使用 Helm 安装 Argo CD 了：</p><pre><code>$ helm install argocd -n argocd argo/argo-cd --values values.yaml</code></pre><p>其中 values.yaml 文件如下所示，用来定制安装的 Argo CD：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server:</span><br><span class="line">  ingress:</span><br><span class="line">    enabled: true</span><br><span class="line">    annotations:</span><br><span class="line">      kubernetes.io/ingress.class: &quot;nginx&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;</span><br><span class="line">      nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;</span><br><span class="line">    hosts:</span><br><span class="line">    - argocd.k8s.local</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>执行上面的安装命令后，Argo CD 就会被安装在 argocd 命名空间之下，可以在本地 <code>/etc/hosts</code> 中添加一个映射，将 argocd.k8s.local 映射到 ingress-nginx 所在的节点即可：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls -n argocd</span><br><span class="line">NAME    NAMESPACE       REVISION        UPDATED                                 STATUS    CHART            APP VERSION</span><br><span class="line">argocd  argocd          2               2020-07-10 15:26:38.259258 +0800 CST    deployed  argo-cd-2.5.0    1.6.1</span><br><span class="line">$ kubectl get pods -n argocd</span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">argocd-application-controller-85c4788ffc-p2m4c   1/1     Running   0          49m</span><br><span class="line">argocd-dex-server-cc65c7546-x78bj                1/1     Running   0          49m</span><br><span class="line">argocd-redis-5f45875bc7-mnx8b                    1/1     Running   0          49m</span><br><span class="line">argocd-repo-server-7bcf647588-h8gtq              1/1     Running   0          49m</span><br><span class="line">argocd-server-7877ff8889-zp7tq                   1/1     Running   0          49m</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>当所有 Pod 变成 Running 状态后，我们就可以通过浏览器访问 Argo CD 的 Dashboard 页面了：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkDyG5dBqSjrkqaHc0ic1ib5BdR9mEMM6iamffu3PxxlJ5mFxpib0bAAx2Tw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>默认的用户名为 admin，密码为 server Pod 的名称，可以通过如下所示的命令来获取：</p><p>$ kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d’/‘ -f 2</p><p>用上面的用户名和密码即可登录成功，接下来我们在 GitLab 中来创建示例项目。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkJNQ7dCTBtUALC3dBo76DHkQZCD0iceFNTVoWJiaClFiafhZZjdEEUibDVg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="GitLab-项目配置"><a href="#GitLab-项目配置" class="headerlink" title="GitLab 项目配置"></a>GitLab 项目配置</h2><p>我们这里使用的示例项目是一个 Golang 程序，在页面上显示一个文本信息和 Pod 名称，代码地址：<a href="https://github.com/cnych/gitops-webapp-demo。我们可以将该项目代码上传到我们自己的" target="_blank" rel="noopener">https://github.com/cnych/gitops-webapp-demo。我们可以将该项目代码上传到我们自己的</a> GitLab 上面去，我这里的 GitLab 安装在 Kubernetes 之上，通过配置域名 git.k8s.local 进行访问，调整过后我们本地的代码仓库地址为：<a href="http://git.k8s.local/course/gitops-webapp" target="_blank" rel="noopener">http://git.k8s.local/course/gitops-webapp</a> 。</p><p>接下来需要添加一些在 GitLab CI 流水线中用到的环境变量（Settings → CI/CD → Variables）：</p><ul><li>CI_REGISTRY - 镜像仓库地址，值为：<a href="https://index.docker.io/v1/" target="_blank" rel="noopener">https://index.docker.io/v1/</a></li></ul><ul><li>CI_REGISTRY_IMAGE - 镜像名称，值为：cnych/gitops-webapp</li></ul><ul><li>CI_REGISTRY_USER - Docker Hub 仓库用户名，值为 cnych</li></ul><ul><li>CI_REGISTRY_PASSWORD - Docker Hub 仓库密码</li></ul><ul><li>CI_PASSWORD - Git 仓库访问密码</li></ul><ul><li>CI_USERNAME - Git 仓库访问用户名</li></ul><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkYIDTCMRrglaBa0mHtYVSsMaZRj2GpXXxZ6cROTs1eAPjYqyzjvyk6A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><h2 id="Argo-CD-配置"><a href="#Argo-CD-配置" class="headerlink" title="Argo CD 配置"></a>Argo CD 配置</h2><p>现在我们可以开始使用 GitOps 来配置我们的 Kubernetes 中的应用了。Argo CD 自带了一套 CRD 对象，可以用来进行声明式配置，这当然也是推荐的方式，把我们的基础设施作为代码来进行托管，下面是我们为开发和生产两套环境配置的资源清单：</p> <figure class="highlight plain"><figcaption><span>gitops-demo-app.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: web-app-dev</span><br><span class="line">  namespace: argocd</span><br><span class="line">spec:</span><br><span class="line">  project: default</span><br><span class="line">  source:</span><br><span class="line">    repoURL: http://git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">    path: deployment/dev</span><br><span class="line">  destination:</span><br><span class="line">    server: https://kubernetes.default.svc</span><br><span class="line">    namespace: dev</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: true</span><br><span class="line">---</span><br><span class="line">apiVersion: argoproj.io/v1alpha1</span><br><span class="line">kind: Application</span><br><span class="line">metadata:</span><br><span class="line">  name: web-app-prod</span><br><span class="line">  namespace: argocd</span><br><span class="line">spec:</span><br><span class="line">  project: default</span><br><span class="line">  source:</span><br><span class="line">    repoURL: http://git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    targetRevision: HEAD</span><br><span class="line">    path: deployment/prod</span><br><span class="line">  destination:</span><br><span class="line">    server: https://kubernetes.default.svc</span><br><span class="line">    namespace: prod</span><br><span class="line">  syncPolicy:</span><br><span class="line">    automated:</span><br><span class="line">      prune: true</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>上面定义的 Application 这个资源，就是 Argo CD 用于描述应用的 CRD 对象：</p><ul><li>name：Argo CD 应用程序的名称</li></ul><ul><li>project：应用程序将被配置的项目名称，这是在 Argo CD 中应用程序的一种组织方式</li></ul><ul><li>repoURL：源代码的仓库地址</li></ul><ul><li>targetRevision：想要使用的 git 分支</li></ul><ul><li>path：Kubernetes 资源清单在仓库中的路径</li></ul><ul><li>destination：Kubernetes 集群中的目标</li></ul><p>然后同样使用 kubectl 工具直接部署上面的资源对象即可，将会创建两个 Application 类型的对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f gitops-demo-app.yaml</span><br><span class="line">application.argoproj.io/web-app-dev created</span><br><span class="line">application.argoproj.io/web-app-prod created</span><br><span class="line">$ kubectl get application -n argocd</span><br><span class="line">NAME           AGE</span><br><span class="line">web-app-dev    25s</span><br><span class="line">web-app-prod   24s</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>此时我们再去 Argo CD 的 Dashboard 首页同样将会看到两个 Application 的信息：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkEnwJ6x3ic9sOvqtJ4a9omlxSKic74ADdIciaDBIriaKCmh3Nj3qwYOUaZQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>点击其中一个就可以看到关于应用的详细信息，我们可以在 gitops-webapp 代码仓库的 <code>deployment/&lt;env&gt;</code> 目录里面找到资源对象。我们可以看到，在每个文件夹下面都有一个 <code>kustomization.yaml</code> 文件，Argo CD 可以识别它，不需要任何其他的设置就可以使用。</p><p>由于我们这里的代码仓库是私有的 GitLab，所以我们还需要配置对应的仓库地址，在页面上 Settings → Repositories，点击 <code>Connect Repo using HTTPS</code> 按钮：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkwbI136LplOtAo1PFmJTt9ic46ZFrambxuy5rmhQkrpoq0DoaXD6ug5g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>添加我们的代码仓库认证信息：</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkHxTR6pFhDytdTh8AkzV67x7In3HiaOQ7ibZyAaw6EjGLNlXrEkc8gw9w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>需要注意的是这里默认使用的是 HTTPS，所以我们需要勾选下方的 <code>Skip server verification</code>，然后点击上方的 <code>CONNECT</code> 按钮添加即可。然后重新同步上面的两个 Application，就可以看到正常的状态了。</p><h2 id="GitLab-CI-流水线"><a href="#GitLab-CI-流水线" class="headerlink" title="GitLab CI 流水线"></a>GitLab CI 流水线</h2><p>接下来我们需要为应用程序创建流水线，自动构建我们的应用程序，推送到镜像仓库，然后更新 Kubernetes 的资源清单文件。</p><p>下面的示例并不是一个多么完美的流水线，但是基本上可以展示整个 GitOps 的工作流。开发人员在自己的分支上开发代码，他们分支的每一次提交都会触发一个阶段性的构建，当他们将自己的修改和主分支合并时，完整的流水线就被触发。将构建应用程序，打包成 Docker 镜像，将镜推送到 Docker 仓库，并自动更新 Kubernetes 资源清单，此外，一般情况下将应用部署到生产环境需要手动操作。</p><p>GitLab CI 中的流水线默认定义在代码仓库根目录下的 <code>.gitlab-ci.yml</code> 文件中，在该文件的最上面定义了一些构建阶段和环境变量、镜像以及一些前置脚本：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stages:</span><br><span class="line">- build</span><br><span class="line">- publish</span><br><span class="line">- deploy-dev</span><br><span class="line">- deploy-prod</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>接下来是阶段的定义和所需的任务声明。我们这里的构建过程比较简单，只需要在一个 golang 镜像中执行一个构建命令即可，然后将编译好的二进制文件保存到下一个阶段处理，这一个阶段适合分支的任何变更：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">build:</span><br><span class="line">  stage: build</span><br><span class="line">  image:</span><br><span class="line">    name: golang:1.13.1</span><br><span class="line">  script:</span><br><span class="line">    - go build -o main main.go</span><br><span class="line">  artifacts:</span><br><span class="line">    paths:</span><br><span class="line">      - main</span><br><span class="line">  variables:</span><br><span class="line">    CGO_ENABLED: 0</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>然后就是构建镜像并推送到镜像仓库，这里我们使用 Kaniko，当然也可以使用 DinD 模式进行构建，只是安全性不高，这里我们可以使用 GIT 提交的 commit 哈希值作为镜像 tag，关于 Docker 镜像仓库的认证和镜像地址信息可以通过项目的参数来进行传递，不过这个阶段只在主分支发生变化时才会触发：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">publish:</span><br><span class="line">  stage: publish</span><br><span class="line">  image:</span><br><span class="line">    name: cnych/kaniko-executor:v0.22.0</span><br><span class="line">    entrypoint: [&quot;&quot;]</span><br><span class="line">  script:</span><br><span class="line">    - echo &quot;&#123;\&quot;auths\&quot;:&#123;\&quot;$CI_REGISTRY\&quot;:&#123;\&quot;username\&quot;:\&quot;$CI_REGISTRY_USER\&quot;,\&quot;password\&quot;:\&quot;$CI_REGISTRY_PASSWORD\&quot;&#125;&#125;&#125;&quot; &gt; /kaniko/.docker/config.json</span><br><span class="line">    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile ./Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA</span><br><span class="line">  dependencies:</span><br><span class="line">    - build</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>下一个阶段就是将应用程序部署到开发环境中，在 GitOps 中就意味着需要更新 Kubernetes 的资源清单，这样 Argo CD 就可以拉取更新的版本来部署应用。这里我们使用了为项目定义的环境变量，包括用户名和 TOKEN，此外在提交消息里面增加 <code>[skip ci]</code> 这样的关键字，这样流水线就不会被触发：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">deploy-dev:</span><br><span class="line">  stage: deploy-dev</span><br><span class="line">  image: cnych/kustomize:v1.0</span><br><span class="line">  before_script:</span><br><span class="line">    - git remote set-url origin http://$&#123;CI_USERNAME&#125;:$&#123;CI_PASSWORD&#125;@git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    - git config --global user.email &quot;gitlab@git.k8s.local&quot;</span><br><span class="line">    - git config --global user.name &quot;GitLab CI/CD&quot;</span><br><span class="line">  script:</span><br><span class="line">    - git checkout -B master</span><br><span class="line">    - cd deployment/dev</span><br><span class="line">    - kustomize edit set image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span><br><span class="line">    - cat kustomization.yaml</span><br><span class="line">    - git commit -am &apos;[skip ci] DEV image update&apos;</span><br><span class="line">    - git push origin master</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>最后添加一个部署到 prod 环境的阶段，和前面非常类似，只是添加了一个手动操作的流程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">deploy-prod:</span><br><span class="line">  stage: deploy-prod</span><br><span class="line">  image: cnych/kustomize:v1.0</span><br><span class="line">  before_script:</span><br><span class="line">    - git remote set-url origin http://$&#123;CI_USERNAME&#125;:$&#123;CI_PASSWORD&#125;@git.k8s.local/course/gitops-webapp.git</span><br><span class="line">    - git config --global user.email &quot;gitlab@git.k8s.local&quot;</span><br><span class="line">    - git config --global user.name &quot;GitLab CI/CD&quot;</span><br><span class="line">  script:</span><br><span class="line">    - git checkout -B master</span><br><span class="line">    - git pull origin master</span><br><span class="line">    - cd deployment/prod</span><br><span class="line">    - kustomize edit set image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span><br><span class="line">    - cat kustomization.yaml</span><br><span class="line">    - git commit -am &apos;[skip ci] PROD image update&apos;</span><br><span class="line">    - git push origin master</span><br><span class="line">  only:</span><br><span class="line">    - master</span><br><span class="line">  when: manual</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>这样我们就完成了整个流水线的定义。</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>接下来我们来看看它们是如何一起工作的。我们将开发和线上两个环境的应用分别部署在了 dev 和 prod 命名空间之下，通过 Ingress 暴露服务，同样需要将两个应用的域名 <a href="http://webapp.dev.k8s.local/" target="_blank" rel="noopener">http://webapp.dev.k8s.local/</a> 与 <a href="http://webapp.prod.k8s.local/" target="_blank" rel="noopener">http://webapp.prod.k8s.local/</a> 在本地 <code>/etc/hosts</code> 中添加映射。</p><p>如果一切正常的话现在我们可以在浏览器中来查看我们部署的 web 应用程序了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkylSD4SV0tgeuyviclKwuKicCJ6LyPqRsZ9oV9fvd7NuxFUNnZ5bf6Q0A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt>Dev web app</p><p>然后我们来尝试修改下代码，编辑 main.go 文件，将变量 welcome 中的 <code>GITOPS</code> 修改为 <code>GITOPS-K8S</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">   welcome := Welcome&#123;&quot;GITOPS-K8S&quot;, time.Now().Format(time.Stamp), os.Getenv(&quot;HOSTNAME&quot;)</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>然后提交代码到 master 分支，然后进入 GitLab 项目 -&gt; CI/CD -&gt; Pipelines，就可以看到一个新的流水线开始构建了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkEz8Po0y2e3IqlSNH8u1WmgBHfokg8zoKXkALuE0Xhac19cXaLymHfA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>等待一会儿，正常情况下会执行到 dev 的部署阶段，然后变成 <code>skipped</code> 的状态，此时流水线已经将代码中的 dev 下的资源清单文件已经更新了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkCyPfBKa2csl9PfKNYkN64sYIBjllnhicEu4jcV6ianUaTiab38K0T0LNw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitLab CI/CD Pipeline</p><p>然后 Argo CD 在自动同步模式下在一分钟内变会更新 Kubernetes 的资源对象，我们也可以在 Argo CD 的页面中看到进度。当 Argo CD 中同步完成后我们再去查看 DEV 环境的应用，就可以看到页面上面的信息已经变成了 <code>GITOPS-K8S</code> 了。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUk5aXjc4XibnmywxOzt1dBr9aR3gPMOqUFU4E7lxXzIRQSia8Nrs8t1yjQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt>Update Dev Web APP</p><p>最后如果需要部署到 prod 环境，我们只需要在 GitLab 的流水线中手动触发即可，之后，prod 中的镜像也会被更新。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkInCosUJFMHtjiaO1quRoQohfICvaa1QO7aZyKs15yKOR4GUyUI0EwYQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>GitLab CI/CD Prod deployment</p><p>下面是同步时 Argo CD 更新的页面状态变化图。</p><p><img src="http://img01.store.sogou.com/net/a/04/link?appid=100520029&amp;url=https://mmbiz.qpic.cn/mmbiz_png/z9BgVMEm7Ytndic91iaEWe4ia2QjIQIYYUkFgM9ujWCgKRLHVJCJxQDcFwAO4BictzbeuO52PTlH3OWIOf4o7lpricw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt></p><p>Argo CD Sync Workflow</p><p>到这里，我们就使用 GitOps 成功的将我们的应用部署到了开发和生产环境之中了。</p></blockquote><blockquote><p> 转发来源  阳明 k8s技术圈 <a href="https://mp.weixin.qq.com/s/tCY86QZ_K3STiN5ZvfhjpQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/tCY86QZ_K3STiN5ZvfhjpQ</a></p></blockquote><blockquote><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://www.weave.works/technologies/gitops/" target="_blank" rel="noopener">https://www.weave.works/technologies/gitops/</a></li></ul><ul><li><a href="https://argoproj.github.io/argo-cd/" target="_blank" rel="noopener">https://argoproj.github.io/argo-cd/</a></li></ul><ul><li><a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">https://docs.gitlab.com/ee/ci/yaml/</a></li></ul><ul><li><a href="https://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6" target="_blank" rel="noopener">https://medium.com/@andrew.kaczynski/gitops-in-kubernetes-argo-cd-and-gitlab-ci-cd-5828c8eb34d6</a></li></ul><ul><li><a href="https://github.com/cnych/gitops-webapp-demo" target="_blank" rel="noopener">https://github.com/cnych/gitops-webapp-demo</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h1 id=&quot;使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践&quot;&gt;&lt;a href=&quot;#使用-GitLab-CI-与-Argo-CD-进行-GitOps-实践&quot; class=&quot;headerlink&quot; title=&quot;使用 GitLab
      
    
    </summary>
    
      <category term="devops" scheme="http://zhang-yu.me/categories/devops/"/>
    
    
      <category term="devops" scheme="http://zhang-yu.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>理解Kubernetes架构</title>
    <link href="http://zhang-yu.me/2020/07/03/%E7%90%86%E8%A7%A3Kubernetes%E6%9E%B6%E6%9E%84/"/>
    <id>http://zhang-yu.me/2020/07/03/理解Kubernetes架构/</id>
    <published>2020-07-03T03:00:00.000Z</published>
    <updated>2020-08-12T08:48:51.849Z</updated>
    
    <content type="html"><![CDATA[<p>架构师波波</p><p><a href="https://blog.csdn.net/yang75108/article/details/100215486" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/100215486</a> </p><blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>理解K8s的架构是运用好K8s的基础，本文波波帮助大家梳理一下K8s的架构。我们先会对K8s的架构进行一个概览，然后分别剖析Master和Worker节点的组件构成，然后把这些组件再集成起来，通过一个发布样例展示这些组件是如何配合工作的，最后展示K8s集群的总体架构。</p><h2 id="架构概览"><a href="#架构概览" class="headerlink" title="架构概览"></a>架构概览</h2><p><img src="https://img-blog.csdnimg.cn/20190902113048702.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图是K8s架构的概览。K8s集群中主要有两类角色，一类是Master节点，另外一类是Worker节点，简单讲，Master节点主要用来管理和调度集群资源的，而Worker节点则是提供资源的。在一个高可用的K8s集群中，Master和Worker一般都有多个节点构成，这些节点可以是物理机，也可以是虚拟机。</p><p>Worker节点提供的资源单位称为Pod，简单理解，Pod就是K8s云平台提供的虚拟机。Pod里头住的是应用容器，比如Docker容器，容器是CPU/Mem资源隔离单位。大部分场景下，一个Pod只住一个应用容器，但是也有一些场景，一个Pod里头可以住多个容器，其中一个是主容器，其它则是辅助容器。一个Pod里头的容器共享Pod的网络栈和存储资源。</p><p>K8s主要解决集群资源调度的问题。简单讲，就是当有应用发布请求过来的时候，K8s需要根据集群资源空闲现状，将这个应用的Pods合理的分配到空闲的Worker节点上去。同时，K8s需要时刻监控集群，如果有节点或者Pods挂了，它要能够重新协调和启动Pods，保证应用高可用，这个术语叫自愈。还有，K8s需要管理集群网络，保证Pod/服务之间可以互通互联。</p><h2 id="Master节点组件"><a href="#Master节点组件" class="headerlink" title="Master节点组件"></a>Master节点组件</h2><p><img src="https://img-blog.csdnimg.cn/2019090211310821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>Master节点是K8s集群大脑，它由如下组件构成：</p><ol><li><strong>Etcd</strong>： 它是K8s的集中状态存储，所有的集群状态数据，例如节点，Pods，发布，配置等等，最终都存储在Etcd中。Etcd是一个分布式KV数据库，采用Raft分布式一致性算法。Etcd高可用部署一般需要至少三个节点。Etcd集群可以独立部署，也可以和Master节点住在一起。</li><li><strong>API server</strong>： 它是K8s集群的接口和通讯总线。用户通过kubectl，dashboard或者sdk等方式操作K8s，背后都通过API server和集群进行交互。集群内的其它组件，例如Kubelet/Kube-Proxy/Scheduler/Controller-Manager等，都通过API server和集群进行交互。API server可以认为是Etcd的一个代理Proxy，它是唯一能够访问操作Etcd数据库的组件，其它组件，都必须通过API server间接操作Etcd。API server不仅接受其它组件的API请求，它还是集群的事件总线，其它组件可以订阅在API server上，当有新事件发生时候，API server会将相关事件通知到感兴趣的组件。</li><li><strong>Scheduler</strong>： 它是K8s集群负责调度决策的组件。Scheduler掌握当前的集群资源使用情况，当有新的应用发布请求被提交到K8s集群，它负责决策相应的Pods应该分布到哪些空闲节点上去。K8s中的调度决策算法是可以扩展的。</li><li><strong>Controller Manager</strong>： 它是保证集群状态最终一致的组件。它通过API server监控集群状态，确保实际状态和预期状态最终一致，如果一个应用要求发布十个Pods，Controller Manager保证这个应用最终启动十个Pods，如果中间有Pods挂了，Controller Manager会负责协调重启Pods，如果Pods启多了，Controller Manager会负责协调关闭多余Pods。也即是说，K8s采用最终一致调度策略，它是集群自愈的背后实现机制。</li></ol><h2 id="Worker节点组件"><a href="#Worker节点组件" class="headerlink" title="Worker节点组件"></a>Worker节点组件</h2><p><img src="https://img-blog.csdnimg.cn/20190902113122457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>Worker节点是K8s集群资源的提供者，它由如下组件构成：</p><ol><li><strong>Kubelet</strong>: 它是Worker节点资源的管理者，相当于一个Agent角色。它监听API server的事件，根据Master节点的指示启动或者关闭Pod等资源，也将本节点状态数据汇报给Master节点。如果说Master节点是K8s集群的大脑，那么Kubelet就是Worker节点的小脑。</li><li><strong>Container Runtime</strong>: 它是节点容器资源的管理者，如果采用Docker容器，那么它就是Docker Engine。Kubelet并不直接管理节点的容器资源，它委托Container Runtime进行管理，比如启动或者关闭容器，收集容器状态等。Container Runtime在启动容器时，如果本地没有镜像缓存，则需要到Docker Registry(或Docker Hub)去拉取相应镜像，然后缓存本地。</li><li><strong>Kube-Proxy</strong>: 它是管理K8s中的服务(Service)网络的组件。Pod在K8s中是ephemeral的概念，也就是不固定的，PodIP可能会变(包括预期和非预期的)。为了屏蔽PodIP的可能的变化，K8s中引入了Servie概念，它可以屏蔽应用的PodIP，并且在调用时进行负载均衡。Kube-Proxy是实现K8s服务(Service)网络的背后机制。另外，当需要把K8s中的服务(Service)暴露给外网时，也需要通过Kube-Proxy进行代理转发。</li></ol><h2 id="流程样例"><a href="#流程样例" class="headerlink" title="流程样例"></a>流程样例</h2><p><img src="https://img-blog.csdnimg.cn/20190902113135346.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>如果我们把Master节点和Worker节点集成起来，就构成上图所示的K8s集群。下面我们通过一个发布流程，展示上面介绍的这些组件是如何配合工作的。</p><ol><li>假设管理员要发布一个新应用，他通过Kubectl命令行工具将发布请求提交到API server，API server将请求存储到Etcd数据库中。</li><li>Scheduler通过API server监听到有新的应用发布请求，它通过调度算法决策，选择若干可发布的空闲节点，并将发布决策更新到API server。</li><li>被选中的Worker节点上的Kubelet通过API server监听到有给自己的新发布任务，它根据任务指示在本地启动相应的Pods(间接通过Container Runtime启动容器)，并将任务执行成功情况报告给API server。</li><li>所有Worker节点上Kube-Proxy通过API server监听到有新的发布，它获取应用的PodIP/ClusterIP/端口等相关数据，更新本地的iptables表规则，让本地的Pods可以通过iptables转发方式，访问到新发布应用的Pods。</li><li>Controller Manager通过API server，时刻监控新发应用的健康状况，保证实际状态和预期状态最终一致。</li></ol><h2 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h2><p><img src="https://img-blog.csdnimg.cn/20190902113147939.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图是一个K8s集群的总体架构。实际K8s集群中还有一个覆盖(Overlay)网络，集群中的Pods通过覆盖网络可以实现IP寻址和通讯。实现覆盖网络的技术有很多，例如Flannel/VxLan/Calico/Weave-Net等等。外网流量如果要访问K8s集群内部的服务，一般要走负载均衡器(Load Balancer)，背后流量会通过Kube-Proxy间接转发到服务Pods上。</p><p>除了上述组件，K8s外围一般还有存储，监控，日志和分析等配套支持服务。</p><h2 id="总结和课程推荐"><a href="#总结和课程推荐" class="headerlink" title="总结和课程推荐"></a>总结和课程推荐</h2><p>下表我把本文讲到的一些K8s关键组件的作用做一个梳理总结，方便大家理解记忆。</p><p><img src="https://img-blog.csdnimg.cn/20190902113204818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;架构师波波&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/100215486&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/yan
      
    
    </summary>
    
      <category term="k8s" scheme="http://zhang-yu.me/categories/k8s/"/>
    
    
      <category term="k8s" scheme="http://zhang-yu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>微服务为什么要配置中心</title>
    <link href="http://zhang-yu.me/2020/06/28/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    <id>http://zhang-yu.me/2020/06/28/微服务为什么要配置中心/</id>
    <published>2020-06-28T03:00:00.000Z</published>
    <updated>2020-06-28T05:45:36.794Z</updated>
    
    <content type="html"><![CDATA[<p>微服务为什么要配置中心?</p><p>架构师波波的专栏</p><p><a href="https://blog.csdn.net/yang75108/article/details/86987941" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/86987941</a></p><blockquote><h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>在系统架构中，和安全、日志、监控等非功能需求一样，配置管理也是一种非功能需求。配置中心是整个微服务基础架构体系中的一个组件，如下图，它的功能看上去并不起眼，无非就是简单配置的管理和存取，但它是整个微服务架构中不可或缺的一环。另外，配置中心如果真得用好了，它还能推动技术组织持续交付和DevOps文化转型。<br><img src="https://img-blog.csdnimg.cn/20200212132045204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" width="100%" height="100%"></p><p>本文介绍在分布式微服务环境下，应用配置管理背后的业务需求，配置的各种分类和一些高级应用场景。</p><h2 id="二、配置定义和形态"><a href="#二、配置定义和形态" class="headerlink" title="二、配置定义和形态"></a>二、配置定义和形态</h2><p><strong>配置其实是独立于程序的可配变量</strong>，同一份程序在不同配置下会有不同的行为，常见的配置有连接字符串，应用配置和业务配置等。</p><p>配置有多种形态，下面是一些常见的：</p><ul><li><strong>程序内部hardcode</strong>，这种做法是反模式，一般我们<strong>不建议！</strong></li><li><strong>配置文件</strong>，比如Spring应用程序的配置一般放在<code>application.properties</code>文件中。</li><li><strong>环境变量</strong>，配置可以预置在操作系统的环境变量里头，程序运行时读取，这是很多PaaS平台，比如Heroku推荐的做法，参考12 factor app[附录9.1]。</li><li><strong>启动参数</strong>，可以在程序启动时一次性提供参数，例如java程序启动时可以通过<code>java -D</code>方式配启动参数。</li><li><strong>基于数据库</strong>，有经验的开发人员会把易变配置放在数据库中，这样可以在运行期灵活调整配置，这个做法和配置中心的思路已经有点接近了。</li></ul><p><img src="https://img-blog.csdnimg.cn/20200212132102689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-9ECzTdDT-1581484808516)(http://jskillcloud.com/img/post/2018060701/config_format.png#pic_center)]"></p><h2 id="三、传统应用配置的痛点"><a href="#三、传统应用配置的痛点" class="headerlink" title="三、传统应用配置的痛点"></a>三、传统应用配置的痛点</h2><p>在没有引入配置中心之前，一般企业研发都会面临如下痛点：</p><h3 id="1-配置散乱格式不标准"><a href="#1-配置散乱格式不标准" class="headerlink" title="1. 配置散乱格式不标准"></a><strong>1. 配置散乱格式不标准</strong></h3><p>有的用properties格式，有的用xml格式，还有的存DB，团队倾向自造轮子，做法五花八门。</p><h3 id="2-主要采用本地静态配置，配置修改麻烦"><a href="#2-主要采用本地静态配置，配置修改麻烦" class="headerlink" title="2. 主要采用本地静态配置，配置修改麻烦"></a><strong>2. 主要采用本地静态配置，配置修改麻烦</strong></h3><p>配置修改一般需要经过一个较长的测试发布周期。在分布式微服务环境下，当服务实例很多时，修改配置费时费力。</p><h3 id="3-易引发生产事故"><a href="#3-易引发生产事故" class="headerlink" title="3. 易引发生产事故"></a><strong>3. 易引发生产事故</strong></h3><p>这个是我亲身经历，之前在一家互联网公司，有团队在发布的时候将测试环境的配置带到生产上，引发百万级资损事故。</p><h3 id="4-配置缺乏安全审计和版本控制功能"><a href="#4-配置缺乏安全审计和版本控制功能" class="headerlink" title="4. 配置缺乏安全审计和版本控制功能"></a><strong>4. 配置缺乏安全审计和版本控制功能</strong></h3><p>谁改的配置？改了什么？什么时候改的？无从追溯，出了问题也无法及时回滚。</p><h2 id="四、现代应用配置核心需求"><a href="#四、现代应用配置核心需求" class="headerlink" title="四、现代应用配置核心需求"></a>四、现代应用配置核心需求</h2><p>近年，持续交付和DevOps理念开始逐步被一线企业接受，微服务架构和容器云也逐渐在一线企业落地，这些都对应用配置管理提出了更高的要求：</p><p><img src="https://img-blog.csdnimg.cn/20200212132118751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-q00lcjAM-1581484808517)(http://jskillcloud.com/img/post/2018060701/core_requirements.png#pic_center)]"></p><h3 id="1-交付件和配置分离"><a href="#1-交付件和配置分离" class="headerlink" title="1. 交付件和配置分离"></a><strong>1. 交付件和配置分离</strong></h3><p>传统做法应用在打包部署时，会为不同环境打出不同配置的包，例如为开发/测试/UAT/生产环境分别制作发布包，每个包里头包含环境特定配置。</p><p>现代微服务提倡云原生(Cloud Native)和不可变基础设施（Immutable Infrastructure）的理念，推荐采用如容器镜像这种方式打包和交付微服务，应用镜像一般只打一份，可以部署到不同环境。这就要求交付件（比如容器镜像）和配置进行分离，交付件只制作一份，并且是不可变的，可以部署到任意环境，而配置由配置中心集中管理，所有环境的配置都可以在配置中心集中配，运行期应用根据自身环境到配置中心动态拉取相应的配置。</p><h3 id="2-抽象标准化"><a href="#2-抽象标准化" class="headerlink" title="2. 抽象标准化"></a><strong>2. 抽象标准化</strong></h3><p>企业应该由框架或者中间件团队提供标准化的配置中心服务(Configuration as a Service)，封装屏蔽配置管理的细节和配置的不同格式，方便用户进行自助式的配置管理。一般用户只需要关注两个抽象和标准化的接口：</p><ol><li>配置管理界面UI，方便应用开发人员管理和发布配置，</li><li>封装好的客户端API，方便应用集成和获取配置。</li></ol><h3 id="3-多环境多集群"><a href="#3-多环境多集群" class="headerlink" title="3. 多环境多集群"></a><strong>3. 多环境多集群</strong></h3><p>现代微服务应用大都采用多环境部署，一般标准化的环境有开发/测试/UAT/生产等，有些应用还需要多集群部署，例如支持跨机房或者多版本部署。配置中心需要支持对多环境和多集群应用配置的集中式管理。</p><h3 id="4-高可用"><a href="#4-高可用" class="headerlink" title="4. 高可用"></a><strong>4. 高可用</strong></h3><p>配置中心必须保证高可用，不能随便挂，否则可能大面积影响微服务。在极端的情况下，如果配置中心不可用，客户端也需要有降级策略，保证应用可以不受影响。</p><h3 id="5-实时性"><a href="#5-实时性" class="headerlink" title="5. 实时性"></a><strong>5. 实时性</strong></h3><p>配置更新需要尽快通知到客户端，这个周期不能太长，理想应该是实时的。有些配置的实时性要求很高，比方说主备切换配置或者蓝绿部署配置，需要秒级切换配置的能力。</p><h3 id="6-治理"><a href="#6-治理" class="headerlink" title="6. 治理"></a><strong>6. 治理</strong></h3><p>配置需要治理，具体包括：</p><ul><li>配置审计，谁、在什么时间、修改了什么配置，需要详细的审计，方便出现问题时能够追溯。</li><li>配置版本控制，每次变更需要版本化，出现问题时候能够及时回滚到上一版本。</li><li>配置权限控制，配置变更发布需要认证授权，不是所有人都能修改和发布配置。</li><li>灰度发布，高级的配置治理支持灰度发布，配置发布时可以先让少数实例生效，确保没有问题再逐步放量。</li></ul><h2 id="五、配置分类"><a href="#五、配置分类" class="headerlink" title="五、配置分类"></a>五、配置分类</h2><p>配置目前还没有特别标准的分类方法，我简单把配置分为静态和动态两大类，每一类再分为若干子类，如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200212132133488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-57ZCzoGH-1581484808518)(http://jskillcloud.com/img/post/2018060701/config_category.png#pic_center)]"></p><h3 id="1-静态配置"><a href="#1-静态配置" class="headerlink" title="1. 静态配置"></a><strong>1. 静态配置</strong></h3><p>所谓静态配置，就是在程序启动前一次性配好，启动时一次性生效，在程序运行期一般不会变化的配置。具体包括：</p><h4 id="1-1-环境相关配置"><a href="#1-1-环境相关配置" class="headerlink" title="1.1 环境相关配置"></a>1.1 环境相关配置</h4><p>有些配置是和环境相关的，每个环境的配置不一样，例如数据库、中间件和其它服务的连接字符串配置。这些配置一次性配好，运行期一般不变。</p><h4 id="1-2-安全配置"><a href="#1-2-安全配置" class="headerlink" title="1.2 安全配置"></a>1.2 安全配置</h4><p>有些配置和安全相关，例如用户名，密码，访问令牌，许可证书等，这些配置也是一次性配好，运行期一般不变。因为涉及安全，相关信息一般需要加密存储，对配置访问需要权限控制。</p><h3 id="2-动态配置"><a href="#2-动态配置" class="headerlink" title="2. 动态配置"></a><strong>2. 动态配置</strong></h3><p>所谓动态配置，就是在程序的运行期可以根据需要动态调整的配置。动态配置让应用行为和功能的调整变得更加灵活，是持续交付和DevOps的最佳实践。具体包括：</p><h4 id="2-1-应用配置"><a href="#2-1-应用配置" class="headerlink" title="2.1 应用配置"></a>2.1 应用配置</h4><p>和应用相关的配置，例如服务请求超时，线程池和队列的大小，缓存过期时间，数据库连接池的容量，日志输出级别，限流熔断阀值，服务安全黑白名单等。一般开发或者运维会根据应用的实际运行情况调整这些配置。</p><h4 id="2-2-业务配置"><a href="#2-2-业务配置" class="headerlink" title="2.2 业务配置"></a>2.2 业务配置</h4><p>和业务相关的一些配置，例如促销规则，贷款额度，利率等业务参数，A/B测试参数等。一般产品运营或开发人员会根据实际的业务需求，动态调整这些参数。</p><h4 id="2-3-功能开关"><a href="#2-3-功能开关" class="headerlink" title="2.3 功能开关"></a>2.3 功能开关</h4><p>在英文中也称Feature Flag/Toggle/Switch，简单的只有真假两个值，复杂的可以是多值参数。功能开关是DevOps的一种最佳实践，在运维中有很多应用场景，比如蓝绿部署，灰度开关，降级开关，主备切换开关，数据库迁移开关等。功能开关在国外互联网公司用得比较多，国内还没有普及开，所以我在下一节会给出一些功能开关的高级应用场景。</p><h2 id="六、配置中心高级应用场景"><a href="#六、配置中心高级应用场景" class="headerlink" title="六、配置中心高级应用场景"></a>六、配置中心高级应用场景</h2><h3 id="场景一、蓝绿部署"><a href="#场景一、蓝绿部署" class="headerlink" title="场景一、蓝绿部署"></a>场景一、蓝绿部署</h3><p>蓝绿部署的传统做法是通过负载均衡器切流量来实现，如下图左边所示。这种做法一般研发人员无法自助操作，需要提交工单由运维介入操作，操作和反馈周期比较长，出了问题回退还需运维人员介入，所以回退也比较慢，总体风险比较高。</p><p><img src="https://img-blog.csdnimg.cn/20200212132148357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ZFkvyLnE-1581484808519)(http://jskillcloud.com/img/post/2018060701/blue_green_deployment.png#pic_center)]"></p><p>蓝绿部署也可以通过配置中心+功能开关的方式来实现，如上图右边所示。开发人员在上线新功能时先将新功能隐藏在动态开关后面，开关的值在配置中心里头配。刚上线时新功能暂不启用，走老功能逻辑，然后开发人员通过配置中心打开开关，这个时候新功能就启用了。一旦发现新功能有问题，可以随时把开关关掉切回老功能。这种做法开发人员可以全程自助实现蓝绿部署，不需要运维人员介入，反馈周期短效率高。</p><h3 id="场景二、限流降级"><a href="#场景二、限流降级" class="headerlink" title="场景二、限流降级"></a>场景二、限流降级</h3><p>当业务团队在搞促销，或者是系统受DDOS攻击的时候，如果没有好的限流降级机制，则系统很容易被洪峰流量冲垮，这个时候所有用户无法访问，体验糟糕，如下图左边所示。</p><p><img src="https://img-blog.csdnimg.cn/20200212132159343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ieWQQ5Mr-1581484808520)(http://jskillcloud.com/img/post/2018060701/rate_limiting_degrade.png#pic_center)]"></p><p>所以我们需要限流降级机制来应对流量洪峰。常见做法，我们一般会在应用的过滤器层或者是网关代理层添加限流降级逻辑，并且和配置中心配合，实现限流降级开关和参数的动态调整。如果促销出现流量洪峰，我们可以通过配置中心启动限流降级策略，比如对于普通用户，我们可以先给出“网络不给力，请稍后再试”的友好提示，对于高级VIP用户，我们仍然保证他们的正常访问。</p><p>国内电商巨头阿里，它内部的系统大量采用<font color="red">限流降级机制，实现方式基于其内部的diamond+sentinel配置管理系统。</font>如果没有限流降级机制的保护，则阿里的系统也无法抵御双十一带来的洪峰流量冲击。</p><h3 id="场景三、数据库迁移"><a href="#场景三、数据库迁移" class="headerlink" title="场景三、数据库迁移"></a>场景三、数据库迁移</h3><p>LaunchDarkly是一家提供配置既服务(Configuration as a Service)的SAAS服务公司，它在其博客上给出了一片关于使用功能开关实现数据库迁移的案例文章，该案例基于其内部一次成功的数据库迁移实践，从MongdoDB迁移到DynamoDB[参考附录9.2]，下图是展示了一个简化的迁移流程：</p><p><img src="https://img-blog.csdnimg.cn/20200212132212489.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-N4Q8XNdr-1581484808520)(http://jskillcloud.com/img/post/2018060701/ff_database_migration-768x1024.jpg#pic_center)]"></p><p>简化迁移腾挪流程如下：</p><ol><li>开发人员先在应用端的DAO层埋好数据双写双读、以及数据比对逻辑。双写双读逻辑由开关控制，开关的值可在配置中心配。</li><li>先保证应用100%读写mongoDB，然后先放开10%的DynamoDB双写，也称金丝雀写(Canary Write)，确保金丝雀写没有功能和性能问题。</li><li>逐步放量DyanamoDB写到100%，确保全量双写没有功能和性能问题。</li><li>放开10%的DynamoDB双读，也称金丝雀读(Canary Read)，通过比对逻辑确保金丝雀读没有逻辑和性能问题。</li><li>逐步放量DynamoDB读到100%，通过比对逻辑确保全量双读没有逻辑和性能问题。</li><li>关闭对mongoDB的读写，迁移完成。</li></ol><p>整个迁移流程受配置中心的开关控制，可以灵活调整开关和参数，有问题可以随时回滚，大大降低迁移风险。</p><h3 id="场景四、A-B测试"><a href="#场景四、A-B测试" class="headerlink" title="场景四、A/B测试"></a>场景四、A/B测试</h3><p>如果我们需要对电商平台的结账(checkout)功能进行改版，考虑到结账功能业务影响面大，一下子上线风险大，为了减低风险，我们可以在配置中心配合下，对结账功能进行A/B测试，简化逻辑如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200212132223128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-xlzC9otk-1581484808521)(http://jskillcloud.com/img/post/2018060701/ab_test.png#pic_center)]"></p><p>我们在配置中心中增加一个<code>ab_test_flag</code>开关，控制A/B测试逻辑：</p><ol><li>如果A/B测试开关是关闭的(<code>ab_test_flag==false</code>)，那么就走老的结账逻辑。</li><li>如果A/B测试开关是打开的(<code>ab_test_flag==true</code>，并且是普通用户(<code>user==regular</code>，可以检查数据库中用户类型)，那么就走老的结账逻辑。</li><li>如果A/B测试开关是打开的(<code>ab_test_flag==true</code>)，并且是beta用户（<code>user==beta</code>），那么就走改版后的新结账逻辑。</li></ol><p>通过配置中心，我们可以灵活调整开关，先对新功能进行充分的beta试验，再考虑全量上线，大大降低关键业务新功能的上线风险。</p><h2 id="七、公司案例和产品"><a href="#七、公司案例和产品" class="headerlink" title="七、公司案例和产品"></a>七、公司案例和产品</h2><p>在一线前沿的互联网公司，配置中心都是其技术体系中的关键基础服务，下图给出一些公司案例产品：</p><p><img src="https://img-blog.csdnimg.cn/20200212132236228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-MA6Ky9GG-1581484808521)(http://jskillcloud.com/img/post/2018060701/config_center_products.png#pic_center)]"></p><ol><li>阿里巴巴中间件部门很早就自研了配置中心Diamond，并且是开源的。Diamond对阿里系统的灵活稳定性发挥了至关重要的作用。开源版本的Diamond由于研发时间比较早，使用的技术比较老，功能也不够完善，目前社区不热已经不维护了。</li><li>Facebook内部也有一整套完善的配置管理体系[可参考其论文，附录9.3]，其中一个产品叫Gatekeeper，目前没有开源。</li><li>Netflix内部有大量的微服务，它的服务的稳定灵活性也重度依赖于配置中心。Netflix开源了它的配置中心的客户端，叫变色龙Archaius[参考附录9.4]，比较可惜的是，Netflix没有开源它的配置中心的服务器端。</li><li>Apollo[参考附录9.5]是携程框架部研发并开源的一款配置中心产品，企业级治理功能完善，目前社区比较火，在github上有超过5k星，在国内众多互联网公司有落地案例。<strong>如果企业打算引入开源的配置中心，那么Apollo是我推荐的首选</strong>。</li><li>百度之前也开源过一个叫Disconf[参考附录9.6]的配置中心产品，作者是前百度资深工程师廖绮绮。在Apollo没有出来之前，Disconf在社区是比较火的，但是自从廖琦琦离开百度之后，他好像没有足够精力投入维护这个项目，目前社区活跃度已经大不如前。</li></ol><h2 id="八、结论"><a href="#八、结论" class="headerlink" title="八、结论"></a>八、结论</h2><ol><li>配置中心是微服务基础架构中不可或缺的核心组件，现代微服务架构和云原生环境，对应用配置管理提出了更高的要求。</li><li>配置中心有众多的应用场景，<strong>配置中心+功能开关是DevOps最佳实践</strong>。用好配置中心，它能帮助技术组织实现持续交付和DevOps文化转型。</li><li>携程开源的Apollo配置中心，企业级功能完善，经过大规模生产验证，社区活跃度高，是开源配置中心产品的首选。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;微服务为什么要配置中心?&lt;/p&gt;
&lt;p&gt;架构师波波的专栏&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/86987941&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;htt
      
    
    </summary>
    
      <category term="配置中心" scheme="http://zhang-yu.me/categories/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
    
      <category term="配置中心" scheme="http://zhang-yu.me/tags/%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>我为啥暂不看好ServiceMesh</title>
    <link href="http://zhang-yu.me/2020/06/28/%E6%88%91%E4%B8%BA%E5%95%A5%E6%9A%82%E4%B8%8D%E7%9C%8B%E5%A5%BDServiceMesh/"/>
    <id>http://zhang-yu.me/2020/06/28/我为啥暂不看好ServiceMesh/</id>
    <published>2020-06-28T03:00:00.000Z</published>
    <updated>2020-06-28T08:26:44.713Z</updated>
    
    <content type="html"><![CDATA[<p>我为啥暂不看好ServiceMesh?</p><p><a href="https://blog.csdn.net/yang75108/article/details/87266458" target="_blank" rel="noopener">https://blog.csdn.net/yang75108/article/details/87266458</a></p><blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>过去的2018年，ServiceMesh(服务网格)概念在社区里头非常火，有人提出2018年是ServiceMesh年，还有人提出ServiceMesh是下一代的微服务架构基础。作为架构师，如果你现在还不了解ServiceMesh的话，是否感觉有点落伍了？</p><p>那么到底什么是ServiceMesh？它诞生的背景是什么？它解决什么问题？企业是否适合引入ServiceMesh？根据近年在一线互联网企业的实践和思考，从个人视角出发，我为大家一一解答这些问题。</p><h2 id="微服务架构的核心技术问题"><a href="#微服务架构的核心技术问题" class="headerlink" title="微服务架构的核心技术问题"></a>微服务架构的核心技术问题</h2><p>在业务规模化和研发效能提升等因素的驱动下，从单块应用向微服务架构的转型(如下图所示)，已经成为很多企业(尤其是互联网企业)数字化转型的趋势。</p><p><img src="https://img-blog.csdnimg.cn/20200211204043954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在微服务模式下，企业内部服务少则几个到几十个，多则上百个，每个服务一般都以集群方式部署，这时自然产生两个问题(如下图所示)：</p><p><img src="https://img-blog.csdnimg.cn/20200211204322439.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p><strong>一、服务发现</strong>：服务的消费方(Consumer)如何发现服务的提供方(Provider)？</p><p><strong>二、负载均衡</strong>：服务的消费方如何以某种负载均衡策略访问集群中的服务提供方实例？</p><p>作为架构师，如果你理解了这两个问题，可以说就理解了微服务架构在技术上的最核心问题。</p><h2 id="三种服务发现模式"><a href="#三种服务发现模式" class="headerlink" title="三种服务发现模式"></a>三种服务发现模式</h2><p>服务发现和负载均衡并不是新问题，业界其实已经探索和总结出一些常用的模式，这些模式的核心其实是代理(Proxy，如下图所以)，以及代理在架构中所处的位置，</p><p><img src="https://img-blog.csdnimg.cn/20200211204059487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在服务消费方和服务提供方之间增加一层代理，由代理负责服务发现和负载均衡功能，消费方通过代理间接访问目标服务。根据代理在架构上所处的位置不同，当前业界主要有三种不同的服务发现模式：</p><h3 id="模式一：传统集中式代理"><a href="#模式一：传统集中式代理" class="headerlink" title="模式一：传统集中式代理"></a>模式一：传统集中式代理</h3><p><img src="https://img-blog.csdnimg.cn/20200211204117995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，F5(4层负载)+Nginx(7层负载)这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。</p><p>这种方式通常在DNS域名服务器的配合下实现服务发现，服务注册(建立服务域名和IP地址之间的映射关系)一般由运维人员在代理上手工配置，服务消费方仅依赖服务域名，这个域名指向代理，由代理解析目标地址并做负载均衡和调用。</p><p>国外知名电商网站eBay，虽然体量巨大，但其内部的服务发现机制仍然是基于这种传统的集中代理模式，国内公司如携程，也是采用这种模式。</p><h3 id="模式二：客户端嵌入式代理"><a href="#模式二：客户端嵌入式代理" class="headerlink" title="模式二：客户端嵌入式代理"></a>模式二：客户端嵌入式代理</h3><p><img src="https://img-blog.csdnimg.cn/2020021120413546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。</p><p>Netflix开源的Eureka(注册中心)和Ribbon(客户端代理)是这种模式的典型案例，国内阿里开源的Dubbo也是采用这种模式。</p><h3 id="模式三：主机独立进程代理"><a href="#模式三：主机独立进程代理" class="headerlink" title="模式三：主机独立进程代理"></a>模式三：主机独立进程代理</h3><p>这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同模式二。</p><p><img src="https://img-blog.csdnimg.cn/20200211204151399.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>Airbnb的SmartStack是这种模式早期实践产品，国内公司唯品会对这种模式也有探索和实践。</p><h2 id="三种服务发现模式的比较"><a href="#三种服务发现模式的比较" class="headerlink" title="三种服务发现模式的比较"></a>三种服务发现模式的比较</h2><p>上面介绍的三种服务发现模式各有优劣，没有绝对的好坏，可以认为是三种不同的架构风格，在不同的公司都有成功实践。下表总结三种服务发现模式的优劣比较，业界案例和适用场景建议，供架构师选型参考：</p><p><img src="https://img-blog.csdnimg.cn/20200211204205282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><h2 id="服务网格ServiceMesh"><a href="#服务网格ServiceMesh" class="headerlink" title="服务网格ServiceMesh"></a>服务网格ServiceMesh</h2><p>所谓的ServiceMesh，其实本质上就是上面提到的模式三~主机独立进程模式，这个模式其实并不新鲜，业界(国外的Airbnb和国内的唯品会等)早有实践，那么为什么现在这个概念又流行起来了呢？我认为主要原因如下：</p><ol><li>上述模式一和二有一些固有缺陷，模式一相对比较重，有单点问题和性能问题；模式二则有客户端复杂，支持多语言困难，无法集中治理的问题。模式三是模式一和二的折中，弥补了两者的不足，它是纯分布式的，没有单点问题，性能也OK，应用语言栈无关，可以集中治理。</li><li>微服务化、多语言和容器化发展的趋势，企业迫切需要一种轻量级的服务发现机制，ServiceMesh正是迎合这种趋势诞生，当然这还和一些大厂(如Google/IBM等)的背后推动有关。</li></ol><p>模式三(ServiceMesh)也被形象称为边车(Sidecar)模式，如下图，早期有一些摩托车，除了主驾驶位，还带一个边车位，可以额外坐一个人。在模式三中，业务代码进程(相当于主驾驶)共享一个代理(相当于边车)，代理除了负责服务发现和负载均衡，还负责动态路由、容错限流、监控度量和安全日志等功能，这些功能是具体业务无关的，属于跨横切面关注点(Cross-Cutting Concerns)范畴。</p><p><img src="https://img-blog.csdnimg.cn/20200211204224981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>在新一代的ServiceMesh架构中(下图上方)，服务的消费方和提供方主机(或者容器)两边都会部署代理SideCar。ServiceMesh比较正式的术语也叫数据平面(DataPlane)，与数据平面对应的还有一个独立部署的控制平面(ControlPlane)，用来集中配置和管理数据平面，也可以对接各种服务发现机制(如K8S服务发现)。术语数据平面和控制平面，估计是偏网络SDN背景的人提出来的。</p><p><img src="https://img-blog.csdnimg.cn/20200211204234911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>上图左下角，每个主机上同时居住了业务逻辑代码(绿色表示)和代理(蓝色表示)，服务之间通过代理发现和调用目标服务，形成服务之间的一种网络状依赖关系，控制平面则可以配置这种依赖调用关系，也可以调拨路由流量。如果我们把主机和业务逻辑剥离，就出现一种网格状架构(上图右下角)，服务网格由此得名。</p><p><img src="https://img-blog.csdnimg.cn/20200211204245278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>Istio是Google/IBM等大厂支持和推进的一个ServiceMesh标准化工作组，上图是Istio给出的ServiceMesh参考架构(注意这个是老版架构，新版有一些调整，但是大框架没变)。Istio专注在控制平面的架构、功能、以及控制平面和数据平面之间API的标准化，它的控制平面功能主要包括：</p><ul><li>Istio-Manager：负责服务发现，路由分流，熔断限流等配置数据的管理和下发</li><li>Mixer：负责收集代理上采集的度量数据，进行集中监控</li><li>Istio-Auth：负责安全控制数据的管理和下发</li></ul><p>Envoy是目前Istio主力支持的数据平面代理，其它主流代理如nginx/kong等也正在陆续加入这个阵营。kubernetes是目前Isito主力支持的容器云环境。</p><h2 id="我的建议"><a href="#我的建议" class="headerlink" title="我的建议"></a>我的建议</h2><p>目前我本人并不特别看好ServiceMesh，也不是特别建议企业在生产上试水ServiceMesh，主要原因如下：</p><ol><li>ServiceMesh其实并不是什么新东西，本质就是上面提到的服务发现模式三~主机独立进程模式，这个模式很早就有公司在探索和实践，但是一直没有普遍流行起来，说明这个模式也是存在落地挑战的。从表面上看，模式三是模式一和模式二的折中，同时解决了模式一和模式二存在的问题，但是在每个主机上独立部署一个代理进程，是有很大运维管理开销的，一方面是规模化部署的问题(考虑服务很多，机器也很多的场景)；另一方面是如何监控治理的问题，代理挂了怎么办？你的团队是否具备自动化运维和监控的能力？另外开发人员在服务调试的时候，会依赖于这个独立的代理，调试排错比较麻烦，这个问题怎么解决？</li><li>Istio的确做了一些标准化工作，但是没有什么特别的创新，可是说换汤不换药，就是把模式三规范化和包装了一下。透过现象看本质，Google/IBM等行业大厂在背后推Isito/ServiceMesh，背后有一些市场利益诉求考虑，例如Google要推进它的kubernates和公有云生态。</li><li>ServiceMesh在年初声音比较大，最近渐渐安静下来，我听到国内只有一些大厂(华为，新浪微博，蚂蚁金服等)在试水，实际生产级落地的案例聊聊无几。大多数企业对ServiceMesh只是观望，很多架构师对ServiceMesh实际落地都存在疑虑。</li></ol><p>所以我的个人建议，对于大部分企业(一般运维和研发能力不是特别强)，采用模式一~集中代理模式就足够了。这个模式比较传统不新鲜，但是在很多一线企业已经切实落地，我甚至认为，除了一些大厂，大部分中小企业的服务发现架构采用的就是集中代理。我本人经历过三家互联网公司，大的有eBay，中等有携程，小的有拍拍贷，都是采用集中式代理模式，而且玩得都很好。我的架构理念很简单，对于生产级应用，不追新，老实采用大部分企业落地过的方案。</p><p>模式一的最大好处是集中治理，应用不侵入，语言栈无关，另外因为模式一是集中部署的，不像模式三是分布式部署，所以模式一的运维开销也远小于模式三。对于模式一，大家最大的顾虑是性能和单点问题，其实性能还是OK的，如果架构和容量规划合理的话，实际生产中经过集中代理的性能开销一般可以控制在小于10个ms，eBay和携程等大流量企业的成功实践已经验证了这点。单点问题一般建议采用两层负载结构，例如硬件F5+软件nginx两层负载，F5以主从HA部署，nginx则以集群多实例部署，这种架构兼顾了高可用和配置的灵活性。</p><p>另外，模式一还可以和服务注册中心结合，从而降低手工配置的复杂性，实现DevOps研发自助部署，一种方案如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200211204300581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmc3NTEwOA==" alt></p><p>服务启动时自动注册到服务注册中心并定期报心跳，Proxy则定期到服务注册中心同步实例。这种方式下，不需要为每个服务申请一个域名，只需一个泛域名即可，消费者访问服务时采用服务名+泛域名即可，整个服务上线流程可以做到DevOps研发自助。目前社区流行的一些开源代理如traefik和kong\等都支持和多种服务注册中心(Consul/Eureka/Etcd/Zookeeper等)进行集成。目前这种方案在拍拍贷有初步成功实践，采用kong和自研服务注册中心Radar同时和容器云调度平台配合，实现了研发全自助式发布上线。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li>服务注册发现和负载均衡是微服务架构在技术上的根本问题，解决的办法是采用代理Proxy。根据代理在架构上的位置不同，服务发现代理一般有三种模式：</li></ol><ul><li>模式一：集中式代理</li><li>模式二：客户端嵌入式代理</li><li>模式三：主机独立进程代理<br>这三种模式没有绝对的好还之分，只是三种不同的架构风格，各有优劣和适用场景，在不同企业都有成功落地案例。</li></ul><ol start="2"><li>ServiceMesh本质上就是模式三~主机独立进程代理，它结合了模式一和模式二的优势，但是分布式部署运维管理开销大。Istio对ServiceMesh的架构、功能和API进行了标准化。</li></ol></blockquote><blockquote><ol start="3"><li>ServiceMesh还在演进中，生产落地仍有挑战，一般企业不建议生产级使用。集中式代理最成熟，对于一般中小企业，建议从集中式代理开始，等达到一定规模和具备一定的研发运维能力，再根据需要考虑其它服务发现模式.</li><li>架构师不要盲目追新，在理解微服务架构原理的基础上，可以学习和试点新技术，但是对于生产级应用，应该以成熟稳定，有大规模落地案例作为选型第一准则。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我为啥暂不看好ServiceMesh?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/yang75108/article/details/87266458&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog
      
    
    </summary>
    
      <category term="ServiceMesh" scheme="http://zhang-yu.me/categories/ServiceMesh/"/>
    
    
      <category term="ServiceMesh" scheme="http://zhang-yu.me/tags/ServiceMesh/"/>
    
  </entry>
  
</feed>
